{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Amar Dhillon","text":""},{"location":"#explore-docs-by-product","title":"Explore docs by product","text":"<ul> <li> <p> Kubernetes</p> <p>Checkout how Kubernetes can bring values to your business and solve your use-cases.</p> <p> See the Kubernetes docs</p> </li> <li> <p> Azure</p> <p>Explore various Azure related technologies.</p> <p> See the Azure docs</p> </li> <li> <p> AWS</p> <p>Documentation and notes for many AWS related technologies</p> <p> See the AWS docs</p> </li> <li> <p> Databricks</p> <p>Dive right into Databricks related data engineering and ML solutions.</p> <p> See the Databricks references</p> </li> <li> <p>Icon-166Artboard 1 AI and ML</p> <p>Talk about AI and ML basiscs, MLOps, Azure and AWS AI solutions</p> <p> See the AI/ML docs</p> </li> <li> <p> Design Patterns</p> <p>Explore pattrens related to micro-services, event-driven architecture, Artificial Intelligence and more.</p> <p> Checkout Design Patterns</p> </li> </ul>"},{"location":"AI/ai_basics/","title":"Artificial I Basics","text":"<p>A machine learning model is a program that can find patterns or make decisions from a previously unseen dataset. For example, in <code>natural language processing</code>, machine learning models can parse and correctly recognize the intent behind previously unheard sentences or combinations of words. In <code>image recognition</code>, a machine learning model can be taught to recognize objects - such as cars or dogs.</p> <p>Info</p> <p>A machine learning algorithm is a mathematical method to find patterns in a set of data. Machine Learning algorithms are often drawn from statistics, calculus, and linear algebra.</p>"},{"location":"AI/ai_basics/#model-training","title":"Model Training","text":"<p>The process of running a machine learning algorithm on a dataset (called <code>training data</code>) and optimizing the algorithm to find certain patterns or outputs is called <code>model training</code>. The resulting function with rules and data structures is called the <code>trained machine learning model</code>.</p>"},{"location":"AI/ai_basics/#ml-types","title":"Ml Types","text":""},{"location":"AI/ai_basics/#supervised","title":"Supervised","text":"<p>In supervised machine learning, the algorithm is provided an input dataset, and is rewarded or optimized to meet a set of specific outputs.</p> <ul> <li><code>Logistic Regression</code>: Logistic Regression is used to determine if an input belongs to a certain group or not </li> <li><code>SVM</code>: SVM, or Support Vector Machines create coordinates for each object in an n-dimensional space and uses a hyperplane to group objects by common features </li> <li><code>Naive Bayes</code>: Naive Bayes is an algorithm that assumes independence among variables and uses probability to classify objects based on features</li> <li><code>Decision Trees</code>: Decision trees are also classifiers that are used to determine what category an input falls into by traversing the leaf's and nodes of a tree </li> <li><code>Linear Regression</code>: Linear regression is used to identify relationships between the variable of interest and the inputs, and predict its values based on the values of the input variables. </li> <li><code>kNN</code>: The k Nearest Neighbors technique involves grouping the closest objects in a dataset and finding the most frequent or average characteristics among the objects.</li> <li><code>Random Forest</code>: Random forest is a collection of many decision trees from random subsets of the data, resulting in a combination of trees that may be more accurate in prediction than a single decision tree. </li> <li><code>Boosting algorithms</code>: Boosting algorithms, such as Gradient Boosting Machine, XGBoost, and LightGBM, use ensemble learning. They combine the predictions from multiple algorithms (such as decision trees) while taking into account the error from the previous algorithm.</li> </ul>"},{"location":"AI/ai_basics/#unsupervised","title":"Unsupervised","text":"<p>In unsupervised machine learning, the algorithm is provided an input dataset, but not rewarded or optimized to specific outputs, and instead trained to group objects by common characteristics. </p> <p><code>For example</code>, recommendation engines on online stores rely on unsupervised machine learning, specifically a technique called clustering.</p> <ul> <li><code>K-Means</code>: The K-Means algorithm finds similarities between objects and groups them into K different clusters. </li> <li><code>Hierarchical Clustering</code>: Hierarchical clustering builds a tree of nested clusters without having to specify the number of clusters.</li> </ul>"},{"location":"AI/ai_basics/#reinforcement-learning","title":"Reinforcement Learning","text":"<p>In reinforcement learning, the algorithm is made to train itself using <code>many trial and error experiments</code>. Reinforcement learning happens when the algorithm interacts continually with the environment, rather than relying on training data. </p> <p>One of the most popular examples of reinforcement learning is autonomous driving.</p>"},{"location":"AI/ai_basics/#model-techniques","title":"Model techniques","text":""},{"location":"AI/ai_basics/#regression","title":"Regression","text":"<p>Regression in data science and machine learning is a statistical method that enables predicting outcomes based on a set of input variables. The outcome is often a variable that depends on a combination of the input variables.</p> <p> </p> <p>Linear, polynomial, ridge, lasso, quantile, and Bayesian regression</p>"},{"location":"AI/ai_basics/#classification","title":"Classification","text":"<p>Binary and multi-label classification</p>"},{"location":"AI/ai_basics/#decision-trees","title":"Decision Trees","text":"<p>A Decision Tree is a predictive approach in ML to determine what class an object belongs to. As the name suggests, a decision tree is a tree-like flow chart where the class of an object is determined step-by-step using certain known conditions.</p> <p> </p>"},{"location":"AI/ai_basics/#random-forests","title":"Random Forests","text":""},{"location":"AI/ai_basics/#xgboost","title":"XGBoost","text":"<p>XGBoost, which stands for Extreme Gradient Boosting, is a scalable, distributed gradient-boosted decision tree (GBDT) machine learning library. It provides parallel tree boosting and is the leading machine learning library for regression, classification, and ranking problems.</p> <p>A Gradient Boosting Decision Trees (GBDT) is a decision tree ensemble learning algorithm similar to random forest, for classification and regression. Ensemble learning algorithms combine multiple machine learning algorithms to obtain a better model.</p>"},{"location":"AI/ai_basics/#deep-learning","title":"Deep Learning","text":"<p>Deep learning models are a class of ML models that imitate the way humans process information. The model consists of several layers of processing (hence the term 'deep') to extract high-level features from the data provided. Each processing layer passes on a more abstract representation of the data to the next layer, with the final layer providing a more human-like insight.</p> <p> </p> <p>Unlike traditional ML models which require data to be labeled, deep learning models can ingest large amounts of unstructured data. </p>"},{"location":"AI/ai_basics/#other-concepts","title":"Other concepts","text":""},{"location":"AI/ai_basics/#gradient-boosting","title":"Gradient Boosting","text":"<p>The term <code>gradient boosting</code> comes from the idea of <code>boosting</code> or improving a single weak model by combining it with a number of other <code>weak models</code> in order to generate a collectively <code>strong model</code>. Gradient boosting is an extension of boosting where the process of additively generating weak models is formalised as a gradient descent algorithm over an objective function.</p>"},{"location":"AI/ai_basics/#gradient-descent","title":"Gradient Descent","text":"<p>Gradient descent is an optimization algorithm which is commonly-used to train machine learning models and neural networks.  </p> <p>Until the function is close to or equal to zero, the model will continue to adjust its parameters to yield the smallest possible error.</p> <p>TLDR</p> <p>Similar to finding the line of best fit in linear regression, the goal of gradient descent is to minimize the cost function, or the error between predicted and actual y. In order to do this, it requires two data points\u2014a direction and a learning rate. These factors determine the partial derivative calculations of future iterations, allowing it to gradually arrive at the local or global minimum (i.e. point of convergence).</p>"},{"location":"AI/ai_basics/#costloss-function","title":"Cost/Loss Function","text":"<p>The cost (or loss) function measures the difference, or error, between <code>actual y</code> and <code>predicted y</code> at its current position. This improves the machine learning model's efficacy by providing feedback to the model so that it can adjust the parameters to minimize the error and find the <code>local or global minimum</code>.</p> <p>It continuously iterates, moving along the direction of steepest descent (or the negative gradient) until the cost function is close to or at zero. At this point, the model will stop learning. Additionally, while the terms, cost function and loss function, are considered synonymous, there is a slight difference between them. It\u2019s worth noting that a loss function refers to the error of one training example, while a cost function calculates the average error across an entire training set.</p>"},{"location":"AI/ai_basics/#learning-rate","title":"Learning rate","text":"<p>Learning rate (also referred to as step size or the alpha) is the size of the steps that are taken to reach the minimum. This is typically a small value, and it is evaluated and updated based on the behavior of the cost function. High learning rates result in larger steps but risks overshooting the minimum. Conversely, a low learning rate has small step sizes. While it has the advantage of more precision, the number of iterations compromises overall efficiency as this takes more time and computations to reach the minimum.</p>"},{"location":"AI/ai_basics/#performance-metrics","title":"Performance Metrics","text":"<ul> <li><code>Classification</code>: Accuracy, Recall, Precision, FPR, FNR, F1, Sensitivity, Specificity</li> <li><code>Regression</code> : MAPE, MAE, RMSE, MSE, R-Squared, Mean Error</li> <li><code>Ranking</code> : NDCG@k, AUC@k</li> <li><code>Ranking Labels</code> : MAP@k, MRR </li> <li><code>AUC / LogLoss</code> : AUC, PR-AUC, Log Loss</li> <li><code>Computer Vision / Object Detection</code> : Accuracy (MAP &amp; IoU coming soon)</li> </ul>"},{"location":"AI/feature_store/","title":"Feature Store","text":""},{"location":"AI/feature_store/#questions","title":"Questions","text":""},{"location":"AI/feature_store/#what-are-features","title":"What are Features?","text":"<p>Features are simply the data that will be used by the models. For example, that data could be a row of an excel sheet or the pixels in a picture.</p>"},{"location":"AI/feature_store/#what-is-feature-engineering","title":"What is feature engineering?","text":"<p>Transforming the source data into features for the ML pipeline to be able to use the features.</p>"},{"location":"AI/feature_store/#what-is-feature-store","title":"What is feature Store?","text":"<p>The Feature Store is where the features are stored and organized for the explicit purpose of being used to either train models (by Data Scientists) or make predictions (by applications that have a trained model). It is a central location where you can either create or update groups of features created from multiple different data sources, or create and update new datasets from those feature groups for training models or for use in applications that do not want to compute the features but just retrieve them when it needs them to make predictions.</p> <p> </p> <p>The Feature Store is a data platform that connects different ML pipelines (feature, training, and inference pipelines). </p>"},{"location":"AI/genai/","title":"GenAI","text":""},{"location":"AI/genai/#genai-use-cases","title":"GenAI Use Cases","text":"<ul> <li>Text summarization</li> <li>Rewriting</li> <li>Information extraction</li> <li>Classification and content moderation</li> <li>Translation: Java to Python for example</li> <li>Source code generation: code from hand writtern sketch</li> <li>Reasoning</li> <li>Mark PII</li> </ul>"},{"location":"AI/genai/#genai-concepts","title":"GenAI Concepts","text":""},{"location":"AI/genai/#generative-discriminative-model","title":"Generative / Discriminative model","text":"<p>A generative AI model could be trained on a dataset of images of cats and then used to generate new images of cats. A discriminative AI model could be trained on a dataset of images of cats and dogs and then used to classify new images as either cats or dogs.</p>"},{"location":"AI/genai/#foundation-model","title":"Foundation model","text":"<p>A foundation model is a large AI model pre-trained on a vast quantity of data that was \"designed to be adapted\u201d (or fine-tuned) to a wide range of downstream tasks, such as sentiment analysis, image captioning, and object recognition.</p> <p>Examples - Llama 2 from Meta - Falcon</p> <p>Info</p> <p><code>Foundation models</code> are very large and complex neural network models consisting of billions of parameters (a.k.a. weights). The model parameters are learned during the training phase\u2014often called pretraining. Foundation models are trained on massive amounts of training data\u2014typically over a period of many weeks and months using large, distributed clusters of CPUs and graphics processing units (GPUs). After learning billions of parameters, these foundation models can represent complex entities such as human language, images, videos, and audio clips</p>"},{"location":"AI/genai/#prompt","title":"Prompt \ud83c\udfb9","text":"<p>A prompt is a short piece of text that is given to the large language model as input, and it can be used to control the output of the model in many ways. </p> <p>What is completion?</p> <p>This prompt is passed to the model during inference time to generate a <code>completion</code></p>"},{"location":"AI/genai/#prompt-engineering","title":"Prompt Engineering","text":"<p><code>Prompt engineering</code> is a practice used to guide an LLM\u2019s response by querying the model in a more specific way. For example, instead of merely asking a model to \u201ccreate a quarterly report,\u201d a user could provide specific figures or data in their input, providing context and leading the LLM to a more relevant and accurate output. However, copying large amounts of information into an LLM\u2019s input is not only cumbersome but is also limited by constraints on the amount of text that can be inputted into these models at once.</p> <p>This is where the concept of \u201csearch and retrieval\u201d can enhance LLM capabilities. By pairing LLMs with an efficient search and retrieval system that has access to your proprietary data, you can overcome the limitations of both static training data and manual input. This approach is the best of both worlds, combining the expansive knowledge of LLMs with the specificity and relevance of your own data, all in real-time.</p> <p> </p>"},{"location":"AI/genai/#tokens","title":"Tokens \ud83c\udf6a","text":"<p>It\u2019s important to note that while <code>text-based prompts</code> and <code>completions</code> are implemented and interpreted by humans as natural language sentences, generative models convert them into <code>sequences of tokens</code>. </p> <p>By combining many of these tokens in different ways, the model is capable of representing an exponential number of words using a relatively small number of tokens often on the order of 30,000\u2013100,000 tokens in the model\u2019s vocabulary.</p> <p>From the model\u2019s standpoint, a document is simply just a sequence of tokens from the model\u2019s vocabulary.</p>"},{"location":"AI/genai/#embeddings","title":"Embeddings","text":"<p>We see embeddings as fundamental to AI and deep learning. Embeddings are the core of how deep learning models represent structures, mappings, hierarchy and manifolds that are learned by models. </p> <p>They proliferate modern deep learning from transformers to encoders, decoders, auto-encoders, recommendation engines, matrix decomposition, SVD, graph neural networks, and generative models \u2014 they are simply everywhere.</p> <p>Embeddings are dense, low-dimensional representations of high-dimensional data. They are an extremely powerful tool for input data representation, compression, and cross-team collaboration. They show you relationships in unstructured data, such as the example below showing that from word embeddings we learn man is to woman as king is to queen (from the corpus the vectors were trained on).</p> <p> </p> <p>Embeddings are foundational because:</p> <ul> <li>They provide a common mathematical representation of your data</li> <li>They compress your data</li> <li>They preserve relationships within your data</li> <li>They are the output of deep learning layers providing comprehensible linear views into complex non-linear relationships learned by models</li> </ul>"},{"location":"AI/genai/#n-shot-learning","title":"n-Shot Learning \ud83d\udc89","text":""},{"location":"AI/genai/#zero-shot-lerarning-0","title":"Zero Shot Lerarning 0\ufe0f\u20e3","text":"<p><code>Zero-shot learning</code> is a technique in which a machine learning model can recognize and classify new concepts without any labeled examples \u2014 hence zero shots. The model leverages knowledge transfer from pre-training on large unlabeled datasets.</p>"},{"location":"AI/genai/#one-shot-lerarning-1","title":"One Shot Lerarning 1\ufe0f\u20e3","text":""},{"location":"AI/genai/#langchain","title":"LangChain \u26d3","text":"<p>LangChain is an open source framework for building applications based on large language models (LLMs).</p> <p>LLMs are large <code>deep-learning models</code> pre-trained on large amounts of data that can generate responses to user queries, for example, answering questions or creating images from text-based prompts. </p> <p>LangChain provides tools and abstractions to improve the customization, accuracy, and relevancy of the information the models generate. For example, developers can use LangChain components to build new prompt chains or customize existing templates.</p> <p>LangChain also includes components that allow LLMs to access new data sets without retraining.</p>"},{"location":"AI/genai/#agents-chains","title":"Agents, Chains \u26d3\ufe0f","text":"<p>What are AI agents? Agents are used in LLMs as a reasoning engine to determine which actions to take and in which order to take them. So instead of using an LLM directly, you can use an agent and give it access to a set of tools and context to deliver a sequence based on your intended task</p> <p>Why use an agent instead of a LLM directly? Agents offer benefits like memory, planning, and learning over time. </p> <p>What are complex agents? However, complex agents require more complex testing scenarios and engineering to build and maintain. These  architectures may include modules for memory, planning, learning over time, and iterative processing</p> <p>What is a chain? A chain is a sequence of calls to components that can be combined to create a single, coherent application. <code>For example</code>, we can create a chain that takes user input, formats it with a prompt template, and then passes the formatted response to an LLM.</p> <p>Custom Chaining? What happens when an applications requires not just a predetermined chain of calls to LLMs, but potentially an unknown chain that depends on the user's input?</p> <p>In these types of chains, there is an \u201cagent\u201d which has access to a suite of tools. </p> <p>Depending on the user input, the agent can then decide which, if any, of these tools to call.</p> <p>LLM Frameworks Frameworks like <code>LlamaIndex</code> and <code>LangChain</code> are great for leveraging agents when developing applications powered by LLMs.</p> <p>ReAct(Reason + Act) Agent Architecture  The Reason and Action (ReAct) framework:  1.Has the agent reason over the next action  2.Constructs a command  3.Executes the action </p>"},{"location":"AI/genai/#rag","title":"RAG","text":"<p>LLMs are trained on vast datasets, but these will not include your specific data (things like company knowledge bases and documentation). Retrieval-Augmented Generation (RAG) addresses this by dynamically incorporating your data as <code>context</code> during the generation process. </p> <p>This is done not by altering the training data of the LLMs but by allowing the model to access and utilize your data in real-time to provide more tailored and contextually relevant responses.</p> <p>In RAG, your data is loaded and prepared for queries. This process is called indexing. User queries act on this index, which filters your data down to the most relevant context. This context and your query then are sent to the LLM along with a prompt, and the LLM provides a response.</p> <p><code>RAG</code> is a critical component for building applications such a <code>chatbots</code> or agents and you will want to know RAG techniques on how to get data into your application.</p> <p> </p>"},{"location":"AI/genai/#context-aware-rag","title":"Context Aware RAG","text":""},{"location":"AI/genai/#rag-steps","title":"RAG Steps","text":"<p>To ensure accuracy and relevance, there are six stages to follow within RAG that will in turn be a part of any larger RAG application.</p> <ul> <li> <p>Data Indexing: Before RAG can retrieve information, the data must be aggregated and organized in an index. This index acts as a reference point for the retrieval engine.</p> </li> <li> <p>Input Query Processing: The user\u2019s input is processed and understood by the system, forming the basis of the search query for the retrieval engine.</p> </li> <li> <p>Search and Ranking: The retrieval engine searches the indexed data and ranks the results in terms of relevance to the input query.</p> </li> <li> <p>Prompt Augmentation: The most relevant information is then combined with the original query. This augmented prompt serves as a richer source for response generation.</p> </li> <li> <p>Response Generation: Finally, the generation engine uses this augmented prompt to create an informed and contextually accurate response.</p> </li> <li> <p>Evaluation: A critical step in any pipeline is checking how effective it is relative to other strategies, or when you make changes. Evaluation provides objective measures on accuracy, faithfulness, and speed of responses.</p> </li> </ul> <p>By following these steps, RAG systems can provide answers that are not just accurate but also reflect the most current and relevant information available. This process is adaptable to both online and offline modes, each with its unique applications and advantages.</p>"},{"location":"AI/genai/#vector-stores","title":"Vector Stores \ud83e\uddee","text":"<p><code>Vector stores</code> and <code>embeddings</code> are used for RAG to efficiently retrieve relevant information from external data sources to augment the data used by a generative model</p> <p>Example: - Amazon OpenSearch Serverless - Amazon Kendra</p>"},{"location":"AI/genai/#genai-project-lifecycle","title":"GenAI Project Lifecycle \u267c","text":""},{"location":"AI/genai/#deployment-optimization","title":"Deployment Optimization \ud83d\udce6","text":""},{"location":"AI/genai/#pruning","title":"Pruning \ud83c\udf84","text":"<p>Pruning is a technique that focuses on removing redundant, or low-impact, parameters that do not contribute, or contribute little, to the model\u2019s performance. Pruning reduces the size of the model, but also increases performance by reducing the number of computations during inference.</p> <p>How is pruning done?</p> <p>The model weights to be eliminated during pruning are those with a value of zero or very close to zero. Pruning during training is accomplished through unstructured pruning (removing weights) or structured pruning (removing entire columns or rows of the weight matrices).</p>"},{"location":"AI/genai/#quantization","title":"Quantization \ud83d\udd2c","text":"<p>Quantization converts a model\u2019s weights from high precision (e.g., 32-bit) to lower precision (e.g., 16-bit). </p> <p>This not only reduces the model\u2019s memory footprint, but also improves model performance by working with smaller number representations. With large generative models, it\u2019s common to reduce the precision further to 8 bits to increase inference performance.</p> <p>PTQ</p> <p>Post Training Quantization (PTQ) aims to transform the model\u2019s learned weights into a lower-precision representation with the goals of reducing the model\u2019s size and the compute requirements when hosting generative models for inference.</p>"},{"location":"AI/genai/#distillation","title":"Distillation \ud83e\uddea","text":"<p>Distillation trains a <code>smaller student model</code> from a <code>larger teacher model</code>. The smaller model is then used for inference to reduce your compute resources yet retain a high percentage of accuracy of your student model.</p> <p>The end result is a student model that retains a high percentage of the teacher\u2019s model accuracy but uses a much smaller number of parameters. The student model is then deployed for inference. The smaller model requires smaller hardware and therefore less cost per inference request.</p> <p>Example</p> <p>A popular distilled student model is DistilBERT from Hugging Face. DistilBERT was trained from the larger BERT teacher model and is an order of magnitude smaller than BERT, yet it retains approximately 97% of the accuracy of the original BERT model. </p>"},{"location":"AI/intro_to_stat/","title":"Stat 101 concepts","text":""},{"location":"AI/intro_to_stat/#descriptive-and-inferential-statistics","title":"Descriptive and Inferential Statistics","text":""},{"location":"AI/intro_to_stat/#descriptive-statistics","title":"Descriptive Statistics","text":"<ul> <li>It provides summary statistics of the existing data. Some examples are:<ul> <li>Mean</li> <li>Median</li> <li>Standard deviation</li> <li>Co-relation</li> </ul> </li> </ul>"},{"location":"AI/intro_to_stat/#inferential-statistics","title":"Inferential Statistics","text":"<ul> <li>It is the concept of deriving the conclusions about population using the sample data. It can be said as an extension of the descriptive statistics.</li> <li>It is used to make the business decisions. </li> <li>It helps us think beyond data.</li> <li><code>Descriptiive Statistics</code> can answer questions like: who were our customers while <code>Infrential Statistics</code> can answer questions like<ul> <li>Will it snow in Ottawa at 2 am on a certain day?</li> <li>Who can be our customers in next 2 years for a particular region? We need to calculate the likeliness for this.</li> <li>How many customers will we gain if we gain/loose if we take some decision?</li> </ul> </li> <li>Generally speaking, the <code>Descriptive Statistics</code> is done on the histogram whereas the <code>Infrential Statistics</code> is done using the underlying distribution</li> </ul>"},{"location":"AI/intro_to_stat/#random-variable","title":"Random variable","text":""},{"location":"AI/intro_to_stat/#discrete-random-variable","title":"Discrete random variable","text":"<ul> <li>the sum of all the probabilities should be one</li> <li>If all the probabilities for the variables can be listed, then we can call it as <code>discrete random variable</code> </li> <li>The term associated with this is <code>probability mass function</code></li> </ul>"},{"location":"AI/intro_to_stat/#continuous-random-variable","title":"Continuous random variable","text":"<ul> <li>We cannot specify all the probabilities in this case.</li> <li>The term associated with this is <code>probability density function</code></li> </ul>"},{"location":"AI/intro_to_stat/#various-distributions","title":"Various distributions","text":""},{"location":"AI/intro_to_stat/#bernoulli-distribution","title":"Bernoulli Distribution","text":"<p>It is associated with yes and no. It is a special case of <code>Binomial distribution</code>. The Bernoulli distribution is a <code>discrete distribution</code> having two possible outcomes labelled by <code>n=0</code> and <code>n=1</code> in which n=1 (\"success\") occurs with probability p and n=0 (\"failure\") occurs with probability <code>q=1-p</code>, where 0&lt;p&lt;1. It therefore has probability density function</p> \\[ P(n) = \\Bigg\\{  \\begin{matrix}                 1-p &amp; n=0\\\\                 p &amp; n=1                 \\end{matrix} \\] <p>Examples: - Stock prices are going up or down? - Item sold or not sold?</p>"},{"location":"AI/intro_to_stat/#bernoulli-trial","title":"Bernoulli Trial","text":"<p>It\u2019s an experiment where you can have one of two possible outcomes. For example, \u201cYes\u201d and \u201cNo\u201d or \u201cHeads\u201d and \u201cTails.\u201d A few examples:</p> <p><code>Coin tosses</code>: record how many coins land heads up and how many land tails up? <code>Births</code>: how many boys and how many girls are born each day? <code>Rolling Dice</code>: the probability of a roll of two die resulting in a double six?</p>"},{"location":"AI/intro_to_stat/#binomial-distribution","title":"Binomial Distribution","text":"<p>It is associated with yes and no. A binomial distribution can be thought of as simply the probability of a SUCCESS or FAILURE outcome in an experiment or survey that is repeated multiple times. The binomial is a type of distribution that has two possible outcomes (the prefix \u201cbi\u201d means two, or twice). For example, a coin toss has only two possible outcomes: heads or tails and taking a test could have two possible outcomes: pass or fail.</p> <p>$$  P(X=x)=\\binom{n}{x} (p)^x  (1\u2212p)^{n\u2212x}  $$</p> <p>The first variable in the binomial formula, n, stands for the number of times the experiment runs.  The second variable, x, represents the probability of one specific outcome.</p> <p>Note</p> <p>Assumptions for this distribution are:</p> <pre><code>- One trial is independent of another \n- There are 2 possible outcomes of each trial are possible\n- Probability of success is p and is same for all trials\n- Number of trials are fixed\n</code></pre> <p>The diff between binomial and Bernoulli distribution is explained below:</p> <p>Tip</p> <p>The <code>Bernoulli distribution</code> is the discrete probability distribution and has only two possible outcomes. The<code>Binomial distribution</code> can be said as discrete probability distribution of the number of times an event is likely to occur within a given period of time.   </p>"},{"location":"AI/intro_to_stat/#uniform-distribution","title":"Uniform Distribution","text":"<p>There is equal likeliness of events happening. </p>"},{"location":"AI/intro_to_stat/#normal-distribution","title":"Normal Distribution","text":"<p>It is associated with yes and no.  </p>"},{"location":"AI/intro_to_stat/#z-score","title":"Z-score","text":""},{"location":"AI/intro_to_stat/#hypothesis-testing","title":"Hypothesis testing","text":""},{"location":"AI/intro_to_stat/#central-limit-theorm","title":"Central limit theorm","text":""},{"location":"AI/intro_to_stat/#area-under-curve","title":"Area under curve","text":"<p>The integral of \\(f(x)\\) corresponds to the computation of the area under the graph of \\(f(x)\\). Area between points \\(a\\) and \\(b\\) is given as </p> \\[    A(a,b) = \\int_{a}^{b} f(x) dx \\]"},{"location":"AI/intro_to_stat/#variability","title":"variability","text":"<p>While the central tendency, or average, tells you where most of your points lie, variability summarizes how far apart they are.  Low variability is ideal because it means that you can better predict information about the population based on sample data. High variability means that the values are less consistent, so it\u2019s harder to make predictions.</p>"},{"location":"AI/intro_to_stat/#what-is-a-test-statistic","title":"What is a test statistic?","text":"<p>A test statistic is a number calculated by a statistical test. It describes how far your observed data is from the null hypothesis of no relationship between variables or no difference among sample groups.</p> <p>The test statistic tells you how different two or more groups are from the overall population mean, or how different a linear slope is from the slope predicted by a null hypothesis. Different test statistics are used in different statistical tests.</p>"},{"location":"AI/intro_to_stat/#nominal-and-ordinal-data","title":"Nominal and Ordinal data","text":"<p>Nominal data is classified without a natural order or rank, whereas ordinal data has a predetermined or natural order</p> <p>What is the difference between ordinal, interval and ratio variables? Why should I care? https://www.graphpad.com/support/faq/what-is-the-difference-between-ordinal-interval-and-ratio-variables-why-should-i-care/</p>"},{"location":"AI/intro_to_stat/#histogram","title":"Histogram","text":""},{"location":"AI/intro_to_stat/#central-tendency-of-data","title":"Central tendency of data?","text":"<ul> <li>using Arithmetic mean</li> </ul>"},{"location":"AI/mlops101/","title":"MLOPS Basics","text":""},{"location":"AI/mlops101/#mlaas","title":"MLaaS","text":"<p>MLaaS describes a variety of machine learning capabilities that are delivered via the cloud. Vendors in the MLaaS market offer tools like image recognition, voice recognition, data visualization, and deep learning.A user typically uploads data to a vendor\u2019s cloud, and then the machine learning computation is processed on the cloud</p>"},{"location":"AI/mlops101/#model-training","title":"Model Training","text":"<p>2 kinds of model training</p>"},{"location":"AI/mlops101/#offlinestatic-model-training","title":"Offline/Static Model training","text":"<ul> <li>A <code>static model</code> is trained offline. That is, we train the model exactly once and then use that trained model for a while.</li> <li>This one is easy to build and test - use batch traing and test, iterate until good</li> <li>This can go stale</li> </ul> <p>Info</p> <p>Currently, the majority of machine learning models are offline. These offline models are trained using trained data and then deployed. After an offline model is deployed, the underlying model doesn\u2019t change as it is exposed to more data. The problem with offline models is that they presume the incoming data will remain fairly consistent.</p> <p>Over the next few years, you will see more machine learning models available for use. As these models are constantly updated with new data, the better the models will be at predictive analytics. However, preferences and trends change, and offline models can\u2019t adapt as the incoming data changes. For example, take the situation where a machine learning model makes predictions on the likelihood that a customer will churn. The model could have been very accurate when it was deployed, but as new, more flexible competitors emerge, and once customers have more options, their likelihood to churn will increase. Because the original model was trained on older data before new market entrants emerged, it will no longer give the organization accurate predictions. On the other hand, if the model is online and continuously adapting based on incoming data, the predictions on churn will be relevant even as preferences evolve and the market landscape changes.</p>"},{"location":"AI/mlops101/#onlinedynamic-model-training","title":"Online/Dynamic Model Training","text":"<ul> <li>A <code>dynamic model</code> is trained online. That is, data is continually entering the system and we're incorporating that data into the model through continuous updates.</li> <li>We need to feed training data over time</li> <li>Use Progressive Validation rather than batch training and test</li> <li>Needs monitoring, model rollback and data quarantine capabilities</li> <li>Staleness is avoided.</li> </ul>"},{"location":"AI/mlops101/#model-inference","title":"Model Inference","text":""},{"location":"AI/mlops101/#offline-inference","title":"Offline Inference","text":"<ul> <li><code>Offline inference</code>, meaning that you make all possible predictions in a batch, using a MapReduce or something similar. You then write the predictions to an SSTable or Bigtable, and then feed these to a cache/lookup table.</li> </ul> <p>Issues</p> <p>We can only predict about the things we know about, for the data that we dont know we cant make the prediction</p>"},{"location":"AI/mlops101/#online-inference","title":"Online Inference","text":"<ul> <li><code>Online inference</code>, meaning that you predict on demand, using a server (model stored in a server).</li> <li>More extensive monitoring needs to be setup.</li> <li>Having a feedback loop from a monitoring system, and refreshing models over time, will help avoid model staleness.</li> </ul> <p>Success</p> <p>They can predict any new data that comes in - good for long tailing.</p>"},{"location":"AI/mlops101/#model-selection","title":"Model Selection","text":""},{"location":"AI/mlops101/#model-deployment","title":"Model Deployment","text":"<p>Model deployment is the process of making a machine learning model available for use on a target environment\u2014for <code>testing</code> or <code>production</code>.</p>"},{"location":"AI/mlops101/#optimization-steps","title":"Optimization Steps","text":"<ul> <li>Using GPU's instead of CPU's when required. In addition to GPUs, researchers are using Field-Programmable Gate Arrays (FPGAs) to successfully run machine learning workloads. Sometimes FPGAs outperform GPUs when running neural network and deep learning operations</li> </ul>"},{"location":"AI/model_observability/","title":"Model Observability","text":"<p>Surveys have found that ML teams spend their &lt;1% of time in the production workflow, while a majority of time is spent in the data preparation, model development and model deployment phases. This is mostly due to the challenges that teams face in resolving these issues in production. </p>"},{"location":"AI/model_observability/#model-performance","title":"Model Performance","text":"<p>The most common problems that plague all teams are around model and data drift performance degradation, and data quality issues.</p> <p>Taking a model from research to production is hard. These models fail silently. The process can be painstaking and no matter how much you work to make sure your models perform well pre-production, you don't know how they're going to perform in the real world. Utilizing every available tool during production to make sure your models are performing optimally is key. </p> <p>ML performance tracing is the methodology for pinpointing the source of a model performance problem and mapping back to the underlying data issue causing that problem.</p> <p>Here are the three most common reasons model performance can drop: - One or more of the features has a data quality issue; - One of more of the features has drifted, or is seeing unexpected values in production; or - There are labeling issues.</p>"},{"location":"AI/model_observability/#drift","title":"Drift","text":"<p>What is drift? </p> <p>Drift is a change in distribution over time. It can be measured for model inputs, outputs, and actuals. Drift can occur because your models have grown stale, bad data is flowing into your model, or even because of adversarial inputs.</p> <p>Models are not static. They are highly dependent on the data they are trained on. Especially in hyper-growth businesses where data is constantly evolving, accounting for drift is important to ensure your models stay relevant.</p>"},{"location":"AI/model_observability/#datafeature-drift","title":"Data/Feature drift","text":"<p>Change in the input to the model is almost inevitable, and your model can\u2019t always handle this change gracefully. Some models are resilient to minor changes in input distributions; however, as these distributions stray far from what the model saw in training, performance on the task at hand will suffer. This kind of drift is known as feature drift or data drift.</p> <p><code>Data drift</code> (aka feature drift, covariate drift, and input drift) refers to a shift in the statistical properties of the independent variable(s), i.e. a distribution change associated with the inputs of a model.</p>"},{"location":"AI/model_observability/#concept-drift","title":"Concept drift","text":"<p>what would happen if the distribution of the correct answers, the actuals, change? Even if your model is making the same predictions as yesterday, it can make mistakes today! This drift in actuals can cause a regression in your model\u2019s performance and is commonly referred to as concept drift or model drift.</p> <p>Concept drift is the shift in the statistical properties of the target/dependent variable(s), i.e a change in the actuals.  - It signifies a change in relationship between current actuals and actuals from a previous time period. - Concept drift can be:\u25cbA gradual change over time\u25cbA recurring or cyclical change\u25cbA sudden or abrupt change</p>"},{"location":"AI/model_observability/#upstream-drift","title":"Upstream Drift","text":"<p>Upstream (or operational data drift) refers to drift caused by changes in the models data pipeline.</p>"},{"location":"AI/model_observability/#explainability","title":"Explainability","text":""},{"location":"AI/model_observability/#global-explainability","title":"Global explainability","text":"<p>Global explainability tells you which features most contributed most to the model\u2019s decisions, global explainability is an average across all predictions. In other words, global explainability lets the model owner determine to what extent each feature contributes to how the model makes its predictions over all of the data.</p>"},{"location":"AI/model_observability/#local-explainability","title":"Local explainability","text":"<p>Local explainability helps answer the question, \u201cfor this event, why did the model make this particular decision?.\u201d The level of specificity is an incredibly useful tool in the toolbox for an ML engineer, but it\u2019s important to note that having local explainability in your system does not imply that you have access to global and cohort explainability.</p> <p>Local explainability is indispensable for getting to the root cause of a particular issue in production. Imagine you just saw that your model has rejected an applicant for a loan and you need to know why this decision was made. Local explainability would help you get to the bottom of which features were most impactful in making this loan rejection.</p>"},{"location":"AI/model_observability/#cohort-explainability","title":"Cohort explainability","text":"<p>Sometimes you need to understand how a model is making its decisions for a particular subset of your data, also known as a cohort. Cohort explainability is the process of understanding to what degree your model\u2019s features contribute to its predictions over a subset of your data.</p> <p>Cohort explainability can serve as a helpful tool in this model validation process by helping to explain the differences in how a model is predicting between a cohort where the model is performing well versus a cohort where the model is performing poorly.</p>"},{"location":"AI/model_observability/#shapley-additive-explanations","title":"Shapley Additive exPlanations","text":"<p>SHAP (Shapley Additive exPlanations) is a method used to break down individual predictions of a complex model. The purpose of SHAP is to compute the contribution of each feature to the prediction in order to identify the impact of each input. The SHAP explanation technique uses principles rooted in cooperative coalitional game theory to compute Shapley values. Much like how cooperative game theory originally looked to identify how the cooperation between groups of players (\u201ccoalitions\u201d) contributed to the gain of the coalition overall, the same technique is used in ML to calculate how features contribute to the model\u2019s outcome. In game theory, certain players contribute more to the outcome and in machine learning certain features contribute more to the model\u2019s prediction and therefore have a higher feature importance.</p>"},{"location":"AI/xgboost/","title":"XGBoost","text":"<p>XGBoost, which stands for Extreme Gradient Boosting, is a scalable, distributed gradient-boosted decision tree (GBDT) machine learning library. It provides <code>parallel tree boosting</code> and is the leading machine learning library for regression, classification, and ranking problems.</p>"},{"location":"AI/aws/aws-ai/","title":"AWS AI Services","text":""},{"location":"AI/aws/aws-ai/#amazon-augmented-ai-amazon-a2i","title":"Amazon Augmented AI (Amazon A2I)","text":"<p>Amazon Augmented AI (Amazon A2I) provides <code>built-in human review workflows</code> for common machine learning use cases, such as content moderation and text extraction from documents. With Amazon A2I, a person can also create their own workflows for machine learning models built on Amazon SageMaker or any other tools.</p>"},{"location":"AI/azure/azure-ai/","title":"Azure AI","text":""},{"location":"AI/azure/azure-ai/#luis","title":"LUIS","text":"<p><code>Language Understanding (LUIS)</code> is a cloud-based conversational AI service that applies custom machine-learning intelligence to a user's conversational, natural language text to predict overall meaning, and pull out relevant, detailed information. It allows users to interact with your applications, bots, and IoT devices by using natural language.</p> <p> </p> <p>As a part of <code>Azure Cognitive Services</code>, LUIS enables your bot to understand natural language by identifying user intents and entities.</p>"},{"location":"AI/azure/azure-ai/#qna-maker","title":"QnA Maker","text":"<p><code>QnA Maker</code> is a cloud-based Natural Language Processing (NLP) service that allows you to create a natural conversational layer over your data. It is used to find the most appropriate answer for any input from your custom <code>Knowledge Base (KB)</code> of information.</p> <p>QnA Maker is commonly used to build conversational client applications, which include social media applications, chat bots, and speech-enabled desktop applications. QnA Maker doesn't store customer data. All customer data (question answers and chat logs) is stored in the region the customer deploys the dependent service instances in. </p>"},{"location":"AI/azure/azure-ai/#text-analytics","title":"Text Analytics","text":"<p><code>Text Analytics</code>: Mine insights in unstructured text using natural language processing (NLP) where no machine learning expertise required. Gain a deeper understanding of customer opinions with sentiment analysis. </p>"},{"location":"AI/azure/azure-ai/#dispatch","title":"Dispatch","text":"<p><code>Dispatch</code> uses sample utterances for each of your bot's different tasks (LUIS, QnA Maker, or custom), and builds a model that can be used to properly route your user's request to the right task, even across multiple bots.</p>"},{"location":"AI/azure/azure-ai/#anomaly-detector","title":"Anomaly Detector","text":"<p><code>Anomaly Detector</code> is an AI service with a set of APIs, which enables you to monitor and detect anomalies in your time series data with little machine learning (ML) knowledge, either batch validation or real-time inference</p> <p>With Anomaly Detector, you can either detect anomalies in one variable using <code>Univariate Anomaly Detector</code>, or detect anomalies in multiple variables with <code>Multivariate Anomaly Detector</code>.</p>"},{"location":"AI/azure/azure-ai/#direct-line-speech","title":"Direct Line Speech","text":"<p>Direct Line Speech is a robust, end-to-end solution for creating a flexible, extensible voice assistant. It is powered by the Bot Framework and its Direct Line Speech channel, that is optimized for voice-in, voice-out interaction with bots.</p> <p> </p>"},{"location":"AI/azure/azure-ai/#azure-data-catalog","title":"Azure Data Catalog","text":"<p>Azure Data Catalog is a fully managed cloud service that lets users discover the data sources they need and understand the data sources they find. At the same time, Data Catalog helps organizations get more value from their existing investments.</p> <p>With Data Catalog, any user (analyst, data scientist, or developer) can discover, understand, and consume data sources in their data landscape. Data Catalog includes a crowdsourcing model of metadata and annotations, so everyone can contribute to making data discoverable and useable. It's a single, central place for all of an organization's users to contribute their knowledge and build a community and culture of data</p>"},{"location":"AI/azure/azure-ai/#azure-ai-video-indexer","title":"Azure AI Video Indexer","text":"<p>Azure AI Video Indexer is a cloud application, part of Azure AI services, built on Azure Media Services and Azure AI services (such as the Face, Translator, Azure AI Vision, and Speech). It enables you to extract the insights from your videos using Azure AI Video Indexer video and audio models.</p> <p>Azure AI Video Indexer analyzes the video and audio content by running 30+ AI models, generating rich insights. Here is an illustration of the audio and video analysis performed by Azure AI Video Indexer in the background:</p> <p> </p>"},{"location":"AI/azure/azure-ai/#named-entity-recognition-ner","title":"Named Entity Recognition (NER)","text":"<p>Named Entity Recognition (NER) is one of the features offered by Azure Cognitive Service for Language, a collection of machine learning and AI algorithms in the cloud for developing intelligent applications that involve written language. The NER feature can identify and categorize entities in unstructured text. For example: people, places, organizations, and quantities.</p>"},{"location":"AI/azure/azure-ai/#kb","title":"KB","text":"<p>Projections are the component of a knowledge store definition that determines where AI enriched content is stored in Azure Storage. Projections determine the type, quantity, and composition of the data structures containing your content.</p> <p>Source and Destination of Projection</p> <p>All projections have source and destination properties. The source is always internal content from an enrichment tree created during skillset execution. The destination is the name and type of an external object that's created and populated in Azure Storage.</p> <p>Except for file projections, which only accept binary images, the source must be:</p> <ul> <li>Valid JSON</li> <li>A path to a node in the enrichment tree (for example, \"source\": \"/document/objectprojection\")</li> </ul>"},{"location":"AI/azure/azure_ai_101/","title":"AI 900 Exam Prep","text":""},{"location":"AI/azure/azure_ai_101/#cognitive-search","title":"Cognitive Search","text":"<p>You can use Azure Cognitive Search's <code>knowledge mining</code> results and populate your knowledge base of your chatbot</p>"},{"location":"AI/azure/azure_ai_101/#nlp","title":"NLP","text":"<ul> <li><code>Natural language processing (NLP)</code> is used for tasks such as sentiment analysis, topic detection, language detection, key phrase extraction, and document categorization.</li> <li><code>Text Analytics</code> is the NLP solution in Azure.</li> </ul>"},{"location":"AI/azure/azure_ai_101/#model-deployment","title":"Model Deployment","text":"<ul> <li>To infer a model, you need:<ul> <li>REST endpoint for your service</li> <li>Key for your service</li> </ul> </li> <li>Real-time endpoints must be deployed to an <code>AKS</code> cluster.</li> </ul>"},{"location":"AI/azure/azure_ai_101/#feature-engineering","title":"Feature Engineering","text":"<ul> <li> <p><code>Feature engineering</code> is the process of using domain knowledge of the data to create features that help ML algorithms learn better. In Azure Machine Learning, <code>scaling</code> and <code>normalization</code> techniques are applied to facilitate feature engineering. Collectively, these techniques and feature engineering are referred to as featurization.</p> </li> <li> <p>In machine learning and statistics, <code>feature selection</code> is the process of selecting a subset of relevant, useful features to use in building an analytical model. Feature selection helps narrow the field of data to the most valuable inputs. Narrowing the field of data helps reduce noise and improve training performance.</p> </li> </ul>"},{"location":"AI/azure/azure_ai_101/#automl","title":"AutoML","text":"<ul> <li> <p>Automated machine learning, also referred to as automated ML or AutoML, is the process of automating the time consuming, iterative tasks of machine learning model development. It allows data scientists, analysts, and developers to build ML models with high scale, efficiency, and productivity all while sustaining model quality.</p> </li> <li> <p>During training, Azure Machine Learning creates a number of pipelines in parallel that try different algorithms and parameters for you. The service iterates through ML algorithms paired with feature selections, where each iteration produces a model with a training score. The higher the score, the better the model is considered to \"fit\" your data.It will stop once it hits the exit criteria defined in the experiment.</p> </li> <li> <p>In machine learning, if you have labeled data, that means your data is marked up, or annotated, to show the target, which is the answer you want your machine learning model to predict.</p> </li> <li><code>Accuracy</code> is simply the proportion of correctly classified instances. It is usually the first metric you look at when evaluating a classifier. However, when the test data is unbalanced (where most of the instances belong to one of the classes), or you are more interested in the performance on either one of the classes, accuracy doesn't really capture the effectiveness of a classifier.</li> </ul>"},{"location":"AI/azure/azure_ai_101/#form-recognizer","title":"Form Recognizer","text":"<p><code>Form Recognizer</code> applies advanced machine learning to accurately extract text, key/ value pairs, and tables from documents. With just a few samples, Form Recognizer tailors its understanding to your documents, both on-premises and in the cloud. Turn forms into usable data at a fraction of the time and cost, so you can focus more time acting on the information rather than compiling it.</p>"},{"location":"AI/azure/azure_ai_101/#regression","title":"Regression","text":"<ul> <li> <p>For regression problems, the label column must contain numeric data that represents the response variable. Ideally the numeric data represents a continuous scale.</p> </li> <li> <p>Linear regression attempts to establish a linear relationship between one or more independent variables and a numeric outcome, or dependent variable.</p> </li> </ul>"},{"location":"AI/azure/azure_ai_101/#classification","title":"Classification","text":"<ul> <li> <p>Classification is a machine learning method that uses data to determine the category, type, or class of an item or row of data.</p> </li> <li> <p><code>Two-class classification</code> provides the answer to simple two-choice questions such as Yes/No or True/False</p> </li> </ul>"},{"location":"AI/azure/azure_ai_101/#clustering","title":"Clustering","text":"<ul> <li>It is a method of grouping data points into similar clusters. It is also called segmentation.</li> </ul>"},{"location":"AI/azure/azure_ai_101/#aml-designer","title":"AML Designer","text":"<ul> <li> <p>Azure Machine Learning designer lets you visually connect datasets and modules on an interactive canvas to create machine learning models.</p> </li> <li> <p>Azure Machine Learning designer is a drag-and-drop UI interface for building machine learning pipelines in Azure Machine Learning Workspaces.</p> </li> <li> <p>You can drag-and-drop datasets and modules onto the canvas.</p> </li> <li> <p>Pipeline Draft: As you edit a pipeline in the designer, your progress is saved as a pipeline draft. You can edit a pipeline draft at any point by adding or removing components, configuring compute targets, creating parameters, and so on.</p> </li> <li> <p>Pipeline Job: Each time you run a pipeline, the configuration of the pipeline and its results are stored in your workspace as a pipeline job. You can go back to any pipeline job to inspect it for troubleshooting or auditing. Clone a pipeline job creates a new pipeline draft for you to continue editing.</p> <p>Note</p> <p>Enterprise workspaces are no longer available as of September 2020. The basic workspace now has all the functionality of the enterprise workspace.</p> </li> </ul>"},{"location":"AI/azure/azure_ai_101/#custom-vision","title":"Custom Vision","text":"<ul> <li> <p><code>Azure AI Custom Vision</code> is an image recognition service that lets you build, deploy, and improve your own image identifier models. An image identifier applies labels to images, according to their visual characteristics. Each label represents a classification or object. Custom Vision allows you to specify your own labels and train custom models to detect them.</p> </li> <li> <p>Custom Vision service can be used only on graphic files.</p> </li> <li> <p>The Custom Vision service uses a machine learning algorithm to analyze images. You, the developer, submit groups of images that feature and lack the characteristics in question. You label the images yourself at the time of submission. Then, the algorithm trains to this data and calculates its own accuracy by testing itself on those same images.</p> </li> <li> <p>Custom Vision functionality can be divided into two features.</p> <ol> <li>Image classification applies one or more labels to an image.</li> <li>Object detection is similar, but it also returns the coordinates in the image where the applied label(s) can be found.</li> </ol> </li> </ul>"},{"location":"AI/azure/azure_ai_101/#validation-set","title":"Validation Set","text":"<ul> <li> <p>The <code>validation dataset</code> is different from the <code>test dataset</code> that is held back from the training of the model</p> </li> <li> <p>A validation dataset is a sample of data that is used to give an estimate of model skill while tuning <code>model's hyperparameters</code></p> </li> </ul>"},{"location":"AI/azure/azure_ai_101/#metrics","title":"Metrics","text":"<ul> <li> <p>The Model evaluation module outputs a <code>confusion matrix</code> showing the number of true positives, false negatives, false positives, and true negatives, as well as <code>ROC</code>, <code>Precision</code>/<code>Recall</code>, and <code>Lift curves</code></p> </li> <li> <p><code>F1 score</code> also known as balanced <code>F-score</code> or <code>F-measure</code> is used to evaluate a classification model.</p> </li> <li> <p><code>aucROC</code> or <code>area under the curve</code> (AUC) is used to evaluate a classification model.</p> </li> <li> <p><code>R-squared (R2)</code>, or Coefficient of determination represents the predictive power of the model as a value between -inf and 1.00. 1.00 means there is a perfect fit, and the fit can be arbitrarily poor so the scores can be negative.</p> </li> <li> <p><code>RMS-loss</code> or <code>Root Mean Squared Error</code>(RMSE) (also called Root Mean Square Deviation, RMSD), measures the difference between values predicted by a model and the values observed from the environment that is being modeled.</p> </li> </ul>"},{"location":"AI/azure/azure_ai_101/#object-detection","title":"Object detection","text":"<p>Object detection is similar to tagging, but the API returns the bounding box coordinates (in pixels) for each object found. For example, if an image contains a dog, cat and person, the Detect operation will list those objects together with their coordinates in the image.</p> <p>You can use this functionality to process the relationships between the objects in an image. It also lets you determine whether there are multiple instances of the same tag in an image.</p>"},{"location":"AI/azure/azure_ai_101/#luis","title":"LUIS","text":"<ul> <li> <p><code>Language Understanding (LUIS)</code> is a cloud-based API service that applies custom machine-learning intelligence to a user's conversational, natural language text to predict overall meaning, and pull out relevant, detailed information.</p> </li> <li> <p>Design your LUIS model with categories of user intentions called intents. Each intent needs examples of user utterances. Each utterance can provide data that needs to be extracted with machine-learning entities.</p> </li> <li> <p><code>Key phrase extraction/ Broad entity extraction</code>: Identify important concepts in text, including key phrases and named entities such as people, places, and organizations.</p> </li> <li> <p><code>Utterances</code> are input from the user that your app needs to interpret. To train LUIS to extract intents and entities from them, it's important to capture a variety of different example utterances for each intent. <code>Active learning</code>, or the process of continuing to train on new utterances, is essential to machine-learned intelligence that LUIS provides.</p> </li> </ul> <p>Relation between intent and utterance</p> <p>Each intent needs to have example utterances, at least 15. If you have an intent that does not have any example utterances, you will not be able to train LUIS. If you have an intent with one or very few example utterances, LUIS will not accurately predict the intent.</p> <ul> <li> <p>Chit-chat can be added to provide the professional greeting and make the bot more user friendly.</p> </li> <li> <p>The <code>None intent</code> is filled with utterances that are outside of your domain.</p> </li> <li> <p>For <code>QnA maker</code> you can extract question-answer pairs from semi-structured content, including FAQ pages, support websites, excel files, SharePoint documents, product manuals and policies.</p> </li> <li> <p>To ensure GDPR compliance with the Language Understanding (LUIS) API in the context of a Bot Framework implementation, it is important to handle user data appropriately. Deleting the utterances from the Review endpoint utterances helps to remove any personal data that may have been collected during the interaction with the bot. This step ensures that user data is not stored or retained longer than necessary, aligning with GDPR principles of data minimization and data retention.</p> </li> <li> <p><code>Text to speech</code> enables your applications, tools, or devices to convert text into humanlike synthesized speech. The text to speech capability is also known as speech synthesis. Use humanlike prebuilt neural voices out of the box, or create a custom neural voice that's unique to your product or brand.</p> </li> </ul> <p>Remember</p> <p>Intent is for tasks or actions</p> <p>Entities is for object - names, dates, times, numbers, measurements, currency</p>"},{"location":"AI/azure/azure_ai_101/#kv-soft-delete","title":"KV soft delete","text":"<p>Soft-delete behavior</p> <ul> <li> <p>With this feature, the DELETE operation on a key vault or key vault object is a soft-delete, effectively holding the resources for a given retention period (90 days), while giving the appearance that the object is deleted. The service further provides a mechanism for recovering the deleted object, essentially undoing the deletion</p> </li> <li> <p>Soft-delete is an <code>optional Key Vault</code> behavior and is not enabled by default in this release. It can be turned on via CLI or Powershell</p> </li> </ul> <p>Purge protection - When purge protection is on, a vault or an object in deleted state cannot be purged until the retention period of 90 days has passed. These vaults and objects can still be recovered, assuring customers that the retention policy will be followed.</p> <ul> <li>Purge protection is an optional Key Vault behavior and is not enabled by default. It can be turned on via CLI or Powershell.</li> </ul>"},{"location":"AI/azure/azureml/","title":"Azure ML","text":""},{"location":"AI/azure/azureml/#aml-workspace","title":"AML Workspace","text":"<p>The workspace is the <code>top-level resource</code> for Azure Machine Learning, providing a centralized place to work with all the artifacts you create.</p> <p>When you create a new workspace, it automatically creates below Azure resources:</p> <ul> <li><code>Storage account</code>: It used to store files used by the workspace as well as data for <code>experiments</code> and <code>model training</code>. It Is used as the <code>default datastore</code> for the workspace. Jupyter notebooks that are used with your Azure Machine Learning compute instances are stored here as well.</li> <li><code>Application Insights</code>: It is used to monitor predictive services in the workspace. </li> <li><code>Key Vault instance</code>: Stores <code>secrets</code> that are used by compute targets and other sensitive information that's needed by the workspace.</li> <li><code>Container/Model registry</code>, used to manage containers for deployed models. </li> <li><code>VMs</code>: provide computing power for your AzureML workspace and are an integral part in deploying and training models.</li> <li><code>Load Balancer</code>: a <code>network load balancer</code> is created for each compute instance and compute cluster to manage traffic even while the compute instance/cluster is stopped.</li> <li><code>Virtual Network</code>: these help Azure resources communicate with one another, the internet, and other on-premises networks.</li> </ul> <p> </p>"},{"location":"AI/azure/azureml/#other-components","title":"Other components:","text":""},{"location":"AI/azure/azureml/#compute-instance","title":"Compute Instance","text":"<p>It is a <code>managed cloud-based workstation</code> for data scientists. Each compute instance has only one owner, although you can share files between multiple compute instances. They can be used for dev and test purposes.</p>"},{"location":"AI/azure/azureml/#compute-target","title":"Compute Target","text":"<p>it is a <code>designated compute resource</code> or environment where you run your training script or host your service deployment. This location might be your local machine or a cloud-based compute resource.</p> <p>Types of Compute Targets</p> <ol> <li>Local \u2014 This is used to run the experiment on the <code>same compute target</code> as the code used to initiate the experiment.</li> <li>Training Cluster \u2014 for high scalable training requirements \u2014 distributed computes, CPU/GPU are enabled and scaled on-demand.</li> <li>Inference Cluster \u2014 containerized clusters to deploy the inference of the trained model as an overall application module.</li> <li>Attached Compute \u2014 to attach already acquired Azure ML VM or Databricks machine</li> </ol>"},{"location":"AI/azure/azureml/#training-cluster","title":"Training Cluster","text":"<ul> <li>AML compute instance</li> <li>AML compute cluster</li> <li>AKS</li> <li>Databricks</li> <li>Batch</li> </ul>"},{"location":"AI/azure/azureml/#inference-cluster","title":"Inference Cluster","text":"<p>Docker container is created before inference</p> <p>When performing inference, AML creates a Docker container that hosts the model and associated resources needed to use it. This container is then used in a <code>compute target</code>.</p> <p>Various inference compute target can be:</p> <ul> <li><code>AML endpoints</code>: Fully managed computes for real-time (managed online endpoints) and batch scoring (batch endpoints) on serverless compute.</li> <li><code>AKS</code>: use managed k8s </li> <li><code>ACI</code>: Run docker container without orchestrator</li> </ul>"},{"location":"AI/azure/azureml/#aml-concepts","title":"AML Concepts","text":""},{"location":"AI/azure/azureml/#model-deployment","title":"Model deployment","text":"<p>After you train an ML model, you need to deploy the model so that others can use it to do inferencing. In Azure Machine Learning, you can use <code>endpoints</code> and <code>deployments</code> to do so.</p>"},{"location":"AI/azure/azureml/#endpoint","title":"Endpoint","text":"<p>Info</p> <p>An endpoint is an HTTPS endpoint that clients can call to receive the inferencing (scoring) output of a trained model. It provides:</p> <pre><code>- Authentication using \"key &amp; token\" based auth\n- SSL termination\n- A stable scoring URI (endpoint-name.region.inference.ml.azure.com)\n</code></pre>"},{"location":"AI/azure/azureml/#deployment","title":"Deployment","text":"<p>A deployment is a set of resources required for hosting the model that does the actual inferencing.</p> <p>Remember</p> <p>A <code>single endpoint</code> can contain multiple deployments. Endpoints and deployments are independent <code>Azure Resource Manager</code> resources that appear in the Azure portal.</p>"},{"location":"AI/azure/azureml/#real-timebatch-scoring","title":"Real-time/batch scoring","text":"<ol> <li> <p><code>Batch scoring</code>, or batch inferencing, involves invoking an endpoint with a reference to data. The batch endpoint runs jobs asynchronously to process data in parallel on compute clusters and store the data for further analysis</p> </li> <li> <p><code>Real-time scoring</code>, or online inferencing, involves invoking an endpoint with one or more model deployments and receiving a response in near-real-time via HTTPs. Traffic can be split across multiple deployments, allowing for testing new model versions by diverting some amount of traffic initially and increasing once confidence in the new model is established.</p> </li> </ol>"},{"location":"AI/azure/azureml/#cli-v2","title":"CLI V2","text":"<p>The AML <code>CLI v2</code> is the latest extension for the Azure CLI. The <code>CLI v2</code> provides commands in the format <code>az ml &lt;noun&gt; &lt;verb&gt; &lt;options&gt;</code> to create and maintain Azure ML assets and workflows. The <code>assets</code> or workflows themselves are defined using a <code>YAML file</code>. The YAML file defines the configuration of the asset or workflow \u2013 what is it, where should it run, and so on.</p> AML commands examples<pre><code>az ml job create --file my_job_definition.yaml\naz ml environment update --name my-env --file my_updated_env_definition.yaml\naz ml model list\naz ml compute show --name my_compute\n</code></pre>"},{"location":"AI/azure/azureml/#aml-pipelines","title":"AML Pipelines","text":"<p>Pipelines are workflows of complete machine learning tasks that can be run independently.  The Azure Machine Learning pipeline service automatically orchestrates all the dependencies between pipeline steps.</p> <p>You can create pipelines without using components, but components offer better amount of flexibility and reuse. <code>Azure ML Pipelines</code> may be defined in YAML and run from the CLI, authored in Python, or composed in Azure ML Studio Designer with a drag-and-drop UI.</p>"},{"location":"AI/azure/azureml/#experiment","title":"Experiment","text":"<p>An Experiment is a <code>container of trials</code> that represent multiple model runs.</p>"},{"location":"AI/azure/azureml/#environment","title":"Environment","text":"<p>AMl environments are an encapsulation of the environment where your ML training happens. They specify the Python packages, environment variables, and software settings around your training and scoring scripts. They also specify runtimes (Python, Spark, or Docker). </p> <p>Environments can broadly be divided into three categories: </p> <ol> <li>Curated: They are provided by Azure Machine Learning and are available in your workspace by default. Intended to be used as is, they contain collections of Python packages and settings to help you get started with various machine learning frameworks</li> <li>User-managed: In this you're responsible for setting up your environment and installing every package that your training script needs on the compute target.</li> <li>System-managed: Conda will manage env.</li> </ol> <p>build the env to docker</p> <p>AML builds environment definitions into <code>Docker images</code> and <code>conda environments</code>. It also caches the environments, so they can be reused in subsequent training jobs and service endpoint deployments.</p>"},{"location":"AI/azure/azureml/#aml-studio","title":"AML Studio","text":""},{"location":"AI/azure/azureml/#aml-designer","title":"AML Designer","text":"<p>We can use the <code>designer</code> to train and deploy ML models without writing any code. Drag and drop datasets and components to create ML pipelines.</p>"},{"location":"AI/azure/azureml/#component","title":"Component","text":"<p>An <code>Azure Machine Learning component</code> is a self-contained piece of code that does one step in a machine learning pipeline. <code>Components</code> are the building blocks of advanced machine learning pipelines. Components can do tasks such as data processing, model training, model scoring, and so on.</p> <p>A component is analogous to a function - it has a name, parameters, expects input, and returns output.</p>"},{"location":"AI/azure/azureml/#aml-compute","title":"AMl Compute","text":"<p>A compute is a designated compute resource where you run your job or host your endpoint. Azure Machine learning supports the following types of compute:</p> <ul> <li><code>Compute cluster</code> - a managed-compute infrastructure that allows you to easily create a cluster of CPU or GPU compute nodes in the cloud.</li> <li><code>Compute instance</code> - a fully configured and managed development environment in the cloud. You can use the instance as a training or inference compute for development and testing. It's similar to a virtual machine on the cloud</li> <li><code>Inference cluster</code> - used to deploy trained machine learning models to Azure Kubernetes Service. You can create an Azure Kubernetes Service (AKS) cluster from your Azure ML workspace, or attach an existing AKS cluster.</li> <li><code>Attached compute</code> - You can attach your own compute resources to your workspace and use them for training and inference.</li> </ul>"},{"location":"AI/azure/azureml/#datastore","title":"Datastore","text":"<p><code>Azure Machine Learning</code> datastores securely keep the connection information to your data storage on Azure, so you don't have to code it in your scripts. You can register and create a datastore to easily connect to your storage account, and access the data in your underlying storage service. The <code>CLI v2</code> and <code>SDK v2</code> support the following types of cloud-based storage services:</p> <ul> <li>Azure Blob Container</li> <li>Azure File Share</li> <li>Azure Data Lake</li> <li>Azure Data Lake Gen2</li> </ul>"},{"location":"AI/azure/azureml/#mlflow","title":"MLFlow","text":"<p><code>MLflow</code> is an open-source framework that's designed to manage the complete ML lifecycle. Its ability to train and serve models on different platforms allows you to use a consistent set of tools regardless of where your experiments are running: locally on your computer, on a remote compute target, on a virtual machine, or on an AML compute instance.</p>"},{"location":"AI/azure/azureml/#model","title":"Model","text":"<p>Azure machine learning models consist of the <code>binary file(s)</code> that represent a machine learning model and any corresponding metadata. Models can be created from a local or remote file or directory. For remote locations https, wasbs and azureml locations are supported </p> <p>The created model will be tracked in the workspace under the specified name and version. Azure ML supports 3 types of storage format for models:</p> <ul> <li>custom_model</li> <li>mlflow_model</li> <li>triton_model</li> </ul> <p>Difference between Artifacts and Models</p> <p>Any file generated (and captured) from an experiment's run or job is an artifact. It may represent a model serialized as a <code>Pickle file</code>, the weights of a PyTorch or TensorFlow model, or even a text file containing the coefficients of a linear regression</p>"},{"location":"AI/azure/azureml/#onnx-format","title":"ONNX format","text":"<p>It is the <code>Open Neural Network Exchange</code> format.</p> <p>Who created ONNX and why?</p> <p>Microsoft and a community of partners created <code>ONNX</code> as an <code>open standard</code> for representing ML models. Models from many frameworks including TensorFlow, PyTorch, SciKit-Learn, Keras, Chainer, MXNet, MATLAB, and SparkML can be exported or converted to the <code>standard ONNX</code> format. Once the models are in the ONNX format, they can be run on a variety of platforms and devices.</p> <p>ONNX Runtime: It is a <code>high-performance inference engine</code> for deploying ONNX models to production. It's optimized for both cloud and edge and works on Linux, Windows, and Mac.</p>"},{"location":"AI/azure/azureml/#mlops","title":"MLOPS","text":"<p><code>DevOps</code> for machine learning models, often called <code>MLOps</code>, is a process for developing models for production. A model's lifecycle from training to deployment must be auditable if not reproducible.</p>"},{"location":"AI/azure/azureml/#model-training-lifecycle","title":"Model training lifecycle","text":"<p>The Azure training lifecycle consists of:</p> <ol> <li>Zipping the files in your project folder, ignoring those specified in <code>.amlignore</code> or <code>.gitignore</code></li> <li>Scaling up your <code>compute cluster</code></li> <li>Building or downloading the <code>dockerfile</code> to the <code>compute node</code><ol> <li>The system calculates a hash of:<ol> <li>The base image</li> <li>Custom docker steps </li> <li>The conda definition YAML </li> </ol> </li> <li>The system uses this hash as the key in a lookup of the workspace <code>Azure Container Registry (ACR)</code></li> <li>If it is not found, it looks for a match in the <code>global ACR</code></li> <li>If it is not found, the system builds a new image (which will be cached and registered with the <code>workspace ACR</code>)</li> </ol> </li> <li>Downloading your zipped project file to temporary storage on the <code>compute node</code>.</li> <li>Unzipping the project file.</li> <li>The compute node executing <code>python &lt;entry script&gt; &lt;arguments&gt;</code></li> <li>Saving logs, model files, and other files written to <code>./outputs</code> to the storage account associated with the workspace.</li> <li>Scaling down compute, including removing temporary storage.</li> </ol>"},{"location":"AI/azure/cognitive-search/","title":"Cognitive Search","text":""},{"location":"AI/azure/cognitive-search/#basics","title":"Basics","text":""},{"location":"AI/azure/cognitive-search/#skillset","title":"Skillset","text":"<p>In Azure AI Search, you can apply <code>Artificial Intelligence (AI)</code> skills as part of the indexing process to enrich the source data with new information, which can be mapped to index fields. The skills used by an indexer are encapsulated in a skillset that defines an enrichment pipeline in which each step enhances the source data with insights obtained by a specific AI skill. Examples of the kind of information that can be extracted by an AI skill include:</p> <ul> <li>The language in which a document is written.</li> <li>Key phrases that might help determine the main themes or topics discussed in a document.</li> <li>A sentiment score that quantifies how positive or negative a document is.</li> <li>Specific locations, people, organizations, or landmarks mentioned in the content.</li> <li>AI-generated descriptions of images, or image text extracted by optical character recognition.</li> <li>Custom skills that you develop to meet specific requirements.</li> </ul>"},{"location":"AI/azure/cognitive-search/#indexer","title":"Indexer","text":"<p>The indexer is the engine that drives the overall indexing process. It takes the outputs extracted using the skills in the skillset, along with the data and metadata values extracted from the original data source, and maps them to fields in the index.</p> <p>An indexer is automatically run when it is created, and can be scheduled to run at regular intervals or run on demand to add more documents to the index. In some cases, such as when you add new fields to an index or new skills to a skillset, you may need to reset the index before re-running the indexer.</p>"},{"location":"AI/azure/cognitive-search/#index","title":"Index","text":"<p><code>Index</code>: The index is the searchable result of the indexing process. It consists of a collection of JSON documents, with fields that contain the values extracted during indexing. Client applications can query the index to retrieve, filter, and sort information.</p> <p>Each index field can be configured with the following attributes:</p> <ul> <li><code>Key</code>: Fields that define a unique key for index records.</li> <li><code>Searchable</code>: Fields that can be queried using full-text search.</li> <li><code>Filterable</code>: Fields that can be included in filter expressions to return only documents that match specified constraints.</li> <li><code>Sortable</code>: Fields that can be used to order the results.</li> <li><code>Facetable</code>: Ensure that users can perform drill down filtering based on category.</li> <li><code>Retrievable</code>: Fields that can be included in search results (by default, all fields are retrievable unless this attribute is explicitly removed).</li> </ul> <p>Indexing Process</p> <p>The indexing process works by creating a document for each indexed entity. During indexing, an enrichment pipeline iteratively builds the documents that combine metadata from the data source with enriched fields extracted by cognitive skills. You can think of each indexed document as a JSON structure, which initially consists of a document with the index fields you have mapped to fields extracted directly from the source data, like this:</p> <pre><code>document\n    metadata_storage_name\n    metadata_author\n    content\n</code></pre> <p>When the documents in the data source contain images, you can configure the indexer to extract the image data and place each image in a normalized_images collection, like this:</p> <pre><code>document\n    metadata_storage_name\n    metadata_author\n    content\n    normalized_images\n        image0\n        image1\n</code></pre> <p>Normalizing the image data in this way enables you to use the collection of images as an input for skills that extract information from image data.</p> <p>Each skill adds fields to the document, so for example a skill that detects the language in which a document is written might store its output in a language field, like this:</p> <pre><code>document\n    metadata_storage_name\n    metadata_author\n    content\n    normalized_images\n        image0\n        image1\n    language\n</code></pre> <p>The document is structured hierarchically, and the skills are applied to a specific context within the hierarchy, enabling you to run the skill for each item at a particular level of the document. For example, you could run an optical character recognition (OCR) skill for each image in the normalized images collection to extract any text they contain:</p> <pre><code>document\n    metadata_storage_name\n    metadata_author\n    content\n    normalized_images\n        image0\n            Text\n        image1\n            Text\n    language\n</code></pre> <p>The output fields from each skill can be used as inputs for other skills later in the pipeline, which in turn store their outputs in the document structure. For example, we could use a merge skill to combine the original text content with the text extracted from each image to create a new merged_content field that contains all of the text in the document, including image text.</p> <pre><code>document\n    metadata_storage_name\n    metadata_author\n    content\n    normalized_images\n        image0\n            Text\n        image1\n            Text\n    language\n    merged_content\n</code></pre>"},{"location":"AI/azure/cognitive-search/#full-text-search","title":"Full text search","text":"<p>Full text search describes search solutions that parse text-based document contents to find query terms. Full text search queries in Azure AI Search are based on the Lucene query syntax, which provides a rich set of query operations for searching, filtering, and sorting data in indexes. Azure AI Search supports two variants of the Lucene syntax:</p> <ul> <li><code>Simple</code> - An intuitive syntax that makes it easy to perform basic searches that match literal query terms submitted by a user.</li> <li><code>Full</code> - An extended syntax that supports complex filtering, regular expressions, and other more sophisticated queries.</li> </ul> <p>Query processing consists of four stages</p> <ol> <li><code>Query parsing</code> - The search expression is evaluated and reconstructed as a tree of appropriate subqueries. Subqueries might include term queries (finding specific individual words in the search expression - for example hotel), phrase queries (finding multi-term phrases specified in quotation marks in the search expression - for example, \"free parking\"), and prefix queries (finding terms with a specified prefix - for example air*, which would match airway, air-conditioning, and airport).</li> <li><code>Lexical analysis</code> - The query terms are analyzed and refined based on linguistic rules. For example, text is converted to lower case and nonessential stopwords (such as \"the\", \"a\", \"is\", and so on) are removed. Then words are converted to their root form (for example, \"comfortable\" might be simplified to \"comfort\") and composite words are split into their constituent terms.</li> <li><code>Document retrieval</code> - The query terms are matched against the indexed terms, and the set of matching documents is identified.</li> <li><code>Scoring</code> - A relevance score is assigned to each result based on a term frequency/inverse document frequency (TF/IDF) calculation.</li> </ol>"},{"location":"AI/azure/cognitive-search/#azure-ai-search","title":"Azure AI Search","text":"<p>Azure AI Search provides a cloud-based solution for indexing and querying a wide range of data sources, and creating comprehensive and high-scale search solutions. With Azure AI Search, you can:</p> <ul> <li>Index documents and data from a range of sources.</li> <li>Use cognitive skills to enrich index data.</li> <li>Store extracted insights in a knowledge store for analysis and integration.</li> </ul>"},{"location":"AI/azure/cognitive-search/#replicas-and-partitions","title":"Replicas and partitions","text":"<p>Depending on the pricing tier you select, you can optimize your solution for scalability and availability by creating replicas and partitions.</p> <ul> <li> <p><code>Replicas</code> are instances of the search service - you can think of them as nodes in a cluster. Increasing the number of replicas can help ensure there is sufficient capacity to service multiple concurrent query requests while managing ongoing indexing operations.</p> </li> <li> <p><code>Partitions</code> are used to divide an index into multiple storage locations, enabling you to split I/O operations such as querying or rebuilding an index.</p> </li> </ul> <p>The combination of replicas and partitions you configure determines the search units used by your solution. Put simply, the number of search units is the number of replicas multiplied by the number of partitions (R x P = SU). For example, a resource with four replicas and three partitions is using 12 search units.</p>"},{"location":"AI/azure/cognitive-search/#cognitive-search","title":"Cognitive Search","text":"<p>Azure Cognitive Search is a search service hosted in Azure that can index content on your premises or in a cloud location.</p> <p> </p> <p>How does indexing happen?</p> <p>During the indexing process, Cognitive Search crawls your content, processes it, and creates a list of words that will be added to the index, together with their location. There are five stages to the indexing process:</p> <ul> <li><code>Document Cracking</code>: In document cracking, the indexer opens the content files and extracts their content.</li> <li><code>Field Mappings</code>: Fields such as titles, names, dates, and more are extracted from the content. You can use field mappings to control how they're stored in the index.</li> <li><code>Skillset Execution</code>: In the optional skillset execution stage, custom AI processing is done on the content to enrich the final index.</li> <li><code>Output field mappings</code>: If you're using a custom skillset, its output is mapped to index fields in this stage.</li> <li><code>Push to index</code>: The results of the indexing process are stored in the index in Azure Cognitive Search.</li> </ul>"},{"location":"AI/azure/cognitive-search/#integrate-cognitive-search-and-azure-ai-document-intelligence","title":"Integrate Cognitive Search and Azure AI Document Intelligence","text":"<p>To integrate Azure AI Document Intelligence into the Cognitive Search indexing process, you must write a Web service that integrates the custom skill interface.</p> <p>If you've developed an Azure AI Document Intelligence solution, you may be using it to accept scanned or photographed forms or documents from users, perhaps from an app on their mobile device. Azure AI Document Intelligence can use either a built-in model or a custom model to analyze the content of these images and return text, structural information, languages used, key-value pairs, and other data.</p> <p>That's the kind of data that may be useful in a Cognitive Search index. For example, if the content that you index includes scanned sales invoices, Azure AI Document Intelligence can identify field such as currency amounts, retailer names, and tax information by using its prebuilt Invoice model. When users search for a retailer, you'd like them to receive a link to invoices from that retailer in their results.</p> <p>To integrate Azure AI Document Intelligence into the Cognitive Search indexing pipeline, you must:</p> <ul> <li>Create an Azure AI Document Intelligence resource in your Azure subscription.</li> <li>Configure one or more models in Azure AI Document Intelligence. You can either select prebuilt models, such as Invoice or Business Card or train your own model for unusual or unique form types.</li> <li>Develop and deploy a web service that can call your Azure AI Document Intelligence resource. In this module, you'll use an Azure Function to host this service.</li> <li>Add a custom web API skill, with the correct configuration to the Cognitive Search skillset. This skill should be configured to send requests to the web service.</li> </ul>"},{"location":"AI/azure/cognitive-search/#custom-skill","title":"Custom Skill","text":"<p>Your custom skill must implement the expected schema for input and output data that is expected by skills in an Azure AI Search skillset.</p> <p>The input schema for a custom skill defines a JSON structure containing a record for each document to be processed. Each document has a unique identifier, and a data payload with one or more inputs, like this:</p> <pre><code>{\n    \"values\": [\n      {\n        \"recordId\": \"&lt;unique_identifier&gt;\",\n        \"data\":\n           {\n             \"&lt;input1_name&gt;\":  \"&lt;input1_value&gt;\",\n             \"&lt;input2_name&gt;\": \"&lt;input2_value&gt;\",\n             ...\n           }\n      },\n      {\n        \"recordId\": \"&lt;unique_identifier&gt;\",\n        \"data\":\n           {\n             \"&lt;input1_name&gt;\":  \"&lt;input1_value&gt;\",\n             \"&lt;input2_name&gt;\": \"&lt;input2_value&gt;\",\n             ...\n           }\n      },\n      ...\n    ]\n}\n</code></pre> <p>The schema for the results returned by your custom skill reflects the input schema. It is assumed that the output contains a record for each input record, with either the results produced by the skill or details of any errors that occurred.</p> <pre><code>{\n    \"values\": [\n      {\n        \"recordId\": \"&lt;unique_identifier_from_input&gt;\",\n        \"data\":\n           {\n             \"&lt;output1_name&gt;\":  \"&lt;output1_value&gt;\",\n              ...\n           },\n         \"errors\": [...],\n         \"warnings\": [...]\n      },\n      {\n        \"recordId\": \"&lt; unique_identifier_from_input&gt;\",\n        \"data\":\n           {\n             \"&lt;output1_name&gt;\":  \"&lt;output1_value&gt;\",\n              ...\n           },\n         \"errors\": [...],\n         \"warnings\": [...]\n      },\n      ...\n    ]\n}\n</code></pre>"},{"location":"AI/azure/cognitive-search/#knowledge-stores","title":"Knowledge stores","text":"<p>While the index might be considered the primary output from an indexing process, the enriched data it contains might also be useful in other ways. For example:</p> <ul> <li>Since the index is essentially a collection of JSON objects, each representing an indexed record, it might be useful to export the objects as JSON files for integration into a data orchestration process using tools such as Azure Data Factory.</li> <li>You may want to normalize the index records into a relational schema of tables for analysis and reporting with tools such as Microsoft Power BI.</li> <li>Having extracted embedded images from documents during the indexing process, you might want to save those images as files.</li> </ul> <p>Azure AI Search supports these scenarios by enabling you to define a knowledge store in the skillset that encapsulates your enrichment pipeline. The knowledge store consists of projections of the enriched data, which can be JSON objects, tables, or image files. When an indexer runs the pipeline to create or update an index, the projections are generated and persisted in the knowledge store.</p>"},{"location":"AI/azure/form-recognizer/","title":"Form Recognizer","text":""},{"location":"AI/azure/form-recognizer/#what-is-azure-document-intelligence","title":"What is Azure Document Intelligence?","text":"<p>Azure Document Intelligence uses Optical Character Recognition (OCR) capabilities and deep learning models to extract text, key-value pairs, selection marks, and tables from documents.</p> <p> </p> <p>OCR captures document structure by creating bounding boxes around detected objects in an image. The locations of the bounding boxes are recorded as coordinates in relation to the rest of the page. Azure Document Intelligence services return bounding box data and other information in a structured form with the relationships from the original file.</p> <p> </p> <p>When to use Vison  or Document Intelligence?</p> <p>if you want to extract simple words and text from a picture of a form or document, without contextual information, Azure AI Vision OCR is an appropriate service to consider. If you want to deploy a complete document analysis solution that enables users to both extract and understand text, consider Azure AI Document Intelligence.</p>"},{"location":"AI/azure/form-recognizer/#document-intelligence-studio","title":"Document Intelligence Studio","text":"<p>If you want to try many features of Azure AI Document Intelligence without writing any code, you can use Azure AI Document Intelligence Studio. This provides a visual tool for exploring and understanding the capabilities of Azure AI Document Intelligence and its support for your forms.</p> <p>To integrate Azure AI Document Intelligence into your own applications you'll need to write code. For example, you could enable users of your sales mobile app to scan receipts with their device's camera and call Azure AI Document Intelligence to obtain prices, costs, and custom details. The app could store this information in your customer relationship management database.</p>"},{"location":"AI/azure/form-recognizer/#how-to-consume-the-app","title":"How to consume the app","text":"<p>When you write an application that uses Azure AI Document Intelligence, you need two pieces of information to connect to the resource:</p> <ul> <li><code>Endpoint</code>. This is the URL where the resource can be contacted.</li> <li><code>Access key</code>. This is unique string that Azure uses to authenticate the call to Azure AI Document Intelligence.</li> </ul>"},{"location":"AI/azure/form-recognizer/#model-types","title":"Model types","text":""},{"location":"AI/azure/form-recognizer/#prebuilt","title":"Prebuilt","text":"<p>Several of the prebuilt models are trained on specific form types:</p> <ul> <li><code>Invoice model</code>: Extracts common fields and their values from invoices.</li> <li><code>Receipt model</code>: Extracts common fields and their values from receipts.</li> <li><code>W2 model</code>: Extracts common fields and their values from the US Government's W2 tax declaration form.</li> <li><code>ID document model</code>: Extracts common fields and their values from US drivers' licenses and international passports.</li> <li><code>Business card model</code>: Extracts common fields and their values from business cards.</li> <li><code>Health insurance card model</code>: Extracts common fields and their values from health insurance cards.</li> </ul> <p>The other models are designed to extract values from documents with less specific structures:</p> <ul> <li><code>Read model</code>: Extracts text and languages from documents.</li> </ul> <p>The Azure AI Document Intelligence <code>read model</code> extracts printed and handwritten text from documents and images. It's used to provide text extraction in all the other prebuilt models. The read model can also detect the language that a line of text is written in and classify whether it's handwritten or printed text.    </p> <ul> <li><code>General document model</code>: Extract text, keys, values, entities and selection marks from documents.The general document model extends the functionality of the read model by adding the detection of key-value pairs, entities, selection marks, and tables. The model can extract these values from structured, semi-structured, and unstructured documents.</li> </ul> <p>The types of entities you can detect include:</p> <ul> <li>Person. The name of a person.</li> <li>PersonType. A job title or role.</li> <li>Location. Buildings, geographical features, geopolitical entities.</li> <li>Organization. Companies, government bodies, sports clubs, musical bands, and other groups.</li> <li>Event. Social gatherings, historical events, anniversaries.</li> <li>Product. Objects bought and sold.</li> <li>Skill. A capability belonging to a person.</li> <li>Address. Mailing address for a physical location.</li> <li>Phone number. Dialing codes and numbers for mobile phones and landlines.</li> <li>Email. Email addresses.</li> <li>URL. Webpage addresses.</li> <li>IP Address. Network addresses for computer hardware.</li> <li>DateTime. Calendar dates and times of day.</li> <li> <p>Quantity. Numerical measurements with their units.</p> </li> <li> <p><code>Layout model</code>: Extracts text and structure information from documents. As well as extracting text, the layout model returns selection marks and tables from the input image or PDF file. It's a good model to use when you need rich information about the structure of a document.</p> </li> </ul> <p>When you digitize a document, it can be at an odd angle. Tables can have complicated structures with or without headers, cells that span columns or rows, and incomplete columns or rows. The layout model can handle all of these difficulties to extract the complete document structure.</p>"},{"location":"AI/azure/form-recognizer/#features-of-prebuilt-models","title":"Features of prebuilt models","text":"<p>The prebuilt models are designed to extract different types of data from the documents and forms users submit. To select the right model for your requirements, you must understand these features:</p> <ol> <li><code>Text extraction</code>: All the prebuilt models extract lines of text and words from hand-written and printed text.</li> <li><code>Key-value pairs</code>: Spans of text within a document that identify a label or key and its response or value are extracted by many models as key-values pairs. For example, a typical key might be Weight and its value might be 31 kg.</li> <li><code>Entities</code>: Text that includes common, more complex data structures can be extracted as entities. Entity types include people, locations, and dates.</li> <li><code>Selection marks</code>: Spans of text that indicate a choice can be extracted by some models as selection marks. These marks include radio buttons and check boxes.</li> <li><code>Tables</code>: Many models can extract tables in scanned forms included the data contained in cells, the numbers of columns and rows, and column and row headings. Tables with merged cells are supported.</li> <li><code>Fields</code>: Models trained for a specific form type identify the values of a fixed set of fields. For example, the Invoice model includes CustomerName and InvoiceTotal fields.</li> </ol>"},{"location":"AI/azure/form-recognizer/#custom","title":"Custom","text":"<p>If the prebuilt models don't suit your purposes, you can create a custom model and train it to analyze the specific type of document users will send to your Azure AI Document Intelligence service. The general document analyzer prebuilt models can extract rich information from these forms and you might be able to use them if your requirements are to obtain general data. However, by using a custom model, trained on forms with similar structures and key-value pairs, you will obtain more predictable and standardized results from your unusual form types.</p> <p>There are two kinds of custom model:</p> <ol> <li> <p><code>Custom template models</code>: A custom template model is most appropriate when the forms you want to analyze have a consistent visual template. If you remove all the user-entered data from the forms and find that the blank forms are identical, use a custom template model. Custom template models support 9 different languages for handwritten text and a wide range of languages for printed text.</p> </li> <li> <p><code>Custom neural models</code>: A custom neural model can work across the spectrum of structured to unstructured documents. Documents like contracts with no defined structure or highly structured forms can be analyzed with a neural model. Neural models work on English with the highest accuracy and a marginal drop in accuracy for Latin based languages like German, French, Italian, Spanish, and Dutch. Try using the custom neural model first if your scenario is addressed by the model.</p> </li> </ol>"},{"location":"AI/azure/form-recognizer/#composed-models","title":"Composed models","text":"<p>A composed model is one that consists of multiple custom models. Typical scenarios where composed models help are when you don't know the submitted document type and want to classify and then analyze it. They are also useful if you have multiple variations of a form, each with a trained individual model. When a user submits a form to the composed model, Document Intelligence automatically classifies it to determine which of the custom models should be used in its analysis. </p>"},{"location":"AI/azure/nlp/","title":"NLP","text":"<p>Info</p> <p>Natural language processing (NLP) is a common AI problem in which software must be able to work with text or speech in the natural language form that a human user would write or speak.</p> <p>Within the broader area of NLP, <code>Natural Language Understanding (NLU)</code> deals with the problem of determining semantic meaning from natural language - usually by using a trained language model.</p> <p> </p> <p>In this design pattern:</p> <ul> <li>An app accepts natural language input from a user.</li> <li>A language model is used to determine semantic meaning (the user's intent).</li> <li>The app performs an appropriate action.</li> </ul>"},{"location":"AI/azure/nlp/#concepts","title":"Concepts","text":"<p>NLP Process</p> <p> </p> <ul> <li>Define labels: Understanding the data you want to classify, identify the possible labels you want to categorize into. In our video game example, our labels would be \"Action\", \"Adventure\", \"Strategy\", and so on.</li> <li>Tag data: Tag, or label, your existing data, specifying the label or labels each file falls under. Labeling data is important since it's how your model will learn how to classify future files. Best practice is to have clear differences between labels to avoid ambiguity, and provide good examples of each label for the model to learn from. For example, we'd label the game \"Quest for the Mine Brush\" as \"Adventure\", and \"Flight Trainer\" as \"Action\".</li> <li>Train model: Train your model with the labeled data. Training will teach our model what types of video game summaries should be labeled which genre.</li> <li>View model: After your model is trained, view the results of the model. Your model is scored between 0 and 1, based on the precision and recall of the data tested. Take note of which genre didn't perform well.</li> <li>Improve model: Improve your model by seeing which classifications failed to evaluate to the right label, see your label distribution, and find out what data to add to improve performance. For example, you might find your model mixes up \"Adventure\" and \"Strategy\" games. Try to find more examples of each label to add to your dataset for retraining your model.</li> <li>Deploy model: Once your model performs as desired, deploy your model to make it available via the API. Your model might be named \"GameGenres\", and once deployed can be used to classify game summaries.</li> <li>Classify text: Use your model for classifying text. </li> </ul>"},{"location":"AI/azure/nlp/#train-test-split","title":"Train test split","text":"<p>During the Train model step, there are two options for how to train your model.</p> <ul> <li> <p><code>Automatic split</code> - Azure takes all of your data, splits it into the specified percentages randomly, and applies them in training the model. This option is best when you have a larger dataset, data is naturally more consistent, or the distribution of your data extensively covers your classes.</p> </li> <li> <p><code>Manual split</code> - Manually specify which files should be in each dataset. When you submit the training job, the Azure AI Language service will tell you the split of the dataset and the distribution. This split is best used with smaller datasets to ensure the correct distribution of classes and variation in data are present to correctly train your model.</p> </li> </ul>"},{"location":"AI/azure/nlp/#utterances-intent-and-entity","title":"Utterances, Intent and Entity","text":"<ul> <li><code>Utterances</code> are the phrases that a user might enter when interacting with an application that uses your language model.</li> <li>An <code>intent</code> represents a task or action the user wants to perform, or more simply the meaning of an utterance. You create a model by defining intents and associating them with one or more utterances.</li> <li><code>Entities</code> are used to add specific context to intents. For example, you might define a TurnOnDevice intent that can be applied to multiple devices, and use entities to define the different devices.</li> </ul> Utterance Intent Entities What is the time? GetTime What time is it in London? GetTime Location (London) What's the weather forecast for Paris? GetWeather Location (Paris) Will I need an umbrella tonight? GetWeather Time (tonight) What's the forecast for Seattle tomorrow? GetWeather Location (Seattle), Time (tomorrow) Turn the light on. TurnOnDevice Device (light) Switch on the fan. TurnOnDevice Device (fan) <p>Defining the intent</p> <p>In your model, you must define the intents that you want your model to understand, so spend some time considering the domain your model must support and the kinds of actions or information that users might request. In addition to the intents that you define, every model includes a None intent that you should use to explicitly identify utterances that a user might submit, but for which there is no specific action required (for example, conversational greetings like \"hello\") or that fall outside of the scope of the domain for this model.</p> <p>After you've identified the intents your model must support, it's important to capture various different example utterances for each intent. Collect utterances that you think users will enter; including utterances meaning the same thing but that are constructed in different ways. Keep these guidelines in mind:</p> <ul> <li>Capture multiple different examples, or alternative ways of saying the same thing</li> <li>Vary the length of the utterances from short, to medium, to long</li> <li>Vary the location of the noun or subject of the utterance. Place it at the beginning, the end, or somewhere in between</li> <li>Use correct grammar and incorrect grammar in different utterances to offer good training data examples</li> <li>The precision, consistency and completeness of your labeled data are key factors to determining model performance.<ul> <li><code>Label precisely</code>: Label each entity to its right type always. Only include what you want extracted, avoid unnecessary data in your labels.</li> <li><code>Label consistently</code>: The same entity should have the same label across all the utterances.</li> <li><code>Label completely</code>: Label all the instances of the entity in all your utterances.</li> </ul> </li> </ul> <p>In some cases, a model might contain multiple intents for which utterances are likely to be similar. You can use the pattern of utterances to disambiguate the intents while minimizing the number of sample utterances.</p> <p>For example, consider the following utterances: <pre><code>- \"Turn on the kitchen light\"\n- \"Is the kitchen light on?\"\n- \"Turn off the kitchen light\"\n</code></pre></p> <p>These utterances are syntactically similar, with only a few differences in words or punctuation. However, they represent three different intents (which could be named <code>TurnOnDevice</code>, <code>GetDeviceStatus</code>, and <code>TurnOffDevice</code>). </p> <p>To correctly train your model, provide a handful of examples of each intent that specify the different formats of utterances</p> <pre><code>TurnOnDevice:\n    \"Turn on the {DeviceName}\"\n    \"Switch on the {DeviceName}\"\n    \"Turn the {DeviceName} on\"\nGetDeviceStatus:\n    \"Is the {DeviceName} on[?]\"\nTurnOffDevice:\n    \"Turn the {DeviceName} off\"\n    \"Switch off the {DeviceName}\"\n    \"Turn off the {DeviceName}\"\n</code></pre>"},{"location":"AI/azure/nlp/#classification-types","title":"Classification types","text":"<p>Custom text classification assigns labels, which in the Azure AI Language service is a class that the developer defines, to text files. For example, a video game summary might be classified as \"Adventure\", \"Strategy\", \"Action\" or \"Sports\".</p> <p>Custom text classification falls into two types of projects:</p> <ul> <li><code>Single label classification</code> - you can assign only one class to each file. Following the above example, a video game summary could only be classified as \"Adventure\" or \"Strategy\".</li> <li><code>Multiple label classification</code> - you can assign multiple classes to each file. This type of project would allow you to classify a video game summary as \"Adventure\" or \"Adventure and Strategy\".</li> </ul>"},{"location":"AI/azure/nlp/#ner","title":"NER","text":"<p>An entity is a person, place, thing, event, skill, or value. <code>Custom named entity recognition (NER)</code> is an Azure API service that looks at documents, identifies, and extracts user defined entities. These entities could be anything from names and addresses from bank statements to knowledge mining to improve search results.</p> <p>Azure AI Language provides certain built-in entity recognition, to recognize things such as a person, location, organization, or URL. Built-in NER allows you to set up the service with minimal configuration, and extract entities. To call a built-in NER, create your service and call the endpoint for that NER service like this:</p> <p>/text/analytics/v3.0/entities/recognition/general"},{"location":"AI/azure/nlp/#service-types","title":"Service Types","text":"<p>Azure AI Language service features fall into two categories: -  <code>Pre-configured features</code> -  <code>Learned features</code> require building and training a model to correctly predict appropriate labels, which is covered in upcoming units of this module.</p>"},{"location":"AI/azure/nlp/#pre-configured","title":"Pre-Configured","text":"<p>The Azure AI Language service provides certain features without any model labeling or training. Once you create your resource, you can send your data and use the returned results within your app.</p> <p>The following features are all pre-configured.</p>"},{"location":"AI/azure/nlp/#summarization","title":"Summarization","text":"<p>Summarization is available for both documents and conversations, and will summarize the text into key sentences that are predicted to encapsulate the input's meaning.</p>"},{"location":"AI/azure/nlp/#named-entity-recognition","title":"Named entity recognition","text":"<p>Named entity recognition can extract and identify entities, such as people, places, or companies, allowing your app to recognize different types of entities for improved natural language responses. For example, given the text \"The waterfront pier is my favorite Seattle attraction\", Seattle would be identified and categorized as a location.</p>"},{"location":"AI/azure/nlp/#personally-identifiable-information-pii-detection","title":"Personally identifiable information (PII) detection","text":"<p>PII detection allows you to identify, categorize, and redact information that could be considered sensitive, such as email addresses, home addresses, IP addresses, names, and protected health information. For example, if the text \"email@contoso.com\" was included in the query, the entire email address can be identified and redacted.</p>"},{"location":"AI/azure/nlp/#key-phrase-extraction","title":"Key phrase extraction","text":"<p>Key phrase extraction is a feature that quickly pulls the main concepts out of the provided text. For example, given the text \"Text Analytics is one of the features in Azure AI Services.\", the service would extract \"Azure AI Services\" and \"Text Analytics\".</p>"},{"location":"AI/azure/nlp/#sentiment-analysis","title":"Sentiment analysis","text":"<p>Sentiment analysis identifies how positive or negative a string or document is. For example, given the text \"Great hotel. Close to plenty of food and attractions we could walk to\", the service would identify that as positive with a relatively high confidence score.</p>"},{"location":"AI/azure/nlp/#language-detection","title":"Language detection","text":"<p>Language detection takes one or more documents, and identifies the language for each. For example, if the text of one of the documents was \"Bonjour\", the service would identify that as French.</p>"},{"location":"AI/azure/nlp/#deploy-model","title":"Deploy Model","text":"<p>Use the REST API</p> <p>One way to build your model is through the REST API. The pattern would be to create your project, import data, train, deploy, then use your model.</p> <p>These tasks are done <code>asynchronously</code>; you'll need to submit a request to the appropriate URI for each step, and then send another request to get the <code>status of that job</code>.</p> <p>For example, if you want to deploy a model for a conversational language understanding project, you'd submit the deployment job, and then check on the deployment job status.</p> <p>Request deployment</p> <p>Submit a POST request to the following endpoint. HTTP</p> <pre><code>{ENDPOINT}/language/authoring/analyze-conversations/projects/{PROJECT-NAME}/deployments/{DEPLOYMENT-NAME}?api-version={API-VERSION}\n</code></pre> Placeholder Value Example {ENDPOINT} The endpoint of your Azure AI Language resource https://.cognitiveservices.azure.com {PROJECT-NAME} The name for your project. This value is case-sensitive AmarsProject {DEPLOYMENT-NAME} The name for your deployment. This value is case-sensitive staging {API-VERSION} The version of the API you're calling 2023-05-01 <p>Analyze Text</p> <p>To query your model using REST, create a POST request to the appropriate URL with the appropriate body specified. For built in features such as language detection or sentiment analysis, you'll query the analyze-text endpoint.</p>"},{"location":"AI/azure/open-ai/","title":"Azure OpenAI Models","text":""},{"location":"AI/azure/open-ai/#models-available","title":"Models available","text":"<p>Azure OpenAI Service provides REST API access to OpenAI's powerful language models including the</p> <ul> <li>GPT-4</li> <li>GPT-3.5-Turbo</li> <li>Embeddings model series. </li> <li>Codex</li> <li>DALL-E</li> </ul> <p>Info</p> <p><code>Azure OpenAI Service</code> provides access to OpenAI's powerful large language models such as ChatGPT, GPT, Codex, and Embeddings models. These models enable various <code>natural language processing (NLP)</code> solutions to understand, converse, and generate content. Users can access the service through REST APIs, SDKs, and <code>Azure OpenAI Studio</code>.</p> <p>Azure OpenAI includes several types of model:</p> <ul> <li><code>GPT-4</code> models are the latest generation of generative pretrained (GPT) models that can generate natural language and code completions based on natural language prompts.</li> <li><code>GPT 3.5</code> models can generate natural language and code completions based on natural language prompts. In particular, GPT-35-turbo models are optimized for chat-based interactions and work well in most generative AI scenarios.</li> <li><code>Embeddings models</code> convert text into numeric vectors, and are useful in language analytics scenarios such as comparing text sources for similarities.</li> <li><code>DALL-E models</code> are used to generate images based on natural language prompts. Currently, DALL-E models are in preview. DALL-E models aren't listed in the Azure OpenAI Studio interface and don't need to be explicitly deployed.</li> </ul>"},{"location":"AI/azure/open-ai/#model-names","title":"Model Names","text":"<p>The model family and capability is indicated in the name of the base model, such as <code>text-davinci-003</code>, which specifies that it's a text model, with <code>davinci level</code> capability, and <code>identifier 3</code>. Details on models, capability levels, and naming conventions can be found on the AOAI Models documentation page.</p>"},{"location":"AI/azure/open-ai/#key-concepts","title":"Key concepts","text":""},{"location":"AI/azure/open-ai/#prompts-completions","title":"Prompts &amp; completions","text":"<ul> <li>A <code>prompt</code> is the text portion of a request that is sent to the deployed model's completions endpoint</li> <li>Responses are referred to as <code>completions</code>, which can come in form of text, code, or other formats. Here's an example of a simple prompt and completion:</li> </ul> <pre><code>    Prompt: \"\"\" count to 5 in a for loop \"\"\"\n\n    Completion: for i in range(1, 6): print(i)\n</code></pre>"},{"location":"AI/azure/open-ai/#tokens","title":"Tokens","text":"<p>Azure OpenAI processes text by breaking it down into tokens. Tokens can be words or just chunks of characters. For example, the word \u201chamburger\u201d gets broken up into the tokens \u201cham\u201d, \u201cbur\u201d and \u201cger\u201d, while a short and common word like \u201cpear\u201d is a single token. Many tokens start with a whitespace, for example \u201c hello\u201d and \u201c bye\u201d.</p>"},{"location":"AI/azure/open-ai/#embedding","title":"Embedding","text":"<p>An embedding is a special format of data representation that machine learning models and algorithms can easily use. The embedding is an information dense representation of the semantic meaning of a piece of text.</p> <p>Remember</p> <p>Each embedding is a vector of floating-point numbers, such that the distance between two embeddings in the vector space is correlated with semantic similarity between two inputs in the original format. For example, if two texts are similar, then their vector representations should also be similar.</p>"},{"location":"AI/azure/open-ai/#rag","title":"RAG","text":"<p>Retrieval Augmented Generation (RAG) is a pattern that works with pretrained Large Language Models (LLM) and your own data to generate responses.</p>"},{"location":"AI/azure/open-ai/#prompt-engineering","title":"Prompt Engineering","text":"<p>Prompt engineering in Azure OpenAI is a technique that involves designing prompts for natural language processing models. This process improves accuracy and relevancy in responses, optimizing the performance of the model.</p> <p>What is prompt engineering?</p> <p>Response quality from large language models (LLMs) in Azure OpenAI depends on the quality of the prompt provided. Improving prompt quality through various techniques is called prompt engineering.</p> <p>For example, if we want an OpenAI model to generate product descriptions, we can provide it with a detailed description that describes the features and benefits of the product. By providing this context, the model can generate more accurate and relevant product descriptions.</p>"},{"location":"AI/azure/open-ai/#endpoints","title":"Endpoints","text":"<p>Some endpoints we have are - Completion - Chat Completion - Embedding</p> <p>It's worth noting that ChatCompletion can also be used for non chat scenarios, where any instructions are included in the system message and user content is provided in the user role message.</p>"},{"location":"AI/azure/open-ai/#model-params","title":"Model Params","text":"<ul> <li>Temprature</li> <li>Top-p: </li> </ul> <p>In particular, temperature and top_p (top_probability) are the most likely to impact a model's response as they both control randomness in the model, but in different ways.</p>"},{"location":"AI/azure/open-ai/#primary-supporting-and-grounding-content","title":"Primary, supporting, and grounding content","text":"<p>Including content for the model to use to respond with allows it to answer with greater accuracy. This content can be thought of in two ways: primary and supporting content.</p> <p><code>Primary content</code> refers to content that is the subject of the query, such a sentence to translate or an article to summarize. This content is often included at the beginning or end of the prompt (as an instruction and differentiated by --- blocks), with instructions explaining what to do with it.</p> <p>For example, say we have a long article that we want to summarize. We could put it in a --- block in the prompt, then end with the instruction.</p> <pre><code>---\n&lt;insert full article here, as primary content&gt;\n---\n\nSummarize this article and identify three takeaways in a bulleted list\n</code></pre> <p><code>Supporting content</code> is content that may alter the response, but isn't the focus or subject of the prompt. Examples of supporting content include things like names, preferences, future date to include in the response, and so on. Providing supporting content allows the model to respond more completely, accurately, and be more likely to include the desired information.</p> <p>For example, given a very long promotional email, the model is able to extract key information. If you then add supporting content to the prompt specifying something specific you're looking for, the model can provide a more useful response. In this case the email is the primary content, with the specifics of what you're interested in as the supporting content</p> <pre><code>---\n&lt;insert full email here, as primary content&gt;\n---\n&lt;the next line is the supporting content&gt;\nTopics I'm very interested in: AI, webinar dates, submission deadlines\n\nExtract the key points from the above email, and put them in a bulleted list:\n</code></pre> <p><code>Grounding content</code> allows the model to provide reliable answers by providing content for the model to draw answer from. Grounding content could be an essay or article that you then ask questions about, a company FAQ document, or information that is more recent than the data the model was trained on. If you need more reliable and current responses, or you need to reference unpublished or specific information, grounding content is highly recommended.</p> <p>Grounding content differs from primary content as it's the source of information to answer the prompt query, instead of the content being operated on for things like summarization or translation. For example, when provided an unpublished research paper on the history of AI, it can then answer questions using that grounding content.</p> <pre><code>---\n&lt;insert unpublished paper on the history of AI here, as grounding content&gt;\n---\n\nWhere and when did the field of AI start?\n\nThis grounding data allows the model to give more accurate and informed answers that may not be part of the dataset it was trained on.\n</code></pre>"},{"location":"AI/azure/open-ai/#cues","title":"Cues","text":"<p>Cues are leading words for the model to build upon, and often help shape the response in the right direction. They often are used with instructions, but don't always. Cues are particularly helpful if prompting the model for code generation. Current Azure OpenAI models can generate some interesting code snippets, however code generation will be covered in more depth in a future module.</p> <p>For example, if you're wanting help creating a SQL query, provide instructions of what you need along with the beginning of the query:code</p> <pre><code>Write a join query to get customer names with purchases in the past 30 days between tables named orders and customer on customer ID. \n\nSELECT\n</code></pre> <p>The model response picks up where the prompt left off, continuing in SQL, even though we never asked for a specific language. </p>"},{"location":"AI/azure/open-ai/#conversation-history","title":"Conversation history","text":"<p>Along with the system message, other messages can be provided to the model to enhance the conversation. Conversation history enables the model to continue responding in a similar way (such as tone or formatting) and allow the user to reference previous content in subsequent queries. This history can be provided in two ways: from an actual chat history, or from a user defined example conversation.</p>"},{"location":"AI/azure/open-ai/#system-message","title":"System message","text":"<p>The system message is included at the beginning of a prompt and is designed to give the model instructions, perspective to answer from, or other information helpful to guide the model's response. This system message might include tone or personality, topics that shouldn't be included, or specifics (like formatting) of how to answer.</p> <p>For example, you could give it some of the following system messages:</p> <ul> <li>\"I want you to act like a command line terminal. Respond to commands exactly as cmd.exe would, in one unique code block, and nothing else.\"</li> <li>\"I want you to be a translator, from English to Spanish. Don't respond to anything I say or ask, only translate between those two languages and reply with the translated text.\"</li> <li>\"Act as a motivational speaker, freely giving out encouraging advice about goals and challenges. You should include lots of positive affirmations and suggested activities for reaching the user's end goal.\"</li> </ul>"},{"location":"AI/azure/vision/","title":"Vision","text":""},{"location":"AI/azure/vision/#services","title":"Services","text":"<p>The Azure AI Vision service is designed to help you extract information from images. It provides functionality that you can use for:</p> <ul> <li> <p><code>Description and tag generation</code> - determining an appropriate caption for an image, and identifying relevant \"tags\" that can be used as keywords to indicate its subject.</p> </li> <li> <p><code>Object detection</code> - detecting the presence and location of specific objects within the image.</p> </li> <li> <p><code>People detection</code> - detecting the presence, location, and features of people in the image.</p> </li> <li> <p><code>Image metadata, color, and type analysis</code> - determining the format and size of an image, its dominant color palette, and whether it contains clip art.</p> </li> <li> <p><code>Category identification</code> - identifying an appropriate categorization for the image, and if it contains any known landmarks.</p> </li> <li> <p><code>Background removal</code> - detecting the background in an image and output the image with the background transparent or a greyscale alpha matte image.</p> </li> <li> <p><code>Moderation rating</code> - determine if the image includes any adult or violent content.</p> </li> <li> <p><code>Optical character recognition</code> - reading text in the image.</p> </li> <li> <p><code>Smart thumbnail generation</code> - identifying the main region of interest in the image to create a smaller \"thumbnail\" version.</p> </li> </ul>"},{"location":"AI/azure/vision/#facial-service","title":"Facial Service","text":"<p>The Face service provides functionality that you can use for:</p> <ul> <li> <p><code>Face detection</code> - for each detected face, the results include an ID that identifies the face and the bounding box coordinates indicating its location in the image.     Face attribute analysis - you can return a wide range of facial attributes, including:         Head pose (pitch, roll, and yaw orientation in 3D space)         Glasses (NoGlasses, ReadingGlasses, Sunglasses, or Swimming Goggles)         Blur (low, medium, or high)         Exposure (underExposure, goodExposure, or overExposure)         Noise (visual noise in the image)         Occlusion (objects obscuring the face)</p> </li> <li> <p><code>Facial landmark location</code> - coordinates for key landmarks in relation to facial features (for example, eye corners, pupils, tip of nose, and so on)</p> </li> <li> <p><code>Face comparison</code> - you can compare faces across multiple images for similarity (to find individuals with similar facial features) and verification (to determine that a face in one image is the same person as a face in another image)</p> </li> <li><code>Facial recognition</code> - you can train a model with a collection of faces belonging to specific individuals, and use the model to identify those people in new images.</li> </ul> <p>Face GUID</p> <p>When a face is detected by the Face service, an ID is assigned to it and retained in the service resource for 24 hours. The ID is a GUID, with no indication of the individual's identity other than their facial features.</p>"},{"location":"AI/azure/vision/#implement-facial-recognition","title":"Implement facial recognition","text":"<p>To train a facial recognition model with the Face service:</p> <ul> <li>Create a Person Group that defines the set of individuals you want to identify (for example, employees).</li> <li>Add a Person to the Person Group for each individual you want to identify.</li> <li>Add detected faces from multiple images to each person, preferably in various poses. The IDs of these faces will no longer expire after 24 hours (so they're now referred to as persisted faces).</li> <li>Train the model.</li> </ul> <p> </p>"},{"location":"AI/azure/vision/#ocr","title":"OCR","text":"<p>OCR allows you to extract text from images, such as photos of street signs and products, as well as from documents \u2014 invoices, bills, financial reports, articles, and more.</p> <p>The Azure AI Vision service offers two APIs that you can use to read text.</p> <ul> <li>The Read API</li> <li>The Read API can be used to process PDF formatted files.</li> <li>Its ideal for this scenario: You need to read a large amount of text with high accuracy. Some of the text is handwritten in English and some of it is printed in multiple languages. </li> <li>There are more than <code>160 languages</code> available for printed text via the Read API.</li> <li>The Image Analysis API</li> </ul> <p>Remember</p> <p>The Read function returns an operation ID, which you can use in a subsequent call to the Get Read Results function in order to retrieve details of the text that has been read. Depending on the volume of text, you may need to poll the Get Read Results function multiple times before the operation is complete.</p>"},{"location":"AI/azure/vision/#azure-video-indexer","title":"Azure Video Indexer","text":"<p>Azure Video Indexer is a service to extract insights from video, including face identification, text recognition, object labels, scene segmentations, and more.</p> <p>Azure OpenAI Service provides access to OpenAI's powerful large language models such as ChatGPT, GPT, Codex, and Embeddings models. These models enable various natural language processing (NLP) solutions to understand, converse, and generate content. Users can access the service through REST APIs, SDKs, and Azure OpenAI Studio.</p> <p>--</p> <p>Azure OpenAI Service provides access to OpenAI's powerful large language models such as ChatGPT, GPT, Codex, and Embeddings models. These models enable various natural language processing (NLP) solutions to understand, converse, and generate content. Users can access the service through REST APIs, SDKs, and Azure OpenAI Studio.</p> <ul> <li>OpenAI Studio: It provides access to model management, deployment, experimentation, customization, and learning resources.</li> </ul> <p>Azure OpenAI includes several types of model:</p> <ul> <li><code>GPT-4</code> models are the latest generation of generative pretrained (GPT) models that can generate natural language and code completions based on natural language prompts.</li> <li><code>GPT 3.5</code> models can generate natural language and code completions based on natural language prompts. In particular, GPT-35-turbo models are optimized for chat-based interactions and work well in most generative AI scenarios.</li> <li><code>Embeddings models</code> convert text into numeric vectors, and are useful in language analytics scenarios such as comparing text sources for similarities.</li> <li><code>DALL-E models</code> are used to generate images based on natural language prompts. Currently, DALL-E models are in preview. DALL-E models aren't listed in the Azure OpenAI Studio interface and don't need to be explicitly deployed.</li> </ul> <p>Prompts and completions - A <code>prompt</code> is the text portion of a request that is sent to the deployed model's completions endpoint - Responses are referred to as <code>completions</code>, which can come in form of text, code, or other formats. Here's an example of a simple prompt and completion.</p> <p>Prompt Engineering With prompt-based models, the user interacts with the model by entering a text prompt, to which the model responds with a text completion. This completion is the model\u2019s continuation of the input text.</p>"},{"location":"Linux/grepContent/","title":"Find if content exists in some file","text":""},{"location":"Linux/grepContent/#simple-search","title":"Simple Search","text":"<p>Lets say we have to sarch if pluign exists in <code>~/.bashrc</code> file, then we can use</p> <pre><code>grep -r plugin  ~/.bashrc\n</code></pre> <p>the output is </p> output<pre><code>/Users/amar/.bashrc:# Which plugins would you like to load? (plugins can be found in ~/.oh-my-bash/plugins/*)\n/Users/amar/.bashrc:# Custom plugins may be added to ~/.oh-my-bash/custom/plugins/\n</code></pre>"},{"location":"Linux/grepContent/#recursive-search","title":"Recursive Search","text":"<pre><code>grep -R \"domain\" /etc/apache2/httpd.conf\n\n# (1)\n/etc/apache2/httpd.conf:# as error documents.  e.g. admin@your-domain.com \n</code></pre> <ol> <li>Output of the grep command</li> </ol>"},{"location":"Linux/grepContent/#finding-exact-text","title":"Finding exact text","text":"<p>Lets imagine we have a file named <code>file.txt</code> with below contents</p> File contents<pre><code>$ cat file.txt \n\nOttawa\nis\nan \nawesome\nplace\nplaces\nto\ntoo\nlive\n</code></pre> <p>Below will find all words which have <code>to</code></p> Simple search<pre><code>$ grep to file.txt \n\nto\ntoo\n</code></pre> <p>Finding the exact <code>to</code></p> Find exact text<pre><code>$grep -w to file.txt \n\nto\n</code></pre>"},{"location":"Linux/updatePaths/","title":"Update $path in Mac","text":"<p>Let's imagine we have to update the Python path in PATH variable</p> <ul> <li>Paths are available in <code>/etc/paths</code>. Update then using</li> </ul> <pre><code>sudo bash\nvim /etc/paths\n</code></pre> <ul> <li> <p>Enter the path of the Python install directory at the end of this list.</p> </li> <li> <p>Reload <code>bashrc</code> using <code>source ~/.bashrc</code></p> </li> </ul>"},{"location":"about/about/","title":"About me","text":"<p>Hi Alien, my name is Amar Dhillon \ud83d\udc4b</p> <p>On this blog I speak in my name and my name only, so, opinions stated here are my own and not my Employer\u2019s</p> <p>Dont forget to chekcout my  Instagram channel </p> <p>Feel free to reach out on  Linkedin profile here</p> <p>Checkout  Fully Simplified Youtube channel </p> <p>Incumbently, I'm working as Enterprise Software Architect: Artificial Intelligence at Air Canada </p> <p>Aspiring content creator \ud83d\udcf8  and music lover \ud83c\udfb8 \ud83c\udfb9 who  exploring new places \ud83d\udeb4\ud83c\udffb \ud83c\udfd6</p>"},{"location":"about/about/#wall-of-certs","title":"Wall of Certs","text":"<ul> <li> <p> Verify </p> </li> <li> <p> Verify </p> </li> <li> <p> Verify </p> </li> </ul> <ul> <li> <p> Verify </p> </li> <li> <p> Verify </p> </li> <li> <p> Verify </p> </li> </ul> <ul> <li> <p> Verify </p> </li> <li> <p> Verify </p> </li> <li> <p> Verify </p> </li> </ul> <ul> <li> Verify </li> </ul>"},{"location":"aws/acl/","title":"ACL","text":"<p>Real world analogy</p> <p>Imagine that you are in an airport \u2708\ufe0f. In the airport, travelers are trying to enter into a different country. You can think of the travelers as packets and the passport control officer as a network ACL. The passport control officer checks travelers\u2019 credentials when they are both entering and exiting out of the country. If a traveler is on an approved list, they are able to get through. However, if they are not on the approved list or are explicitly on a list of banned travelers, they cannot come in.</p> <ul> <li>Each AWS account includes a default network ACL. When configuring your VPC, you can use your account\u2019s default network ACL or create custom network ACLs. </li> <li>Default ACL allows all the inbound and outbound traffic.</li> <li>Custom ACL denies all inbound and outbound traffic.</li> <li>Additionally, all network ACLs have an explicit deny rule. This rule ensures that if a packet doesn\u2019t match any of the other rules on the list, the packet is denied. </li> <li>We can block IP\u2019s using ACL but not SG.</li> </ul>"},{"location":"aws/acl/#stateless-packet-filtering","title":"Stateless packet filtering","text":"<p>Network ACLs perform <code>stateless packet filtering</code>. They remember nothing and check packets that cross the subnet border each way: <code>inbound and outbound</code>. </p> <p>Recall the previous example of a traveler who wants to enter into a different country. This is similar to sending a request out from an Amazon EC2 instance and to the internet.</p> <p>When a packet response for that request comes back to the subnet, the network ACL does not remember your previous request. The network ACL checks the packet response against its list of rules to determine whether to allow or deny.</p>"},{"location":"aws/api_gw/","title":"API Gateway","text":"<p>Amazon API Gateway is an AWS service for creating, publishing, maintaining, monitoring, and securing REST, HTTP, and WebSocket APIs at any scale. This includes handling traffic management, Cross Origin Resource Sharing (CORS) support, authorization and access control, throttling, monitoring, and API version management. </p> <p>API Gateway creates RESTful APIs that:</p> <ul> <li>Are HTTP-based.</li> <li>Enable <code>stateless</code> client-server communication.</li> <li>Implement standard HTTP methods such as GET, POST, PUT, PATCH, and DELETE.</li> </ul> <p>Want to have full-duplex communication: there is a support for web-sockets too?</p> <p>API Gateway creates WebSocket APIs that:</p> <ul> <li>Adhere to the WebSocket protocol, which enables stateful, full-duplex communication between client and server.</li> <li>Route incoming messages based on message content.</li> </ul> <p>WebSocket APIs are often used in real-time applications such as chat applications, collaboration platforms, multiplayer games, and financial trading platforms.</p> <p> </p>"},{"location":"aws/api_gw/#features","title":"Features \ud83d\udccb","text":""},{"location":"aws/api_gw/#version","title":"Version \ud83e\uddee","text":"<p>With API Gateway, you can run multiple versions of the same API simultaneously so that you can quickly iterate, test, and release new versions. You can make changes to your API and host multiple versions of it for different users also.</p>"},{"location":"aws/api_gw/#transform-data","title":"Transform data \ud83d\udcca","text":"<p>With API Gateway, you can also transform and validate both incoming and outgoing requests. With this feature, you can use API Gateway as a fully managed environment for transforming requests as they come into your API before they are passed to your backend.</p>"},{"location":"aws/api_gw/#reduced-latency","title":"Reduced Latency \ud83d\udd70\ufe0f","text":"<p>API Gateway provides end users with the lowest possible latency for API requests and responses by taking advantage of the Amazon CloudFront global network of edge locations. </p>"},{"location":"aws/api_gw/#api-throtlling","title":"API Throtlling  \ud83e\udea3","text":"<p>You can configure throttling and quotas for your APIs to help protect them from being overwhelmed by too many requests. Both throttles and quotas are applied on a best-effort basis and should be thought of as targets rather than guaranteed request ceilings.</p> <p>API Gateway throttles requests to your API using the token bucket algorithm, where a token counts for a request. Specifically, API Gateway examines the rate and a burst of request submissions against all APIs in your account, per Region.</p> <p>When request submissions exceed the steady-state request rate and burst limits, API Gateway begins to throttle requests. Clients may receive <code>429 Too Many Requests error</code> responses at this point. Upon catching such exceptions, the client can resubmit the failed requests in a way that is rate limiting.</p> <p> </p> <p>Info</p> <p>In a usage plan, you can set a per-method throttling target for all methods at the API or stage level. You can specify a throttling rate, which is the rate, in requests per second, that tokens are added to the token bucket. You can also specify a throttling burst, which is the capacity of the token bucket.</p> <p> </p>"},{"location":"aws/api_gw/#custom-lambda-authorizer","title":"Custom Lambda authorizer \ud83c\udfab","text":"<p>TLDR</p> <p>A <code>Lambda authorizer</code> (formerly known as a custom authorizer) is an API Gateway feature that uses a Lambda function to control access to your API.</p> <p>A <code>Lambda authorizer</code> is useful if you want to implement a custom authorization scheme that uses a <code>bearer token authentication strategy</code> such as <code>OAuth</code> or <code>SAML</code>, or that uses request parameters to determine the caller's identity.</p> <p>When a client makes a request to one of your API's methods, API Gateway calls your Lambda authorizer, which takes the caller's identity as input and returns an IAM policy as output.</p>"},{"location":"aws/api_gw/#types-of-lambda-authorizers","title":"Types of Lambda authorizers","text":"<p>Info</p> <p>There are two types of Lambda authorizers:</p> <ul> <li> <p>A token-based Lambda authorizer (also called a TOKEN authorizer) receives the caller's identity in a bearer token, such as a JSON Web Token (JWT) or an OAuth token. For an example application, see Open Banking Brazil - Authorization Samples on GitHub.</p> </li> <li> <p>A request parameter-based Lambda authorizer (also called a REQUEST authorizer) receives the caller's identity in a combination of headers, query string parameters, stageVariables, and $context variables.</p> </li> </ul> <p>For WebSocket APIs, only request parameter-based authorizers are supported.</p> <p>Using custom authorizer</p> <p>It is possible to use an AWS Lambda function from an AWS account that is different from the one in which you created your API. For more information,</p> <p> </p>"},{"location":"aws/api_gw/#authorization-workflow","title":"Authorization workflow \u23f3","text":"<ol> <li>The client calls a method on an API Gateway API method, passing a bearer token or request parameters.</li> </ol> <p>What is Bearer token?</p> <p><code>Bearer authentication</code> (also called token authentication) is an <code>HTTP authentication</code> scheme that involves security tokens called bearer tokens. The name \u201cBearer authentication\u201d can be understood as \u201cgive access to the bearer of this token.\u201d The bearer token is a cryptic string, usually generated by the server in response to a login request. The client must send this token in the Authorization header when making requests to protected resources</p> <ol> <li><code>API Gateway</code> checks whether a <code>Lambda authorizer</code> is configured for the method. If it is, API Gateway calls the Lambda function.</li> <li> <p>The Lambda function authenticates the caller by means such as the following:</p> <ol> <li>Calling out to an <code>OAuth provider</code> to get an OAuth access token</li> <li>Calling out to a <code>SAML provider</code> to get a SAML assertion.</li> <li>Generating an <code>IAM policy</code> based on the request parameter values.</li> <li>Retrieving credentials from a database.</li> </ol> </li> <li> <p>If the call succeeds, the <code>Lambda function</code> grants access by returning an output object containing at least an <code>IAM policy</code> and a <code>principal identifier</code>.</p> </li> <li> <p>API Gateway evaluates the policy.</p> <ol> <li>If access is denied, API Gateway returns a suitable HTTP status code, such as <code>403 ACCESS_DENIED</code>.</li> <li>If access is allowed, API Gateway executes the method. If caching is enabled in the authorizer settings, API Gateway also caches the policy so that the Lambda authorizer function doesn't need to be invoked again.</li> </ol> </li> </ol>"},{"location":"aws/api_gw/#auth-using-cognito","title":"Auth using Cognito \ud83d\udea8","text":"<p> API Gw auth using Cognito <p></p>"},{"location":"aws/api_gw/#uri","title":"URI \ud83d\udd17","text":"<p> Anatomy of URI <p></p> <p>All of the APIs you create with API Gateway will follow the same pattern as you see in the invoke URL above, reflecting the <code>ID</code> of the API and the <code>Region</code> in which you created it, followed by a <code>stage</code>, and then the <code>resource</code> and <code>resource path</code> you want to expose.</p>"},{"location":"aws/api_gw/#endpoint-types","title":"Endpoint Types","text":"<p>An API endpoint type refers to the hostname of the API. The API endpoint type can be of 3 types depending on where the majority of your API traffic originates from.</p> <ol> <li>Edge-optimized</li> <li>Regional</li> <li>Private</li> </ol>"},{"location":"aws/api_gw/#edge-optimized","title":"Edge Optimized \u21f1","text":"<p>An edge-optimized API endpoint typically routes requests to the nearest CloudFront Point of Presence (POP), which could help in cases where your clients are geographically distributed.</p> <p>This is the default endpoint type for API Gateway REST APIs.</p> <p>How to prevent DDOS attack?</p> <p>AWS WAF is a web application firewall that helps protect web applications and APIs from attacks. It enables you to configure a set of rules called a web access control list (web ACL) that allow, block, or count web requests based on customizable web security rules and conditions that you define. For more information, see How AWS WAF Works.</p> <p>You can use AWS WAF to protect your API Gateway REST API from common web exploits, such as SQL injection and cross-site scripting (XSS) attacks. These could affect API availability and performance, compromise security, or consume excessive resources. For example, you can create rules to allow or block requests from specified IP address ranges, requests from CIDR blocks, requests that originate from a specific country or region, requests that contain malicious SQL code, or requests that contain malicious script. </p> <p> </p>"},{"location":"aws/api_gw/#regional","title":"Regional \ud83c\udf0f","text":"<p>A <code>Regional API endpoint</code> is intended for clients in the same region.</p> <p>Example</p> <p>When a client running on an <code>EC2 instance</code> calls an API in the same region, or when an API is intended to serve a small number of clients with high demands, a Regional API reduces connection overhead.</p>"},{"location":"aws/api_gw/#private","title":"Private \ud83d\udd10","text":"<p>A private API endpoint is an API endpoint that can only be accessed from your Amazon VPC using an interface VPC endpoint, which is an endpoint network interface (ENI) that you create in your VPC. </p>"},{"location":"aws/api_gw/#integrations","title":"Integrations \ud83c\udf10","text":""},{"location":"aws/api_gw/#http-endpoint","title":"HTTP Endpoint \ud83d\udedc","text":"<p>HTTP integration endpoints are useful for public web applications where you want clients to interact with the endpoint. This type of integration lets an API expose HTTP endpoints in the backend.</p>"},{"location":"aws/api_gw/#lambda-fn","title":"Lambda Fn \u019b","text":"<p>When you are using API Gateway as the gateway to a Lambda function, you\u2019ll use the Lambda integration. This will result in requests being proxied to Lambda with request details available to your function handler in the event parameter, supporting a streamlined integration setup. </p>"},{"location":"aws/api_gw/#aws-service","title":"AWS Service \ud83d\ude9a","text":"<p>This type of integration lets an API expose AWS service actions. In AWS integration, you must configure both the <code>integration request</code> and <code>integration response</code> and set up necessary data mappings from the method request to the integration request, and from the integration response to the method response.</p> <p>Example</p> <p>You might drop a message directly into an Amazon SQS queue.</p>"},{"location":"aws/api_gw/#mock","title":"Mock \ud83c\udccf","text":"<p>Mock lets API Gateway return a response without sending the request further to the backend. This is a good idea for a health check endpoint to test your API. Anytime you want a hardcoded response to your API call, use a <code>Mock integration</code>.</p>"},{"location":"aws/api_gw/#request-response-lifecycle","title":"Request Response Lifecycle \u267b\ufe0f","text":""},{"location":"aws/application-choice/","title":"Choosing AWS Service","text":""},{"location":"aws/application-choice/#fargate","title":"Fargate","text":"<p>Choose Fargate if </p> <ul> <li>You are doing <code>Lift and shift</code> with minimal rework</li> <li>Longer-running processes or larger deployment packages than 15 mins</li> <li>Predictable, consistent workload</li> <li>Need more than 3 GB of memory</li> <li>Application with a non-HTTP/S listener</li> <li>Run side cars with your service (agents only supported as side cars)</li> <li><code>Container image portability</code> with Docker runtime</li> </ul>"},{"location":"aws/application-choice/#lambda","title":"Lambda","text":"<ul> <li>Tasks that run less than 15 minutes</li> <li>Spiky, unpredictable workloads</li> <li>Unknown demand</li> <li>Lighter-weight, application-focused stateless computing</li> <li>Simplified IT automation</li> <li>Real-time data processing</li> <li>Reduced complexity for development and operations</li> </ul>"},{"location":"aws/application-choice/#s3","title":"S3","text":"<ul> <li>Data lakes</li> <li>Economical state store</li> <li>Claim-check pattern</li> <li>Filter data retrieved by Lambda (S3 Select)</li> </ul>"},{"location":"aws/appsync/","title":"App Sync","text":"<p>AWS AppSync provides a robust, scalable <code>GraphQL interface</code> for application developers to combine data from multiple sources, including Amazon DynamoDB Icon-Architecture/64/Arch_Amazon-DynamoDB_64Created with Sketch. , AWS Lambda  Icon-Architecture/64/Arch_AWS-Lambda_64 Created with Sketch.  , and HTTP APIs.</p>"},{"location":"aws/appsync/#features","title":"Features","text":"<ul> <li>Powerful <code>GraphQL schema</code> editing through the <code>AWS AppSync console</code>, including automatic GraphQL schema generation from DynamoDB</li> <li>Efficient data caching</li> <li>Integration with <code>Amazon Cognito</code> user pools for fine-grained access control at a per-field level</li> </ul>"},{"location":"aws/appsync/#architecture","title":"Architecture","text":""},{"location":"aws/appsync/#concepts","title":"Concepts","text":""},{"location":"aws/appsync/#graphql-proxy","title":"GraphQL Proxy","text":"<p>A component that runs the GraphQL engine for processing requests and mapping them to logical functions for data operations or triggers. The data resolution process performs a batching process (called the Data Loader) to your data sources. This component also manages conflict detection and resolution strategies.</p>"},{"location":"aws/appsync/#operation","title":"Operation","text":"<p>AWS AppSync supports the three GraphQL operations: query (<code>read-only</code> fetch), mutation (write followed by a fetch), and subscription (long-lived requests that receive data in response to events).</p>"},{"location":"aws/appsync/#action","title":"Action","text":"<p>There is one action that AWS AppSync defines. This action is a notification to connected subscribers, which is the result of a mutation. Clients become subscribers through a handshake process following a GraphQL subscription.</p>"},{"location":"aws/appsync/#data-source","title":"Data Source","text":"<p>A persistent storage system or a trigger, along with credentials for accessing that system or trigger. Your application state is managed by the system or trigger defined in a data source. Examples of data sources include NoSQL databases, relational databases, AWS Lambda functions, and HTTP APIs.</p>"},{"location":"aws/appsync/#resolver","title":"Resolver","text":"<p>A function that converts the GraphQL payload to the underlying storage system protocol and executes if the caller is authorized to invoke it. Resolvers are comprised of request and response mapping templates, which contain transformation and execution logic.</p>"},{"location":"aws/appsync/#unit-resolver","title":"Unit Resolver","text":"<p>A unit resolver is a resolver that performs a single operation against a predefined data source.</p>"},{"location":"aws/appsync/#pipeline-resolver","title":"Pipeline Resolver","text":"<p>A pipeline resolver is a resolver that allows executing multiple operations against one or more data sources. A pipeline resolver is composed of a list of functions. Each function is executed in sequence and can execute a single operation against a predefined data source.</p>"},{"location":"aws/appsync/#function","title":"Function","text":"<p>A function defines a single operation that can be used across pipeline resolvers. Functions can be reused to perform redundant logic throughout the GraphQL Proxy. Functions are comprised of a request and a response mapping template, a data source name, and a version.</p>"},{"location":"aws/appsync/#identity","title":"Identity","text":"<p>A representation of the caller based on a set of credentials, which must be sent along with every request to the GraphQL proxy. It includes permissions to invoke resolvers. Identity information is also passed as context to a resolver and the conflict handler to perform additional checks.</p>"},{"location":"aws/appsync/#aws-appsync-client","title":"AWS AppSync Client","text":"<p>The location where GraphQL operations are defined. The client performs appropriate authorization wrapping of request statements before submitting to the GraphQL proxy. Responses are persisted in an offline store and mutations are made in a write-through pattern.</p>"},{"location":"aws/artifacts/","title":"AWS Artifacts","text":"<p>AWS Artifact(opens in a new tab) is a service that provides on-demand access to AWS security and compliance reports and select online agreements. AWS Artifact consists of two main sections: AWS Artifact Agreements and AWS Artifact Reports.</p>"},{"location":"aws/artifacts/#aws-artifact-agreements","title":"AWS Artifact Agreements","text":"<p>Example</p> <p>Suppose that your company needs to sign an agreement with AWS regarding your use of certain types of information throughout AWS services. You can do this through AWS Artifact Agreements. </p> <p>In AWS Artifact Agreements, you can review, accept, and manage agreements for an individual account and for all your accounts in AWS Organizations. Different types of agreements are offered to address the needs of customers who are subject to specific regulations, such as the Health Insurance Portability and Accountability Act (HIPAA).</p>"},{"location":"aws/artifacts/#aws-artifact-reports","title":"AWS Artifact Reports","text":"<p>When to use report?</p> <p>Suppose that a member of your company\u2019s development team is building an application and needs more information about their responsibility for complying with certain regulatory standards. You can advise them to access this information in AWS Artifact Reports.</p> <p>AWS Artifact Reports provide compliance reports from third-party auditors. These auditors have tested and verified that AWS is compliant with a variety of global, regional, and industry-specific security standards and regulations. AWS Artifact Reports remains up to date with the latest reports released. You can provide the AWS audit artifacts to your auditors or regulators as evidence of AWS security controls. </p>"},{"location":"aws/athnea/","title":"Athnea","text":"<ul> <li> <p>It Interactive query service which allows analyzing the S3 database using the SQL.</p> </li> <li> <p>Athena is serverless, so there is no infrastructure to manage, and you pay only for the queries that you run.</p> </li> <li> <p>It will work with a number of data formats including \"JSON\", \"Apache Parquet\", \"Apache ORC\" amongst others, but \"XML\" is not a format that is supported.</p> </li> <li> <p>No need to set up the ETL processes</p> </li> </ul> <p>Athena is used to</p> <ol> <li>Generate Query log files in s3.</li> <li>Generate reports from s3.</li> </ol> <p> </p>"},{"location":"aws/autoScaling/","title":"Auto Scaling","text":""},{"location":"aws/autoScaling/#auto-scaling","title":"Auto Scaling","text":""},{"location":"aws/autoScaling/#auto-scaling-group","title":"Auto Scaling Group","text":"<p>When you create an <code>Auto Scaling group</code>, you can set the minimum number of Amazon EC2 instances. The minimum capacity is the number of Amazon EC2 instances that launch immediately after you have created the Auto Scaling group. In this example, the Auto Scaling group has a minimum capacity of one Amazon EC2 instance.</p> <p> Auto Scaling Example <p></p>"},{"location":"aws/autoScaling/#placement-group","title":"Placement Group","text":"<ul> <li>A placement group is concerned primarily with network throughput and reducing latency among EC2 instances within a single availability zone. AWS does support a placement group spanning multiple AZs via spread placement groups, but unless \u201cspread\u201d is specifically mentioned, you should assume the question references a \u201cnormal\u201d (or \u201ccluster\u201d) placement group.</li> <li>Cluster placement group is the default type of placement group.</li> <li>Spread placement groups can span availability zones and support up to 7 instances per zone.</li> </ul>"},{"location":"aws/aws-azure-gcp/","title":"AWS Azure and GCP comparison","text":"<p> Service Comparison on AWS, Azure and GCP <p> </p>"},{"location":"aws/aws-config/","title":"AWS Config","text":"<p>What is AWS Config?</p> <p><code>AWS Config</code> provides a detailed view of the resources associated with your AWS account, including how they are configured, how they are related to one another, and how the configurations and their relationships have changed over time.</p> <p> </p> <p>Below diagram shown the various servers which we need to configure</p> <p> </p> <p>You can use AWS Config rules to represent your <code>desired configuration settings</code> for specific AWS resources or for an entire AWS account. If a resource violates a rule, AWS Config flags the resource and the rule as noncompliant and notifies you through Amazon SNS</p> <p>AWS Config also provides the following features:</p> <ul> <li>A normalized snapshot of how your resources are configured and the ability to create rules that enforce the compliant state of those resources</li> <li>Customizable, predefined rules to help you get started, in addition to prebuilt remediation actions and the option to automatically remediate an issue</li> </ul>"},{"location":"aws/aws-config/#config-aggregators","title":"Config Aggregators \u269b\ufe0f","text":"<p>An aggregator is an AWS Config resource type that collects AWS Config configuration and compliance data from the following:</p> <ul> <li>Multiple accounts and multiple AWS Regions.</li> <li>Single account and multiple AWS Regions.</li> <li>An organization in AWS Organizations and all the accounts in that organization which have AWS Config enabled.</li> </ul> <p>Use an aggregator to view the resource configuration and compliance data recorded in AWS Config. An aggregator uses an Amazon S3 bucket to store aggregated data. It periodically retrieves configuration snapshots from the source accounts and stores them in the designated S3 bucket.</p> <p> </p>"},{"location":"aws/aws-config/#config-recorder","title":"Config Recorder \ud83c\udfa5","text":"<p>The configuration recorder stores the configurations of the supported resources in your account as configuration items. You must first create and then start the configuration recorder before you can start recording. You can stop and restart the configuration recorder at any time.</p> <p> </p>"},{"location":"aws/aws-config/#config-snapshot","title":"Config Snapshot \ud83d\udcf7","text":"<p>A configuration snapshot is a collection of the configuration items for the supported resources that exist in your account. This configuration snapshot is a complete picture of the resources that are being recorded and their configurations. The configuration snapshot can be a useful tool for validating your configuration.</p> <p> </p>"},{"location":"aws/aws-config/#config-history","title":"Config History \u231a\ufe0f","text":"<p>A configuration history is a collection of the configuration items for a given resource over any time period. A configuration history can help you answer questions about, for example, when the resource was first created, how the resource has been configured over the last month, and what configuration changes were introduced yesterday at 9 AM. </p> <p> </p> <p>Difference between Config History and Config Snapshot</p> <p> </p>"},{"location":"aws/aws-config/#config-rule","title":"Config Rule \ud83d\udc6e","text":"<p>AWS Config provides AWS managed rules, which are predefined, customizable rules that AWS Config uses to evaluate whether your AWS resources comply with common best practices. </p> <p>For example, you could use a managed rule to quickly start assessing whether your Amazon Elastic Block Store (Amazon EBS) volumes are encrypted or whether specific tags are applied to your resources.</p> <p> </p>"},{"location":"aws/aws-databases/","title":"AWS Databases","text":""},{"location":"aws/aws-databases/#rds","title":"RDS","text":"<p>Amazon Relational Database Service (Amazon RDS) is a web service that makes it easier to set up, operate, and scale a relational database in the AWS Cloud. </p> <p>It provides cost-efficient, resizable capacity for an industry-standard relational database and manages common database administration tasks.</p>"},{"location":"aws/aws-databases/#document-db","title":"Document DB","text":"<p><code>Amazon DocumentDB</code> is a document database service that supports MongoDB workloads. (MongoDB is a document database program.)</p>"},{"location":"aws/aws-databases/#neptune","title":"Neptune","text":"<p>You can use <code>Amazon Neptune</code> to build and run applications that work with highly connected datasets, such as recommendation engines, fraud detection, and knowledge graphs.</p>"},{"location":"aws/aws-databases/#amazon-managed-blockchain","title":"Amazon Managed Blockchain","text":"<p>Amazon Managed Blockchain is a service that you can use to create and manage blockchain networks with open-source frameworks. </p>"},{"location":"aws/aws-databases/#amazon-elasticache","title":"Amazon ElastiCache","text":"<p>Amazon ElastiCache is a service that adds caching layers on top of your databases to help improve the read times of common requests. </p>"},{"location":"aws/aws-databases/#amazon-dynamodb-accelerator-dax","title":"Amazon DynamoDB Accelerator (DAX)","text":"<p>Amazon DynamoDB Accelerator (DAX) is an <code>in-memory cache</code> for DynamoDB. </p>"},{"location":"aws/aws-gateways/","title":"AWS Gateways","text":"<p> AWS Gateways <p></p>"},{"location":"aws/bastionhost/","title":"Bation Host","text":"<p>An AWS <code>Bastion Host</code> is a specially configured EC2 instance that acts as an intermediary between your local machine (client) and the private instances residing within your VPC. </p> <p>It enables secure and controlled access to private instances, which are typically not directly accessible from the public internet. </p> <p>It has its own <code>security group</code> we call it as a bastion host security group, then we also have a security group for our EC2 instance in the private subnet.</p> <p>&lt;center  </p> Bastion Host Setup: bare minimum <p></p> <p><code>Bastion hosts</code> are also sometimes called jump servers because they allow a connection to \u201cjump\u201d to the bastion and then into a private subnet.</p> <p>Bastion host example</p> <p>This solution sets up the following:</p> <p> Bastion Host Example Setup </p> <ul> <li> <p>A highly available architecture that spans two Availability Zones.</p> </li> <li> <p>A virtual private cloud (VPC) configured with public and private subnets, according to AWS best practices, to provide you with your own virtual network on AWS.*</p> </li> <li> <p>In the public subnets:</p> <ul> <li> <p>Managed network address translation (NAT) gateways to allow outbound internet access for resources in the private subnets.*</p> </li> <li> <p><code>1\u20134 Linux bastion hosts</code> in an Amazon Elastic Compute Cloud (Amazon EC2) Auto Scaling group for connecting to Amazon EC2 instances and other resources deployed in public and private subnets.</p> </li> </ul> </li> <li> <p>An Amazon CloudWatch log group to hold the Linux bastion host shell history logs.</p> </li> <li> <p>AWS Systems Manager for access to the bastion host.</p> </li> </ul> <p>&lt;center   Bastion Host Setup: another example </p>"},{"location":"aws/bastionhost/#benefits","title":"Benefits \ud83d\ude00","text":"<ul> <li> <p><code>Enhanced Security</code>: Bastion hosts serve as a single entry point into your VPC, reducing the exposure of private instances to potential security threats.</p> </li> <li> <p><code>Controlled Access</code>: By setting up proper security groups and IAM roles, you can tightly control who can access your private instances via the bastion host.</p> </li> <li> <p><code>Simplified Network</code> Management: Bastion hosts streamline network management, providing a central point of access for your private instances.</p> </li> <li> <p><code>Auditing and Logging</code>: Bastion hosts can be equipped with monitoring and logging tools to track access activities and improve auditing capabilities.</p> </li> </ul>"},{"location":"aws/batch/","title":"AWS Batch","text":"<p>AWS Batch helps you to run batch computing workloads on the AWS Cloud. Batch computing is a common way for developers, scientists, and engineers to access large amounts of compute resources. </p> <p>As a fully managed service, AWS Batch helps you to run batch computing workloads of any scale. AWS Batch automatically provisions compute resources and optimizes the workload distribution based on the quantity and scale of the workloads. With AWS Batch, there's no need to install or manage batch computing software, so you can focus your time on analyzing results and solving problems.</p> <p>How to run an AWS Batch job?</p> <p>AWS Batch simplifies running batch jobs across multiple Availability Zones within a Region. You can create AWS Batch compute environments within a new or existing VPC.</p> <p>After a compute environment is up and associated with a job queue, you can define job definitions that specify which Docker container images to run your jobs.</p> <p>Container images are stored in and pulled from container registries, which may exist within or outside of your AWS infrastructure.</p>"},{"location":"aws/batch/#components-of-batch","title":"Components of Batch","text":""},{"location":"aws/batch/#jobs","title":"Jobs","text":"<p>A unit of work (such as a shell script, a Linux executable, or a Docker container image) that you submit to AWS Batch. It has a name, and runs as a containerized application on AWS Fargate or Amazon EC2 resources in your compute environment, using parameters that you specify in a job definition. Jobs can reference other jobs by name or by ID, and can be dependent on the successful completion of other jobs. For more information, see Jobs.</p>"},{"location":"aws/batch/#job-definitions","title":"Job Definitions","text":"<p>A job definition specifies how jobs are to be run. You can think of a job definition as a blueprint for the resources in your job. You can supply your job with an IAM role to provide access to other AWS resources. You also specify both memory and CPU requirements. The job definition can also control container properties, environment variables, and mount points for persistent storage. Many of the specifications in a job definition can be overridden by specifying new values when submitting individual Jobs. For more information, see Job definitions</p>"},{"location":"aws/batch/#job-queues","title":"Job Queues","text":"<p>When you submit an AWS Batch job, you submit it to a particular job queue, where the job resides until it's scheduled onto a compute environment. You associate one or more compute environments with a job queue. </p> <p>Priority in queues</p> <p>You can also assign <code>priority values</code> for these compute environments and even across job queues themselves. For example, you can have a <code>high priority queue</code> that you submit time-sensitive jobs to, and a <code>low priority queue</code> for jobs that can run anytime when compute resources are cheaper.</p>"},{"location":"aws/batch/#compute-environment","title":"Compute Environment","text":"<p>A compute environment is a set of managed or unmanaged compute resources that are used to run jobs. With managed compute environments, you can specify desired compute type (Fargate or EC2) at several levels of detail. </p> <p>You can set up compute environments that use a particular type of EC2 instance, a particular model such as <code>c5.2xlarge</code> or <code>m5.10xlarge</code></p> <p> Batch Env's <p></p>"},{"location":"aws/cert-notes/","title":"Cert Notes","text":""},{"location":"aws/cert-notes/#professional-architect","title":"Professional Architect","text":"<ul> <li> <p><code>Elastic Beanstalk</code> provides support for running Amazon RDS instances in your Elastic Beanstalk environment. This works great for development and testing environments, but is not ideal for a production environment because it ties the lifecycle of the database instance to the lifecycle of your application's environment.  If you terminate the environment, the database instance is terminated as well. An integrated database instance also cannot be removed from your environment once added.</p> </li> <li> <p>The primary goal of caching is typically to offload reads from your database or other primary data source. In most apps, you have hot spots of data that are regularly queried, but only updated periodically. Think of the front page of a blog or news site, or the top 100 leaderboard in an online game. In this type of case, your app can receive dozens, hundreds, or even thousands of requests for the same data before it's updated again. Having your caching layer handle these queries has several advantages. First, it's considerably cheaper to add an in-memory cache than to scale up to a larger database cluster. Second, an in-memory cache is also easier to scale out, because it's easier to distribute an in-memory cache horizontally than a relational database.</p> </li> <li> <p>The <code>EBS-Optimized throughput</code> limits the total IOPS that can be utilized, so using an <code>EBS-Optimized instance</code> that provides larger throughput would help increase the total random I/O performance. By using an instance type that supports higher EBS-Optimized throughput, we can  utilize the increased number of EBS volumes and achieve the desired IOPS performance.</p> </li> <li> <p>Volume gateway provides an iSCSI target, which enables you to create volumes and mount them as iSCSI devices from your on-premises application servers. The volume gateway runs in either a cached or stored mode. In either mode, you can take point-in-time snapshots of your volumes and store them in Amazon S3, enabling you to make space-efficient versioned copies of your volumes for data protection and various data reuse needs. </p> <ol> <li>In the <code>cached mode</code>, your primary data is written to S3, while you retain some portion of it locally in a cache for frequently accessed data.</li> <li>In the <code>stored mode</code>, your primary data is stored locally and your entire dataset is available for low-latency access while asynchronously backed up to AWS.</li> </ol> </li> <li> <p>Stateless instances are ideal for scalability since they don't maintain any session information. If one instance fails, users can be directed to another instance without any disruption.</p> </li> <li> <p>The AWS GovCloud (US) Region authentication is completely isolated from Amazon.com. If the organization is planning to host on EC2 in AWS GovCloud then it will be billed to standard AWS account of organization since AWS GovCloud billing is linked with the standard AWS account and is NOT billed separately.</p> </li> <li> <p>In <code>Amazon ElastiCache</code>, in-memory caching improves application performance by storing critical pieces of data in memory for low-latency access. Cached information may include the results of I/O-intensive database queries or the results of computationally intensive calculations.</p> </li> <li> <p><code>Provisioned IOPS</code> volumes are designed to meet the needs of I/O-intensive workloads, particularly database workloads that are sensitive to storage performance and consistency in random access I/O throughput. Provisioned IOPS volumes are designed to meet the needs of I/O-intensive workloads, particularly database workloads, that are sensitive to storage performance and consistency in random access I/O throughput business applications, database workloads, such as MongoDB, RDBMS</p> </li> <li> <p><code>As-describe-launch-configs</code> describes all the launch config parameters created by the AWS account in the specified region. Generally, it returns values, such as Launch Config name, Instance Type and AMI ID. If the user wants additional parameters, such as the IAM Profile used in the config, he has to run command:</p> <pre><code>as-describe-launch-configs \u2013show-long\n</code></pre> </li> <li> <p>Amazon EC2 uses an instance profile as a container for an IAM role. When you create an IAM role using the console, the console creates an instance profile automatically and gives it the same name as the role it corresponds to. If you use the AWS CLI, API, or an AWS SDK to create a role, you create the role and instance profile as separate actions, and you might give them different names.</p> </li> <li> <p>The Condition element in IAM (or Condition block) lets you specify conditions for when a policy is in effect. The Condition element is optional.</p> </li> <li> <p>Only one role can be assigned to an EC2 instance at a time, and all applications on the instance share the same role and permissions.</p> </li> <li> <p>When using string conditions within IAM, short versions of the available comparators can be used instead of the more verbose versions. For instance, <code>streqi</code> is the short version of <code>StringEqualsIgnoreCase</code> that checks for the exact match between two strings ignoring their case.</p> </li> <li> <p>Attempts, one of the three types of items associated with a schedule pipeline in AWS Data Pipeline, provides robust data management. AWS Data Pipeline retries a failed operation. It continues to do so until the task reaches the maximum number of allowed retry attempts. Attempt objects track the various attempts, results, and failure reasons if applicable. Essentially, it is the instance with a counter. AWS Data Pipeline performs retries using the same resources from the previous attempts, such as Amazon EMR clusters and EC2 instances.</p> </li> <li> <p>AWS Data Pipeline is a web service that helps you reliably process and move data between different AWS compute and storage services as well as on premise data sources at specified intervals. With AWS Data Pipeline, you can regularly access your data where it's stored, transform and process it at scale, and efficiently transfer the results to another AWS. AWS Data Pipeline helps you easily create complex data processing workloads that are fault tolerant, repeatable, and highly available. AWS Data Pipeline also allows you to move and process data that was previously locked up in on premise data silos.</p> </li> <li> <p>AWS Direct Connect itself has no specific resources for you to control access to. Therefore, there are no AWS Direct Connect ARNs for you to use in an IAM policy. You use an asterisk (*) as the resource when writing a policy to control access to AWS Direct Connect actions.</p> </li> <li> <p>A <code>task runner</code> is an application that polls AWS Data Pipeline for tasks and then performs those tasks. You can either use Task Runner as provided by <code>AWS Data Pipeline</code>, or create a custom Task Runner application.</p> </li> <li> <p>Regardless of how your Lambda function is invoked, AWS Lambda always executes the function. At the time you create a Lambda function, you specify an IAM role that AWS Lambda can assume to execute your Lambda function on your behalf. This role is also referred to as the <code>execution role</code>.</p> </li> <li> <p>Within an IAM policy, <code>IfExists</code> can be added to the end of any condition operator except the Null condition. It can be used to indicate that conditional comparison needs to happen if the policy key is present in the context of a request; otherwise, it can be ignored.</p> </li> <li> <p>A <code>service account</code> is a special Google account that can be used by applications to access Google services programmatically. This account belongs to your application or a virtual machine (VM), instead of to an individual end user. Your application uses the service account to call the Google API of a service, so that the users aren't directly involved.</p> </li> <li> <p>Currently the STS API command <code>GetSessionToken</code> is available to every IAM user in your account without previous permission. In contrast, the <code>GetFederationToken</code> command is restricted and explicit permissions need to be granted so a user can issue calls to this particular Action.</p> </li> <li> <p>Example for RDS: Each DB subnet group should have subnets in at least two Availability Zones in a given region. If the RDS instance is required to be accessible from the internet the organization must enable the VPC attributes, DNS hostnames and DNS resolution. For each RDS DB instance that the user runs in a VPC, he should reserve at least one address in each subnet in the DB subnet group for use by Amazon RDS for recovery actions.</p> </li> <li> <p>If you create a VPN connection, you must specify the type of routing that you plan to use, which will depend upon on the make and model of your VPN devices. If your VPN device supports Border Gateway Protocol (BGP), you need to specify dynamic routing when you configure your VPN connection. If your device does not support BGP, you should specify static routing.</p> </li> <li> <p>EC2 allows the user to launch On-Demand instances. If the organization is using an application temporarily only for demo purposes the best way to assign an elastic IP would be:</p> <ol> <li>Launch an instance with a VPC and assign an EIP to the primary network interface. This way on every instance start it will have the same IP</li> <li>Create a bootstrapping script and provide it some metadata, such as user data which can be used to assign an EIP</li> <li>Create a controller instance which can schedule the start and stop of the instance and provide an EIP as a parameter so that the controller instance can check the instance boot and assign an EIP The instance metadata gives the current instance data, such as the public/private IP. It can be of no use for assigning an EIP.</li> </ol> </li> <li></li> </ul>"},{"location":"aws/cloudformation/","title":"CloudFormation","text":"<p>With AWS CloudFormation, you can treat your <code>infrastructure as code</code>. This means that you can build an environment by writing lines of code instead of using the AWS Management Console to individually provision resources.</p>"},{"location":"aws/cloudformation/#cdk","title":"CDK","text":"<p>The AWS Cloud Development Kit (AWS CDK) is a software development framework for defining cloud infrastructure in code and provisioning it through CloudFormation.</p> <p>You can use the AWS CDK to define your cloud resources in a familiar programming language. The AWS CDK supports TypeScript, JavaScript, Python, Java, .NET, and Go.</p>"},{"location":"aws/cloudfront/","title":"Cloudfront","text":"<p>What is cloudfront? Icon-Architecture/64/Arch_Amazon-CloudFront_64Created with Sketch. </p> <ul> <li>It is a CDN which have 2 types of distributions<ol> <li><code>Web distribution</code>: fFor static and dynamic content, media files.</li> <li><code>RTMP</code>: It is used to speed up the distribution of streaming media files using Adobe Flash\u2019s RTMP (Real-Time Messaging Protocol). Using this the user can begin playing - the file before it\u2019s downloaded from the server.</li> </ol> </li> </ul> <ul> <li><code>Origin</code> is the actual location where the resource is (it can be S3), <code>Edge</code> is where the user.</li> <li>Edge location will cache the data for some period called TTL (<code>Time To Live</code>)</li> <li>Distribution is the name given to the CDN, which is the collection of edge locations. \ud83d\udca1</li> <li>You can clear the contents in the cache, but you will be charged for this.</li> <li>Invalidation removes the objects from the edge cache.</li> <li><code>/*</code> will invalidate everything.</li> <li>Creating and deleting distribution takes some time.</li> </ul> <p>Remember</p> <ul> <li> <p>The invalidation API is the fastest way to remove a file or object, although it will typically incur additional costs.</p> </li> <li> <p>First, remove the file from the origin servers; then set the expiration time on the CloudFront distribution to 0 to remove the file from the CloudFront </p> </li> <li> <p>==RDS instance can be an origin server--.</p> </li> <li> <p>You can read and write objects directly to an edge location. You cannot delete or update them directly; only the CloudFront service can handle that.</p> </li> <li> <p>CloudFront allows interaction via CloudFormation, the AWS CLI, the AWS console, the AWS CLI, the AWS APIs, and the various SDKs that AWS provides.</p> </li> <li> <p>CloudFront can front several AWS services: </p> </li> <li>AWS Shield</li> <li>S3</li> <li>ELBs (including ALBs) </li> <li> <p>EC2 instances.</p> </li> <li> <p>CloudFront automatically provides AWS Shield (standard) to protect from DDoS, and it also can integrate with AWS WAF and AWS Shield advanced. These combine to secure content at the edge.</p> </li> <li> <p>CloudFront is easy to set up and lets you create a global content delivery network without contracts. It\u2019s also a mechanism for distributing content at low latency.</p> </li> <li> <p>When you create a CloudFront distribution, you register a domain name for your static and dynamic content. This domain should then be used by clients.</p> </li> <li> <p>There is no charge associated with data moving from any region to a CloudFront edge location.</p> </li> <li> <p>CloudFront supports a variety of origin servers, including a non-AWS origin server. It supports EC2  regardless of region, as well. It does not support RDS or SNS.</p> </li> <li> <p>Edge locations can be set to have a <code>0-second expiration</code> period, which effectively means no caching occurs.</p> </li> <li> <p>CloudFront can distribute content from an ELB, rather than directly interfacing with S3, and can do the same with a Route 53 record set. These allow the content to come from multiple instances.</p> </li> <li> <p>CloudFront serves content from origin servers, usually static files, and dynamic responses. These origin servers are often S3 buckets for static content and EC2 instances for dynamic content.</p> </li> </ul>"},{"location":"aws/cloudtrail/","title":"Cloud Trail","text":"<p>AWS CloudTrail is an AWS service that helps you enable operational and risk auditing, governance, and compliance of your AWS account. Actions taken by a user, role, or an AWS service are recorded as events in CloudTrail.</p> <p>AWS CloudTrail records <code>API calls</code> for your account. The recorded information includes the identity of the API caller, the time of the API call, the source IP address of the API caller, and more. </p> <p> </p> <p>Events are typically updated in CloudTrail within <code>15 minutes after an API call</code>. </p>"},{"location":"aws/cloudtrail/#type-of-trails","title":"Type of Trails \ud83d\udee3\ufe0f","text":"<p>A trail is a configuration that enables delivery of CloudTrail events to an S3 bucket, with optional delivery to CloudWatch Logs and Amazon EventBridge. You can use a trail to choose the CloudTrail events you want delivered, encrypt your CloudTrail event log files with an AWS KMS key, and set up Amazon SNS notifications for log file delivery.</p> <p> </p>"},{"location":"aws/cloudtrail/#multi-region-trail","title":"Multi-Region trail \ud83c\udf0e","text":"<p>When you create a multi-Region trail, CloudTrail records events in all AWS Regions in the AWS partition in which you are working and delivers the CloudTrail event log files to an S3 bucket that you specify.</p> <p>If an AWS Region is added after you create a multi-Region trail, that new Region is automatically included, and events in that Region are logged. Creating a multi-Region trail is a recommended best practice since you capture activity in all Regions in your account. </p>"},{"location":"aws/cloudtrail/#single-region-trail","title":"Single-Region trail \ud83c\udf10","text":"<p>When you create a single-Region trail, CloudTrail records the events in <code>that Region only</code>. It then delivers the CloudTrail event log files to an <code>Amazon S3 bucket</code> that you specify. </p> <p>You can only create a single-Region trail by using the <code>AWS CLI</code>. If you create additional single trails, you can have those trails deliver CloudTrail event log files to the same S3 bucket or to separate buckets.</p>"},{"location":"aws/cloudtrail/#organization-trail","title":"Organization trail \ud83c\udfe2","text":"<p>An organization trail is a configuration that enables delivery of CloudTrail events in the <code>management account</code> and all <code>member accounts in an AWS Organizations</code> organization to the same <code>Amazon S3 bucket</code>, <code>CloudWatch Logs</code>, and <code>Amazon EventBridge</code>.</p> <p>Creating an organization trail helps you define a uniform event logging strategy for your organization. </p>"},{"location":"aws/cloudtrail/#cloudtrail-lake","title":"Cloudtrail Lake \ud83d\udc1f","text":"<p>AWS CloudTrail Lake lets you run <code>SQL-based queries</code> on your events. CloudTrail Lake converts existing events in row-based JSON format to Apache ORC format. ORC is a <code>columnar storage</code> format that is optimized for fast retrieval of data. Events are aggregated into event data stores, which are immutable collections of events based on criteria that you select by applying advanced event selectors. </p> <p>You can keep the event data in an event data store for up to 3,653 days (about 10 years) if you choose the One-year extendable retention pricing option, or up to 2,557 days (about 7 years) if you choose the Seven-year retention pricing option.</p> <p> </p>"},{"location":"aws/cloudwatch/","title":"Cloudwatch","text":"<p>Cloudwatch monitors performance. Standard monitoring is <code>5 mins</code> while detailed monitoring is <code>1 minute</code>.</p> <p>AWS services send metrics to CloudWatch. CloudWatch then uses these metrics to create graphs automatically that show how performance has changed over time. </p> <p> CloudWatch Metrics Time  <p></p> <p>Cloudwatch can monitor</p> <ul> <li>EC2</li> <li>Autoscaling groups</li> <li>Elastic Load Balancers</li> <li>Route 53 checks</li> <li>EBS</li> <li>CloudFront</li> </ul> <p> CloudWatch Components  <p></p> <p>CloudTrail and Cloudwatch difference?</p> <p>CloudTrail is different from CloudWatch as it records what <code>API calls</code> are made to AWS management console and which actions are taken.</p> <p>We can see <code>who</code> made the call, what was the <code>IP</code>, and <code>when</code> calls were made.</p>"},{"location":"aws/cloudwatch/#concepts","title":"Concepts \ud83d\udcd9","text":""},{"location":"aws/cloudwatch/#namespaces","title":"Namespaces \ud83e\ude90","text":"<p>A namespace is a container for CloudWatch metrics. Metrics in different namespaces are isolated from each other, so that metrics from different applications are not mistakenly aggregated into the same statistics.</p> <p>There is no default namespace. You must specify a namespace for each data point you publish to CloudWatch. You can specify a namespace name when you create a metric.</p>"},{"location":"aws/cloudwatch/#dimentions","title":"Dimentions \u2331","text":"<p>A dimension is a name/value pair that is part of the identity of a metric. You can assign up to 30 dimensions to a metric.</p> <p>Every metric has specific characteristics that describe it, and you can think of dimensions as categories for those characteristics. Dimensions help you design a structure for your statistics plan. Because dime</p> <pre><code>Dimensions: Server=Prod, Domain=Frankfurt, Unit: Count, Timestamp: 2016-10-31T12:30:00Z, Value: 105\nDimensions: Server=Beta, Domain=Frankfurt, Unit: Count, Timestamp: 2016-10-31T12:31:00Z, Value: 115\nDimensions: Server=Prod, Domain=Rio,       Unit: Count, Timestamp: 2016-10-31T12:32:00Z, Value: 95\nDimensions: Server=Beta, Domain=Rio,       Unit: Count, Timestamp: 2016-10-31T12:33:00Z, Value: 97\n</code></pre>"},{"location":"aws/cloudwatch/#resolution","title":"Resolution \u267e\ufe0f","text":"<p>Each metric is one of the following:</p> <ul> <li>Standard resolution: with data having a one-minute granularity</li> <li>High resolution: with data at a granularity of one second</li> </ul> <p>Metrics produced by AWS services are <code>standard resolution by default</code>. When you publish a custom metric, you can define it as either standard resolution or high resolution.</p> <p>When you publish a <code>high-resolution metric</code>, CloudWatch stores it with a resolution of 1 second, and you can read and retrieve it with a period of 1 second, 5 seconds, 10 seconds, 30 seconds, or any multiple of 60 seconds.</p>"},{"location":"aws/cloudwatch/#cloudwatch-alarms","title":"CloudWatch Alarms \ud83d\udea8","text":"<p>With CloudWatch, you can create alarms that automatically perform actions if the value of your metric has gone above or below a predefined threshold. </p> <p> CloudWatch Alarm  <p></p> <p>Save \ud83d\udcb6 by using Alarms</p> <p>For example, suppose that your company\u2019s developers use Amazon EC2 instances for application development or testing purposes. If the developers occasionally forget to stop the instances, the instances will continue to run and incur charges. </p> <p>In this scenario, you could create a CloudWatch alarm that automatically stops an Amazon EC2 instance when the CPU utilization percentage has remained below a certain threshold for a specified period. When configuring the alarm, you can specify to receive a notification whenever this alarm is triggered.</p>"},{"location":"aws/cloudwatch/#cloudwatch-logs-insights","title":"CloudWatch Logs Insights \ud83d\udcb9","text":"<p><code>CloudWatch Logs Insights</code> enables you to interactively query your log data in CloudWatch Logs. If a team wants to search and query their logs for their API, CloudWatch Logs Insights would be the best option.</p> <p> CloudWatch Log Insghts  <p></p>"},{"location":"aws/cloudwatch/#anomaly-detection","title":"Anomaly Detection \ud83d\udd75\ufe0f","text":"<p> CloudWatch Anomaly <p></p>"},{"location":"aws/cognito/","title":"Cognito","text":"<p>When to use Cognito over IAM?</p> <p><code>AWS IAM</code> focuses on managing access to AWS resources within an organization, while <code>Amazon Cognito</code> provides scalable user management for web and mobile applications with features like user sign-up, sign-in, federated identity, and single sign-on capabilities.</p>"},{"location":"aws/concepts/","title":"AWS Concepts","text":""},{"location":"aws/concepts/#availability-zone","title":"Availability Zone \ud83c\udf10","text":"<p>An Availability Zone is a <code>single data center</code> or a <code>group of data centers</code> within a Region. Availability Zones are located tens of miles apart from each other. This is close enough to have low latency (the time between when content requested and received) between Availability Zones. However, if a disaster occurs in one part of the Region, they are distant enough to reduce the chance that multiple Availability Zones are affected.</p> <p> Availability Zone Example <p></p> <p>Each AZ will always have at least one other AZ that is geographically located within the same area, usually a city, linked by highly resilient and very low latency private fiber-optic connections. However, each AZ will be isolated from the others using separate power and network connectivity that minimizes impact to other AZs should a single AZ fail.</p> <p>These low latency links between AZs are used by many AWS services to replicate data for high availability and resilience purposes. </p> <p>Example</p> <p>When RDS (Relational Database Service) is configured for \u2018Multi-AZ\u2019 deployments, AWS will use synchronous replication between its primary and secondary database and asynchronous replication for any read replicas that have been created.</p> <p>Often, there are three, four, or even five AZs linked together via these low latency connections. This localized geographical grouping of multiple AZs, which would include multiple data centers, is defined as an AWS Region</p> <p>Naming for AZ's</p> <p>Availability Zones are always referenced by their Code Name, which is defined by the AZs Region Code Name that the AZ belongs to, followed by a letter. For example, the AZs within the eu-west-1 region (EU Ireland), are:</p> <ul> <li>eu-west-1a</li> <li>eu-west-1b</li> <li>eu-west-1c</li> </ul>"},{"location":"aws/concepts/#region","title":"Region \ud83c\udf0e","text":"<p> Region Example <p></p> <p><code>Multiple AZs</code> within a region allows you to create highly available and resilient applications and services. By architecting your solutions to utilize resources across more than one AZ ensures that minimal or no impact will occur to your infrastructure should an AZ experience a failure, which does happen.</p> <p>Having global regions also allows for compliance with regulations, laws, and governance relating to data storage (at rest and in transit). For example, you may be required to keep all data within a specific location, such as Europe. Having multiple regions within this location allows an organization to meet this requirement.</p>"},{"location":"aws/concepts/#edge-locations","title":"Edge Locations \ud83d\udcf2","text":"<p>Edge Locations are AWS sites deployed in major cities and highly populated areas across the globe. They far outnumber the number of availability zones available.</p> <p>While Edge Locations are not used to deploy your main infrastructures such as EC2 instances, EBS storage, VPCs, or RDS resources like AZs, they are used by AWS services such as AWS CloudFront and AWS Lambda@Edge (currently in Preview) to cache data and reduce latency for end-user access by using the Edge Locations as a global Content Delivery Network (CDN).</p> <p>As a result, Edge Locations are primarily usedby end users who are accessing and using your services.</p> <p>Example</p> <p>You may have your website hosted on <code>EC2 instances</code> and <code>S3</code> (your origin) within the Ohio region with a configured CloudFront distribution associated. When a user accesses your website from Europe, they would be re-directed to their closest Edge Location (in Europe) where cached data could be read on your website, significantly reducing latency.</p>"},{"location":"aws/concepts/#regional-edge-cache","title":"Regional Edge Cache \ud83d\udce6","text":"<p> Regional Cache Example <p></p> <p>In November 2016, AWS announced a new type of Edge Location, called a <code>Regional Edge Cache</code>. These sit between your <code>CloudFront Origin servers</code> and the <code>Edge Locations</code>.</p> <p>A Regional Edge Cache has a larger cache-width than each of the individual Edge Locations, and because data expires from the cache at the Edge Locations, the data is retained at the Regional Edge Caches.Therefore, when data is requested at the Edge Location that is no longer available, the Edge Location can retrieve the cached data from the Regional Edge Cache instead of the Origin servers, which would have a higher latency</p>"},{"location":"aws/concepts/#local-zones","title":"Local Zones \ud83c\udf10","text":"<p>In 2022, Amazon announced that it had launched its first 16 Local Zones, a new type of infrastructure deployment designed to place core AWS Compute, Storage, Networking, and Database services near highly populated areas such as major cities that do not already have an AWS Region nearby. </p> <p>Example</p> <p>Eastern United States has two Regions: <code>us-east-1</code> in northern Virginia and <code>us-east-2</code> in Ohio. However, there are also very large metropolitan areas around Boston, New York City, Philadelphia, Atlanta, and Miami, all of which are 100 miles or more from the data centers in that Region\u2019s nearest <code>Availability Zones</code></p> <p>AWS Local Zones allow customers in these areas to deploy resources and applications that require single-digit millisecond latency that would otherwise not be attainable given the geographic distance to the nearest Regions. They are also useful where data residency requirements may dictate that data be stored within certain geographic boundaries.</p>"},{"location":"aws/concepts/#outposts","title":"Outposts \ud83d\udcec","text":"<p>AWS Outposts brings the capabilities of the AWS cloud to your <code>on-premises data center</code>. This includes the same hardware used by AWS within their data centers, allowing you to use native AWS services, including the same tools and APIs you would use when running your infrastructure within AWS.</p> <p>Outposts are available as 1U or 2U rack-mountable servers, or as entire 42U racks that can be scaled to deployments of up to 96 racks. Outposts may be connected to AWS using either a Direct Connect or VPN connection. Outposts allow you to run AWS services such as EC2, ECS, EKS, S3, RDS, and EMR on-premises. Customers can also make use of PrivateLink gateway endpoints to securely and privately connect to other services and resources, such as DynamoDB. There are a wide number of EC2 instance types available on AWS Outposts. </p> <p>These include M5, C5, and R5 instances, as well as storage options for EBS volumes, local disks, and localinstance storage.</p> <p>Because AWS Outposts are fully managed, you do not need to maintain a level of patch management across your infrastructure or worry about installing or updating any software. AWS will ensure your Outposts are patched and updated as needed.</p>"},{"location":"aws/concepts/#bastion-host","title":"Bastion Host \ud83d\udd12","text":"<p>A <code>bastion host</code> is a special-purpose computer on a network specifically designed and configured to withstand attacks. The computer generally hosts a single application, for example, a proxy server, and all other services are removed or limited to reduce the threat to the computer. It is hardened</p> <p>In AWS, a bastion host is a server whose purpose is to provide access to a private network from an external network, such as the Internet. Because of its exposure to potential attacks, a bastion host must minimize the chances of penetration.</p> <p>For users to talk to a private instance, we place a bastion host in a public subnet that is connected to the instance in a private subnet</p>"},{"location":"aws/concepts/#pre-signed-url","title":"Pre Signed URL \ud83d\udd17","text":"<p>Info</p> <p>A user who does not have AWS credentials or permission to access an S3 object can be granted temporary access by using a <code>pre-signed URL</code></p> <p>A pre-signed URL is generated by an AWS user who has access to the object. The generated URL is then given to the unauthorized user. The pre-signed URL can be entered in a browser or used by a program or HTML webpage. The credentials used by the pre-signed URL are those of the AWS user who generated the URL</p>"},{"location":"aws/concepts/#dedicated-instances","title":"Dedicated instances \ud83d\udcbe","text":"<p>Instances run on hardware that is dedicated to a host.</p>"},{"location":"aws/concepts/#dedicated-host","title":"Dedicated Host \ud83c\udfe2","text":"<p>In this case, the whole server is dedicated to a particular host. It is used in case, where a company has a security policy.</p>"},{"location":"aws/concepts/#vpc-peering","title":"VPC peering \u267e\ufe0f","text":"<p>TLDR</p> <p>A <code>VPC peering</code> connection is a networking connection between two VPCs that enables you to route traffic between them using private IPv4 addresses or IPv6 addresses. Instances in either VPC can communicate with each other as if they are within the same network.</p> <p>You can create a VPC peering connection between your own VPCs, or with a VPC in another AWS account. The VPCs can be in different regions (also known as an <code>inter-region VPC peering</code> connection). This allows VPC resources including EC2 instances, Amazon RDS databases, and Lambda functions that run in different AWS Regions to communicate with each other using private IP addresses, without requiring gateways, VPN connections, or separate network appliances. The traffic remains in the private IP space.\u00a0All inter-region traffic is encrypted with no single point of failure, or bandwidth bottleneck.\u00a0</p> <p>A VPC peering connection is a one to one relationship between two VPCs. You can create multiple VPC peering connections for each VPC that you own, but transitive peering relationships are not supported.</p>"},{"location":"aws/concepts/#nat-gateway","title":"NAT Gateway \ud83c\udf09","text":"<ul> <li>NAT gateway is a managed NAT service. </li> <li>We create <code>NAT instance</code> in a public subnet so that it can talk to the internet.</li> </ul>"},{"location":"aws/concepts/#lift-and-shift","title":"Lift and shift \ud83c\udfcb\ufe0f\u200d\u2640\ufe0f","text":"<ul> <li><code>Lift and shift</code> is a strategy for migrating a workload to the cloud without redesigning the application or making code changes. </li> <li>It is also called as <code>rehosting</code>.</li> </ul>"},{"location":"aws/concepts/#apn","title":"APN \ud83d\udedc","text":"<p>The AWS Partner Network (APN) is a global community that leverages AWS technologies, programs, expertise, and tools to build solutions and services for customers. The APN has more than <code>130,000 partners</code> from over 200 countries, with 70% headquartered outside of the United States (As of October 2023)</p> <ul> <li> <p>CSP: Cloud Service Provider like AWS, Azure</p> </li> <li> <p>MSP: Managed Service Provider like CloudReach or CloudOps</p> </li> </ul> <p>The <code>AWS Cloud Adoption Framework</code> (AWS CAF) identifies common cloud migration activities and best practices to optimize your migrations and cloud adoption outcomes. </p> <p>The AWS <code>Cloud Adoption Readiness Tool</code> (CART) helps customers develop efficient and effective plans for cloud adoption and enterprise cloud migrations</p>"},{"location":"aws/control-tower/","title":"Control Tower","text":"<p>AWS Control Tower offers the easiest way to set up and govern a secure, multi-account AWS environment. It establishes a landing zone that is based on <code>best-practices blueprints</code>, and it enables governance using controls you can choose from a pre-packaged list.</p> <p>Control Tower Features are shown below  </p> <p><code>Control Tower</code> can manage the AWS Organizations as shown below</p> <p> </p>"},{"location":"aws/control-tower/#landing-zone","title":"Landing Zone \ud83d\udeec","text":"<p>The landing zone is a well-architected, <code>multi-account baseline</code> that follows AWS best practices. Controls implement governance rules for security, compliance, and operations.</p> <p> </p> <p>Who should use Control Tower?</p> <p>If you want to create or manage your existing multi-account AWS environment with best practices, use AWS Control Tower. It offers prescriptive guidance to govern your AWS environment at scale. It gives you control over your environment without sacrificing the speed and agility AWS provides for builders.</p> <p>You will benefit if you are building a new AWS environment, starting out on your journey on AWS, starting a new cloud initiative, are completely new to AWS, or if you have an existing multi-account AWS environment but prefer a solution with built-in blueprints and controls.</p> <p>Landing Zone Structure is shown below</p> <p> </p>"},{"location":"aws/cost-management/","title":"Cost Management","text":"<p>The set of cost reporting and monitoring tools provided by AWS is collectively referred to as Billing and Cost Management. With these tools, you can view your paid and unpaid bills, manage payment methods, and monitor and analyze your costs and usage.</p> <p>The Cost Management console is integrated closely with the Billing console, and you can use both together to manage your costs in a holistic manner. You can use Billing console resources to manage ongoing payments and generate reports, while using Cost Management console resources to optimize future costs.</p>"},{"location":"aws/cost-management/#using-billing-and-cost-management","title":"Using Billing and Cost Management","text":"<ul> <li>Estimate and plan your AWS costs.</li> <li>Receive alerts if your costs exceed or approach a threshold.</li> <li>Assess your biggest investments in AWS resources.</li> <li>Streamline your accounting if you work with multiple AWS accounts.</li> </ul>"},{"location":"aws/cost-management/#cost-explorer","title":"Cost Explorer","text":"<p><code>Cost Explorer</code> is a Cost Management feature you can use to visualize and better understand your costs and usage. After activating this service, you can review historical cost data spanning the last 12 months, and Cost Explorer can use that data to forecast how much you're likely to spend for the next 12 months. You can view this data at a higher, overall level, or apply a diverse range of filters that empower you to dive deeper for detailed analysis. </p>"},{"location":"aws/cost-management/#cost-and-usage-reports","title":"Cost and Usage Reports","text":"<p>You can customize the content and delivery of your reports, and manage them from the <code>Cost and Usage Reports</code> page of the AWS Billing Dashboard. Cost and Usage Reports are published to an Amazon Simple Storage Service (Amazon S3) bucket that you own, and you can configure the report to automatically update the .csv file once each day.</p>"},{"location":"aws/cost-management/#aws-budgets","title":"AWS Budgets","text":"<p><code>AWS Budgets</code> is a Cost Management feature you can use to track and manage your AWS costs. When you create a budget, you effectively create an upper boundary you would like your costs to remain within for a configured time period. You can track cost in depth by adding filters related to AWS services, member accounts, AWS Regions, tags, and more. </p>"},{"location":"aws/cost-management/#cost-anomaly-detection","title":"Cost Anomaly Detection","text":"<p>AWS <code>Cost Anomaly Detection</code> is an AWS Cost Management feature that uses <code>machine learning</code> to continuously monitor your cost and usage to detect unusual spending. This tool can be used as another mitigating factor against receiving unexpected bills at the end of the month. </p>"},{"location":"aws/data-sync/","title":"Data Sync","text":"<p> AWS DataSync Architecture <p></p> <p>AWS DataSync is a secure, online service that automates and accelerates moving data between on premises and AWS Storage services. DataSync can copy data between Network File System (NFS) shares, Server Message Block (SMB) shares, Hadoop Distributed File Systems (HDFS), self-managed object storage, AWS Snowcone, Amazon Simple Storage Service (Amazon S3) buckets, Amazon Elastic File System (Amazon EFS) file systems, Amazon FSx for Windows File Server file systems, Amazon FSx for Lustre file systems, Amazon FSz for OpenZFS file systems, and Amazon FSx for NetApp ONTAP file systems. </p>"},{"location":"aws/dc/","title":"DC Gateway","text":"<p>AWS Direct Connect is a service that lets you to establish a dedicated private connection between your <code>data center</code> and a <code>VPC</code>.  </p> <p>AWS Direct Connect links your internal network to an AWS Direct Connect location over a standard Ethernet fiber-optic cable. </p> <p>One end of the cable is connected to your router, the other to an AWS Direct Connect router. With this connection, you can create virtual interfaces directly to public AWS services (for example, to Amazon S3) or to Amazon VPC, bypassing internet service providers in your network path. An AWS Direct Connect location provides access to AWS in the Region with which it is associated. </p> <p> DC Example <p></p> <p>Suppose that there is an apartment building with a hallway directly linking the building to the coffee shop. Only the residents of the apartment building can travel through this hallway. </p> <p>This private hallway provides the same type of dedicated connection as <code>AWS Direct Connect</code>. Residents are able to get into the coffee shop without needing to use the public road shared with other customers. </p> <p> DC Responsibility </p>"},{"location":"aws/dc/#dc-architecture","title":"DC Architecture","text":"<p>The customer then works with their communications or networking partner to make the connection to the Direct Connect port from their data center. Unlike a VPN connection, Direct Connect requires physical connectivity to a specific DX location and it could take weeks or even months to run the required cabling between the DX location and the customer data center. With that said, let's get an overview of AWS Direct Connect architecture. An AWS Direct Connect typically involves three entities. </p> <p>The DX location is usually a large regional colocation facility in which AWS rents space. Within its space, AWS has deployed some number of AWS-managed routers which are used as the endpoints of the DX service. To connect to the authorized DX port, a customer can rent space within this colocation facility to install their own routers. Or to avoid deploying equipment within this colocation facility, the customer can connect to the AWS DX port using routers provided by a DX partner. </p> <p> DC Architecture <p></p>"},{"location":"aws/dc/#vif","title":"VIF","text":"<p>With these connections, you can create virtual interfaces directly to public AWS services (for example, to Amazon Simple Storage Service (Amazon S3) or Amazon Connect) or to Amazon VPC, bypassing internet service providers in your network path. An AWS Direct Connect point-of-presence (AWS DX POP), carrier interconnection, and data center interconnection provides access to AWS in the Region with which it is associated. You can use a single connection in an AWS Region or AWS GovCloud (US) to access public AWS services in all other Regions. </p> <p> VIF Types <p></p> <ol> <li>A public virtual interface (public VIF) enables access to public services such as Amazon S3 or Amazon Connect. </li> <li>A private virtual interface (private VIF) enables access to your VPC and hosted workloads. </li> <li>A transit virtual interface (transit VIF) is used to access one or more Amazon Transit Gateways associated with Direct Connect gateways. </li> </ol> <p> VIF Reference daigram over BGP <p></p>"},{"location":"aws/dynamo/","title":"Dynamo DB","text":"<p>Amazon DynamoDB is a serverless, NoSQL, fully managed database with <code>single-digit millisecond</code> performance at any scale. </p> <p>DynamoDB is a database that supports <code>key-value</code> and <code>document-based</code> table design. It is fully-managed and provides high performance at scale. Interestingly, its performance improves with scale Icon-Architecture/64/Arch_Amazon-DynamoDB_64Created with Sketch.</p>"},{"location":"aws/dynamo/#schema","title":"Schema","text":"<p>The only schema constraint you have when inserting an item in a table is that the item should have a unique primary key. As long as we provide a unique primary key, we can insert whatever data we want to insert.</p>"},{"location":"aws/dynamo/#scaling","title":"Scaling \ud83d\udcc8","text":"<p>It works better on large scale due to <code>consistent hashing</code>.</p> <p> </p> <p>You can choose <code>on-demand</code> or <code>provisioned mode</code>, depending on how predictable your capacity needs are.</p> <ol> <li> <p>On demand: When you choose <code>on-demand mode</code>, your tables automatically scale read and write throughput based on each prior peak. On-demand capacity instantly handles up to double the previous traffic peak on a table and will then use the latest peak as the baseline from which it can instantly double capacity for the next peak. </p> <p>Warning</p> <p>If you get a new peak that is more than double the previous, DynamoDB will still give you more capacity, but your requests could get throttled if you exceed double your previous peak within 30 minutes.</p> </li> <li> <p>Provisioned: If you choose <code>provisioned mode</code> for your DynamoDB tables, you specify capacity in terms of <code>read capacity units (RCU)</code> and <code>write capacity units (WCU)</code>. Provisioned throughput is the maximum amount of capacity the table can consume. If you exceed this in a table or index, requests will get throttled. Throttled requests will return an error, and the AWS SDK has built in support for retries and exponential backoff.</p> </li> </ol> <p>You can use auto scaling with provisioned capacity to define lower and upper capacity limits and target a utilization percentage within the range. DynamoDB auto scaling will work to maintain the target utilization as the workload increases or decreases. You can set the target between 20 and 90 percent. With auto scaling, a table will increase its read and write capacity to handle sudden increases in traffic without getting throttled.</p> <p>Which one to choose?</p> <p><code>On-demand</code> is a really great fit for serverless applications because you don\u2019t have to worry about provisioning any capacity. You pay a set amount for each read and write. This simplifies evaluating the cost of a transaction, because the cost is directly reflected in the reads and writes performed by that transaction.</p> <p><code>Provisioned capacity</code> may be the better choice if you have a very consistent, predictable workload. With provisioned capacity you are paying a set rate for the amount of read and write capacity you have provisioned.</p>"},{"location":"aws/dynamo/#dax","title":"DAX \u23e9","text":"<p>If your application is really <code>read heavy</code> and requires even lower latency than DynamoDB offers, you can add <code>Amazon DynamoDB Accelerator</code>, called DAX.</p> <p>DAX is an in-memory cache. Things like real-time bidding, social gaming, and trading applications are good candidates for using DAX. </p> <p> DAX Architecture <p></p> <p>How DAX processes requests?</p> <p>A DAX cluster consists of one or more nodes. Each node runs its own instance of the DAX caching software. One of the nodes serves as the primary node for the cluster. Additional nodes (if present) serve as read replicas. For more information, see Nodes.</p> <p>Your application can access DAX by specifying the endpoint for the DAX cluster. The DAX client software works with the cluster endpoint to perform intelligent load balancing and routing. Read operations</p> <p>DAX can respond to the following API calls:</p> <ul> <li>GetItem</li> <li>BatchGetItem</li> <li>Query</li> <li>Scan</li> </ul> <p>If the request specifies eventually consistent reads (the default behavior), it tries to read the item from DAX:</p> <ul> <li> <p>If DAX has the item available (a cache hit), DAX returns the item to the application without accessing DynamoDB.</p> </li> <li> <p>If DAX does not have the item available (a cache miss), DAX passes the request through to DynamoDB. When it receives the response from DynamoDB, DAX returns the results to the application. But it also writes the results to the cache on the primary node.</p> </li> </ul>"},{"location":"aws/dynamo/#change-data-capture","title":"Change data capture \u26a1\ufe0f","text":"<p>DynamoDB supports streaming of item-level change data capture (CDC) records in near-real time. </p> <p>It offers two streaming models for CDC: 1. DynamoDB Streams  2.  Kinesis Data Streams for DynamoDB.</p> <p>Whenever an application creates, updates, or deletes items in a table, streams records a time-ordered sequence of every item-level change in near-real time. This makes DynamoDB Streams ideal for applications with event-driven architecture to consume and act upon the changes.</p>"},{"location":"aws/dynamo/#partitioningre-partitioning","title":"Partitioning/Re-partitioning \ud83c\udfac","text":"<p>AS we know that data is horizontally scaled across multiple servers in Dynamo. This means that the data needs to be partitioned. Also, servers are added and removed all the time, which means that they have to be re-partitioned frequently. You don\u2019t have to worry about that this either, since the partitioning and repartitioning processes are managed by AWS without downtime.</p> <p>Storing large files in Dynamo</p> <p>Although we can save up to 400 KB of data in one item, if we are trying to save larger files in DynamoDB, we will be at a loss in terms of cost. It would be better for us to use <code>AWS S3</code> to store the files and have links to the files in our database \ud83d\ude04</p> <p>Tip</p> <p>DynamoDb is running on SSD and have minimum of 3 instances.</p>"},{"location":"aws/dynamo/#secondary-indexes","title":"Secondary Indexes \ud83d\udcc7","text":"<p>DynamoDB offers the option to create both <code>global and local secondary indexes</code>, which let you query the table data using an alternate key. With these secondary indexes, you can access data with attributes other than the primary key, giving you maximum flexibility in accessing your data.</p>"},{"location":"aws/dynamo/#global-tables","title":"Global Tables \ud83c\udf0d","text":"<p>DynamoDB global tables enable a 99.999% availability SLA and multi-Region resilience. This helps you build resilient applications and optimize them for the lowest recovery time objective (RTO) and recovery point objective (RPO).</p> <p>Global tables also integrates with AWS Fault Injection Service (AWS FIS) to perform fault injection experiments on your global table workloads. </p> <p> Global Tables <p></p>"},{"location":"aws/dynamo/#access-patterns","title":"Access Patterns \ud83d\udd10","text":"<p>As mentioned earlier in this guide, you can choose from three access patterns to perform create, read, update, and delete (CRUD) operations on DynamoDB tables: </p> <ol> <li>Object persistence interface</li> <li>Document interfaces</li> <li>Low-level API interface </li> </ol>"},{"location":"aws/dynamo/#dynamo-data-model","title":"Dynamo Data Model \ud834\udf2d","text":""},{"location":"aws/dynamo/#items","title":"Items \ud83d\udd20","text":"<p>Items in DynamoDB are similar to the rows in relational databases. An item belongs to a table and can contain multiple attributes. An item in DynamoDB can also be represented as a JSON object (a collection of key-value pairs).</p>"},{"location":"aws/dynamo/#attributes","title":"Attributes \ud83c\udf9b\ufe0f","text":"<p>Each individual key-value pair of an item is known as an attribute. An item is built from multiple attributes. We can think of attributes as the properties of the item when we think of the item as a JSON object. Values of attributes can have many scalar and composite data types</p>"},{"location":"aws/dynamo/#primary-key","title":"Primary key \ud83d\udd11","text":"<p>Each table in DynamoDB contains a <code>primary key</code>. A primary key is a special set of attributes. Its value is unique for every item and is used to identify the item in the database. Under the hood, it is used to partition and store the data in order.</p> <p>There are two types of primary keys:</p> <ul> <li> <p><code>Partition key</code>: Here, we have a unique key of scalar type (string, number, boolean), which determines the storage partition the item will go into.</p> </li> <li> <p><code>Partition key and Sort key</code>: Here, we have two keys. The partition key determines the partition where the item goes into the storage and the sort key determines the rank of the item in the partition. Neither of these two keys need to be unique. However, their combination should be unique.</p> </li> </ul>"},{"location":"aws/dynamo/#dynamo-db-limitations","title":"Dynamo DB Limitations \u203c\ufe0f \u267e\ufe0f","text":"<p>There are a few limits you must understand to model properly in DynamoDB. If you're not aware of them, you can run into a brick wall \ud83e\uddf1. But if you understand them and account for them, you remove the element of surprise once your app hits production.</p> <p>Those limits are:</p> <ul> <li>The item size limit: </li> <li>The page size limit for Query and Scan operations: It is about a collection of items that are read together in a single request</li> <li>The partition throughput limits: It is about the number and size of concurrent requests in a single DynamoDB partition.</li> </ul>"},{"location":"aws/dynamo/#item-size-limit","title":"Item Size Limit \ud83d\udce6","text":"<p>The first important limit to know is the item size limit. An individual record in DynamoDB is called an item, and a single DynamoDB item cannot exceed 400KB.</p> <p>While 400KB is large enough for most normal database operations, it is significantly lower than the other options. MongoDB allows for <code>documents to be 16MB</code>, while Cassandra allows blobs of up to <code>2GB</code>. And if you really want to get beefy, Postgres allows rows of up to <code>1.6TB</code> (1600 columns X 1GB max per field)!</p> <p>So what accounts for this limitation? DynamoDB is pointing you toward how you should model your data in an OLTP database.</p> <p>Online transaction processing (or OLTP) systems are characterized by large amounts of small operations against a database. They describe most of how you interact with various services -- fetch a LinkedIn profile, show my Twitter timeline, or view my Gmail inbox. For these operations, you want to quickly and efficiently filter on specific fields to find the information you want, such as a username or a Tweet ID. OLTP databases often make use of indexes on certain fields to make lookups faster as well as holding recently-accessed data in RAM.</p> <p>How to store large objects in Dynamo?</p> <p>But if you have a large piece of data associated with your record, such as an image file, some user-submitted prose, or just a giant blob of JSON, it might not be best to store that directly in your database. You'll clog up the RAM and churn your disk I/O as you read and write that blob.</p> <p>Put the blob in an object store instead. Amazon S3 is a cheap, reliable way to store blobs of data. Your database record can include a pointer to the object in S3, and you can load it out when it's needed. S3 has a better pricing model for reading and writing large blobs of data, and it won't put extra strain on your database.</p>"},{"location":"aws/dynamo/#page-size-limit-for-query-scan","title":"Page size limit for Query &amp; Scan \ud83d\udda8\ufe0f","text":"<p>While the first limit we discussed involved an individual item, the second limit involves a grouping of items.</p> <p>DynamoDB has two APIs for fetching a range of items in a single request. The Query operation will fetch a range of items that have the same partition key, whereas the Scan operation will fetch a range of items from your entire table.</p> <p>For both of these operations, there is a 1MB limit on the size of an individual request. If your Query parameters match more than 1MB of data or if you issue a Scan operation on a table that's larger than 1MB, your request will return the initial matching items plus a LastEvaluatedKey property that can be used in the next request to read the next page.</p>"},{"location":"aws/dynamo/#partition-throughput-limits","title":"Partition throughput limits \ud83d\udeb0","text":"<p>A DynamoDB table isn't running on some giant supercomputer in the cloud. Rather, your data will be split across multiple partitions. Each partition contains roughly 10GB of data.</p> <p>Each item in your DynamoDB table will contain a primary key that includes a partition key. This partition key determines the partition on which that item will live. This allows for DynamoDB to provide fast, consistent performance as your application scales.</p>"},{"location":"aws/dynamo/#designing-no-sql-schema","title":"Designing No-SQL Schema \ud83c\udfa8","text":"<p>NoSQL design requires a different mindset than RDBMS design. For an RDBMS, you can go ahead and create a normalized data model without thinking about access patterns.</p> <p>In particular, it is important to understand three fundamental properties of your application's access patterns before you begin:</p> <ul> <li><code>Data size</code>: Knowing how much data will be stored and requested at one time will help determine the most effective way to partition the data.</li> <li><code>Data shape</code>: Instead of reshaping data when a query is processed (as an RDBMS system does), a NoSQL database organizes data so that its shape in the database corresponds with what will be queried. This is a key factor in increasing speed and scalability.</li> <li><code>Data velocity</code>: DynamoDB scales by increasing the number of physical partitions that are available to process queries, and by efficiently distributing data across those partitions. Knowing in advance what the peak query loads will be might help determine how to partition data to best use I/O capacity.</li> </ul>"},{"location":"aws/dynamo/#no-sql-best-practices","title":"No-SQL best practices \ud83d\uddd2\ufe0f","text":""},{"location":"aws/dynamo/#keep-related-data-together","title":"Keep related data together","text":"<p>The single most important factor in speeding up response time: keeping related data together in one place. </p> <p>Tip</p> <p>As a general rule, you should maintain as few tables as possible in a DynamoDB application.Exceptions are cases where high-volume time series data are involved, or datasets that have very different access patterns. A single table with inverted indexes can usually enable simple queries to create and retrieve the complex hierarchical data structures required by your application.</p>"},{"location":"aws/dynamo/#use-sort-order","title":"Use sort order \u29e1","text":"<p>Related items can be grouped together and queried efficiently if their key design causes them to sort together. </p>"},{"location":"aws/dynamo/#distribute-queries","title":"Distribute queries \ud834\udf2d","text":"<p>It is also important that a high volume of queries not be focused on one part of the database, where they can exceed I/O capacity. Instead, you should design data keys to distribute traffic evenly across partitions as much as possible, avoiding \"hot spots.\"</p>"},{"location":"aws/dynamo/#use-global-secondary-indexes","title":"Use global secondary indexes \ud83c\udf10","text":"<p>By creating specific global secondary indexes, you can enable different queries than your main table can support, and that are still fast and relatively inexpensive.</p>"},{"location":"aws/ebs/","title":"EBS Icon-Architecture/64/Arch_Amazon-Elastic-Block-Store_64Created with Sketch.","text":"<p>What is block storage?</p> <p>When you're using Amazon EC2 to run your business applications, those applications need access to CPU, memory, network, and storage. EC2 instances give you access to all those different components, and right now, let's focus on the storage access. As applications run, they will oftentimes need access to block-level storage. </p> <p>You can think of block-level storage as a place to store files. A file being a series of bytes that are stored in blocks on disc. When a file is updated, the whole series of blocks aren't all overwritten. Instead, it updates just the pieces that change. This makes it an efficient storage type when working with applications like databases, enterprise software, or file systems. </p> <p>When you use your laptop or personal computer, you are accessing block-level storage. All block-level storage is, in this case, is your hard drive. EC2 instances have hard drives as well. And there are a few different types.</p> <p> </p> <p>When you launch an EC2 instance, depending on the type of the EC2 instance you launched, it might provide you with local storage called instance store volumes. These volumes are physically attached to the host, your EC2 instances running on top of. And you can write to it just like a normal hard drive. The catch here is that since this volume is attached to the underlying physical host, if you stop or terminate your EC2 instance, all data written to the instance store volume will be deleted. The reason for this, is that if you start your instance from a stop state, it's likely that EC2 instance will start up on another host. A host where that volume does not exist. Remember EC2 instances are virtual machines, and therefore the underlying host can change between stopping and starting an instance. </p> <ul> <li>We can think them of to be like <code>virtual hard drives</code></li> <li>They can only be used with EC2.</li> <li>Can only be tied to single AZ</li> <li>Officially, instances can have up to 28 EBS attachments. One of those attachments is the network interface attachment, leaving 27 attachments available for EBS volumes. However, the better approach is to remember that an instance can attach to a root volume and several more volumes (more than two).</li> <li><code>EBS volumes</code> store data within a single Availability Zone. <code>Amazon EFS</code> file systems store data across multiple Availability Zones.</li> </ul> <p>As shown in the image below, the EC2 instance storage is locked to EC2 instance while EBS are not locked to an EC2. We can also create snapshot from EBS</p> <p> </p> <p>!!! question 'Why do we need snapshots?'     - Cost effective backup stragegy     - To share data-sets with others users/accounts     - Migrate a system to new AZ/Region     - They are used to convert unencrypted volume to an encrypted one</p> <p>Each EBS is replicated in it\u2019s AZ by default.</p>"},{"location":"aws/ebs/#encryption","title":"Encryption","text":"<ul> <li>EBS volumes can be encrypted when they are created.</li> <li>You cannot encrypt an existing EBS volume \u201con the fly.\u201d You must create a snapshot and then encrypt that snapshot as you copy it to another, encrypted snapshot. You can then restore from that new snapshot.</li> </ul> <p>What all is encrypted in EBS?</p> <p>There are four types of data encrypted when an EBS volume is encrypted: </p> <pre><code>- Data at rest on the volume.\n- Data moving between the volume and the instance.\n- Any snapshots created from the volume.\n- Any volumes created from those snapshots.\n</code></pre> <p>Points to remember</p> <ul> <li>You must make a copy of an unencrypted snapshot to apply encryption.</li> <li>You cannot encrypt an existing EBS volume.</li> <li>You cannot encrypt a snapshot that is unencrypted.</li> <li>You can encrypt a copy of a snapshot and restore an encrypted snapshot to a volume that is encrypted.</li> </ul>"},{"location":"aws/ebs/#different-types-of-ebs","title":"Different types of EBS","text":"<ul> <li>General-purpose SSD</li> <li>Provisioned IOPS SSD</li> <li>Throughput optimized HDD</li> <li>Cold HDD: Cold HDD is the lowest cost HDD</li> <li>EBS magnetic HDD</li> </ul>"},{"location":"aws/ebs/#ebs-backupsnapshot","title":"EBS Backup/Snapshot","text":"<p>Take backup of EBS</p> <p>Since the use case for EBS volumes is to have a hard drive that is persistent, that your applications can write to, it's probably important that you back that data up. EBS allows you to take <code>incremental backups</code> of your data called snapshots. It's very important that you take regular snapshots of your EBS volumes This way, if a drive ever becomes corrupted, you haven't lost your data. And you can restore that data from a snapshot.</p> <p>An EBS snapshot is an <code>incremental backup</code>. This means that the first backup taken of a volume copies all the data. For subsequent backups, only the blocks of data that have changed since the most recent snapshot are saved. </p>"},{"location":"aws/ebs/#move-the-ebs-from-one-az-to-another","title":"Move the EBS from one AZ to another","text":""},{"location":"aws/ebs/#option-a","title":"Option A","text":"<ul> <li>creates a snapshot from a volume.</li> <li>create an AMI from a snapshot.</li> </ul>"},{"location":"aws/ebs/#option-b","title":"Option B","text":"<ul> <li>Copy the AMI to a different region. Then go to that region and choose the AZ AZ\u2019s can be changed when we choose from the subnets</li> </ul>"},{"location":"aws/ebs/#types-of-root-volumes","title":"Types of root volumes","text":"<p>2 types of root volumes are there:</p> <ol> <li><code>Instance storage</code> : They are called <code>Ephemeral storage</code> as we are going to lose all the data if it\u2019s stopped. They can not be stopped.</li> <li><code>EBS backed</code>: they can be stopped, they are persisted as well.</li> </ol> <p>Remember</p> <p>By default, root volumes do get deleted when the associated instance terminates. However, you can configure this to not be the case.</p> <ul> <li>We can create AMI from both snapshots and volumes.</li> <li>For an EBS volume that is used as a root volume, if you want to take a consistent - snapshot then stop the instance as it is writing the data to EBS.</li> <li>ENI (Elastic Network Interface):</li> <li>EN (Enhanced Networking): This has a high speed of 100 Gbps and is used for high - performance. It uses the ENA (Enhanced network adapter).</li> <li>EFA (Elastic Fiber Adapter): use for HPC and ML.</li> </ul> <p>Warning</p> <ul> <li>If we close the instance, then the root volume is deleted but the persisted data - remains.</li> <li>Volumes exist on EBS whereas the snapshots exist on S3</li> <li>Snapshot is a delta (incremental). Taking the first snapshot may take a lot of time.</li> <li>We can change the EBS volume size on the fly.</li> <li>Volumes are always in the same AZ as the instance is.</li> <li>We can create AMI from snapshots.</li> <li>Move EC2 to at new AZ</li> <li>take a snapshot \u2192 to create AMI \u2192 use AMI to launch in new AZ</li> <li>Move EC2 to a new region</li> <li>Take a snapshot \u2192 to create AMI \u2192 copy AMI to new region \u2192  use AMI to launch in new - AZ</li> </ul>"},{"location":"aws/ec2/","title":"EC2","text":""},{"location":"aws/ec2/#ami","title":"AMI","text":"<p> AMI Example <p></p>"},{"location":"aws/ec2/#instance-types-for-ec2","title":"Instance Types for EC2","text":""},{"location":"aws/ec2/#on-demand","title":"On Demand \ud83d\udcc8","text":"<p>On-Demand Instances are ideal for short-term, irregular workloads that cannot be interrupted. No upfront costs or minimum contracts apply. The instances run continuously until you stop them, and you pay for only the compute time you use.</p>"},{"location":"aws/ec2/#reserved-instances","title":"Reserved Instances \ud83d\udd16","text":"<p>Reserved Instances are a billing discount applied to the use of On-Demand Instances in your account. There are two available types of Reserved Instances:</p> <ul> <li>Standard Reserved Instances</li> <li>Convertible Reserved Instances</li> </ul>"},{"location":"aws/ec2/#savings-plan","title":"Savings Plan \ud83d\udcb0","text":"<p>EC2 Instance Savings Plans reduce your EC2 instance costs when you make an hourly spend commitment to an instance family and Region for a 1-year or 3-year term. This term commitment results in savings of up to 72 percent compared to On-Demand rates. Any usage up to the commitment is charged at the discounted Savings Plans rate (for example, $10 per hour). Any usage beyond the commitment is charged at regular On-Demand rates.</p> <p>How are Savings Plan different from Reserved Instances Icon-Architecture/64/Arch_Amazon-EC2_64Created with Sketch. ?</p> <p>Unlike Reserved Instances, however, you don't need to specify up front what EC2 instance type and size (for example, m5.xlarge), OS, and tenancy to get a discount. Further, you don't need to commit to a certain number of EC2 instances over a 1-year or 3-year term. Additionally, the EC2 Instance Savings Plans don't include an EC2 capacity reservation option.</p>"},{"location":"aws/ec2/#spot-instances","title":"Spot Instances \ud83d\udc1e","text":"<p><code>Spot Instances</code> are ideal for workloads with flexible start and end times, or that can withstand interruptions. Spot Instances use unused Amazon EC2 computing capacity and offer you cost savings at up to 90% off of On-Demand prices.</p> <p>Example</p> <p>If you make a Spot request and Amazon EC2 capacity is available, your Spot Instance launches. However, if you make a Spot request and Amazon EC2 capacity is unavailable, the request is not successful until capacity becomes available. The unavailable capacity might delay the launch of your background processing job.</p>"},{"location":"aws/ec2/#tenancy","title":"Tenancy \ud83c\udfe2","text":""},{"location":"aws/ec2/#shared-tenancy","title":"Shared Tenancy","text":"<p>By default, EC2 instances run on <code>shared tenancy</code> hardware. This means that multiple AWS accounts might share the same physical hardware.</p>"},{"location":"aws/ec2/#dedicated-instance","title":"Dedicated Instance \ud83c\udfd7\ufe0f","text":"<p>Dedicated Instances are EC2 instances that run on hardware that's dedicated to a single AWS account. This means that Dedicated Instances are physically isolated at the host hardware level from instances that belong to other AWS accounts, even if those accounts are linked to a single payer account. However, Dedicated Instances might share hardware with other instances from the same AWS account that are not Dedicated Instances.</p>"},{"location":"aws/ec2/#dedicated-hosts","title":"Dedicated Hosts \ud83c\udfe0","text":"<p><code>Dedicated Instances</code> provide no visibility or control over instance placement, and they do not support host affinity. If you stop and start a Dedicated Instance, it might not run on the same host. Similarly, you cannot target a specific host on which to launch or run an instance. Additionally, <code>Dedicated Instances</code> provide limited support for Bring Your Own License (BYOL).</p> <p>When to use Dedicated Host?</p> <p>If you require visibility and control over instance placement and more comprehensive BYOL support, consider using a <code>Dedicated Host</code> instead.</p>"},{"location":"aws/ec2/#boot-volumes","title":"Boot volumes \ud83e\udd7e","text":"<p>This is easiest to remember by noting that HDD types are not available to use as boot volumes. General SSD and Provisioned IOPS are.</p>"},{"location":"aws/ec2/#user-data","title":"User Data \ud83d\udc71","text":"<p>When you launch an Amazon EC2 instance, you can pass user data to the instance that is used to perform automated configuration tasks, or to run scripts after the instance starts.</p> <p>How user data is handled in Linux and Windows?</p> <p>On Linux instances, you can pass two types of user data to Amazon EC2: </p> <ul> <li>shell scripts </li> <li>cloud-init directives</li> </ul> <p>You can also pass this data into the launch instance wizard as <code>plain text</code>, as a <code>file</code> (this is useful for launching instances with the command line tools), or as <code>base64-encoded text</code> (for API calls).</p> <p>On Windows instances, the launch agents handle your user data scripts. </p>"},{"location":"aws/ec2/#ec2-placement-groups","title":"EC2 Placement groups \ud83c\udfa1","text":"<p>When you launch a new EC2 instance, the EC2 service attempts to place the instance in such a way that all of your instances are spread out across underlying hardware to minimize correlated failures. You can use placement groups to influence the placement of a group of interdependent instances to meet the needs of your workload.</p> <p>Depending on the type of workload, you can create a placement group using one of the following placement strategies:</p> <p><code>Cluster</code>: It means placing the instances in the same rack and AZ so that there is low latency and high throughput. It packs instances close together inside an Availability Zone. This strategy enables workloads to achieve the low-latency network performance necessary for tightly-coupled node-to-node communication that is typical of HPC applications.</p> <p><code>Spread</code>: A spread placement group is a group of instances that are each placed on distinct racks, with each rack having its own network and power source. The following image shows seven instances in a single Availability Zone that are placed into a spread placement group. The seven instances are placed on seven different racks. Spread placement groups are recommended for applications that have a small number of critical instances that should be kept separate from each other.</p> <p><code>Partition</code>: Same as Spread but with a difference that in this, we can have multiple instances in the same partition. In this case, the different racks have a different power source. Use case: HDFS, Cassandra, etc.</p> <p>Partition placement groups help reduce the likelihood of correlated hardware failures for your application. When using partition placement groups, Amazon EC2 divides each group into logical segments called partitions. Amazon EC2 ensures that each partition within a placement group has its own set of racks. Each rack has its own network and power source. No two partitions within a placement group share the same racks, allowing you to isolate the impact of a hardware failure within your application.</p> <p>The following image is a simple visual representation of a partition placement group in a single Availability Zone. It shows instances that are placed into a partition placement group with three partitions\u2014Partition 1, Partition 2, and Partition 3. Each partition comprises multiple instances. The instances in a partition do not share racks with the instances in the other partitions, allowing you to contain the impact of a single hardware failure to only the associated partition.</p>"},{"location":"aws/ec2/#storage-options","title":"Storage Options \ud83d\udce6","text":""},{"location":"aws/ec2/#persistent","title":"Persistent","text":"<p>For data that you want to retain longer, or if you want to encrypt the data, use Amazon EBS volumes instead. EBS volumes have the following features:</p> <ul> <li>EBS volumes preserve their data through instance stops and terminations</li> <li>You can back up EBS volumes with <code>EBS snapshots</code></li> <li>You can remove EBS volumes from one instance, and reattach them to another</li> <li>EBS volumes support <code>full-volume encryption</code></li> </ul>"},{"location":"aws/ec2/#emphemralinstance-backed","title":"Emphemral/Instance-backed","text":"<p>Some Amazon EC2 instance types come with a form of directly attached, block-device storage known as an instance store. Use the instance store for temporary storage.</p> <p>Data will be lost</p> <p>Data that's stored in instance store volumes isn't persistent through instance stops, terminations, or hardware failures.</p> <p>Data will remain if instance reboots</p>"},{"location":"aws/ec2/#security-group","title":"Security Group \ud83d\udd10","text":"<p>What is SG?</p> <p>Its an instance level firewall.</p> <ul> <li> <p>Security groups only contain allow rules, not deny rules.</p> </li> <li> <p>A security group can actually have no inbound or outbound rules.</p> </li> <li> <p>A security group does require a name and description, though.</p> </li> <li> <p>A security group can be attached to multiple constructs, like an EC2 instance, - but is ultimately associated with a network interface, which in turn is - attached to individual instances.</p> </li> </ul> <p>By default SG has      - Outbound   <code>0.0.0.0/0</code> for all protocols allowed for IPV4     - Outbound <code>::/0</code> for all protocols allowed for IPV6</p> <ul> <li> <p>Security group rules have a protocol and a description. They do not have a - subnet, although they can have CIDR blocks or single IP addresses. Instances - can associate with a security group, but a security group does not itself refer - to a specific instance.</p> </li> <li> <p>Default security groups prevent all incoming traffic and allow all traffic out - whereas the new SG allows all the outgoing traffic and blocks all the incoming traffic.</p> </li> </ul>"},{"location":"aws/ec2/#auto-scaling","title":"Auto Scaling \ud83d\udcc8","text":"<p>Types of Auto Scaling</p> <p>There are a number of valid scaling policies for Auto Scaling:</p> <ul> <li><code>Maintain current instance levels</code>: used to ensure that a specific number of instances is running at all times.</li> <li><code>Manual scaling</code>: Manual scaling allows you to specify a minimum and a maximum number of instances as well as the desired capacity. The Auto Scaling policy then handles maintaining that capacity.</li> <li><code>Schedule-based scaling</code>: </li> <li><code>Demand-based scaling</code>: Demand-based scaling allows you to specify parameters to control scaling. One of those parameters can be CPU utilization</li> </ul>"},{"location":"aws/ec2/#encryption-settings","title":"Encryption Settings \ud83e\uddde\u200d\u2642\ufe0f","text":"<ul> <li> <p>Client-side encryption involves the client (you, in this example) managing the - entire encryption and decryption process. AWS only provides storage.</p> </li> <li> <p>SSE-S3, SSE-KMS, and SSE-C are all valid approaches to server-side S3 - encryption.</p> </li> <li> <p>SSE-KMS provides a very good audit trail and security.</p> </li> <li> <p>SSE-S3 requires that Amazon S3 manage the data and master encryption keys while - SSE-KMS requires that AWS manage the data key but you manage the customer - master key (CMK) in AWS KMS.</p> </li> </ul>"},{"location":"aws/ec2/#status-checks","title":"Status Checks \u2611\ufe0f","text":"<p>Amazon EC2 performs automated checks on every running EC2 instance to identify hardware and software issues. You can view the results of these status checks to identify specific and detectable problems.</p> <p>We can't disable these checks</p> <p>Status checks are performed <code>every minute</code>, returning a <code>pass</code> or a <code>fail</code> status. </p> <p>If all checks pass, the overall status of the instance is <code>OK</code></p> <p>If one or more checks fail, the overall status is <code>impaired</code> </p> <p>Status checks are built into Amazon EC2, so they cannot be disabled or deleted.</p> <p>There are three types of status checks</p>"},{"location":"aws/ec2/#system-check","title":"System Check \u2705","text":"<p>Its something on AWS end</p>"},{"location":"aws/ec2/#instance-check","title":"Instance Check","text":"<p>Its something on our end and will require your input</p>"},{"location":"aws/ec2/#attached-ebs-check","title":"Attached EBS Check","text":"<p>Attached EBS status checks monitor if the Amazon EBS volumes attached to an instance are reachable and able to complete I/O operations. The StatusCheckFailed_AttachedEBS metric is a binary value that indicates impairment if one or more of the EBS volumes attached to the instance are unable to complete I/O operations. </p>"},{"location":"aws/ec2/#notes","title":"Notes \ud83d\udcdd","text":"<ul> <li>EC2 instances, as well as <code>ECS containers</code>, can both be scaled up and down by <code>Auto Scaling</code></li> <li> <p>A launch configuration contains an <code>AMI ID</code>, <code>key pair</code>, <code>instance type</code>, <code>security groups</code>, and possibly a <code>block device mapping</code>.</p> </li> <li> <p>InService and Standby are valid states for an instance in an auto-scaling group.</p> </li> <li>You have to create a launch configuration first, then an Auto Scaling group, - and then you can verify your configuration and group.   </li> <li>A <code>launch configuration</code> needs a single AMI ID to use for all instances it - launches.</li> <li>It is generally better to allow AWS to handle encryption in cases where you - want to ensure all encryption is the same across a data store.</li> <li>A bastion host is a publicly accessible host that allows traffic to connect to - it. Then, an additional connection is made from the bastion host into a private - subnet and the hosts within that subnet.</li> <li>The security of the bastion must be different from the hosts in the private - subnet. The bastion host should be hardened significantly as it is public, but - also accessible; this is in many ways the opposite of the security requirements - of hosts within a private subnet.</li> <li>For private subnet instances, you need a route out to a NAT gateway, and that - NAT gateway must be in a public subnet\u2014otherwise, it would not itself be able - to provide outbound traffic access to the Internet.</li> <li>NAT gateway is essentially a managed service and a NAT instance as an instance (- which you manage) for networking.</li> <li>all custom NACLs disallow all inbound and outbound traffic. It is only a VPC\u2019s - default NACL that has an \u201callow all\u201d policy.</li> <li>An instance has a primary network interface in all cases but can have - additional network interfaces attached.</li> <li>You can only assign a single role to an instance.</li> <li>By default, root volumes are terminated on instance deletion, and by default, additional EBS volumes attached to an instance are not.</li> </ul>"},{"location":"aws/ecr/","title":"ECR","text":"<p><code>Amazon Elastic Container Registry</code> (Amazon ECR) is a <code>fully managed</code> container registry offering high-performance hosting, so you can reliably deploy application images and <code>artifacts anywhere</code>.</p> <p>Amazon ECR supports private repositories with resource-based permissions using AWS IAM. This is so that specified users or Amazon EC2 instances can access your container repositories and images. </p> <p>You can use your preferred CLI to push, pull, and manage <code>Docker images</code>, <code>Open Container Initiative (OCI) images</code>, and OCI compatible artifacts.</p>"},{"location":"aws/ecr/#components-of-ecr","title":"Components of ECR","text":""},{"location":"aws/ecr/#registry","title":"Registry","text":"<p>An Amazon ECR private registry <code>hosts</code> your container images in a highly available and scalable architecture. You can use your private registry to manage private image repositories consisting of Docker and Open Container Initiative (OCI) images and artifacts. Each AWS account is provided with a default private Amazon ECR registry. </p>"},{"location":"aws/ecr/#repository","title":"Repository","text":"<p>An Amazon ECR repository contains your Docker images, Open Container Initiative (OCI) images, and OCI compatible artifacts. For more information, see Amazon ECR private repositories.</p>"},{"location":"aws/ecr/#authorization-token","title":"Authorization token","text":"<p>Your client must authenticate to an Amazon ECR private registry as an AWS user before it can push and pull images. For more information, see Private registry authentication in Amazon ECR.</p>"},{"location":"aws/ecr/#repository-policy","title":"Repository policy","text":"<p>You can control access to your repositories and the contents within them with  - <code>Repository Policy</code> - <code>IAM Policy</code></p>"},{"location":"aws/ecr/#image","title":"Image","text":"<p>You can <code>push</code> and <code>pull</code> container images to your repositories. You can use these images locally on your development system, or you can use them in <code>Amazon ECS task definitions</code> and Amazon EKS pod specifications.</p>"},{"location":"aws/ecs/","title":"Elastic Container Service","text":"<p>Amazon <code>Elastic Container Service</code> (ECS) is a <code>fully managed container orchestration service</code> that helps you to more efficiently deploy, manage, and scale containerized applications</p> <p>This applicaiton is region specific.</p> <p>Fargagte as compute for ECS and EKS</p> <p>On both Amazon EKS and Amazon ECS, you have the option of running your containers on the following compute options:</p> <ul> <li> <p><code>AWS Fargate</code> \u2014 a \u201cserverless\u201d container compute engine where you only pay for the resources required to run your containers. Suited for customers who do not want to worry about managing servers, handling capacity planning, or figuring out how to isolate container workloads for security.</p> </li> <li> <p><code>EC2 instances</code> \u2014 offers widest choice of instance types including processor, storage, and networking. Ideal for customers who want to manage or customize the underlying compute environment and host operating system.</p> </li> <li> <p><code>AWS Outposts</code> \u2014 run your containers using AWS infrastructure on premises for a consistent hybrid experience. Suited for customers who require local data processing, data residency, and hybrid use cases.</p> </li> <li> <p><code>AWS Local Zones</code> \u2014 an extension of an AWS Region. Suited for customers who need the ability to place resources in multiple locations closer to end users.</p> </li> <li> <p><code>AWS Wavelength</code> \u2014 ultra-low-latency mobile edge computing. Suited for 5G applications, interactive and immersive experiences, and connected vehicles.</p> </li> </ul>"},{"location":"aws/ecs/#fargate","title":"Fargate","text":"<p>AWS Fargate is a engine that you can use with <code>Amazon ECS</code> to run containers without having to manage servers or clusters of Amazon EC2 instances.</p> <p>Each Fargate task has its own isolation boundary and does not share the underlying kernel, CPU resources, memory resources, or elastic network interface with another task.</p>"},{"location":"aws/ecs/#ecs-and-eks-diff","title":"ECS and EKS diff","text":"<p>Amazon ECS delivers an <code>AWS-opinionated solution</code> for running containers at scale and is not an option for running Kubernetes workloads</p>"},{"location":"aws/ecs/#ecs-anywhere","title":"ECS Anywhere","text":"<p>The following provides a high-level system architecture overview of Amazon ECS Anywhere. Your on-premises server has both the Amazon ECS agent and the SSM agent installed.</p> <p><code>Amazon ECS Anywhere</code> provides support for registering an external instance such as an on-premises server or virtual machine (VM), to your Amazon ECS cluster. </p> <p>External instances are optimized for running applications that generate outbound traffic or process data.</p> <p>If your application requires inbound traffic, the lack of Elastic Load Balancing support makes running these workloads less efficient. Amazon ECS added a new EXTERNAL launch type that you can use to create services or run tasks on your external instances.</p> <p> ECS AnywhereExample <p></p>"},{"location":"aws/efs/","title":"Elastic File System","text":"<p>When to use EFS ?</p> <p>Compared to block storage and object storage, file storage is ideal for use cases in which a large number of services and resources need to access the same data at the same time. </p> <p><code>Amazon Elastic File System</code> is a scalable file system used with AWS Cloud services and on-premises resources.</p> <p>As you add and remove files, Amazon EFS grows and shrinks automatically. It can scale on demand to petabytes without disrupting applications. </p> <ul> <li> <p>Sun created NFS (which is a distributed File System). EFS is an implementation for NFS file share</p> </li> <li> <p>EFS is designed to provide serverless, fully elastic file storage that lets you share file data without provisioning or managing storage capacity and performance</p> </li> <li> <p>It is a great way of sharing the files between the two instances.</p> </li> <li> <p>Unlike the EBS where we have to define/provision space, in the EFS we do not need to do that as they are <code>backed by S3</code>.</p> </li> <li> <p>It can support thousands of concurrent EFS connections</p> </li> <li> <p>Data is stored across multiple AZ\u2019s in a region.</p> </li> <li> <p>Read after write consistency.</p> </li> </ul> <p>Protocol support for  Icon-Architecture/64/Arch_Amazon-EFS_64 </p> <p>Amazon EFS supports the <code>Network File System</code> version 4 (<code>NFSv4.1</code> and <code>NFSv4.0</code>) protocol, so the applications and tools that you use today work seamlessly with Amazon EFS. </p> <p>EFS is accessible across most types of Amazon Web Services compute instances, including Amazon EC2, Amazon ECS, Amazon EKS, AWS Lambda, and AWS Fargate. </p>"},{"location":"aws/efs/#storage-classes","title":"Storage Classes \ud83c\udf9b\ufe0f","text":"<ol> <li> <p><code>EFS</code>: for Linux based systems that need the distributed storage.</p> </li> <li> <p><code>Amazon FSx for Windows</code>: When you need storage for windows-based systems for Sharepoint, SQL Server, etc.</p> </li> <li> <p><code>Amazon FSx for Lustre</code>: For HPC and ML in windows env. It can store data on S3.</p> </li> </ol> <p>AWS currently offers file storage using 5 different services. These services are divided into two categories -  Amazon's own cloud native file storage  -  Amazon FSx file storage offerings: FSx stands for \"file system X\". These offering implement managed files storage using the commonly available file systems for on-premises solutions. </p> <p>Each file service offers different feature sets to meet your requirements. </p> <ol> <li> <p>Amazon Elastic File System (Amazon EFS) is a multi-Availability Zone file storage service that uses <code>NFS access protocol</code>. </p> </li> <li> <p>Amazon FSx for Lustre is built using the Lustre file system and is designed for <code>high performance computing (HPC)</code> and machine learning (ML) workloads. FSx for Lustre uses the Lustre client's POSIX-compliant access protocol.</p> </li> <li> <p>Amazon FSx for Windows File Server is built using Windows File Server. The access protocol is <code>Server Message Block (SMB)</code> and designed for your Microsoft applications and Windows workloads.</p> </li> <li> <p>Amazon FSx for NetApp ONTAP is built using the NetApp ONTAP operating system and is designed to provide both <code>NetApp block and file storage</code>. The access protocols are <code>iSCSI</code> for block storage, <code>NFS</code> and <code>SMB</code> for file storage.</p> </li> <li> <p>Amazon FSx for OpenZFS  fully managed shared file storage built on the <code>OpenZFS file system</code>. With Amazon FSx for OpenZFS, you can migrate your on-premises OpenZFS storage to AWS with minimal effort. You can use the same access protocols now in the AWS Cloud.</p> </li> </ol>"},{"location":"aws/efs/#storage-types","title":"Storage Types \ud83d\udccb","text":""},{"location":"aws/efs/#regional-recommended","title":"Regional (Recommended) \ud83c\udf10","text":"<p>Regional file systems (recommended) store data redundantly across multiple geographically separated Availability Zones within the same AWS Region.</p> <p>Storing data across multiple Availability Zones provides continuous availability to the data, even when one or more Availability Zones in an AWS Region are unavailable.</p>"},{"location":"aws/efs/#one-zone","title":"One Zone \u26f3\ufe0f","text":"<p>One Zone file systems store data within a <code>single Availability Zone</code>. Storing data in a single Availability Zone provides continuous availability to the data. </p> <p>In the unlikely case of the loss or damage to all or part of the Availability Zone, however, data that is stored in these types of file systems might be lost.</p>"},{"location":"aws/eks/","title":"EKS","text":"<p>Amazon Elastic Kubernetes Service (Amazon EKS) Icon-Architecture/64/Arch_Amazon-EKS-Cloud_64 is a fully managed service that you can use to run <code>Kubernetes on AWS</code>. Kubernetes is a <code>single tenant orchestrator</code>, i.e. a single instance of the <code>control plane</code> is shared among all the tenants within a cluster.</p> <p>How auth is managed?</p> <p>In AWS, any auth-related stuff is handled with <code>Identity &amp; Access Management (IAM)</code>, and EKS is no exception. The EKS relies on IAM for the authentication flow, meaning that the user/bot's identity needs to be present in the IAM to be allowed to talk to Kubernetes' control plane.Jan 30, 2023</p>"},{"location":"aws/eks/#clis-used","title":"CLI's used","text":"<p><code>kubectl</code> \u2013 A command line tool for working with Kubernetes clusters.</p> <p><code>eksctl</code> \u2013 A command line tool for working with EKS clusters that automates many individual tasks. </p>"},{"location":"aws/eks/#types-of-nodes-in-eks","title":"Types of nodes in EKS","text":"<p><code>Fargate \u2013 Linux</code>: Select this type of node if you want to run Linux applications on AWS Fargate. Fargate is a serverless compute engine that l ets you deploy Kubernetes pods without managing Amazon EC2 instances.</p> <p><code>self-managed nodes \u2013 Linux</code> : Select this type of node if you want to run Amazon Linux applications on Amazon EC2 instances.</p> <pre><code>eksctl create cluster --name my-cluster --region region-code --fargate\n\nkubectl get nodes -o wide # view nodes\n</code></pre>"},{"location":"aws/eks/#notes","title":"Notes","text":"<ul> <li><code>GuardDuty EKS Protection</code> is a GuardDuty feature that monitors Amazon EKS cluster control plane activity by analyzing Amazon EKS audit logs.</li> <li><code>Amazon ECR</code> integrates with <code>Amazon Inspector</code> to provide automated, continuous scanning of your repositories. Your container images are scanned for both operating systems and programing language package vulnerabilities.</li> <li>The Amazon <code>VPC CNI plugin</code> for Kubernetes add-on is deployed on each Amazon EC2 node in your Amazon EKS cluster.</li> <li>The add-on creates <code>elastic network interfaces</code> and attaches them to your Amazon EC2 nodes. The add-on also assigns a private IPv4 or IPv6 address from your VPC to each Pod and service.</li> <li>It comes pre-installed with EKS</li> <li>Runs as DeamonSet named <code>aws-node</code></li> <li>CoreDNS: </li> <li>It assigns DNS names to Services</li> <li>Comes pre-installed </li> </ul>"},{"location":"aws/elastic-beanstalk/","title":"Elastic beanstalk","text":"<p>With Elastic Beanstalk, you can quickly deploy and manage applications in the AWS Cloud without having to learn about the infrastructure that runs those applications.</p> <p>Elastic Beanstalk reduces management complexity without restricting choice or control. You simply upload your application, and Elastic Beanstalk automatically handles the details of capacity provisioning, load balancing, scaling, and application health monitoring.</p> <p>Elastic Beanstalk supports applications developed in Go, Java, .NET, Node.js, PHP, Python, and Ruby. When you deploy your application, Elastic Beanstalk builds the selected supported platform version and provisions one or more AWS resources, such as <code>Amazon EC2</code> instances, to run your application.</p> <p>It has its own CLI</p> <p>You can interact with Elastic Beanstalk by using the Elastic Beanstalk console, the AWS Command Line Interface (AWS CLI), or eb, a high-level CLI designed specifically for Elastic Beanstalk. </p>"},{"location":"aws/elastic-beanstalk/#environments","title":"Environments","text":"<p> Beanstalk Regions <p></p>"},{"location":"aws/elastic-beanstalk/#web-server-env","title":"Web Server Env","text":"<p>The following diagram shows an example Elastic Beanstalk architecture for a web server environment tier, and shows how the components in that type of environment tier work together.</p> <p> Web Server Env <p></p> <p>The environment is the heart of the application. In the diagram, the environment is shown within the top-level solid line. When you create an environment, Elastic Beanstalk provisions the resources required to run your application. AWS resources created for an environment include one elastic load balancer (ELB in the diagram), an Auto Scaling group, and one or more Amazon Elastic Compute Cloud (Amazon EC2) instances.</p>"},{"location":"aws/elastic-beanstalk/#worker-env","title":"Worker Env","text":"<p>or the worker environment tier, Elastic Beanstalk also creates and provisions an Amazon SQS queue if you don\u2019t already have one. When you launch a worker environment, Elastic Beanstalk installs the necessary support files for your programming language of choice and a daemon on each EC2 instance in the Auto Scaling group. The daemon reads messages from an Amazon SQS queue. The daemon sends data from each message that it reads to the web application running in the worker environment for processing. If you have multiple instances in your worker environment, each instance has its own daemon, but they all read from the same Amazon SQS queue.</p>"},{"location":"aws/elasticache/","title":"Elasticache","text":"<p>Amazon ElastiCache makes it easy to set up, manage, and scale distributed in-memory cache environments in the AWS Cloud. It provides a high-performance, resizable, and cost-effective in-memory cache, while removing the complexity associated with deploying and managing a distributed cache environment.</p> <p>ElastiCache works with both of the following  1. Redis OSS  2. Memcached engines. </p> <ul> <li>In general, pulling an image from a cache is far faster than performing a database read.</li> <li>ElastiCache uses shards as a grouping mechanism for individual Redis nodes. So a single node is part of a shard, which in turn is part of a cluster.</li> <li>Consider ElastiCache as only useful for storing transient data. Further, it\u2019s not a persistent store; therefore, it\u2019s great for caching data from a message queue or providing very fast ephemeral storage.</li> </ul> <p> Using consistent Hashing in MemCacheD <p></p> <p> Async Replication for read replica <p></p> <p> Using shards <p></p> <p> Failover process in Cache <p></p>"},{"location":"aws/elasticache/#serverless-caching","title":"Serverless Caching","text":"<p>ElastiCache serverless caching creates a highly available cache that spans multiple Availability Zones. You can specify subnets from different availability zones and same VPC as you create your cache or ElastiCache will choose subnets automatically from your default VPC. </p>"},{"location":"aws/elasticip/","title":"Elastic IP","text":"<p>What is elastic IP?</p> <ul> <li> <p>An\u00a0Elastic IP address\u00a0is a static and public IPv4 address designed for dynamic cloud computing.</p> </li> <li> <p>An Elastic IP address is associated with your AWS account.</p> </li> <li> <p>With an Elastic IP address, you can mask the failure of an instance or software by rapidly remapping the address to another instance in your account.</p> </li> </ul> <p>An\u00a0Elastic IP address\u00a0is a <code>reserved public IP address</code> that you can assign to any <code>EC2 instance</code> in a particular region until you choose to release it.\u00a0</p> <p>Reuse not allowed after release</p> <p>When you associate an Elastic IP address with an instance or its primary network interface, the instance's public IPv4 address (if it had one) is released back into Amazon's pool of public IPv4 addresses. You cannot reuse a public IPv4 address, and you cannot convert a public IPv4 address to an Elastic IP address.</p> <p> Elastic IP Overview <p></p> <p>When you associate an Elastic IP address with an instance that previously had a public IPv4 address, the public DNS hostname of the instance changes to match the Elastic IP address.</p> <p>To use an elastic IP, you must:</p> <ul> <li>Allocate it for use in a VPC.</li> <li>Associate it with an instance in that VPC.</li> </ul> <p>Quota</p> <ul> <li>By\u00a0default, all\u00a0AWS\u00a0accounts are limited to <code>5\u00a0Elastic IP addresses</code>\u00a0per Region, because public (IPv4) internet\u00a0addresses\u00a0are a scarce public resource.</li> </ul>"},{"location":"aws/elasticip/#eip-basics","title":"EIP basics \ud83c\udf10","text":"<p>The following are the basic characteristics of an Elastic IP address:</p> <ul> <li> <p>An Elastic IP address is <code>static</code>; it does not change over time.</p> </li> <li> <p>An Elastic IP address is for use in a specific Region only, and cannot be moved to a different Region</p> </li> <li> <p>An Elastic IP address comes from Amazon's pool of IPv4 addresses, or from a custom IPv4 address pool that you have brought to your AWS account. We do not support Elastic IP addresses for IPv6</p> </li> <li> <p>To use an Elastic IP address, you first allocate one to your account, and then associate it with your instance or a network interface</p> </li> <li> <p>When you associate an Elastic IP address with an instance, it is also associated with the <code>instance's primary network interface</code>. When you associate an Elastic IP address with a network interface that is attached to an instance, it is also associated with the instance.</p> </li> </ul> <p> Associate Elastic IP <p></p> <ul> <li> <p>When you associate an Elastic IP address with an instance or its primary network interface, if the instance already has a public IPv4 address associated with it, that public IPv4 address is released back into Amazon's pool of public IPv4 addresses and the Elastic IP address is associated with the instance instead. You cannot reuse the public IPv4 address previously associated with the instance and you cannot convert that public IPv4 address to an Elastic IP address. </p> </li> <li> <p>You can disassociate an Elastic IP address from a resource, and then associate it with a different resource. To avoid unexpected behavior, ensure that all active connections to the resource named in the existing association are closed before you make the change. After you have associated your Elastic IP address to a different resource, you can reopen your connections to the newly associated resource</p> </li> <li> <p>A <code>disassociated *Elastic IP address*</code> remains allocated to your account until you <code>explicitly release</code> it. You are charged for all *Elastic IP address*es in your account, regardless of whether they are associated or disassociated with an instance.</p> </li> <li> <p>When you associate an Elastic IP address with an instance that previously had a public IPv4 address, the public DNS host name of the instance changes to match the Elastic IP address</p> </li> <li> <p>We resolve a public DNS host name to the public IPv4 address or the Elastic IP address of the instance outside the network of the instance, and to the private IPv4 address of the instance from within the network of the instance</p> </li> <li> <p>When you allocate an Elastic IP address from an IP address pool that you have brought to your AWS account, it does not count toward your Elastic IP address limits.</p> </li> <li> <p>When you allocate the *Elastic IP address*es, you can associate the *Elastic IP address*es with a network border group. This is the location from which we advertise the CIDR block. Setting the network border group limits the CIDR block to this group. If you do not specify the network border group, we set the border group containing all of the Availability Zones in the Region (for example, us-west-2)</p> </li> <li> <p>An Elastic IP address is for use in a specific network border group only.</p> </li> </ul>"},{"location":"aws/eni/","title":"ENI","text":"<p>An <code>Elastic Network Interface (ENI)</code> is virtual and can have:</p> <ul> <li>multiple IPv4 and IPv6 addresses</li> <li>security groups</li> <li>a MAC address</li> <li>a <code>source/destination check</code> flag.</li> </ul> <p> ENI Overivew <p></p> <p>Why is ENI requried?</p> <p> ETH0 Interface <p></p> <p>Tip</p> <ul> <li>Traffic follows the network interface rather than sticking to any particular instance. So in the case when ENI is moved from one instance to another, the traffic is redirected to the new instance but stays targeted at the elastic network interface  </li> <li>An elastic network interface can only be attached to a single instance at one time but can be moved from one instance to another.</li> <li>We can attach multiple elastic network interfaces to an instance</li> </ul> <p> Example to show 2 ENIs attached to EC2 </p> <ul> <li>An instance\u2019s primary ENI cannot be detached.</li> </ul> <p> Example to show how all of the parts \u2014 VPC, subnets, routing tables, and ENIs fit together <p></p>"},{"location":"aws/event-bridge/","title":"EventBridge","text":"<p>Amazon EventBridge is a serverless, fully managed, and scalable event bus that enables integrations between AWS services, Software as a services (SaaS), and your applications.</p> <p> Event Bridge Example <p></p> <p>Remember</p> <p>EventBridge was formerly called <code>Amazon CloudWatch Events</code>. Amazon CloudWatch Events and EventBridge are the same underlying service and API, however, EventBridge provides more features.</p> <p>EventBridge can be used to initiate processes on downstream services <code>asynchronously</code></p> <ul> <li> <p>EventBridge makes it easier to connect applications. You can ingest, filter, transform and deliver events without writing custom code.</p> </li> <li> <p>EventBridge allows you to build <code>event-driven architectures</code>, which are loosely coupled and distributed. This improves developer agility as well as application resiliency and it takes care of event ingestion and delivery, security, authorization, and error-handling for you. It enables you to decouple your architectures to make it faster to build and innovate, using routing rules to deliver events to selected targets.</p> </li> <li> <p><code>EventBridge</code> delivers a stream of real-time data from your applications, SaaS applications, and AWS services to targets such as AWS Lambda functions, HTTP invocation endpoints using API destinations, or event buses in other AWS accounts.</p> </li> </ul> <p>Example of e-comm site using EventBridge</p> <p> Event-driven architecture example using EB </p> <p>Remember</p> <p>EventBridge was formerly called Amazon CloudWatch Events. The default event bus and the rules you created in CloudWatch Events also display in the EventBridge console. EventBridge uses the same CloudWatch Events API, so your code that uses the CloudWatch Events API stays the same.</p> <p> What is Eventbridge?  <p></p>"},{"location":"aws/event-bridge/#event-bus","title":"Event Bus \ud83d\ude8c","text":"<p> What is CloudWatch Event Bus?  <p></p> <p>An event bus is a router that receives events and delivers them to zero or more destinations,or targets. Use an event bus when you need to route events from many sources to many targets, with optional transformation of events prior to delivery to a target.</p> <p>Your account includes a default event bus that automatically receives events from AWS services. You can also:</p> <ul> <li>Create additional event buses, called custom event buses, and specify which events they receive.</li> <li>Create partner event buses, which receive events from SaaS partners.</li> </ul> <p> CloudWatch Event Bridge Architecture  <p></p> <p>An <code>event bus</code> is a pipeline that receives events, 3 buses are shown below</p> <ol> <li><code>Default event bus</code> \u2013 Every AWS account has a default event bus, which receives events from AWS services in that account.</li> <li><code>Custom event buses</code> \u2013 Custom event buses are used to receive events from custom applications, different AWS accounts in the same AWS Region, or from different Regions.</li> <li><code>Partner event buses</code> \u2013 Receive events from a SaaS partner through a partner event source.</li> </ol> <p> Event Bridge Connectivity <p></p>"},{"location":"aws/event-bridge/#event-pipe","title":"Event Pipe \ud83e\ude88","text":"<p>A pipe routes events from a <code>single source</code> to a <code>single target</code>. The pipe also includes the ability to filter for specific events, and to perform enrichments on the event data before it is sent to the target.</p> <p> CloudWatch Event Bus  <p></p>"},{"location":"aws/event-bridge/#rules","title":"Rules \ud83d\udcd0","text":"<p>Rules evaluate events and send them to interested consumers, or targets. Rules are assigned to a single event bus and are invoked in one of two ways. First, when an event on an event bus matches a rule's event pattern. Second, when you have defined a schedule and EventBridge invokes based on the identified times.</p> <p>Example</p> <p>For example, you can create a rule to identify when an Amazon S3 bucket is publicly readable and to also change the Amazon S3 bucket permissions to remove public access.</p>"},{"location":"aws/event-bridge/#targets","title":"Targets \ud83c\udfaf","text":"<p>A target is a resource where EventBridge sends events when a rule's event pattern matches. You can associate multiple targets for a rule to send the same event to multiple other consumers without creating multiple rules.</p> <p>EventBridge can directly integrate with AWS services, SaaS applications, and external API destinations. </p>"},{"location":"aws/event-bridge/#content-filtering","title":"Content filtering \ud83d\udcdd","text":"<p>You can filter event patterns on standard operations such as equals, empty, and boolean AND/OR for values; however, EventBridge supports more complex event patterns using content filtering. With content filtering, you can match events in very specific scenarios, such as identifying when a field of the event is within a specific numeric range. There are special filters for evaluating IP addresses and even checking that a field doesn't exist in the event JSON.</p>"},{"location":"aws/event-bridge/#scheduling","title":"Scheduling \u23f0","text":"<p>With EventBridge Scheduler, you can schedule one-time or recurrently tens of millions of tasks across many AWS services without provisioning or managing underlying infrastructure. EventBridge Scheduler targets over 270 AWS services through the AWS SDK, giving you many options to invoke the service you need.</p>"},{"location":"aws/event-bridge/#api-destinations","title":"API destinations \ud83c\udd7f\ufe0f","text":"<p>With API destinations, you can send events to on-premise applications, SaaS applications, or any web-based application with a web address without worrying about writing custom code or using additional infrastructure.</p> <p>An API destination is made up of an HTTP endpoint, HTTP method (such as GET, POST, PUT), and a connection, which defines the type of authorization and credentials used to call the endpoint</p>"},{"location":"aws/failover/","title":"HA and DR","text":""},{"location":"aws/failover/#ha-and-drfault-tolerance-difference","title":"HA and DR/Fault Tolerance difference","text":"<p>HA example is shown below</p> <p> </p> <p>DR Example</p> <p> </p>"},{"location":"aws/failover/#business-continuity","title":"Business Continuity","text":""},{"location":"aws/failover/#rpo","title":"RPO \ud83d\udd3b","text":""},{"location":"aws/failover/#rto","title":"RTO \u231a\ufe0f","text":""},{"location":"aws/failover/#notes","title":"Notes","text":"<ul> <li> <p>If all your clients are in one region, then there is little advantage to adding <code>read replicas</code> to additional regions. Instead, providing replicas in the same region as the clients gives them the fastest access.</p> </li> <li> <p>You also cannot failover to a read replica. You can convert it to a stand-alone instance, but that is a manual process that is not a failover.</p> </li> </ul>"},{"location":"aws/fargate/","title":"Fargate","text":"<ul> <li>AWS Fargate is a <code>serverless compute engine</code> for containers.</li> <li>It works with both <code>Amazon ECS</code> and <code>Amazon EKS</code></li> </ul>"},{"location":"aws/glacier/","title":"AWS Glacier","text":"<ul> <li>It is called as <code>cold storage</code></li> <li>It is used by AWS storage gateway Virtual Tape library</li> <li>Used in S3 via lifecycle management </li> </ul> <p>Tip</p> <p>Glacier is a service in itself. You don't need to use S3 to use Glacier.</p> <p> </p>"},{"location":"aws/global-accelerator/","title":"Global Accelerator","text":"<p>AWS Global Accelerator is a service that allows you to route traffic to your applications using the <code>AWS global network</code> instead of the internet. The internet can be congested and AWS claim that by using their private network infrastructure you can improve the connection speed and performance by as much as 60%.</p> <p> GA Overview <p></p> <p>Global Accelerator provides two global static public IPs that act as a fixed entry point to your application endpoints, such as Application Load Balancers, Network Load Balancers, Amazon Elastic Compute Cloud (EC2) instances, and elastic IPs.</p> <p> GA Overview <p></p> <p>GA then optimises the path from those IP addresses to your application which results in lower latency and better network performance.</p> <p>Why use GA?</p> <p>Whether your users are distributed locally or across the globe, you are always going to want them to connect to your application using the fastest reliable connection with the lowest latency which is something the public internet may not be able to provide consistently. </p> <p>Because GA can detect unhealthy endpoints and swap traffic to a healthy option within 30 seconds, it provides a seamless method of ensuring you have the fastest method of delivering traffic to your application endpoints and resolving issues as they occur.</p>"},{"location":"aws/glue/","title":"AWS Glue","text":"<ul> <li> <p>AWS Glue is a <code>fully-managed ETL</code> (extract, transform, and load) service that makes it simple and cost-effective to categorize your data, clean it, enrich it, and move it reliably between various <code>data stores</code> and data streams</p> </li> <li> <p>AWS Glue consists of a central metadata repository known as the <code>Glue Data Catalog</code>, an <code>ETL engine</code> that automatically generates Python or Scala code, and a flexible <code>scheduler</code> that handles dependency resolution, job monitoring, and retries</p> </li> <li> <p>AWS Glue is serverless</p> </li> </ul>"},{"location":"aws/glue/#how-it-works","title":"How it works","text":"<ul> <li> <p>AWS Glue is designed to work with semi-structured data.</p> </li> <li> <p>It introduces a component called a dynamic frame, which you can use in your ETL scripts. </p> </li> <li> <p>A dynamic frame is similar to an <code>Apache Spark dataframe</code>, which is a data abstraction used to organize data into rows and columns, except that each record is self-describing so no schema is required initially.</p> </li> <li> <p>With dynamic frames, you get schema flexibility and a set of advanced transformations specifically designed for dynamic frames.</p> </li> <li> <p>You can convert between dynamic frames and Spark dataframes, so that you can take advantage of both <code>AWS Glue</code> and <code>Spark transformations</code> to do the kinds of analysis that you want.</p> </li> </ul>"},{"location":"aws/glue/#use-cases","title":"Use cases","text":"<ul> <li> <p>You can use the AWS Glue console to discover data, transform it, and make it available for search and querying. The console calls the underlying services to orchestrate the work required to transform your data.</p> </li> <li> <p>You can also use the AWS Glue API operations to interface with AWS Glue services. Edit, debug, and test your Python or Scala Apache Spark ETL code using a familiar development environment.</p> </li> </ul>"},{"location":"aws/glue/#reference-architecture-for-streaming-data-pipeline","title":"Reference architecture for streaming data pipeline","text":"<p> Reference architecture for streaming data pipeline <p> </p> <ol> <li> <p>Data Source\u2014 In the previous architecture, there are multiple data sources. Near real-time data is generated through streaming data sources such as IoT devices, log and diagnostic data from application servers, and change data capture (CDC) from transactional data stores.</p> </li> <li> <p>Data steaming \u2014 Messages and events are streamed into streaming services such as Amazon Kinesis Data Streams or Amazon MSK.</p> </li> <li> <p>Stream data processing \u2014 In this step, you can create streaming ETL jobs that run continuously and consume data from streaming sources such as Amazon Kinesis Data Streams and Amazon MSK. The jobs cleanse and transform the data.</p> </li> <li> <p>Stream data loading\u2014 The processed data is typically loaded into S3 data lakes or joint database connectivity (JDBC) data stores such as Amazon Redshift or NoSQL data sources such as Amazon DynamoDB or Amazon OpenSearch Service. After the data is loaded, the data can be consumed using services such as Amazon QuickSight, Amazon Athena, Amazon SageMaker, Amazon Managed Grafana, and so on.</p> </li> <li> <p>Amazon QuickSight \u2014 Amazon QuickSight allows everyone in your organization to understand your data by asking questions in their native language, exploring through interactive dashboards, or automatically looking for patterns and outliers powered by ML.</p> </li> <li> <p>Amazon Athena\u2014 Amazon Athena provides capability for ad hoc querying capability on the data stored in the data lake.</p> </li> <li> <p>Amazon SageMaker \u2014 Amazon SageMaker and AWS AI services can be used to build, train, and deploy ML models, and add intelligence to your applications.</p> </li> <li> <p>Amazon Managed Grafana \u2014 Amazon Managed Grafana is an open-source analytics platform that can be used to query, visualize, alert on, and understand metrics, no matter where they are stored.</p> </li> <li> <p>Logging, monitoring, and notification \u2014 Amazon CloudWatch can be used for monitoring, Amazon SNS can be used for notification, and AWS CloudTrail can be used for event logging. </p> </li> </ol>"},{"location":"aws/iam/","title":"IAM    Icon-Architecture/64/Arch_AWS-Single-Sign-On_64 Created with Sketch.","text":"<ul> <li>IAM is universal service and it does not apply to a single region; it is cross-region</li> <li>The <code>root account</code> is the account created for the first time and it has full access (also called as God mode \ud83e\uddda )</li> <li>New users have no permissions when created </li> <li><code>Access key id</code> and <code>secret access key</code> are required for programmatic access to AWS. They can\u2019t be used to login to the console.</li> <li>Use MFA in the root account. We can use the Google Auth app for this \ud83d\udcf1</li> <li>IAM is concerned with the raw AWS resources, not access to running web applications.</li> </ul> <p>Tip</p> <p>Setting up a cross-account IAM role is currently the only method that will allow IAM users to access cross-account S3 buckets both programmatically and via the AWS console.</p>"},{"location":"aws/iam/#iam-root-user","title":"IAM Root User \ud83c\udf34","text":"<p>When you first create an Amazon Web Services (AWS) account, you begin with a single sign-in identity that has complete access to all AWS services and resources in the account. </p> <p>This identity is called the <code>AWS account root user</code> and is accessed by signing in with the email address and password that you used to create the account.</p> <p> </p> <p>How to secure AWS Root Account  Icon-Architecture/64/Arch_AWS-Single-Sign-On_64 Created with Sketch. </p> <p>You can do the following:</p> <p>Implement MFA </p> <p>Delete the Access Keys \u274c</p> <p> </p>"},{"location":"aws/iam/#iam-user","title":"IAM User \ud83d\udc68\u200d\ud83e\uddb1","text":"<p>An IAM user is an identity that you create in AWS. It represents the person or application that interacts with AWS services and resources. It consists of a name and credentials.</p> <p> </p> <p>By default, when you create a new IAM user in AWS, it has no permissions associated with it. To allow the IAM user to perform specific actions in AWS, such as launching an Amazon EC2 instance or creating an Amazon S3 bucket, you must grant the IAM user the necessary permissions.</p> <p> </p>"},{"location":"aws/iam/#iam-policy","title":"IAM Policy \ud83e\uddd1\u200d\ud83d\udcbc","text":"<p>An IAM policy is a document that allows or denies permissions to AWS services and resources.  </p> <p> </p>"},{"location":"aws/iam/#policy-types","title":"Policy Types","text":"<p>There are four types of policies in IAM:-</p> <ul> <li>Identity-based</li> <li>Resource-based</li> <li>Organization SCPs</li> <li>Permission boundaries</li> </ul> <p>You can provide console access and programmatic access via IAM. Programmatic access includes API and CLI access.</p> <p>Remember</p> <ul> <li>IAM is not the managed service for handling MFA Delete setup on S3 buckets</li> <li>Users, groups, roles, permissions, and similar constructs are part of IAM</li> <li>Organizations and organizational units are part of AWS Organizations, a different facility.</li> <li>User policies are not part of IAM but permissions are.</li> <li>IAM policies are written in JSON.</li> <li>IAM policies can be attached to users, groups, and roles in the case of identity-based policies, and AWS services and components via resource-based policies.</li> <li>Restoring revoked permissions for a user and changing the support options need the root user access.</li> <li>IAM changes apply immediately to all users across the system; there is no lag, and no need to log out and back in.</li> <li>Power-user access is a predefined policy that allows access to all AWS services with the exception of group or user management within IAM</li> </ul> <p>Warning</p> <ul> <li>AWS strongly recommends you delete your root user access keys and create IAM users for everyday use.</li> <li>IAM root user account is needed for very privileged access; in this case, that\u2019s creating a CloudFront key pair, which essentially provides signed access to applications and is a very trusted action.</li> <li>AWS firmly believes that root account access should be highly limited, but also not confined to a single user. Having a very small group of engineers (ideally AWS certified) is the best approach to reducing root account level access as much as possible.</li> </ul> <ul> <li>You will always need to provide <code>non-root sign-in URLs</code> for new users.</li> <li>New users have no access to AWS services. They are \u201cbare\u201d or \u201cempty\u201d or \u201cnaked\u201d users, as they can merely login to the AWS console (if a URL is provided). They cannot make any changes to AWS services or even view services.</li> <li>AWS usernames have to be unique across the AWS account in which that user exists \ud83d\udd11</li> <li>If you have an external Active Directory, you\u2019d want to federate those users into AWS. This allows you to use the existing user base, not re-create each individual user.</li> </ul>"},{"location":"aws/iam/#identity-based","title":"Identity-based \ud83c\udd94","text":""},{"location":"aws/iam/#resource-based","title":"Resource-based \u26fa\ufe0f","text":""},{"location":"aws/iam/#organization-scps","title":"Organization SCPs \ud83c\udfe2","text":"<p>SCP's are used by AWS organizations and attached to AWS accounts or organizational units, OUs, to define the maximum permissions allowed for the members of the associated account or OU. So in a way they act in a similar fashion to that of permission boundaries, but at the account level and affect all members of those accounts.</p> <p> </p> <p>Example</p> <p>let's say a user within an AWS account had full access to S3, RDS and EC2 via an identity-based policy. If the SCP associated with that AWS account denied access to the S3 service, then that user would only be able to access RDS and EC2 despite having full access to S3. The SCP would serve to prevent service from being used within the AWS account and so have the overriding precedence and determine the maximum level of permissions allowed.</p> <p>So to be clear, an SCP does not grant access, they add a guide route to define what is allowed. You'll still need to configure your identity-based or resource-based policies to identities, granting permission to carry out actions within your accounts. If you want to use service control policies to help you manage your security at an account level, then you need to ensure that you deploy AWS organizations using the enable all feature setting.</p> <p>Within IAM, you have the ability to view any SCPs that are applicable to your AWS account, the policy statement itself, it's ARN, the number of entities it affects and you can also review access activity to learn when a principal within the organization last accessed a service. However, if you wanted to update or edit the SCP, then you'd have to do that from within the AWS organization service itself. The SCP can't be edited from within IAM. </p>"},{"location":"aws/iam/#permission-boundaries","title":"Permission boundaries \ud83e\uddf1","text":"<p>These policies can be associated with a role or user, but they don't actually grant permissions themselves, instead they define the maximum level of permissions that can be granted to an entity. Organization service control policies, SCPs. These are very similar to permission boundaries in the fact that they do not grant permissions.</p> <p>They define a boundary of maximum permissions. However, these service control policies are associated with an AWS account or organizational unit, an OU, when working with AWS organizations and govern the maximum permissions to the members of those accounts.</p>"},{"location":"aws/iam/#iam-group","title":"IAM Group \ud83d\udc68\u200d\ud83d\udc69\u200d\ud83d\udc66\u200d\ud83d\udc66","text":"<p>An IAM group is a collection of IAM users. When you assign an IAM policy to a group, all users in the group are granted permissions specified by the policy.</p> <p> </p>"},{"location":"aws/iam/#iam-role","title":"IAM Role \ud83c\udfab","text":"<p>An IAM role is an identity that you can assume to gain temporary access to permissions.  </p> <p>Best Practice</p> <p>IAM roles are ideal for situations in which access to services or resources needs to be granted temporarily, instead of long-term.  </p>"},{"location":"aws/iam/#on-behalf-of-role","title":"On-Behalf of Role \ud83e\udde2","text":"<p>Many <code>AWS services</code> require that you use roles to allow the service to access resources in other services on your behalf. A role that a service assumes to perform actions on your behalf is called a <code>service role</code>.</p> <p>When a role serves a specialized purpose for a service, it is categorized as a service role for EC2 instances (for example), or a service-linked role. </p>"},{"location":"aws/iam/#identity-federation","title":"Identity Federation \ud83e\udeaa","text":"<p>IDP: The user credentials and other identifying information are stored and managed by an Identity Provider (IdP) centralized system. IdP is a trusted system that provides access to other websites and applications. </p> <p>SP: In the context of <code>SSO</code>, a <code>service provider</code> is responsible for providing services to the end user. However, service providers do not authenticate users themselves. Instead, they rely on an <code>identity provider (IdP)</code> to verify the user\u2019s identity and manage specific attributes related to the user. Put simply, <code>service providers</code> request authentication decisions from the <code>IdP</code>, which holds the account information and unique attributes related to the user for the given service. </p> <p> </p> <p>Web identity federation and SAML difference?</p> <p>Whereas <code>web identity federation</code> is generally used for large, wide scale of access from unknown users, <code>SAML 2.0</code> is generally used to authenticate your employees using existing directory services that you might already be using. </p> <p>SAML, which stands for Security Assertion Markup Language, is a standard that's used to exchange authentication and authorization identities between different security domains, which uses security tokens containing assertions to pass information about a user between a <code>SAML Identity Provider</code> and a <code>SAML consumer</code>. </p> <p>Federated access example</p> <p> </p>"},{"location":"aws/inspector/","title":"Inspector","text":"<p>Amazon Inspector helps to improve the security and compliance of applications by running automated security assessments. </p> <p>It checks applications for security vulnerabilities and deviations from security best practices, such as open access to Amazon EC2 instances and installations of vulnerable software versions. </p> <p>Report</p> <p>After Amazon Inspector has performed an assessment, it provides you with a list of security findings. The list prioritizes by severity level, including a detailed description of each security issue and a recommendation for how to fix it. </p>"},{"location":"aws/kinesis/","title":"Kinesis","text":"<p>You can use Amazon Kinesis Data Streams to collect and process large streams of data records in real time. You can create data-processing applications, known as Kinesis Data Streams applications. A typical Kinesis Data Streams application reads data from a data stream as data records. These applications can use the Kinesis Client Library, and they can run on Amazon EC2 instances.</p> <p> </p>"},{"location":"aws/kinesis/#kinesis-data-stream","title":"Kinesis Data Stream","text":""},{"location":"aws/kinesis/#resharding","title":"ReSharding","text":"<p>Amazon Kinesis Data Streams supports resharding, which lets you adjust the number of shards in your stream to adapt to changes in the rate of data flow through the stream. Resharding is considered an advanced operation.</p> <p> </p> <p>There are two types of resharding operations:</p> <ul> <li>Shard split</li> <li>Shard merge</li> </ul> <p>In a shard split, you divide a single shard into two shards. In a shard merge, you combine two shards into a single shard. Resharding is always pairwise in the sense that you cannot split into more than two shards in a single operation, and you cannot merge more than two shards in a single operation. The shard or pair of shards that the resharding operation acts on are referred to as parent shards. The shard or pair of shards that result from the resharding operation are referred to as child shards. </p>"},{"location":"aws/kinesis/#kinesis-data-firehouse","title":"Kinesis Data Firehouse","text":"<p>Amazon Data Firehose is a fully managed service for delivering real-time streaming data to destinations such as Amazon Simple Storage Service (Amazon S3), Amazon Redshift, Amazon OpenSearch Service, Amazon OpenSearch Serverless, Splunk, Apache Iceberg Tables, and any custom HTTP endpoint or HTTP endpoints owned by supported third-party service providers, including Datadog, Dynatrace, LogicMonitor, MongoDB, New Relic, Coralogix, and Elastic.</p> <p> </p>"},{"location":"aws/kinesis/#concepts","title":"Concepts","text":""},{"location":"aws/kinesis/#firehose-stream","title":"Firehose stream","text":"<p>The underlying entity of Amazon Data Firehose. You use Amazon Data Firehose by creating a Firehose stream and then sending data to it.</p>"},{"location":"aws/kinesis/#record","title":"Record","text":"<p>The data of interest that your data producer sends to a Firehose stream. A record can be as large as 1,000 KB.</p>"},{"location":"aws/kinesis/#data-producer","title":"Data producer","text":"<p>Producers send records to Firehose streams. For example, a web server that sends log data to a Firehose stream is a data producer. You can also configure your Firehose stream to automatically read data from an existing Kinesis data stream, and load it into destinations.</p>"},{"location":"aws/kinesis/#buffer-size-and-buffer-interval","title":"Buffer size and buffer interval","text":"<p>Amazon Data Firehose buffers incoming streaming data to a certain size or for a certain period of time before delivering it to destinations. Buffer Size is in MBs and Buffer Interval is in seconds.</p>"},{"location":"aws/kinesis/#kinesis-data-analytics","title":"Kinesis Data Analytics","text":""},{"location":"aws/kms/","title":"Key Management Service (KMS)","text":"<p><code>AWS Key Management Service</code> enables you to perform encryption operations through the use of cryptographic keys. A cryptographic key is a random string of digits used for locking (encrypting) and unlocking (decrypting) data. </p> <p>You can use AWS KMS to create, manage, and use cryptographic keys. You can also control the use of keys across a wide range of services and in your applications.</p>"},{"location":"aws/lambda/","title":"Lambda basics","text":"<ul> <li>Serverless computing allows you to build and run applications and services without thinking about servers \ud83d\udda5\ufe0f</li> <li>With serverless computing, your application still runs on servers, but all the server management is done by AWS </li> <li>It scales out instead of scaling up \u2b06\ufe0f with the number of requests.</li> <li>Lambda functions are <code>stateless</code>, with no affinity to the underlying infrastructure</li> <li>Lambda functions are independent, meaning that they will get replicated with each event \ud83d\udd25</li> </ul> <p>Tip</p> <p>Using AWS and its Serverless Platform, you can build and deploy applications on cost-effective services that provide built-in application availability and flexible scaling capabilities. This lets you focus on your application code instead of worrying about provisioning, configuring, and managing servers.</p> <p> Non-monolith Lambda based on Micro-service Architecture <p></p>"},{"location":"aws/lambda/#which-services-are-serverless","title":"Which services are serverless?","text":"<ul> <li><code>Lambda</code> is serverless  Icon-Architecture/64/Arch_AWS-Lambda_64 Created with Sketch. </li> <li>RDS is not serverless as it has downtime.</li> <li>Aurora is serverless.</li> <li>Dynamo, S3 is serverless.</li> </ul>"},{"location":"aws/lambda/#concepts","title":"Concepts","text":""},{"location":"aws/lambda/#function","title":"Function \u26a1\ufe0f","text":"<ul> <li>A function is a resource that you can invoke to run your code in Lambda.</li> <li>A function has code to process the events that you pass into the <code>function</code> or that other AWS services send to the function.</li> </ul>"},{"location":"aws/lambda/#configuration","title":"Configuration \ud83d\udcac","text":"<p>When building and testing a function, you must specify three primary configuration settings: memory, timeout, and concurrency</p> <p> Config params for Lambda <p></p> <p>15 min max timeout</p> <p>The <code>AWS Lambda timeout</code> value dictates how long a function can run before Lambda terminates the Lambda function. At the time of this publication, the maximum timeout for a Lambda function is 900 seconds. This limit means that a single invocation of a Lambda function cannot run longer than 900 seconds (which is 15 minutes). </p>"},{"location":"aws/lambda/#trigger","title":"Trigger \ud83d\udd2b","text":"<p>A trigger is a resource or configuration that invokes a Lambda function. Triggers include AWS services that you can configure to invoke a function and event source mappings. </p> <p>Tip</p> <p>An <code>event source mapping</code> is a resource in Lambda that reads items from a stream or queue and invokes a function.</p> <p>Trigger Types</p>"},{"location":"aws/lambda/#sync","title":"Sync \u267b\ufe0f","text":"<p>When you invoke a function <code>synchronously</code>, Lambda runs the function and waits for a response. When the function completes, Lambda returns the response from the function's code with additional data, such as the version of the function that was invoked. Synchronous events expect an immediate response from the function invocation. </p> <p>Remember</p> <p>With this model, there are no built-in retries. You must manage your retry strategy within your application code.</p>"},{"location":"aws/lambda/#async","title":"ASync \u26f0\ufe0f","text":"<p>When you invoke a function asynchronously, events are queued and the requestor doesn't wait for the function to complete. This model is appropriate when the client doesn't need an immediate response. With the asynchronous model, you can make use of destinations.</p>"},{"location":"aws/lambda/#polling","title":"Polling \ud83d\udcca","text":"<p>With this type of integration, AWS will manage the poller on your behalf and perform synchronous invocations of your function. </p> <p>Tip</p> <p>With this model, the retry behavior varies depending on the event source and its configuration.</p>"},{"location":"aws/lambda/#event","title":"Event \u2604\ufe0f","text":"<p>An event is a JSON-formatted document that contains data for a <code>Lambda function</code> to process. The runtime converts the event to an object and passes it to your function code. When you invoke a function, you determine the structure and contents of the event.</p> Example custom event \u2013 Weather data<pre><code>{\n  \"TemperatureK\": 281,\n  \"WindKmh\": -3,\n  \"HumidityPct\": 0.55,\n  \"PressureHPa\": 1020\n}\n</code></pre> <p>When an AWS service invokes your function, the service defines the shape of the event.</p> Example service event \u2013 Amazon SNS notification<pre><code>{\n  \"Records\": [\n    {\n      \"Sns\": {\n        \"Timestamp\": \"2019-01-02T12:45:07.000Z\",\n        \"Signature\": \"tcc6faL2yUC6dgZdmrwh1Y4cGa/ebXEkAi6RibDsvpi+tE/1+82j...65r==\",\n        \"MessageId\": \"95df01b4-ee98-5cb9-9903-4c221d41eb5e\",\n        \"Message\": \"Hello from SNS!\",\n        ...\n</code></pre>"},{"location":"aws/lambda/#execution-environment","title":"Execution environment \u2699\ufe0f","text":"<p>An execution environment provides a <code>secure and isolated runtime environment</code> for your Lambda function. An execution environment manages the processes and resources that are required to run the function. </p> <p> Lambda execution environment <p></p> <p>How execution env is setup?</p> <p>When you create your Lambda function, you specify configuration information, such as the amount of available memory and the maximum invocation time allowed for your function. Lambda uses this information to set up the execution environment.</p> <p> Execution Env for Customer A and B <p></p> <ol> <li>Init Phase: In this phase, Lambda creates or unfreezes an execution environment with the configured resources, downloads the code for the function and all layers, initializes any extensions, initializes the runtime, and then runs the function\u2019s initialization code (the code outside the main handler). </li> <li>Invoke Phase: In this phase, Lambda invokes the function handler. After the function runs to completion, Lambda prepares to handle another function invocation. </li> <li>Shutdown Phase: If the Lambda function does not receive any invocations for a period of time, this phase initiates. In the Shutdown phase, Lambda shuts down the runtime, alerts the extensions to let them stop cleanly, and then removes the environment. Lambda sends a shutdown event to each extension, which tells the extension that the environment is about to be shut down.</li> </ol>"},{"location":"aws/lambda/#cold-start-and-warm-start","title":"Cold Start and Warm Start \u2603\ufe0f","text":"<ul> <li><code>Cold Start</code>: A cold start occurs when a new execution environment is required to run a Lambda function</li> <li><code>Warm Start</code>: In a warm start, the Lambda service retains the environment instead of destroying it immediately. This allows the function to run again within the same execution environment. This saves time by not needing to initialize the environment.  </li> </ul> <p> Cold Start Vs Warm Start <p></p>"},{"location":"aws/lambda/#policies","title":"Policies \ud83d\udc6e","text":""},{"location":"aws/lambda/#execution-role","title":"Execution role \ud83d\ude46\u200d\u2642\ufe0f","text":"<p>The <code>execution role</code> gives your function permissions to interact with other services. You provide this role when you create a function, and Lambda assumes the role when your function is invoked. The policy for this role defines the actions the role is allowed to take \u2014 for example, writing to a DynamoDB table.</p> <p> IAM Policy and Execution role <p></p> <p>The role must include a <code>trust policy</code> that allows Lambda to \u201cAssumeRole\u201d so that it can take that action for another service. </p> <p>What is trust policy</p> <p>A trust policy defines what actions your role can assume. The trust policy allows Lambda to use the role's permissions by giving the service principal <code>lambda.amazonaws.com</code> permission to call the <code>AWS Security Token Service</code> (AWS STS) AssumeRole action.</p> <p>This example illustrates that the principal <code>\"Service\":\"lambda.amazonaws.com\"</code> can take the <code>\"Action\":\"sts:AssumeRole\"</code> allowing Lambda to assume the role and invoke the function on your behalf.    </p>"},{"location":"aws/lambda/#iam-resource-policy","title":"IAM resource policy","text":"<p>A <code>resource policy</code> (also called a <code>function policy</code>) tells the Lambda service which principals have permission to invoke the Lambda function. An AWS principal may be a user, role, another AWS service, or another AWS account.</p>"},{"location":"aws/lambda/#instruction-set-architecture","title":"Instruction set architecture","text":"<p>The instruction set architecture determines the type of computer processor that Lambda uses to run the function. Lambda provides a choice of instruction set architectures:</p> <ul> <li> <p><code>arm64</code> \u2013 64-bit ARM architecture, for the AWS Graviton2 processor.</p> </li> <li> <p><code>x86_64</code> \u2013 64-bit x86 architecture, for x86-based processors.</p> </li> </ul>"},{"location":"aws/lambda/#deployment-package","title":"Deployment package \ud83d\udce6","text":"<p>You deploy your Lambda function code using a deployment package. Lambda supports two types of deployment packages:</p> <ul> <li> <p>A .zip file archive that contains your function code and its dependencies. Lambda provides the operating system and runtime for your function.</p> </li> <li> <p>A container image that is compatible with the <code>Open Container Initiative (OCI)</code> specification. You add your function code and dependencies to the image. You must also include the operating system and a Lambda runtime.</p> </li> </ul>"},{"location":"aws/lambda/#runtime","title":"Runtime \u23f0","text":"<p>The runtime provides a language-specific environment that runs in an execution environment. The runtime relays invocation events, context information, and responses between Lambda and the function. You can use runtimes that Lambda provides, or build your own. If you package your code as a .zip file archive, you must configure your function to use a runtime that matches your programming language. For a container image, you include the runtime when you build the image.</p>"},{"location":"aws/lambda/#layer","title":"Layer \ud83d\udcda","text":"<p>A Lambda layer is a <code>.zip file archive</code> that can contain additional code or other content. A layer can contain libraries, a custom runtime, data, or configuration files.</p> <p>Remember</p> <p>Layers provide a convenient way to package libraries and other dependencies that you can use with your Lambda functions. Using layers reduces the size of uploaded deployment archives and makes it faster to deploy your code. Layers also promote code sharing and separation of responsibilities so that you can iterate faster on writing business logic.</p> <p>You can include up to five layers per function. Layers count towards the standard Lambda deployment size quotas. When you include a layer in a function, the contents are extracted to the /opt directory in the execution environment.</p> <p>By default, the layers that you create are private to your AWS account. You can choose to share a layer with other accounts or to make the layer public. If your functions consume a layer that a different account published, your functions can continue to use the layer version after it has been deleted, or after your permission to access the layer is revoked. However, you cannot create a new function or update functions using a deleted layer version.</p> <p>Tldr</p> <p>Functions deployed as a container image do not use layers. Instead, you package your preferred runtime, libraries, and other dependencies into the container image when you build the image.</p>"},{"location":"aws/lambda/#destination","title":"Destination \ud83d\uddfb","text":"<p><code>Destinations</code> are AWS resources that receive a <code>record of an invocation</code> after success or failure. You can configure Lambda to send invocation records when your function is invoked asynchronously, or if your function processes records from a stream. The contents of the invocation record and supported destination services vary by source.</p>"},{"location":"aws/lambda/#extension","title":"Extension","text":"<p><code>Lambda extensions</code> enable you to augment your functions. For example, you can use extensions to integrate your functions with your preferred monitoring, observability, security, and governance tools. </p>"},{"location":"aws/lambda/#concurrency","title":"Concurrency","text":"<p>Concurrency is the number of requests that your function is serving at any given time. When your function is invoked, Lambda provisions an instance of it to process the event. When the function code finishes running, it can handle another request. If the function is invoked again while a request is still being processed, another instance is provisioned, increasing the function's concurrency.</p> <p> Concurrency Example for Lambda <p></p>"},{"location":"aws/lambda/#throtlling","title":"Throtlling \u26a0\ufe0f","text":"<p>what is Throtlling?</p> <p>At the highest level, throttling just means that Lambda will intentionally reject one of your requests and so what we see from the user side is that when making a client call, Lambda will throw a throttling exception, which you need to handle. Typically, people handle this by backing off for some time and retrying. But there are also some different mechanisms that you can use, so that\u2019s interesting, Lambda will reject your request. </p> <p>Why does it occur? Throttling occurs when your <code>concurrent execution count</code> &gt; <code>concurrency limit</code>.</p> <p>Now, just as a reminder, if this wasn\u2019t clear, Lambda can handle multiple instance invocations at the same time and the sum of all of those invocations amounts to your concurrency execution count. So, assume we\u2019re at a particular instant in time if you have more invocations that are running that exceed your configured limit, all new requests to your Lambda function will get a throttling exception.</p> <p>What are the configure limits?</p> <p>Lambda has a default <code>1000 concurrency limit</code> that\u2019s specified per region within an account. But it does get a little bit more complicated in terms of how this rule applies when you have multiple Lambda functions in the same region and the same account.</p>"},{"location":"aws/lambda/#authentication-in-lambda","title":"Authentication in Lambda \ud83d\udd11","text":"<p>We have 3 options:</p>"},{"location":"aws/lambda/#use-cognito","title":"Use Cognito","text":"<p>As an alternative to using IAM roles and policies or Lambda authorizers (formerly known as custom authorizers), you can use an Amazon Cognito user pool to control who can access your API in Amazon API Gateway. </p>"},{"location":"aws/lambda/#use-iam","title":"Use IAM","text":"<p>IAM is best for clients that are within your AWS environment or can otherwise retrieve IAM temporary credentials to access your environment.</p>"},{"location":"aws/lambda/#use-custom-authorizer","title":"Use Custom Authorizer","text":"<p>Use this when you are using other IDP such as AAD</p> <p> Custom Auth Workflow <p></p>"},{"location":"aws/lambda/#function-url","title":"Function URL \ud83d\udd17","text":"<p>A function URL is a dedicated HTTP(S) endpoint for your Lambda function. You can create and configure a function URL through the Lambda console or the Lambda API. When you create a function URL, Lambda automatically generates a unique URL endpoint for you. Once you create a function URL, its URL endpoint never changes. Function URL endpoints have the following format:</p> <pre><code>https://&lt;url-id&gt;.lambda-url.&lt;region&gt;.on.aws\n</code></pre> <p>Info</p> <p>Function URLs are dual stack-enabled, supporting IPv4 and IPv6.</p>"},{"location":"aws/lambda/#code-editor","title":"Code editor","text":"<p>The <code>code editor</code> supports languages that do not require compiling, such as <code>Node.js</code> and <code>Python</code>. The code editor suppports only <code>.zip</code> archive deployment packages, and the size of the deployment package must be less than 3 MB.</p>"},{"location":"aws/lambda/#lambda-and-private-link","title":"Lambda and Private Link","text":"<ul> <li>To establish a private connection between your VPC and Lambda, create an interface VPC endpoint.</li> <li>Interface endpoints are powered by <code>AWS PrivateLink</code> Icon-Architecture/64/Arch_AWS-PrivateLink_64Created with Sketch., which enables you to privately access Lambda APIs without an internet gateway, NAT device, VPN connection, or AWS Direct Connect connection. </li> </ul>"},{"location":"aws/lambda/#state-store-not-data-store","title":"State Store (not Data Store) \ud83d\uddf3\ufe0f","text":"<p>No information about state should be saved within the context of the function itself. Because your functions only exist when there is work to be done, it is particularly important for serverless applications to treat each function as stateless.</p> <p>Consider one of the following options for storing state data:</p> <ul> <li> <p><code>Amazon DynamoDB</code> Icon-Architecture/64/Arch_Amazon-DynamoDB_64Created with Sketch. is serverless and scales horizontally to handle your Lambda invocations. It also has single-millisecond latency, which makes it a great choice for storing state information. </p> </li> <li> <p><code>Amazon ElastiCache</code> may be less expensive than <code>DynamoDB</code> if you have to put your Lambda function in a VPC. </p> </li> <li> <p><code>Amazon S3</code> Icon-Architecture/64/Arch_Amazon-Simple-Storage-Service_64  can be used as an inexpensive way to store state data if throughput is not critical and the type of state data you are saving will not change rapidly.</p> </li> </ul>"},{"location":"aws/lambda/#aws-sam","title":"AWS SAM \ud83e\uddd9\u200d\u2640\ufe0f","text":"<p><code>AWS SAM</code> is an open-source framework for building serverless applications. It provides shorthand syntax to express functions, APIs, databases, and event source mappings. With just a few lines per resource, you can define the application you want and model it using YAML.</p> <p>You provide AWS SAM with simplified instructions for your environment and during deployment AWS SAM transforms and expands the AWS SAM syntax into AWS CloudFormation syntax (a fully detailed CloudFormation template)</p> <p> SAM Framework (extension of CloudFormation) <p></p> <p>Testing made easy with SAM</p> <p><code>AWS SAM CLI</code> launches a Docker container that you can interact with to test and debug your Lambda function code before deploying to the cloud.With AWS SAM CLI for testing, you can do the following:</p> <ul> <li>Invoke functions and run automated tests locally.</li> <li>Generate sample event source payloads.</li> <li>Run API Gateway locally.</li> <li>Debug code.</li> <li>Review Lambda function logs.</li> <li>Validate AWS SAM templates.</li> </ul>"},{"location":"aws/lambda/#deploy-using-sam","title":"Deploy using SAM","text":"<p> AWS SAM Workflow <p></p> <ul> <li>You begin by writing your Lambda function code and defining all of your serverless resources inside an AWS SAM template.</li> <li>You can use the SAM CLI to emulate the Lambda environment and perform local tests on your Lambda functions. </li> <li>After the code and templates are validated, you can then use the SAM package command to create a deployment package, which is essentially a <code>.zip file</code> that SAM stores in Amazon S3. </li> <li>After that, the SAM deploy command instructs AWS CloudFormation to deploy the .zip file to create resources inside of your AWS console.</li> </ul>"},{"location":"aws/lambda/#pricing","title":"Pricing \ud83e\udd11","text":"<p>Price depends on the amount of memory you allocate to your function, not the amount of memory your function uses. If you allocate 10 GB to a function and the function only uses 2 GB, you are charged for the 10 GB. This is another reason to test your functions using different memory allocations to determine which is the most beneficial for the function and your budget. </p>"},{"location":"aws/lambda/#lambda-monitoring","title":"Lambda Monitoring \ud83d\udcca","text":""},{"location":"aws/lambda/#cloud-watch","title":"Cloud Watch \u231a\ufe0f","text":"<p>AWS Lambda automatically monitors Lambda functions on your behalf and reports metrics through Amazon CloudWatch. To help you monitor your code when it runs, Lambda automatically tracks the following:</p> <ul> <li>Number of requests: The number of times your function code is run, including successful runs and runs that result in a function error. If the invocation request is throttled or otherwise resulted in an invocation error, invocations aren't recorded. </li> <li>Invocation duration per request</li> <li>Number of requests that result in an error</li> </ul>"},{"location":"aws/lambda/#xray","title":"Xray \ud83e\uddb4","text":"<p>You can use X-Ray for: - Tuning performance - Identifying the call flow of Lambda functions and API calls - Tracing path and timing of an invocation to locate bottlenecks and failures</p> <p>  XRAY example for Lambda <p></p>"},{"location":"aws/lambda/#dlq","title":"DLQ \u2620\ufe0f","text":"<p><code>Dead-letter queues (DLQ)</code> help you capture application errors that must receive a response, such as an ecommerce application that processes orders. If an order fails, you cannot ignore that order error. You move that error into the dead-letter queue and manually look at the queue and fix the problems. </p>"},{"location":"aws/lambda/#performance-tuning","title":"Performance tuning \u2699\ufe0f","text":""},{"location":"aws/lambda/#concurrency-limits","title":"Concurrency limits \ud83d\udcc8","text":"<p>Account concurrency limits are in place to help avoid unexpected consequences if you have a \"function gone wild\" scenario. For example, a runaway function could incur very high costs, and impact the performance of downstream resources.</p> <p>You can also set function concurrency limits to reserve a part of your account limit pool for a function. You might do this to guarantee concurrency availability for a function, or to avoid overwhelming a downstream resource that your function is interacting with.</p> <p>Competing functions</p> <p>If your test results uncover situations where functions from different applications or different environments are competing with each other for concurrency, you probably need to rethink your account segregation strategy and consider moving to a multi-account strategy. You don\u2019t want to end up with a non-production function impacting production functions because they\u2019re in the same account.</p>"},{"location":"aws/lambda/#memory-config","title":"Memory config","text":"<p>When you configure functions, there is only one setting that can impact performance\u2014memory. However, both CPU and I/O scale linearly with memory configuration. For example, a function with 256 MB of memory has twice the CPU and I/O as a 128 MB function.</p> <p>Memory assignment will impact how long your function runs and, at a larger scale, can impact when functions are throttled.</p>"},{"location":"aws/lambda/#alias","title":"Alias","text":"<ul> <li>A Lambda alias is a pointer to a specific function version. </li> <li>By default, an alias points to a single Lambda version.</li> <li>When the alias is updated to point to a different function version, all incoming request traffic will be redirected to the updated Lambda function version.</li> </ul> <p> Alias Deployment <p></p>"},{"location":"aws/lb/","title":"Elastic Load Balancer","text":"<ul> <li>ELB is HA service managed by AWS</li> <li>Elastic Load Balancing automatically distributes your incoming traffic across multiple targets, such as EC2 instances, containers, and IP addresses, in one or more Availability Zones.</li> <li>It monitors the health of its registered targets, and routes traffic only to the healthy targets.</li> <li>Elastic Load Balancing scales your load balancer capacity automatically in response to changes in incoming traffic.</li> </ul> <p>Load Balancer and Auto Scaling group work hand in hand</p> <p><code>Elastic Load Balancing</code> is the AWS service that automatically distributes incoming application traffic across multiple resources, such as Amazon EC2 instances.</p> <p>A load balancer acts as a single point of contact for all incoming web traffic to your <code>Auto Scaling group</code>. This means that as you add or remove Amazon EC2 instances in response to the amount of incoming traffic, these requests route to the load balancer first. Then, the requests spread across multiple resources that will handle them.</p>"},{"location":"aws/lb/#types-of-elb","title":"Types of ELB","text":""},{"location":"aws/lb/#classic-lb","title":"Classic LB","text":"<p><code>Classic load balancers</code> support both IPv4 and IPv6. They support HTTP/1 and HTTP/1.1, but only application load balancers support HTTP/2. </p> <p>Why not to use Classic LB?</p> <p>You must register individual instances, rather than target groups, with classic load balancers; registering target groups is a functionality only available with application load balancers.</p> <p>An ELB is an elastic load balancer and generally refers to a classic load balancer. An ALB is an application load balancer.</p> <p>Classic load balancers operate at both the connection (Level 4) and the request (Level 7) layer of the TCP stack.</p>"},{"location":"aws/lb/#application-lb-l7","title":"Application LB (L7)","text":"<p>An ALB offers SSL termination and makes the SSL offload process very simple through tight integration with SSL processes. While an ELB will handle SSL termination, it does not offer the management features that ALBs do.</p>"},{"location":"aws/lb/#network-lb-l4","title":"Network LB (L4)","text":"<ul> <li>A Network Load Balancer functions at the 4<sup>th</sup> layer of the Open Systems Interconnection (OSI) model.</li> <li>It can handle <code>millions</code> of requests per second.</li> <li>After the <code>load balancer</code> receives a connection request, it selects a <code>target</code> from the <code>target group</code> for the default rule. It attempts to open a <code>TCP connection</code> to the selected target on the port specified in the listener configuration.</li> <li>By default, each load balancer node distributes traffic across the registered targets in its Availability Zone only. If you enable cross-zone load balancing, each load balancer node distributes traffic across the registered targets in all enabled Availability Zones. </li> <li>For TCP traffic, the load balancer selects a target using a flow hash algorithm based on the protocol, source IP address, source port, destination IP address, destination port, and TCP sequence number. The TCP connections from a client have different source ports and sequence numbers, and can be routed to different targets. Each individual TCP connection is routed to a single target for the life of the connection.</li> <li>For UDP traffic, the load balancer selects a target using a flow hash algorithm based on the protocol, source IP address, source port, destination IP address, and destination port. A UDP flow has the same source and destination, so it is consistently routed to a single target throughout its lifetime. Different UDP flows have different source IP addresses and ports, so they can be routed to different targets.</li> </ul> <p> Network Load Balancer <p> </p>"},{"location":"aws/lb/#components-of-elb","title":"Components of ELB","text":"<p> components of ELB <p></p>"},{"location":"aws/lb/#listener","title":"Listener","text":"<p>A listener checks for connection requests from clients, using the <code>protocol</code> and <code>port</code> that you configure.</p> <p>Where is your certificate?</p> <p>To create an <code>HTTPS listener</code>, you must deploy at least one SSL server certificate on your load balancer. The load balancer uses a server certificate to terminate the front-end connection and then decrypt requests from clients before sending them to the targets.</p> <p>You must also specify a security policy, which is used to negotiate secure connections between clients and the load balancer.</p>"},{"location":"aws/lb/#rule","title":"Rule","text":"<p>The rules that you define for a listener determine how the load balancer routes requests to its registered targets. </p> <p>Each rule consists of a <code>priority</code>, one or more actions, and one or more conditions. When the conditions for a rule are met, then its actions are performed. You must define a default rule for each listener, and you can optionally define additional rules.</p>"},{"location":"aws/lb/#target-target-group","title":"Target/ Target Group","text":"<p>Each <code>target group</code> routes requests to one or more registered targets, such as EC2 instances, using the protocol and port number that you specify. </p> <p>You can register a target with <code>multiple target groups</code>. You can configure <code>health checks</code> on a per target group basis. Health checks are performed on all targets registered to a target group that is specified in a listener rule for your load balancer.</p>"},{"location":"aws/lightsail/","title":"Lightsail","text":"<p>Amazon Lightsail is the easiest way to get started with Amazon Web Services (AWS) for anyone who needs to build websites or web applications.</p> <p>It includes everything you need to launch your project quickly\u2014instances (virtual private servers), container services, managed databases, content delivery network (CDN) distributions, load balancers, SSD-based block storage, static IP addresses, DNS management of registered domains, and resource snapshots (backups)\u2014for a low, predictable monthly price.</p>"},{"location":"aws/macie/","title":"Macie","text":"<ul> <li><code>Amazon Macie</code> is a security service that uses machine learning to automatically discover, classify, and protect sensitive data in AWS. </li> <li>Amazon Macie recognizes sensitive data such as personally identifiable information (PII) or intellectual property and provides you with dashboards and alerts that give visibility into how this data is being accessed or moved.</li> <li>The fully managed service continuously monitors data access activity for anomalies and generates detailed alerts when it detects the risk of unauthorized access or inadvertent data leaks. Amazon Macie is available to protect data stored in Amazon S3.</li> <li>Used AI to analyze data in S3 to find sensitive data.</li> <li>Analyze cloud trail logs to find suspicious activity. </li> <li>It has dashboards and reporting features.</li> <li>Used to prevent ID theft.</li> </ul>"},{"location":"aws/natgw/","title":"NAT Gateway","text":"<ul> <li> <p>A NAT gateway is a <code>Network Address Translation (NAT)</code> service. </p> </li> <li> <p>You can use a NAT gateway so that instances in a <code>private subnet</code> can connect to services outside your VPC but external services cannot initiate a connection with those instances.</p> </li> </ul> <p>When you create a NAT gateway, you specify one of the following connectivity types:</p> <ul> <li> <p><code>Public</code> \u2013 (Default) Instances in private subnets can connect to the internet through a public NAT gateway, but cannot receive unsolicited inbound connections from the internet. You create a public NAT gateway in a public subnet and must associate an elastic IP address with the NAT gateway at creation. You route traffic from the NAT gateway to the internet gateway for the VPC. Alternatively, you can use a public NAT gateway to connect to other VPCs or your on-premises network. In this case, you route traffic from the NAT gateway through a transit gateway or a virtual private gateway.</p> </li> <li> <p><code>Private</code> \u2013 Instances in private subnets can connect to other VPCs or your on-premises network through a private NAT gateway. You can route traffic from the NAT gateway through a transit gateway or a virtual private gateway. You cannot associate an elastic IP address with a private NAT gateway. You can attach an internet gateway to a VPC with a private NAT gateway, but if you route traffic from the private NAT gateway to the internet gateway, the internet gateway drops the traffic.</p> </li> </ul> <p>Gateway and router difference</p> <p>A gateway connects networks, and a router delivers data within a network. Gateways and routers are usually separate devices. However, it's becoming more common for their functions to be combined in a router. For example, in your home network, your router can also be your default gateway. </p> <p>NAT Gateway is a highly available AWS managed service that makes it easy to connect to the Internet from instances within a private subnet in an Amazon Virtual Private Cloud (Amazon VPC). Previously, you needed to launch a NAT instance to enable NAT for instances in a private subnet. </p> <p>Remember</p> <p>A <code>NAT gateway</code> is preferable to a <code>NAT instance</code> because it is managed by AWS rather than you, the architect.</p>"},{"location":"aws/natinstance/","title":"NAT Instance","text":""},{"location":"aws/networking/","title":"Networking","text":"<p> Various networking services <p></p>"},{"location":"aws/networking/#shared-responsibility","title":"Shared responsibility","text":"<p> Shared Responsibility Model <p></p> <p>The shared responsibility model balances the agility of your network with the need to improve the security of the data as it traverses your network using network controls, configurations, and so on.</p> <p>\ud83d\udcb0 No inbound charge</p> <p>There is no charge for inbound data transfer across all services in all Regions. However, data transfer from AWS to the internet is charged per service, with rates specific to the originating Region.</p>"},{"location":"aws/opensearch/","title":"OpenSearch","text":""},{"location":"aws/orgs/","title":"AWS Org","text":"<p>The following diagram shows a basic organization that consists of five accounts that are organized into four organizational units (OUs) under the root. The organization also has several policies that are attached to some of the OUs or directly to accounts. </p> <p> </p> <p>A simplified OU Tree is shown below</p> <p> </p> <ul> <li>AWS Orgs used to <code>centrally manage</code> the AWS account using the <code>Org Units</code> (OU).</li> <li>When you create an organization, AWS Organizations automatically creates a root, which is the parent container for all the accounts in your organization. </li> <li>You can get one bill per AWS account.</li> <li>Here the <code>star account</code> \u2b50 is the master account.</li> <li>Enable/ Disable the services on AWS using the <code>Service Control Policies</code> (SCP).</li> </ul> <p>Do not use the paying account to deploy the resources.</p>"},{"location":"aws/orgs/#root","title":"Root \ud83c\udf34","text":"<p>The <code>parent container</code> for all the accounts for your organization. </p> <p>Remember!</p> <p>If you apply an <code>authorization policy</code> to the root, it applies to all organizational units (OUs) and member accounts in the organization but If you apply a <code>management policy</code> to the root, it applies to all organizational units (OUs) and accounts including the management account in the organization.</p>"},{"location":"aws/orgs/#organizational-units-ou","title":"Organizational Units (OU) \ud83c\udfdb\ufe0f","text":"<p>In <code>AWS Organizations</code>, you can group <code>accounts</code> into <code>OUs</code> to make it easier to manage accounts with similar business or security requirements.</p> <p>When you apply a policy to an OU, all the accounts in the OU automatically inherit the permissions specified in the policy.  </p>"},{"location":"aws/orgs/#account","title":"Account \ud83c\udd94","text":"<p>An account in Organizations is a <code>standard AWS account</code> that contains your AWS resources and the identities that can access those resources. </p> <p> </p> <p>Warning</p> <p>An AWS account isn't the same thing as a user account. An AWS user is an identity that you create using AWS Identity and Access Management (IAM) and takes the form of either an IAM user with long-term credentials, or an IAM role with short-term credentials. A single AWS account can, and typically does contain many users and roles.</p>"},{"location":"aws/orgs/#scp","title":"SCP \ud83d\udc6e\u200d\u2640\ufe0f","text":"<p><code>Service control policies (SCPs)</code> are a type of organization policy that you can use to manage permissions in your organization. SCPs offer central control over the maximum available permissions for the IAM users and IAM roles in your organization. SCPs help you to ensure your accounts stay within your organization\u2019s access control guidelines.</p> <p>SCPs are available only in an organization that has all features enabled. SCPs aren't available if your organization has enabled only the consolidated billing features. </p> <p>SCP's dont grant permissions and they are just GuardRails\ud83d\udc82</p> <p>SCPs do not grant permissions to the IAM users and IAM roles in your organization. No permissions are granted by an SCP. An SCP defines a permission guardrail, or sets limits, on the actions that the IAM users and IAM roles in your organization can perform. </p> <p> </p> <p>SCP has overriding precedence as shown below (on S3 bucket)</p> <p> </p>"},{"location":"aws/outposts/","title":"Outposts","text":"<p>AWS Outposts is a fully managed service that extends AWS infrastructure, services, APIs, and tools to customer premises. </p> <p>By providing local access to AWS managed infrastructure, AWS Outposts enables customers to build and run applications on premises using the same programming interfaces as in AWS Regions, while using local compute and storage resources for lower latency and local data processing needs.</p> <p>Outposts solutions allow you to extend and run native AWS services on premises, and is available in a variety of form factors, from <code>1U</code> and <code>2U</code> Outposts servers to <code>42U</code> Outposts racks, and multiple rack deployments.</p>"},{"location":"aws/outposts/#key-concepts","title":"Key concepts","text":"<p>These are the key concepts for AWS Outposts.</p>"},{"location":"aws/outposts/#outpost-site","title":"Outpost site \ud83c\udfe2","text":"<p>The <code>customer-managed physical buildings</code> where AWS will install your Outpost. A site must meet the facility, networking, and power requirements \ud83d\udd0b for your Outpost.</p>"},{"location":"aws/outposts/#outpost-capacity","title":"Outpost capacity \ud83e\uddfa","text":"<p>Compute and storage resources available on the Outpost. You can view and manage the capacity for your Outpost from the AWS Outposts console.</p>"},{"location":"aws/outposts/#outpost-equipment","title":"Outpost equipment \u2699\ufe0f","text":"<p>Physical hardware that provides access to the AWS Outposts service. The hardware includes racks, servers, switches, and cabling owned and managed by AWS.</p>"},{"location":"aws/outposts/#outposts-racks","title":"Outposts racks \ud83d\udda5\ufe0f","text":"<p>An Outpost form factor that is an industry-standard <code>42U rack</code>. Outpost racks include rack-mountable servers, switches, a network patch panel, a power shelf and blank panels.</p>"},{"location":"aws/outposts/#outposts-ace-racks","title":"Outposts ACE racks \ud83c\udf9b\ufe0f","text":"<p>The <code>Aggregation, Core, Edge (ACE)</code> rack acts as a network aggregation point for multi-rack Outpost deployments. The ACE rack reduces the number of physical networking port and logical interface requirements by providing connectivity between multiple Outpost compute racks in your logical Outposts and your on-premise network.</p> <p>You must install an ACE rack if you have five or more compute racks. If you have less than five compute racks but plan to expand to five or more racks in the future, we recommend that you install an ACE rack at the earliest.</p>"},{"location":"aws/outposts/#outposts-servers","title":"Outposts servers \ud83d\uddb2\ufe0f","text":"<p>An Outpost form factor that is an industry-standard 1U or 2U server, which can be installed in a standard EIA-310D 19 compliant 4 post rack. Outpost servers provide local compute and networking services to sites that have limited space or smaller capacity requirements.</p>"},{"location":"aws/outposts/#service-link","title":"Service link \ud83d\udd17","text":"<p>Network route that enables communication between your Outpost and its associated AWS Region. Each Outpost is an extension of an Availability Zone and its associated Region.</p>"},{"location":"aws/outposts/#local-gateway-lgw","title":"Local gateway (LGW) \u26e9\ufe0f","text":"<p>A logical interconnect virtual router that enables communication between an Outpost rack and your on-premises network.</p>"},{"location":"aws/outposts/#local-network-interface","title":"Local network interface \ud83e\udd45","text":"<p>A network interface that enables communication from an Outpost server and your on-premises network.</p>"},{"location":"aws/parameter-store/","title":"Parameter Store","text":"<p>Why to use Param Store?</p> <p><code>AWS Systems Manager</code> has an additional capability called <code>Parameter Store</code> that provides you with secure, hierarchical storage for configuration data management and secrets management. You can store data such as passwords, database strings, Amazon Machine Image (AMI) IDs, and license codes as parameter values.</p> <p>In addition, <code>Parameter Store</code> includes the following:</p> <ul> <li>Free, fully managed, centralized storage system for configuration data and secret management.</li> <li>Data can be stored in plain text or also encrypted with AWS Key Management System (AWS KMS).</li> <li>Parameter Store tracks all parameter changes through versioning, so if you need to roll back your deployment, you can also choose to use an earlier version of your configuration data.</li> </ul>"},{"location":"aws/private-link/","title":"Private Link","text":"<p>AWS PrivateLink establishes private connectivity between <code>Virtual Private Cloud (VPC)</code> and supported AWS services, services hosted by other AWS accounts, and <code>supported AWS Marketplace services</code>.</p> <p>You do not need to use an internet gateway, NAT device, AWS Direct Connect connection, or AWS Site-to-Site VPN connection to communicate with the service.</p> <p>How to create Private Link</p> <p>To use AWS PrivateLink, create a VPC endpoint in your VPC, specifying the name of the service and a subnet. This creates an elastic network interface in the subnet that serves as an entry point for traffic destined to the service.</p> <p>You can also create your own VPC endpoint service, powered by AWS PrivateLink and enable other AWS customers to access your service. PrivateLink enables the creation of private API endpoints, allowing organizations to expose their own services securely to other AWS customers. </p>"},{"location":"aws/ram/","title":"RAM","text":"<p>In a nutshell, the RAM service enables you to share AWS resources developed in a single AWS account with multiple AWS accounts. They could be accounts from your company, organisational units (OUs), or even accounts from third parties. </p> <p>AWS Resource Access Manager, or RAM, supports the sharing of the Subnets, transit gateways, resolver rules, capacity reservations, license configurations, Aurora DB clusters, and traffic mirror targets.</p>"},{"location":"aws/ram/#how-to-share","title":"How to share? \ud83d\udd17","text":"<p>AWS RAM service that allows you to simply and securely share AWS resources with any AWS account or, if you are a member of AWS Organizations, with Organizational Units (OUs) or your entire organization. If you share resources with accounts outside of your Organization, those accounts will receive an invitation to the Resource Share and can begin using the shared resources after they accept the invitation.</p> <ul> <li>Only the master account has the ability to share with AWS Organizations.</li> <li>All functionalities must be enabled for the organisation.</li> </ul>"},{"location":"aws/rds/","title":"RDS","text":"<p>Tldr</p> <ul> <li>Amazon <code>Relational Database Service</code> is a service that enables you to run relational databases in the AWS Cloud.</li> <li>Amazon RDS is a managed service that automates tasks such as hardware provisioning, database setup, patching, and backups.</li> <li>With these capabilities, you can spend less time completing administrative tasks and more time using data to innovate your applications. </li> </ul>"},{"location":"aws/rds/#rds-offerings","title":"RDS offerings \ud83d\udd4a\ufe0f","text":"<p>Amazon RDS is available on 6 database engines, which optimize for memory, performance, or input/output (I/O). Supported database engines include:</p> <ul> <li>Amazon Aurora</li> <li>PostgreSQL</li> <li>MySQL</li> <li>MariaDB</li> <li>Oracle Database</li> <li>Microsoft SQL Server</li> </ul> <p>DB Instance A DB instance is an isolated database environment in the AWS Cloud. The basic building block of Amazon RDS is the DB instance. Your DB instance can contain one or more user-created databases. The following diagram shows a virtual private cloud (VPC) that contains two Availability Zones, with each AZ containing two DB instances.</p> <p> DB Instance Example <p></p>"},{"location":"aws/rds/#rds-proxy","title":"RDS Proxy \u26a1\ufe0f","text":"<p>Many applications, including those built on modern serverless architectures, can have a large number of open connections to the database server and may open and close database connections at a high rate, exhausting database memory and compute resources.</p> <p>Amazon RDS Proxy allows applications to <code>pool</code> and <code>share connections</code> established with the database, improving database efficiency and application scalability. </p> <p>Handling surge and throttling requests?</p> <p>Using RDS Proxy, you can handle unpredictable surges in database traffic. Otherwise, these surges might cause issues due to oversubscribing connections or new connections being created at a fast rate. RDS Proxy establishes a database connection pool and reuses connections in this pool. This approach avoids the memory and CPU overhead of opening a new database connection each time. To protect a database against oversubscription, you can control the number of database connections that are created. </p> <p>RDS Proxy queues or throttles application connections that can't be served immediately from the connection pool. Although latencies might increase, your application can continue to scale without abruptly failing or overwhelming the database. If connection requests exceed the limits you specify, RDS Proxy rejects application connections (that is, it sheds load). At the same time, it maintains predictable performance for the load that RDS can serve with the available capacity. </p> <p> RDS Proxy Architecture <p></p>"},{"location":"aws/rds/#basics","title":"Basics \ud83d\udcdd","text":""},{"location":"aws/rds/#connection-pooling","title":"Connection Pooling \ud83c\udfca","text":"<p>Connection pooling is an optimization that enables applications to share and re-use database connections, thus reducing the load on the database itself. Opening and closing a new database connection is CPU-intensive whereas additional memory is needed for each open connection. Connection pooling also removes the need to worry about database connections in the application code.</p> <p>Each database transaction uses one underlying database connection which can be reused once the transaction has finished. This transaction-level reuse is called connection multiplexing (or connection reuse).</p>"},{"location":"aws/rds/#pinnning","title":"Pinnning \ud83d\udccc","text":"<p>In some cases, RDS proxy can\u2019t safely reuse a database connection outside of the current session. In such scenarios, the same connection is used for the session until the session ends. This behavior is called pinning.</p> <p>AWS recommends trying to avoid pinning as much as possible since it makes it harder to share connections and thus reduces the benefits of using RDS proxy.</p> <p>Some reasons why a connection might be pinned are:</p> <ul> <li>Change of session variable</li> <li>Change of configuration parameter</li> </ul>"},{"location":"aws/rds/#benefits","title":"Benefits \ud83d\udcb5","text":"<ul> <li> <p>Better Performance: Your Amazon RDS Proxy instance maintains a pool of established connections to your RDS database instances, reducing the stress on database compute and memory resources that typically occurs when new connections are established. RDS Proxy also shares infrequently used database connections, so that fewer connections access the RDS database. This connection pooling enables your database to efficiently support a large number and frequency of application connections so that your application can scale without compromising performance. </p> </li> <li> <p>Increased Availability: RDS Proxy minimizes application disruption from outages affecting the availability of your database by automatically connecting to a new database instance while preserving application connections. When failovers occur, RDS Proxy routes requests directly to the new database instance.</p> </li> </ul>"},{"location":"aws/rds/#monitor-rds-proxy","title":"Monitor RDS Proxy \ud83d\udd2d","text":"<p>RDS proxy can be monitored by using Amazon CloudWatch. CloudWatch is well integrated with RDS proxy and provides useful metrics that can be used to understanding the performance and behavior of the proxy.</p> <p>Some key metrics to keep an eye are:</p> <ul> <li> <p>DatabaseConnections: Number of database connections to the backend database</p> </li> <li> <p>DatabaseConnectionsCurrentlyBorrowed: Number of connections currently being used by your application. Important to set an alarm on this metric.</p> </li> <li> <p>DatabaseConnectionsCurrentlySessionPinned: Number of connections in the pinned state. This number should ideally be as low as possible to maximize RDS proxy performance.- </p> </li> </ul>"},{"location":"aws/rds/#aurora","title":"Aurora \ud83c\udf1f","text":"<p>Amazon Aurora is an enterprise-class relational database. It is compatible with MySQL and PostgreSQL relational databases. It is up to 5 times faster than <code>standard MySQL databases</code> and up to 3 times faster than <code>standard PostgreSQL databases</code>.</p> <p>Amazon Aurora helps to reduce your database costs by reducing unnecessary input/output (I/O) operations, while ensuring that your database resources remain reliable and available. </p> <p>Consider Amazon Aurora if your workloads require high availability. It replicates 6 copies of your data across 3 Availability Zones and continuously backs up your data to Amazon S3.</p>"},{"location":"aws/rds/#backup-and-restore","title":"Backup and Restore \ud83d\udce6","text":"<p>You can turn on <code>automated backups</code>, or manually create your own backup snapshots. You can use these backups to restore a database. The Amazon RDS restore process works reliably and efficiently.</p> <p>By default, Amazon RDS creates and saves automated backups of your DB instance securely in Amazon S3 for a user-specified retention period. In addition, you can create snapshots, which are user-initiated backups of your instance that are kept until you explicitly delete them. </p>"},{"location":"aws/rds/#automated-backups","title":"Automated Backups \ud83d\udefa","text":"<p>Turned on by default, the automated backup feature of Amazon RDS will backup your databases and transaction logs. Amazon RDS automatically creates a storage volume snapshot of your DB instance, backing up the entire DB instance and not just individual databases.  </p> <p>This backup occurs during a daily user-configurable <code>30 minute</code> period known as the backup window. Automated backups are kept for a configurable number of days (called the <code>backup retention period</code>). Your automatic backup retention period can be configured to up to <code>35 days</code>.</p>"},{"location":"aws/rds/#point-in-time-restores","title":"Point-in-time Restores \u23f0","text":"<p>You can restore your DB instance to any specific time during the <code>backup retention period</code>, creating a new DB instance. </p> <p>To determine the latest restorable time for a DB instance, use the AWS Console or Command Line Interface to look at the value returned in the <code>LatestRestorableTime field</code> for the DB instance. The latest restorable time for a DB instance is typically within 5 minutes of the current time.  </p>"},{"location":"aws/rds/#database-snapshots","title":"Database Snapshots \ud83d\udcf8","text":"<p>Database snapshots are user-initiated backups of your instance stored in Amazon S3 that are kept until you explicitly delete them. You can create a new instance from a database snapshots whenever you desire. Although database snapshots serve operationally as full backups, you are billed only for incremental storage use.</p> <p> RDS Snapshots <p></p> <p>If you copy a snapshot to a different region then you must ensure that the target region supports cross-region snapshot copies.</p> <p>Copy an encrypted database</p> <p> RDS Snapshots </p> <p>If you are working with an encrypted snapshot then there are a few things you need to be aware of.  If a snapshot is taken from an encrypted database, then the snapshot will also be encrypted through the use of a KMS key.  If you copy the snapshot to the same region as the source snapshot, then you can use the same KMS encryption key.  However, if you copy the encrypted snapshot to a different region, then during the copy configuration you will need to specify a new KMS key in the target region, this is because KMS is a regional service, and KMS keys only exist in one region.  </p> <p>As a part of the copy process, you can choose to encrypt an unencrypted snapshot by selecting a KMS key.  This provides the opportunity to create an encrypted version of an unencrypted database.  You can take your source database, create a snapshot, copy the snapshot, and select a new KMS key to encrypt it with, and then restore the database using that encrypted snapshot.  This will result in a new encrypted database instance.</p>"},{"location":"aws/rds/#snapshot-copies","title":"Snapshot Copies \ud83d\uddd2\ufe0f","text":"<p>With Amazon RDS, you can copy DB snapshots and DB cluster snapshots. You can copy automated or manual snapshots. After you copy a snapshot, the copy is a manual snapshot. You can copy a snapshot within the same AWS Region, you can copy a snapshot across AWS Regions, and you can copy a snapshot across AWS accounts.</p>"},{"location":"aws/rds/#snapshot-sharing","title":"Snapshot Sharing \ud83d\udd17","text":"<p>Using Amazon RDS, you can share a manual DB snapshot or DB cluster snapshot with other AWS accounts. Sharing a manual DB snapshot or DB cluster snapshot, whether encrypted or unencrypted, enables authorized AWS accounts to copy the snapshot.</p> <p>Sharing an unencrypted manual DB snapshot enables authorized AWS accounts to directly restore a DB instance from the snapshot instead of taking a copy of it and restoring from that. This isn't supported for encrypted manual DB snapshots.</p> <p>Sharing a manual DB cluster snapshot, whether encrypted or unencrypted, enables authorized AWS accounts to directly restore a DB cluster from the snapshot instead of taking a copy of it and restoring from that.</p>"},{"location":"aws/rds/#ha","title":"HA \ud83c\udf9b\ufe0f","text":"<p>You can get <code>high availability</code> with a primary DB instance and a synchronous secondary DB instance that you can fail over to when problems occur. You can also use read replicas to increase read scaling.</p> <p> Highly Available Architecture for RDS <p></p>"},{"location":"aws/rds/#multi-region-deployment","title":"Multi-Region deployment \ud83c\udf0d","text":"<p>The following scenario shows an RDS DB instance in one Region that replicates asynchronously to a standby DB instance in a different Region. If one Region becomes unavailable, the instance in the other Region is still available.</p> <p> HA in another region with Async replication for RDS <p></p>"},{"location":"aws/rds/#multi-az-deployment","title":"Multi-AZ deployment \ud83c\udf10","text":"<p>You can run your DB instance in several Availability Zones, an option called a Multi-AZ deployment. When you choose this option, Amazon automatically provisions and maintains one or more secondary standby DB instances in a different AZ. Your primary DB instance is replicated across Availability Zones to each secondary DB instance.</p> <p>A Multi-AZ deployment provides the following advantages:</p> <ul> <li>Providing data redundancy and failover support</li> <li>Eliminating I/O freezes </li> <li>Minimizing latency spikes during system backups</li> <li>Serving read traffic on secondary DB instances (Multi-AZ DB clusters deployment only)</li> </ul> <p> HA in same region with Sync replication <p></p>"},{"location":"aws/rds/#database-performance","title":"Database Performance \ud83d\udcca","text":"<ul> <li><code>Average Active Sessions (AAS)</code>: is the unit for the DBLoad metric in Performance Insights. It measures how many sessions are concurrently active on the database.</li> <li><code>Wait event</code>: It causes a SQL statement to wait for a specific event to happen before it can continue running. Wait events are an important dimension, or category, for DB load because they indicate where work is impeded.</li> </ul> <p>Every active session is either running on the CPU or waiting. For example, sessions consume CPU when they search memory for a buffer, perform a calculation, or run procedural code. When sessions aren't consuming CPU, they might be waiting for a memory buffer to become free, a data file to be read, or a log to be written to. The more time that a session waits for resources, the less time it runs on the CPU. </p> <p> Performance Insights Concepts <p></p> <p>Example of CPU saturation is shown below</p> <p> CPU Saturation Example <p></p>"},{"location":"aws/rds/#performace-insights","title":"Performace Insights","text":"<p>With the Performance Insights dashboard, you can visualize the database load on your Amazon RDS DB instance load and filter the load by waits, SQL statements, hosts, or users</p>"},{"location":"aws/readReplica/","title":"Read Replicas","text":"<ul> <li>A <code>multi-AZ instance</code> can not be promoted to a read-replica.</li> <li>There is no charge for primary-to-secondary data replication.</li> <li>Although it\u2019s not particularly common, you can set up a read replica in an on-premises instance. Additionally, read replicas are often created in separate regions from the primary instance, to improve performance for clients closer to different regions than the primary instance.</li> <li>Currently, read replicas in RDS are only supported by MariaDB, MySQL, and PostgreSQL. Not for DynamoDB</li> <li>Read replicas are updated via asynchronous replication\u2014  the most performant approach.</li> <li>Read replica setup only allows for five read replicas. This is not a limit that can be raised by AWS.</li> <li>Read replica has its own database engine active so that it can be promoted to a primary DB.</li> </ul>"},{"location":"aws/readReplica/#promoting-read-replica","title":"Promoting Read Replica","text":"<p>The read replica maintains a secure <code>asynchronous link</code> between itself and the primary database. At this point, read-only traffic can be directed to the read replica to serve queries, perhaps on business intelligence tools. </p> <p>By implementing read replicas, it helps to offload this traffic from the <code>primary instance</code>, and therefore, helping with the overall performance.</p> <p>Remember</p> <p>Do be aware when thinking about deploying read replicas that they are only available for MySQL, MariaDB, and PostgreSQL database engines. However, for the latest supported engines for read replicas, it is always best to consult the AWS documentation, as this can change over time. Thankfully, it is possible to deploy more than one read replica for a primary database, and there are a number of different reasons as to why you might want to do this. </p> <p>By adding more than one read replica, it allows you to scale your read performance to a wider range of tools and applications that need to query the data without being restricted to a single read replica. It is also possible to deploy a read replica in a different region, which significantly helps to enhance your DR capabilities. </p> <p> </p> <p>It's also possible to promote an existing read replica to replace the <code>primary database</code> in the event of an incident (as shown in the below diagram). Also, during any maintenance that is being perform on your primary instance where I/O requests may have been suspended, then read traffic can still be served by a read replica.</p> <p> </p> <p>Tip</p> <ul> <li>Multi-AZ setup is focused on disaster recovery and fault tolerance, while read replicas provide performance and scalability.</li> <li>You can manually promote (not automatically ) a read replica instance to a stand-alone instance if you have to.</li> <li>Read replicas do not create automatic backups, but the primary database instance must have automatic backups enabled to create read replicas.</li> <li>There is no particular performance increase in a Multi-AZ deployment unless read replicas are also turned on.</li> </ul>"},{"location":"aws/redshift/","title":"Redshift","text":"<p>Amazon Redshift is a data-warehousing service that you can use for big data analytics. It offers the ability to collect data from many sources and helps you to understand relationships and trends across your data.</p>"},{"location":"aws/redshift/#redshift-cluster","title":"Redshift Cluster","text":"<p>An Amazon Redshift cluster is a set of nodes, which consists of a <code>leader node</code> and one or more <code>compute nodes</code>. </p> <p>The type and number of compute nodes that you need depends on the size of your data, the number of queries you will run, and the query runtime performance that you need.</p> <p> Redshift Cluster <p></p>"},{"location":"aws/route53/","title":"Route 53","text":"<p>Route 53 has the ability to manage the <code>DNS records</code> for domain names. You can register new domain names directly in Route 53. You can also transfer DNS records for existing domain names managed by other domain registrars. This enables you to manage all of your domain names within a single location.</p> <p>Route 53 supports up to 50 domain names by default, but this limit can be raised if requested.</p> <p>A\u00a0<code>naked domain</code>\u00a0is a\u00a0DNS\u00a0name\u00a0that can't be a canonical\u00a0name\u00a0record (CNAME). An example is <code>hello.com</code>, without the <code>www</code> subdomain</p> <p>Type of DNS records</p> <ul> <li>A\u00a0record : It maps a name to one or more IPv4 addresses when the IP is known and stable. <pre><code>192.0.2.1\n</code></pre></li> </ul> <ul> <li>AAAA\u00a0record  : It maps a name to one or more IPv6 addresses</li> </ul> Example for the Amazon Route 53 consoleExample for the Amazon Route 53 API <pre><code>2001:0db8:85a3:0:0:8a2e:0370:7334\n</code></pre> <pre><code>&lt;Value&gt;2001:0db8:85a3:0:0:8a2e:0370:7334&lt;/Value&gt;\n</code></pre> <ul> <li>CNAME\u00a0record : It maps a hostname to another hostname. It should only be used when there are no other records on that name.</li> </ul> <p>Example</p> <p>A CNAME record maps DNS queries for the name of the current record, such as acme.example.com, to another domain (example.com or example.net) or subdomain (acme.example.com or zenith.example.org). </p> <ul> <li>ALIAS\u00a0record : It maps a name to another name but can coexist with other records on that name.</li> </ul> <ul> <li>MX: An MX record specifies the names of your mail servers and, if you have two or more mail servers, the priority order. Each value for an MX record contains two values, priority and domain name.</li> </ul> MX record example <pre><code>10 mail.example.com\n</code></pre> <ul> <li>SOA: A start of authority (SOA) record provides information about a domain and the corresponding Amazon Route 53 hosted zone. For information about the fields in an SOA record.</li> </ul> SOA Example <pre><code>ns-2048.awsdns-64.net hostmaster.awsdns.com 1 1 1 1 60\u00df\n</code></pre> <ul> <li>PTR: Getting reverse DNS going is done by finding the\u00a0PTR records\u00a0in use by a DNS server.\u00a0</li> </ul> <ul> <li> <p>TXT: Used to privide info in a text format to the systems outside of your domain. A TXT record contains one or more strings that are enclosed in double quotation marks (\"). When you use the simple routing policy, include all values for a domain (example.com) or subdomain (www.example.com) in the same TXT record.</p> <p>A single string can include up to 255 characters, including the following:</p> <ul> <li>a-z</li> <li>A-Z</li> <li>0-9</li> <li>Space</li> <li> <ul> <li>(hyphen)</li> </ul> </li> <li>! \" # $ % &amp; ' ( ) * + , - / : ; &lt; = &gt; ? @ [  ] ^ _ ` { | } ~ . </li> </ul> <p>If you need to enter a value longer than 255 characters, break the value into strings of 255 characters or fewer, and enclose each string in double quotation marks (\"). In the console, list all the strings on the same line:</p> <pre><code>\"String 1\" \"String 2\" \"String 3\"\n</code></pre> </li> </ul> <ul> <li>NS (NameServer): This\u00a0record\u00a0indicates which DNS server is <code>authoritative for that domain</code> (which server contains the actual DNS records). A domain will often have multiple\u00a0NS\u00a0records which can indicate primary and backup name servers for that domain.</li> </ul> <ul> <li>Alias Record: This is unique to AWS using which  maps requests to AWS resources.</li> </ul> <p>Some rules:</p> <ul> <li>The\u00a0A,\u00a0CNAME, and\u00a0ALIAS\u00a0records cause a name to resolve to an IP. Conversely, the\u00a0URL\u00a0record redirects the name to a destination. The\u00a0URL\u00a0record is a simple and effective way to apply a redirect for one name to another name, for example redirecting\u00a0<code>www.example.com</code>\u00a0to\u00a0example.com.</li> <li>The\u00a0A\u00a0name must resolve to an IP. The\u00a0CNAME\u00a0and\u00a0ALIAS\u00a0records must point to a name.</li> </ul>"},{"location":"aws/route53/#concepts","title":"Concepts","text":""},{"location":"aws/route53/#alias-record","title":"Alias record","text":"<p>A type of record that you can create with Amazon Route 53 to route traffic to AWS resources such as <code>Amazon CloudFront distributions</code> and <code>Amazon S3 buckets</code> </p>"},{"location":"aws/route53/#auth-name-server","title":"Auth Name Server","text":"<p>A name server that has definitive information about one part of the Domain Name System (DNS) and that responds to requests from a <code>DNS resolver</code> by returning the applicable information. </p> <p>Example</p> <p>An authoritative name server for the <code>.com top-level domain</code> (TLD) knows the names of the name servers for every registered <code>.com domain</code>. When a <code>.com</code> authoritative name server receives a request from a DNS resolver for <code>example.com</code>, it responds with the names of the name servers for the DNS service for the <code>example.com</code> domain.</p> <p>Route 53 name servers are the authoritative name servers for every domain that uses Route 53 as the DNS service. The name servers know how you want to route traffic for your domain and subdomains based on the records that you created in the hosted zone for the domain. (Route 53 name servers store the hosted zones for the domains that use Route 53 as the DNS service.)</p> <p>If a Route 53 name server receives a request for <code>www.example.com</code>, it finds that record and returns the IP address, such as 192.0.2.33, that is specified in the record.</p>"},{"location":"aws/route53/#cidr-block","title":"CIDR block \ud83c\udf9b\ufe0f","text":"<p>A CIDR block is an IP range used with <code>IP-based routing</code></p> <p>In Route 53 You can specify CIDR block from <code>/0</code> to <code>/24</code> for IPv4 and <code>/0</code> to <code>/48</code> for IPv6. For example, a <code>/24</code> IPv4 CIDR block includes 256 contiguous IP addresses. You can group sets of CIDR blocks (or IP ranges) into CIDR locations, which are in turn grouped into reusable CIDR collections.</p>"},{"location":"aws/route53/#dns-query","title":"DNS query","text":"<p>Usually a request that is submitted by a device, such as a computer or a smart phone, to the Domain Name System (DNS) for a resource that is associated with a domain name. The most common example of a DNS query is when a user opens a browser and types the domain name in the address bar. The response to a DNS query typically is the IP address that is associated with a resource such as a web server. The device that initiated the request uses the IP address to communicate with the resource. For example, a browser can use the IP address to get a web page from a web server.  DNS resolver</p> <p>A DNS server, often managed by an internet service provider (ISP), that acts as an intermediary between user requests and DNS name servers. When you open a browser and enter a domain name in the address bar, your query goes first to a DNS resolver. The resolver communicates with DNS name servers to get the IP address for the corresponding resource, such as a web server. A DNS resolver is also known as a recursive name server because it sends requests to a sequence of authoritative DNS name servers until it gets the response (typically an IP address) that it returns to a user's device, for example, a web browser on a laptop computer.</p>"},{"location":"aws/route53/#domain-name-system-dns","title":"Domain Name System (DNS)","text":"<p>A worldwide network of servers that help computers, smart phones, tablets, and other IP-enabled devices to communicate with one another. The Domain Name System translates easily understood names such as example.com into the numbers, known as IP addresses, that allow computers to find each other on the internet.</p>"},{"location":"aws/route53/#hosted-zone","title":"Hosted zone","text":"<p>A container for records, which include information about how you want to route traffic for a domain (such as example.com) and all of its subdomains (such as <code>www.example.com</code>, retail.example.com, and seattle.accounting.example.com). A hosted zone has the same name as the corresponding domain.</p> <p>Example</p> <p>The hosted zone for example.com might include a record that has information about routing traffic for <code>www.example.com</code> to a web server that has the IP address 192.0.2.243, and a record that has information about routing email for example.com to two email servers, mail1.example.com and mail2.example.com. Each email server also requires its own record.</p>"},{"location":"aws/route53/#ip-address","title":"IP address","text":"<p>A number that is assigned to a device on the internet\u2014such as a laptop, a smart phone, or a web server\u2014that allows the device to communicate with other devices on the internet. IP addresses are in one of the following formats:</p> <ul> <li>Internet Protocol version 4 (IPv4) format, such as 192.0.2.44</li> <li>Internet Protocol version 6 (IPv6) format, such as 2001:0db8:85a3:0000:00000001:2345</li> </ul> <p>Route 53 supports both IPv4 and IPv6 addresses for the following purposes:</p> <ol> <li>You can create records that have a type of A, for IPv4 addresses, or a type of AAAA, for IPv6 addresses.</li> <li>You can create health checks that send requests either to IPv4 or to IPv6 addresses.</li> <li>If a DNS resolver is on an IPv6 network, it can use either IPv4 or IPv6 to submit requests to Route 53.</li> </ol>"},{"location":"aws/route53/#name-servers","title":"Name servers","text":"<p>Servers in the Domain Name System (DNS) that help to translate domain names into the IP addresses that computers use to communicate with one another. Name servers are either recursive name servers (also known as DNS resolver) or authoritative name server.</p>"},{"location":"aws/route53/#private-dns","title":"Private DNS","text":"<p>A local version of the Domain Name System (DNS) that lets you route traffic for a domain and its subdomains to Amazon EC2 instances within one or more Amazon virtual private clouds (VPCs). </p>"},{"location":"aws/route53/#record-dns-record","title":"Record (DNS record)","text":"<p>An object in a hosted zone that you use to define how you want to route traffic for the domain or a subdomain. For example, you might create records for <code>example.com</code> and <code>www.example.com</code> that route traffic to a web server that has an IP address of 192.0.2.234.</p>"},{"location":"aws/route53/#routing-policy","title":"Routing policy","text":"<p>A setting for records that determines how Route 53 responds to DNS queries. Route 53 supports the following routing policies:</p> <ol> <li> <p>Simple routing policy \u2013 Use to route internet traffic to a single resource that performs a given function for your domain, for example, a web server that serves content for the example.com website.</p> </li> <li> <p>Failover routing policy \u2013 Use when you want to configure active-passive failover.</p> </li> <li> <p>Geolocation routing policy \u2013 Use when you want to route internet traffic to your resources based on the location of your users.</p> </li> <li> <p>Geoproximity routing policy \u2013 Use when you want to route traffic based on the location of your resources and, optionally, shift traffic from resources in one location to resources in another.</p> </li> <li> <p>Latency routing policy \u2013 Use when you have resources in multiple locations and you want to route traffic to the resource that ==provides the best latency=.</p> </li> <li> <p>IP-based routing policy \u2013 Use when you want to route traffic based on the location of your users, and have the IP addresses that the traffic originates from.</p> </li> <li> <p>Multivalue answer routing policy \u2013 Use when you want Route 53 to respond to DNS queries with up to 8 healthy records selected at random.</p> </li> <li> <p>Weighted routing policy \u2013 Use to route traffic to multiple resources in proportions that you specify.</p> </li> </ol>"},{"location":"aws/route53/#subdomain","title":"Subdomain","text":"<p>A domain name that has one or more labels prepended to the registered domain name. For example, if you register the domain name example.com, then <code>www.example.com</code> is a subdomain. If you create the hosted zone accounting.example.com for the example.com domain, then seattle.accounting.example.com is a subdomain.</p>"},{"location":"aws/route53/#time-to-live-ttl","title":"Time to live (TTL)","text":"<p>The amount of time, in seconds, that you want a DNS resolver to cache (store) the values for a record before submitting another request to Route 53 to get the current values for that record. If the DNS resolver receives another request for the same domain before the TTL expires, the resolver returns the cached value.</p> <p>A longer TTL reduces your Route 53 charges, which are based in part on the number of DNS queries that Route 53 responds to. A shorter TTL reduces the amount of time that DNS resolvers route traffic to older resources after you change the values in a record, for example, by changing the IP address for the web server for <code>www.example.com</code>.</p>"},{"location":"aws/s3/","title":"S3   Icon-Architecture/64/Arch_Amazon-Simple-Storage-Service_64","text":"<ul> <li> <p>It is object-based storage.</p> </li> <li> <p>Files can be from 0 bytes to 5 TB.</p> </li> <li> <p>Buckets are created in a <code>region</code>, not in an <code>AZ</code>. It means that S3 is a region-based and fully managed service.</p> </li> <li> <p>A bucket is a folder.</p> </li> <li> <p>The object has a:</p> <ul> <li><code>key</code></li> <li><code>value</code> (5 Tb max size)</li> <li><code>version id</code></li> <li><code>metadata</code></li> <li><code>subresources</code> (such as ACL and bucket policy)</li> </ul> </li> <li> <p>S3 has a flat structure but we can create a directory by using the prefixes.</p> </li> <li> <p>It has <code>read after write</code> consistency.</p> </li> <li> <p><code>Eventual consistency</code> for overwriting.</p> </li> <li> <p>It has a simple web service interface.</p> </li> <li> <p>A <code>multipart upload</code> allows an application to upload a large object as a set of smaller parts uploaded in parallel. Upon completion, S3 combines the smaller pieces into the original larger object.</p> </li> <li> <p>We can use tags in objects to group various objects and later retrieve them.</p> </li> <li> <p>S3 is a <code>restful web service</code>.</p> </li> <li> <p>Tags can be used in <code>cloud-trail</code>, <code>cloud-watch</code>, and <code>lifecycle management</code>, etc.</p> </li> <li> <p>The lifecycle of tiers means we can move them from one tier to another tier (this can be automated as well).</p> </li> <li> <p>Use <code>MFA</code> for delete to make sure someone else does not delete data in bucket.</p> </li> <li> <p><code>ACL</code> is used to grant fine-grained access on bucket.</p> </li> <li> <p><code>First-byte latency</code> is the time between requesting an object from the service and when that data starts to arrive. With S3 (for example) that time is measured in milliseconds and in many cases could be considered instant.</p> </li> <li> <p><code>Cross-region replication</code> will replicate the data across the regions.</p> </li> </ul> <p> </p> <ul> <li>Users can save data to the edge locations in case of <code>transfer-acceleration</code>.</li> </ul> <p>Can I install OS?</p> <p>We can not install an operating system on S3.</p> <ul> <li> <p>We can create up to 100 buckets by default in one account.</p> </li> <li> <p>Buckets have <code>sub-resources</code>; which are the resources that can not exist on its own.</p> </li> </ul> <p>What are we geting charged for?</p> <ul> <li>Object tagging</li> <li>Storage</li> <li>Requests</li> <li>Transfer acceleration</li> </ul> <p>Remember</p> <ul> <li><code>Standard IA</code> will charge every time we are going to use the data. It is usually used for backups etc. </li> <li><code>Glacier</code> has minimum storage for 3 months and <code>Glacier deep archive</code> has it for a minimum of 6 months.</li> <li>For <code>Intelligent tiering</code>, it will put the data automatically to <code>IA</code> if it is not used for 30 days and there will be no archival feel associated with it as well (special case).</li> <li>Object size less than 128 kb will not be moved by using the Intelligent tiering, it will remain in the standard tier.</li> <li>S3 is a universal namespace, so it has to be unique.</li> </ul>"},{"location":"aws/s3/#s3-storage-tiers","title":"S3 storage tiers \ud83e\udea3","text":"<p> S3 Lifecycle Layers <p></p>"},{"location":"aws/s3/#standard","title":"Standard","text":"<p>Standard S3 is the most expensive tier.</p> <p>Designed for frequently accessed data and it stores data in a minimum of 3 Availability Zones</p>"},{"location":"aws/s3/#infrequently-accessed-standard-ia","title":"Infrequently Accessed (Standard-IA)","text":"<p>Similar to Amazon <code>S3 Standard</code> but has a lower storage price and higher retrieval price.</p>"},{"location":"aws/s3/#one-zone","title":"One zone \ud83c\udfaf","text":"<p>It costs less as we use the only 1 zone to keep data. </p> <p>Tip</p> <p>All other tiers except one-zone replicate data in 3 or more AZ's</p>"},{"location":"aws/s3/#one-zone-ia","title":"One zone -IA \ud83d\udce6","text":"<p>S3 One Zone-IA is intended for use cases with infrequently accessed data that is re-creatable, such as storing secondary backup copies of on-premises data or for storage that is already replicated in another AWS Region for compliance or disaster recovery purposes.</p>"},{"location":"aws/s3/#intelligent-tiering","title":"Intelligent tiering \ud83e\udde0","text":"<ul> <li>It will use ML to move data between tiers.</li> <li>Requires a small monthly monitoring and automation fee per object</li> <li>In the S3 Intelligent-Tiering storage class, Amazon S3 monitors objects\u2019 access patterns. If you haven\u2019t accessed an object for 30 consecutive days, Amazon S3 automatically moves it to the infrequent access tier, S3 Standard-IA. If you access an object in the infrequent access tier, Amazon S3 automatically moves it to the frequent access tier, S3 Standard.</li> </ul>"},{"location":"aws/s3/#s3-glacier-instant-retrieval","title":"S3 Glacier Instant Retrieval \u2603\ufe0f","text":"<p>You can retrieve objects stored in the S3 Glacier Instant Retrieval storage class within milliseconds, with the same performance as S3 Standard.</p>"},{"location":"aws/s3/#glacier-flexible-retrival","title":"Glacier Flexible Retrival \ud83c\udfd4\ufe0f","text":"<p>It is used for data archival, mainly for compliance reasons. Retrival time is from minutes to hours</p>"},{"location":"aws/s3/#glacier-deep-archive","title":"Glacier deep archive \ud83d\udd11","text":"<p>Long term data archieve with retrival time &lt; 12 hours</p>"},{"location":"aws/s3/#s3-outposts","title":"S3 Outposts \ud83d\udceb","text":"<p>Amazon <code>S3 Outposts</code> delivers object storage to your on-premises AWS Outposts environment. Amazon S3 Outposts is designed to store data durably and redundantly across multiple devices and servers on your Outposts. It works well for workloads with local data residency requirements that must satisfy demanding performance needs by keeping data close to on-premises applications.</p>"},{"location":"aws/s3/#lifecycle-management","title":"Lifecycle Management \ud83d\udd04","text":"<p>An S3 Lifecycle configuration is a set of rules that define actions that Amazon S3 applies to a group of objects. There are two types of actions:</p> <ol> <li><code>Transition actions</code> \u2013 These actions define when objects transition to another storage class. </li> </ol> <p>Example</p> <p>You might choose to transition objects to the <code>S3 Standard-IA</code> storage class 30 days after creating them, or archive objects to the <code>S3 Glacier Flexible Retrieval storage class</code> one year after creating them. </p> <p>In an S3 Lifecycle configuration, you can define rules to <code>transition objects</code> from one storage class to another to save on storage costs. When you don't know the access patterns of your objects, or if your access patterns are changing over time, you can transition the objects to the S3 Intelligent-Tiering storage class for automatic cost savings</p> <p> S3 Lifecycle Transitions <p></p> <ol> <li><code>Expiration actions</code> \u2013 These actions define when objects expire. Amazon S3 deletes expired objects on your behalf. Lifecycle expiration costs depend on when you choose to expire objects.</li> </ol> <p> S3 Intelligent Tiering <p></p>"},{"location":"aws/s3/#structure-of-s3","title":"Structure of S3 \ud83c\udfdb\ufe0f","text":"<p>S3 consist of the following:</p> <ul> <li>Key: it is the object name.</li> <li>Value: it is the actual data we are storing inside the object.</li> <li>Version id</li> <li>Metadata</li> <li>Sub-resources</li> <li>ACL</li> <li>Torrents</li> </ul>"},{"location":"aws/s3/#consistency-in-s3","title":"Consistency in S3 \u267e\ufe0f","text":"<ul> <li> <p><code>Read after write</code>: you can read after writing.</p> </li> <li> <p><code>Eventual consistency</code> for update and delete.</p> </li> </ul> <p>In S3-IA, we are charged a retrieval fee.</p> <p>In S3 Glacier, we can configure the retrieval time from minutes to hours.</p>"},{"location":"aws/s3/#pricing","title":"Pricing \ud83d\udcb0","text":"<p>Prices are <code>S3</code> &gt;<code>S3 IA</code> &gt; <code>S3 intelligent tiering</code> &gt; <code>S3 one zone</code> &gt; <code>S3 glacier</code> &gt; <code>S3 glacier deep archive</code></p>"},{"location":"aws/s3/#access","title":"Access \u274c","text":""},{"location":"aws/s3/#acl","title":"ACL \ud83d\udccb","text":"<p>Amazon S3 <code>access control lists (ACLs)</code> enable you to manage access to buckets and objects. Each bucket and object has an ACL attached to it as a subresource. It defines which AWS accounts or groups are granted access and the type of access. </p> <p>When a request is received against a resource, Amazon S3 checks the corresponding ACL to verify that the requester has the necessary access permissions. </p> <p>Recommendation is not to use ACL's</p> <p>A majority of modern use cases in <code>Amazon S3</code> no longer require the use of ACLs. We recommend that you keep ACLs disabled, except in unusual circumstances where you need to control access for each object individually. With ACLs disabled, you can use policies to control access to all objects in your bucket, regardless of who uploaded the objects to your bucket. </p> <p>Note</p> <ul> <li> <p>They are XML docs use to give access to both objects and the bucket.</p> </li> <li> <p>Each bucket and the object has the ACL attached to it in the sub-resources part.</p> </li> <li> <p>The default ACL provides full access to a resource owner</p> </li> <li> <p>A grantee can be an AWS account or one of the predefined Amazon S3 groups.</p> </li> <li> <p>When an object is created, then the only ACL is created not the user policy or bucket policy.</p> </li> <li> <p>ACL\u2019s can be used to grant permissions to pre-defined groups but not to an IAM user.</p> </li> <li> <p>With ACL we can NOT provide the deny rules and conditional access. All we can do is provide the basic read/write permissions.</p> </li> </ul> <p><code>Canned ACL</code>: Amazon S3 supports a set of predefined grants, known as canned ACLs. Each canned ACL has a predefined set of <code>grantees</code> and <code>permissions</code>. They are an easy way to grant access.</p> <p>When you create a bucket or an object, Amazon S3 creates a default ACL that grants the resource owner full control over the resource. This is shown in the following sample bucket ACL (the default object ACL has the same structure):</p> <pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;AccessControlPolicy xmlns=\"http://s3.amazonaws.com/doc/2006-03-01/\"&gt;\n  &lt;Owner&gt;\n    &lt;ID&gt;*** Owner-Canonical-User-ID ***&lt;/ID&gt;\n    &lt;DisplayName&gt;owner-display-name&lt;/DisplayName&gt;\n  &lt;/Owner&gt;\n  &lt;AccessControlList&gt;\n    &lt;Grant&gt;\n      &lt;Grantee xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" \n               xsi:type=\"Canonical User\"&gt;\n        &lt;ID&gt;*** Owner-Canonical-User-ID ***&lt;/ID&gt;\n        &lt;DisplayName&gt;display-name&lt;/DisplayName&gt;\n      &lt;/Grantee&gt;\n      &lt;Permission&gt;FULL_CONTROL&lt;/Permission&gt;\n    &lt;/Grant&gt;\n  &lt;/AccessControlList&gt;\n&lt;/AccessControlPolicy&gt; \n</code></pre>"},{"location":"aws/s3/#policies","title":"Policies \ud83d\ude94","text":"<p>2 types of policies are</p> <p> S3 Policies: IAM and Bucket <p></p>"},{"location":"aws/s3/#resource-based-aka-bucket-policy","title":"Resource-based AKA Bucket policy","text":"<ul> <li><code>Access control list (ACL)</code>: used for objects</li> <li>Bucket policy(JSON) and bucket ACL (XML)</li> </ul>"},{"location":"aws/s3/#useridentity-basediam","title":"User/identity-based/IAM","text":"<p>By default, users and roles don't have permission to create or modify Amazon S3 resources. They also can't perform tasks by using the AWS Management Console, AWS Command Line Interface (AWS CLI), or AWS API. To grant users permission to perform actions on the resources that they need, an IAM administrator can create IAM policies. The administrator can then add the IAM policies to roles, and users can assume the roles.</p> <ul> <li>ACL is legacy while <code>bucket policy</code> is new</li> <li><code>Root user</code> does not have the IAM policy</li> <li>Deny access always wins.</li> <li>You pay for storage, data replication, requests, management (monitoring)</li> <li>Bucket policy work at bucket level which the ACL works at the object level.</li> </ul>"},{"location":"aws/s3/#s3-access-points","title":"S3 Access Points \u2b55\ufe0f","text":"<p>Amazon <code>S3 access points</code> simplify data access for any AWS service or customer application that stores data in S3. Access points are named network endpoints that are attached to buckets that you can use to perform S3 object operations, such as <code>GetObject</code> and <code>PutObject</code>. </p> <p>We can have multiple access points for single bucket.</p> <p>Each access point has distinct permissions and network controls that S3 applies for any request that is made through that access point. </p> <p>Each access point enforces a <code>customized access point policy</code> that works in conjunction with the <code>bucket policy</code> that is attached to the underlying bucket.</p> <p>You can configure any access point to accept requests only from a virtual private cloud (VPC) to restrict Amazon S3 data access to a private network. You can also configure custom block public access settings for each access point.</p>"},{"location":"aws/s3/#s3-access-analyzer","title":"S3 Access Analyzer","text":"<p>IAM Access Analyzer for S3 alerts you to S3 buckets that are configured to allow access to anyone on the internet or other AWS accounts, including AWS accounts outside of your organization.</p> <p>For each public or shared bucket, you receive findings into the source and level of public or shared access.</p> <p>IAM Access Analyzer Example</p> <p>IAM Access Analyzer for S3 might show that a bucket has read or write access provided through a bucket access control list (ACL), a bucket policy, a Multi-Region Access Point policy, or an access point policy.</p>"},{"location":"aws/s3/#cors","title":"CORS \ud83d\ude45\u200d\u2640\ufe0f","text":"<p><code>Cross-origin resource sharing (CORS)</code> defines a way for client web applications that are loaded in one domain to interact with resources in a different domain. With CORS support, you can build rich client-side web applications with Amazon S3 and selectively allow cross-origin access to your Amazon S3 resources. </p> <p>How does Amazon S3 evaluate the CORS configuration on a bucket?</p> <p>When Amazon S3 receives a preflight request from a browser, it evaluates the CORS configuration for the bucket and uses the first <code>CORSRule</code> rule that matches the incoming browser request to enable a cross-origin request. For a rule to match, the following conditions must be met:</p> <ul> <li> <p>The <code>Origin header</code> in a CORS request to your bucket must match the origins in the <code>AllowedOrigins</code> element in your CORS configuration.</p> </li> <li> <p>The HTTP methods that are specified in the <code>Access-Control-Request-Method</code> in a CORS request to your bucket must match the method or methods listed in the <code>AllowedMethods</code> element in your CORS configuration.</p> </li> <li> <p>The headers listed in the <code>Access-Control-Request-Headers</code> header in a pre-flight request must match the headers in the <code>AllowedHeaders</code> element in your CORS configuration.</p> </li> </ul>"},{"location":"aws/s3/#object-lock","title":"Object Lock \ud83d\udd12","text":"<p>Why do we need object locking?</p> <p><code>S3 Object Lock</code> can help prevent Amazon S3 objects from being deleted or overwritten for a fixed amount of time or indefinitely. Object Lock uses a <code>write-once-read-many</code> (WORM) model to store objects.</p> <p>You can use Object Lock to help meet regulatory requirements that require WORM storage, or to add another layer of protection against object changes or deletion.</p> <ul> <li> <p>It can be done at time of creating bucket</p> </li> <li> <p>We need to enable versioning to enable locks. If you put an object into a bucket that already contains an existing protected object with the same object key name, Amazon S3 creates a new version of that object. The existing protected version of the object <code>remains locked</code> according to its retention configuration.</p> </li> </ul> <ul> <li> <p>Retention period \u2013 A retention period protects an object version for a fixed amount of time. When you place a retention period on an object version, Amazon S3 stores a timestamp in the object version's metadata to indicate when the retention period expires. After the retention period expires, the object version can be overwritten or deleted.</p> </li> <li> <p>Legal hold \u2013 A legal hold provides the same protection as a retention period, but it has no expiration date. Instead, a legal hold remains in place until you explicitly remove it. Legal holds are independent from retention periods and are placed on individual object versions.</p> </li> <li> <p>Retention modes</p> <ol> <li> <p>Governance mode: In governance mode, users can't overwrite or delete an object version or alter its lock settings unless they have special permissions. With governance mode, you protect objects against being deleted by most users, but you can still grant some users permission to alter the retention settings or delete the objects if necessary. You can also use governance mode to test retention-period settings before creating a compliance-mode retention period. </p> </li> <li> <p>Compliance mode: In compliance mode, a protected object version can't be overwritten or deleted by any user, including the root user in your AWS account. When an object is locked in compliance mode, its retention mode can't be changed, and its retention period can't be shortened. Compliance mode helps ensure that an object version can't be overwritten or deleted for the duration of the retention period.</p> </li> </ol> </li> </ul>"},{"location":"aws/s3/#encryption","title":"Encryption \ud83d\udd10","text":"<ul> <li>Encryption in transit is done using <code>SSL</code> or <code>TLS</code></li> <li>Encryption in transit is optional.</li> </ul> <p>What is delete marker?</p> <p>If we delete the files in the S3 bucket with versioning turned on, then it will place a <code>delete marker</code>. We can restore the files if we delete the <code>delete marker</code>.</p> <p>Lifecycle rules can be used in conjunction with versioning.</p> <p>Encryption is of 2 types as shown below</p>"},{"location":"aws/s3/#server-side","title":"Server-side \ud83e\udd16","text":"<ul> <li>S3 managed keys AKA SSE- S3 (USE AES-256): Here keys are managed by AWS</li> <li>Key management service (KMS): Here client has more transparency </li> </ul>"},{"location":"aws/s3/#client-side","title":"Client-side \ud83d\udc68","text":"<ul> <li>Here we use your own keys</li> <li>Once versioning is enabled, It can not be disabled but can only be suspended.</li> </ul>"},{"location":"aws/s3/#server-access-logs","title":"Server Access Logs \ud83d\udcbf","text":"<p><code>Server access logs</code> provides detailed records for the requests that are made to an Amazon S3 bucket. Server access logs are useful for many applications. </p> <p>For example, access log information can be useful in security and access audits. This information can also help you learn about your customer base and understand your Amazon S3 bill.</p> <p>This feature is not enabled by default</p> <p>By default, Amazon S3 doesn't collect server access logs. </p> <p>When you enable logging, Amazon S3 delivers access logs for a <code>source bucket</code> to a <code>destination bucket</code> (also known as a target bucket) that you choose. </p> <p>The destination bucket must be in the <code>same AWS Region</code> and <code>AWS account</code> as the source bucket. </p> <p>Here are some key points to remember</p> <p>Key points</p> <ul> <li> <p>There is no extra charge for enabling server access logging on an Amazon S3 bucket. However, any log files that the system delivers to you will accrue the usual charges for storage.</p> </li> <li> <p>You can delete the log files at any time. We do not assess data-transfer charges for log file delivery, but we do charge the normal data-transfer rate for accessing the log files.</p> </li> <li> <p>Your destination bucket should NOT have server access logging enabled. You can have logs delivered to any bucket that you own that is in the same Region as the source bucket, including the source bucket itself. However, delivering logs to the source bucket will cause an infinite loop of logs and is not recommended. </p> </li> <li> <p>S3 buckets that have <code>S3 Object Lock</code> enabled can't be used as destination buckets for server access logs. Your destination bucket must NOT have a default retention period configuration.</p> </li> </ul>"},{"location":"aws/s3/#file-replication","title":"File replication \u23e9\ufe0f","text":"<ul> <li> <p><code>Cross-region replication</code> needs the <code>versioning</code> to be enabled for both the source and destination buckets.</p> </li> <li> <p>The existing files in the bucket which were added before the replication are not replicated, so we have to replicate them manually.</p> </li> <li> <p>The subsequently updated files will be replicated automatically.</p> </li> <li> <p>For the replicated files, if you put a <code>delete marker</code>/or delete a file, then the file is not deleted on the replicated.</p> </li> </ul>"},{"location":"aws/s3/#static-file-hosting","title":"Static File Hosting \ud83c\udf0f","text":"<p>You can use Amazon S3 to host a static website. On a static website, individual webpages include static content. They might also contain client-side scripts.</p> <p>By contrast, a dynamic website relies on server-side processing, including server-side scripts, such as <code>PHP</code>, <code>JSP</code>, or <code>ASP.NET</code>. Amazon S3 does not support server-side scripting, but AWS has other resources for hosting dynamic websites.</p> <p>Remember</p> <p>When you configure a bucket as a static website, you must enable static website hosting, configure an index document, and set permissions.</p>"},{"location":"aws/s3/#requester-pays","title":"Requester Pays \ud83d\udcb3","text":"<p>In general, <code>bucket owners</code> pay for all Amazon S3 storage and data transfer costs that are associated with their bucket. However, you can configure a bucket to be a Requester Pays bucket.</p> <p>With <code>Requester Pays buckets</code>, the requester instead of the bucket owner pays the cost of the request and the data download from the bucket. The bucket owner always pays the cost of storing data.</p> <p>Typically, you configure buckets to be Requester Pays buckets when you want to share data but not incur charges associated with others accessing the data. For example, you might use Requester Pays buckets when making available large datasets, such as zip code directories, reference data, geospatial information, or web crawling data</p> <p>Anonymous access is not allowed with Requester Pays</p> <p>You must authenticate all requests involving Requester Pays buckets. The request authentication enables Amazon S3 to identify and charge the requester for their use of the Requester Pays bucket. </p>"},{"location":"aws/s3/#s3-transfer-acceleration","title":"S3 Transfer Acceleration \ud83d\ude85","text":"<p>It uses the CloudFront to fasten the process of uploading. The users will first upload to the edge location from where the files are uploaded to bucked using Amazon\u2019s backbone network.</p> <p>!!! question 'Which policies should I use?'     We should use the User policy or the bucket policy as it will help in providing us in access at a much fine-grained level. ACL\u2019s are the legacy tech.</p>"},{"location":"aws/s3/#user-groups","title":"User Groups \ud83d\udc71","text":"<p><code>S3 pre-defined groups</code>: Amazon S3 has a set of predefined groups. When granting account access to a group, you specify one of our URIs instead of a canonical user ID. We provide the following predefined groups:</p> <ol> <li>Authenticated Users group: all AWS accounts</li> <li>All Users group: authenticated and anonymous users.</li> <li>Log Delivery group </li> </ol> <p>Tip</p> <p>The canonical user ID is an alpha-numeric identifier, such as <code>79a59df900b949e55d96</code> , that is an obfuscated form of the AWS account ID. You can use this ID to identify an AWS account when granting cross-account access to buckets and objects using Amazon S3.</p>"},{"location":"aws/s3/#server-side-encryption","title":"Server-side encryption \ud83e\ude84","text":"<p>Server-side encryption is the encryption of data at its destination by the application or service that receives it. Amazon S3 encrypts your data at the object level as it writes it to disks in its data centers and decrypts it for you when you access it.</p>"},{"location":"aws/s3/#sse-s3","title":"SSE-S3 \ud83e\udea3","text":"<p>Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3): When you use <code>SSE-S3</code>, each object is encrypted with a unique key. As an additional safeguard, it encrypts the key itself with a master key that it regularly rotates.</p> <ul> <li>All the objects are encrypted using different keys</li> <li>We can not manage keys in this case</li> </ul> <p> Encryption for SSE-S3 <p></p>"},{"location":"aws/s3/#sse-kms","title":"SSE-KMS \ud83d\udd10","text":"<p>Server-Side Encryption with Customer Master Keys (CMKs) Stored in AWS Key Management Service (SSE-KMS): The SSE-KMS is similar to <code>SSE-S3</code>, but with some additional benefits and charges for using this service. There are separate permissions for the use of a CMK that provides added protection against unauthorized access of your objects in Amazon S3. </p> <ul> <li> <p>We can create and use data keys, master keys, and rotate keys as well.</p> </li> <li> <p>It gives us a lot of control as a user can choose a key to encrypt the object.</p> </li> <li> <p>We have access to data keys and master keys.</p> </li> <li> <p>AWS does not have access to the keys in this case.</p> </li> <li> <p>We can audit the use of KMS using Cloudtrail that shows when your CMK was used and by whom.</p> </li> </ul>"},{"location":"aws/s3/#sse-c","title":"SSE-C \ud83d\udc68\u200d\ud83e\uddb0","text":"<p>Server-Side Encryption with Customer-Provided Keys (SSE-C): with SSE-C you manage the encryption keys and Amazon S3 manages the encryption, as it writes to disks and decryption when you access your objects.</p> <ul> <li> <p>The Key is managed by the user.</p> </li> <li> <p>A user generates the key and uploads it with data.</p> </li> <li> <p>Must use <code>HTTPS</code> to upload the objects.</p> </li> <li> <p>If we lose the key, so we lose the data.</p> </li> <li> <p>S3 will discard the key after using it in this case.</p> </li> </ul>"},{"location":"aws/s3/#s3-and-ebs-difference","title":"S3 and EBS difference \ud83e\udea3","text":"<p><code>S3 (Simple Storage Service)</code> and <code>EBS (Elastic Block Store)</code> are two file storage services provided by Amazon. The main difference between them is with what they can be used with. EBS is specifically meant for EC2 (<code>Elastic Computing Cloud</code>) instances and is not accessible unless mounted to one.</p> <p>On the other hand, S3 is not limited to EC2. The files within an S3 bucket can be retrieved using HTTP protocols and even with BitTorrent. Many sites use S3 to hold most of their files because of its accessibility to HTTP clients; web browsers for example.</p> <p>As already stated above, you need some type of software in order to read or write information with S3. With EBS, a volume can be mounted on an EC2 instance and it would appear just like a hard disk partition. It can be formatted with any file system and files can be written or read by the EC2 instance just like it would to a hard drive.</p> <p>When it comes to the total amount that you can store, S3 still has the upper hand. EBS has a standard limit of 20 volumes with each volume holding up to 1TB of data. With S3, the standard limit is at 100 buckets with each bucket having an unlimited data capacity. S3 users do not need to worry about filling a bucket and the only concern is having enough buckets for your needs.</p> <p>S3 can't be used concurrently</p> <p>A limitation of EBS is its inability to be used by multiple instances at once. Once it is mounted by an instance, no other instance can use it. </p> <p>S3 can have multiple images of its contents so it can be used by many at the same time. </p> <p>An interesting side-effect of this capability is something called \u2018eventual consistency\u2019. With EBS, data read or write occurs almost instantly. With S3, the changes are not written immediately so if you write something, it may not be the data that a read operation returns.</p> <ul> <li> <p>EBS can only be used with EC2 instances while S3 can be used outside EC2.</p> </li> <li> <p>EBS appears as a <code>mountable volume</code> while the S3 requires software to read and write data.</p> </li> <li> <p>EBS can accommodate a smaller amount of data than S3.</p> </li> <li> <p>EBS can only be used by one EC2 instance at a time while S3 can be used by multiple instances.</p> </li> <li> <p>S3 typically experiences write delays while EBS does not as EBS is attached to an instance.</p> </li> </ul>"},{"location":"aws/s3/#limits","title":"Limits \u2140","text":"<p>Until 2018 there was a hard limit on S3 puts of 100 PUTs per second. To achieve this care needed to be taken with the structure of the name Key to ensuring parallel processing. </p> <p>As of July 2018, the limit was raised to 3500 and the need for the Key design was basically eliminated.</p>"},{"location":"aws/s3/#notes","title":"Notes \ud83d\udcd5","text":"<p>Some points to remember</p> <ul> <li> <p>The <code>S3 policy</code> is a JSON doc. </p> </li> <li> <p>HTTP and HTTPS are both enabled in S3 by default, but we can disable the HTTP by using the bucket policy.</p> </li> <li> <p>IAM is Universal.</p> </li> <li> <p>S3 is not suitable to install OS or a database as it is object-based.</p> </li> <li> <p>By default, the <code>buckets are private</code> and we have to make them <code>public</code> (remember this).</p> </li> <li> <p>We can log the requests made to the S3 and then later these logs can be sent to another account as well.</p> </li> <li> <p>S3 supports <code>bittorrent protocol</code> to retrieve any publicaly available object using torrent files (peer-to-peer).</p> </li> <li> <p>Data is encrypted by the client and the encrypted data is uploaded in case of Client-side encryption.</p> </li> <li> <p>HTTPS uses asymmetric encryption.</p> </li> <li> <p>We can apply <code>lifecycle rules</code> to a whole bucket or a subset. </p> </li> <li> <p>We can have 1000 lifecycle policies per bucket.</p> </li> <li> <p>Lifecycle is defined as XML and is stored in the sub-resources section.</p> </li> <li> <p>Lifecycle configuration on <code>multi-factor authentication (MFA)-enabled buckets</code> is NOT supported. This is because the  MFA needs human intervention.</p> </li> <li> <p>For glacier, we are charged for at least 90 days(3 months) and 180 days(6 months) for the deep archive.</p> </li> <li> <p>The difference between <code>Security Group</code> and <code>ACLs</code> is that the Security Group acts as a firewall for associated Amazon EC2 instances, controlling both inbound and outbound traffic at the instance level, while ACLs act as a firewall for associated subnets, controlling both inbound and outbound traffic at the subnet level.</p> </li> <li> <p>A particular folder cannot be tagged separately from other folders; only an entire bucket can be tagged.</p> </li> <li> <p>With the exception of Glacier, retrieving data from the various S3 storage classes should be virtually identical (network issues notwithstanding).</p> </li> </ul>"},{"location":"aws/sagemaker/","title":"SageMaker","text":""},{"location":"aws/sagemaker/#bedrock","title":"Bedrock \ud83e\uddd7\u200d\u2642\ufe0f","text":"<p>Amazon Bedrock is a <code>fully managed service</code> that offers a choice of high-performing foundation models (FMs) from leading AI companies like AI21 Labs, Anthropic, Cohere, Meta, Mistral AI, Stability AI, and Amazon through a single API, along with a broad set of capabilities you need to build generative AI applications with security, privacy, and responsible AI. </p> <p>Why use Bedrock?</p> <p>Using <code>Amazon Bedrock</code>, you can easily experiment with and evaluate top FMs for your use case, privately customize them with your data using techniques such as <code>fine-tuning</code> and <code>Retrieval Augmented Generation (RAG)</code>, and build agents that execute tasks using your enterprise systems and data sources.</p> <p>Since Amazon Bedrock is serverless, you don't have to manage any infrastructure, and you can securely integrate and deploy generative AI capabilities into your applications using the AWS services you are already familiar with.</p>"},{"location":"aws/sagemaker/#advatages","title":"Advatages","text":"<ul> <li> <p>Leading FM's: Amazon Bedrock helps you rapidly adapt and take advantage of the latest generative AI innovations with easy access to a choice of high-performing FMs from leading AI companies like AI21 Labs, Anthropic, Cohere, Meta, Mistral AI, Stability AI, and Amazon. </p> <p>The single-API access of Amazon Bedrock, regardless of the models you choose, gives you the flexibility to use different FMs and upgrade to the latest model versions with minimal code changes.</p> </li> <li> <p>Fine Turning: Model customization helps you deliver differentiated and personalized user experiences. To customize models for specific tasks, you can privately fine-tune FMs using your own labeled datasets in just a few quick steps.</p> </li> <li> <p>RAG: To equip the FM with <code>up-to-date proprietary information</code>, organizations use RAG, a technique that involves fetching data from company data sources and enriching the prompt with that data to deliver more relevant and accurate responses.</p> <p><code>Knowledge Bases</code> for Amazon Bedrock is a fully managed RAG capability that allows you to customize FM responses with contextual and relevant company data. </p> </li> <li> <p>Agenting: Agents for Amazon Bedrock plan and execute multistep tasks using company systems and data sources\u2014from answering customer questions about your product availability to taking their orders. With Amazon Bedrock, you can create an agent in just a few quick steps by first selecting an FM and providing it access to your enterprise systems, knowledge bases, and AWS Lambda functions to securely execute your APIs. </p> <p>An agent analyzes the user request and automatically calls the necessary APIs and data sources to fulfill the request. Agents for Amazon Bedrock offer enhanced security and privacy\u2014no need for you to engineer prompts, manage session context, or manually orchestrate tasks.</p> </li> </ul>"},{"location":"aws/sagemaker/#sagemaker_1","title":"SageMaker \ud83e\uddd8","text":""},{"location":"aws/sagemaker/#sagemaker-jumpstart","title":"Sagemaker Jumpstart \ud83c\udfa9","text":"<p>Amazon SageMaker JumpStart is a <code>machine learning hub</code> that provides access to a wide range of <code>public ML models</code> and seamlessly integrates them into AWS infrastructure managed by SageMaker. </p> <p>The hub is particularly useful for applications that need to implement common use cases, which publicly available models can solve</p>"},{"location":"aws/sagemaker/#codewhisperer","title":"CodeWhisperer \ud83d\udd0a","text":"<p>Amazon CodeWhisperer provides generative coding capabilities across multiple coding languages, supporting productivity enhancements such as code gen\u2010 eration, proactively scanning for vulnerabilities and suggesting code remediations, with automatic suggestions for code attribution.</p>"},{"location":"aws/sagemaker/#trainium","title":"Trainium \ud83d\ude8a","text":"<p>AWS Trainium is the <code>machine learning (ML) chip</code> thsat AWS purpose built for deep learning (DL) training of 100B+ parameter models. Each EC2 Trn1 instance deploys up to <code>16 Trainium accelerators</code> to deliver a high-performance, low-cost solution for DL training in the cloud.</p> <p>How does trainium and Infrentia works</p> <p><code>AWS Neuron SDK</code> helps developers train models on <code>Trainium accelerators</code> (and deploy them on <code>AWS Inferentia accelerators</code>). </p> <p>It natively integrates with popular frameworks, such as <code>PyTorch</code> and <code>TensorFlow</code>, so that you can continue to train on Trainium accelerators and use your existing code and workflows.</p>"},{"location":"aws/sagemaker/#infernia","title":"Infernia \ud83d\udcc8","text":""},{"location":"aws/sagemaker/#sagemaker-deployment-types","title":"Sagemaker Deployment Types","text":"<p>After you train your machine learning model, you can deploy it using <code>Amazon SageMaker</code> to get predictions. Amazon SageMaker supports the following ways to deploy a model, depending on your use case:</p> <p> </p>"},{"location":"aws/sagemaker/#real-time-inference","title":"Real-time Inference \u23e9","text":"<p>Real-time inference is ideal for inference workloads where you have real-time, interactive, low latency requirements. </p> <p>These endpoints are <code>fully managed</code> and support autoscaling</p> <p>Deployments and Endpoints</p> <p>You can deploy one or more models to an endpoint with Amazon SageMaker. When multiple models share an endpoint, they jointly utilize the resources that are hosted there, such as the ML compute instances, CPUs, and accelerators.</p>"},{"location":"aws/sagemaker/#serverless-inference","title":"Serverless Inference \u2699\ufe0f","text":"<p>Workloads that have idle periods between traffic spikes and can tolerate cold starts, use Serverless Inference. </p>"},{"location":"aws/sagemaker/#asynchronous-inference","title":"Asynchronous Inference \ud83d\udcde","text":"<p><code>SageMaker Asynchronous Inference</code> is a capability in SageMaker that queues incoming requests and processes them asynchronously. This option is ideal for requests with large payload sizes (up to 1GB), long processing times (up to one hour), and near real-time latency requirements.</p> <p> </p> <p>Optimizing Model Inference with Amazon SageMaker: A Step-by-Step Guide</p> <p>In today's data-driven world, leveraging the power of models for inference is crucial. Amazon SageMaker offers a robust solution for asynchronous model inference, allowing organizations to efficiently handle large volumes of requests. Here are the 10 steps that illustrate how SageMaker manages async model inference seamlessly:</p> <ol> <li>Input Upload: The user begins by uploading the input request file to an Amazon S3 bucket, which serves as the storage for data to be processed.</li> <li>Invoking the Async Endpoint: Once the input is securely stored, a Data Scientist (DS) invokes the asynchronous endpoint, providing a reference to the original S3 bucket path of the uploaded data.</li> <li>Webhook for Status Checks: The asynchronous endpoint responds by returning a webhook endpoint. This allows users or services to check the status of their inference requests as needed.</li> <li>Enqueuing the Request: The endpoint then enqueues the model request into an internal queue, prioritizing the requests based on specific needs and optimizing workflow management.</li> <li>Triggering SageMaker Compute: Following queuing, the endpoint triggers the SageMaker compute resources, initiating the inference process.</li> <li>Batch File Processing: The compute resources retrieve the model input batch file, parse its contents, divide the data into manageable chunks, and commence parallel processing across multiple nodes. This parallelization enhances efficiency and reduces latency.</li> <li>Batch Inference Execution: SageMaker conducts batch inference, applying the trained model to the input data and generating predictions.</li> <li>Storing Outputs: Upon completion of the inference, the model outputs are written back to the designated S3 bucket for further use.</li> <li>Notifying the SNS Topic: If configured, SageMaker sends a success or failure notification to an Amazon Simple Notification Service (SNS) topic, keeping stakeholders informed about the inference results.</li> <li>User/Service Notification: Lastly, the SNS service notifies the user or service regarding the completion status of the inference request, ensuring streamlined communication and timely updates.</li> </ol> <p>By following these structured steps, Amazon SageMaker enables organizations to efficiently process large-scale model inferences asynchronously, ultimately enhancing productivity and decision-making capabilities.</p> <p>Tip</p> <p>Asynchronous Inference enables you to save on costs by autoscaling the instance count to zero when there are no requests to process, so you only pay when your endpoint is processing requests.</p>"},{"location":"aws/sagemaker/#batch-inference","title":"Batch Inference \ud83d\udce6","text":"<p>To get predictions for an entire dataset, use SageMaker batch transform. See Use batch transform to run inference with Amazon SageMaker.</p>"},{"location":"aws/secret-manager/","title":"Secret Manager","text":"<p>WS Secrets Manager helps you manage, retrieve, and rotate database credentials, application credentials, OAuth tokens, API keys, and other secrets throughout their lifecycles. Many AWS services store and use secrets in Secrets Manager.</p> <p>Secrets Manager helps you improve your security posture, because you no longer need <code>hard-coded credentials</code> in application source code. Storing the credentials in Secrets Manager helps avoid possible compromise by anyone who can inspect your application or the components. You replace hard-coded credentials with a runtime call to the Secrets Manager service to retrieve credentials dynamically when you need them.</p> <p>With Secrets Manager, you can configure an automatic rotation schedule for your secrets. This enables you to replace <code>long-term secrets</code> with <code>short-term ones</code>, significantly reducing the risk of compromise. Since the credentials are no longer stored with the application, rotating credentials no longer requires updating your applications and deploying changes to application clients.</p> <p> Secret Expiry <p></p>"},{"location":"aws/serverless/","title":"Serverless","text":"<p>The AWS serverless platform includes a number of fully managed services that are tightly integrated with AWS Lambda and well-suited for serverless applications. Developer tools, including the AWS Serverless Application Model (AWS SAM), help simplify deployment of your Lambda functions and serverless applications.</p> <p> Serverless Services in AWS <p> </p>"},{"location":"aws/serverless/#event-driven-architecture","title":"Event Driven Architecture","text":"<p>An <code>event-driven architecture</code> uses events to initiate actions and communication between <code>decoupled services</code>.</p> <p>Wht is an event?</p> <p>An <code>event</code> is a change in state, a user request, or an update, like an item being placed in a shopping cart in an e-commerce website. When an event occurs, the information is published for other services to consume it. In event-driven architectures, events are the primary mechanism for sharing information across services. </p> <p> <p></p> <p>SNS event driven architecture</p> <p>Here the SQS is introduced to take care of 30 second timeout of API Gateway</p> <p> </p>"},{"location":"aws/sg/","title":"Security Group","text":"<p>Difference from an ACL as its also an virtual Firewall</p> <p>A <code>security group</code> is a virtual firewall that controls inbound and outbound traffic for an Amazon EC2 instance.</p> <ul> <li>By default, a security group denies all inbound traffic and allows all outbound traffic. You can add custom rules to configure which traffic should be allowed; any other traffic would then be denied</li> <li>There are slight differences between a normal 'new' Security Group and a 'default' security group in the default VPC. For a 'new' security group nothing is allowed in by default.</li> <li>Security groups evaluate all the rules on the group before deciding how to handle the traffic.</li> <li>Security groups only provide for allow rules.</li> <li>Security groups operate at the instance level.</li> </ul>"},{"location":"aws/sg/#stateful-packet-filtering","title":"Stateful packet filtering","text":"<p>Security groups perform stateful packet filtering. They remember previous decisions made for incoming packets.</p> <p>Consider the same example of sending a request out from an Amazon EC2 instance to the internet. </p> <p>When a packet response for that request returns to the instance, the security group remembers your previous request. The security group allows the response to proceed, regardless of inbound security group rules.</p>"},{"location":"aws/snowball/","title":"Snow-Family","text":""},{"location":"aws/snowball/#snow-cone","title":"Snow Cone \ud83c\udf66","text":"<p>AWS Snowcone(opens in a new tab) is a small, rugged, and secure edge computing and data transfer device. </p> <p>It features 2 CPUs, 4 GB of memory, and up to 14 TB of usable storage.</p>"},{"location":"aws/snowball/#snowball","title":"SnowBall \u2744\ufe0f","text":"<ul> <li>Icon-Architecture/64/Arch_AWS-Snowball_64Created with Sketch. is used for large data transfer</li> <li>It has less cost, fast and is secure.</li> <li>A SnowBall edge connects to your existing applications and infrastructure using the standard interfaces.</li> </ul> <p>AWS Snowball offers two types of devices:</p> <ul> <li><code>Snowball Edge Storage Optimized</code> devices are well suited for large-scale data migrations and recurring transfer workflows, in addition to local computing with higher capacity needs.         </li> <li><code>Snowball Edge Compute Optimized</code> provides powerful computing resources for use cases such as machine learning, full motion video analysis, analytics, and local computing stacks. </li> </ul>"},{"location":"aws/snowball/#aws-snowmobile","title":"AWS Snowmobile \ud83d\udefb","text":"<p>AWS Snowmobile is an <code>exabyte-scale data transfer service</code> used to move large amounts of data to AWS. </p> <p>You can transfer up to 100 petabytes of data per Snowmobile, a <code>45-foot long ruggedized shipping container</code>, pulled by a semi trailer truck.</p>"},{"location":"aws/sns/","title":"SNS","text":"<p>Amazon Simple Notification Service (Amazon SNS) is a publish/subscribe service. Using Amazon SNS topics, a publisher publishes messages to subscribers. This is similar to the coffee shop; the cashier provides coffee orders to the barista who makes the drinks.</p> <p>In Amazon SNS, subscribers can be web servers, email addresses, AWS Lambda functions, or several other options. </p>"},{"location":"aws/sns/#amazon-sns-message-filtering","title":"Amazon SNS message filtering","text":"<p>In the Amazon SNS message filtering pattern, the subscriber assigns a filter policy to the topic subscription. The filter policy contains attributes that define which messages that subscriber receives, and Amazon SNS compares message attributes to the filter policy for each subscription. If the attributes don\u2019t match, the message doesn\u2019t get sent to that subscriber.</p> <p>  SNS filtering example <p></p>"},{"location":"aws/sqs/","title":"SQS","text":"<p>Tldr</p> <p>Using <code>Amazon SQS</code>, you can send, store, and receive messages between software components, without losing messages or requiring other services to be available. In Amazon SQS, an application sends messages into a queue. </p> <p>A user or service retrieves a message from the queue, processes it, and then deletes it from the queue (as its a pull-based system).</p> <p> SQS pull-based architecture <p></p> <p>SQS will guarantee that a message is delivered <code>at least once</code>, but that message may be redelivered.</p> <p>SQS queues only make an \u201cattempt\u201d to deliver messages in order (more or less a FIFO approach) but do not guarantee FIFO. If strict FIFO is needed, that option can be selected.</p> <p>SNS vs SQS</p> <ul> <li>SNS manages notifications and SQS manages messages</li> <li>SNS is a <code>push-based system</code> while SQS is a <code>pull-based system</code></li> </ul>"},{"location":"aws/sqs/#longshort-polling","title":"Long/Short Polling","text":"<p>Amazon SQS offers short and long polling options for receiving messages from a queue. Consider your application's requirements for responsiveness and cost efficiency when choosing between these two polling options:</p> <p>Short polling (default) \u2013 The <code>ReceiveMessage</code> request queries a subset of servers (based on a weighted random distribution) to find available messages and sends an immediate response, even if no messages are found.</p> <p> </p> <p>The following diagram shows the short-polling behavior of messages returned from a standard queue after one of your system components makes a receive request. Amazon SQS samples several of its servers (in gray) and returns messages A, C, D, and B from these servers. Message E isn't returned for this request, but is returned for a subsequent request.</p> <p> </p> <p>Long polling \u2013 <code>ReceiveMessage</code> queries all servers for messages, sending a response once at least one message is available, up to the specified maximum. An empty response is sent only if the polling wait time expires. This option can reduce the number of empty responses and potentially lower costs.</p> <p> </p>"},{"location":"aws/sqs/#types-of-sqs","title":"Types of SQS","text":""},{"location":"aws/sqs/#standard-default","title":"Standard (Default)","text":"<p>Amazon SQS offers standard as the <code>default queue</code> type. Standard queues support a nearly unlimited number of API calls per second, per API action (<code>SendMessage</code>, <code>ReceiveMessage</code>, or <code>DeleteMessage</code>). Standard queues support <code>at-least-once</code> message delivery. </p> <p>Messages delivered out of order</p> <p>However, occasionally (because of the highly distributed architecture that allows nearly unlimited throughput), more than one copy of a message might be delivered <code>out-of-order</code>. Standard queues provide best-effort ordering which ensures that messages are generally delivered in the same order as they're sent.</p>"},{"location":"aws/sqs/#fifo","title":"FIFO","text":"<p><code>FIFO (First-In-First-Out) queues</code> have all the capabilities of the standard queues, but are designed to enhance messaging between applications when the order of operations and events is critical, or where duplicates can't be tolerated.</p>"},{"location":"aws/sqs/#sns-vs-sqs","title":"SNS vs SQS","text":"<p>There are some key distinctions between SNS and SQS:</p> <ul> <li>SNS is a <code>pub/sub system</code>, while SQS is a <code>queueing system</code>. You'd typically use SNS to send the same message to multiple consumers via topics. In comparison, in most scenarios, each message in an SQS queue is processed by only one consumer.</li> <li>With SQS, messages are delivered through a <code>long polling (pull)</code> mechanism, while SNS uses a push mechanism to immediately deliver(push) messages to subscribed endpoints.</li> <li>SNS is typically used for applications that need realtime notifications, while SQS is more suited for message processing use cases.</li> <li>SNS does not persist messages - it delivers them to subscribers that are present, and then deletes them. In comparison, SQS can persist messages (from 1 minute to 14 days).</li> </ul>"},{"location":"aws/sqs/#features","title":"Features","text":""},{"location":"aws/sqs/#dlq","title":"DLQ","text":""},{"location":"aws/sqs/#visibility-timeout","title":"Visibility Timeout","text":""},{"location":"aws/sqs/#best-practices","title":"Best Practices","text":"<ul> <li>It is a best practice to set the visibility timeout on your SQS queue to 6x the Lambda function timeout to allow for retries if the function is getting throttled.</li> <li>Retries \u2013 Use the maxReceiveCount on the queue's policy to limit the number of times Lambda will retry to process a failed execution.</li> </ul>"},{"location":"aws/stepfunctions/","title":"Step Functions","text":"<p>AWS Step Functions is a <code>serverless orchestration service</code> that lets you integrate with AWS Lambda functions and other AWS services to build business-critical applications. Through Step Functions' graphical console, you see your application\u2019s workflow as a series of event-driven steps.</p> <p>Why is step function required?</p> <p><code>AWS Step Functions</code> can coordinate the distributed components of your application and keep the need for orchestration out of your code. It automatically launches and tracks each step, and can retry steps when there are errors. </p> <p>As your applications run, Step Functions maintains the <code>application state</code>, tracking exactly which workflow step your application is in, and stores an <code>event log of data</code> that is passed between application components. That means if the workflow is interrupted for any reason, your application can pick up right where it left off.</p> <p> Step function example <p> </p>"},{"location":"aws/stepfunctions/#use-cases","title":"Use cases","text":"<ul> <li>ML: Step Functions lets you to orchestrate end-to-end machine learning workflows on SageMaker. These workflows can include data preprocessing, post-processing, feature engineering, data validation, and model evaluation</li> <li>Micro-services Orchestration:</li> <li>IT and Security Automation</li> </ul>"},{"location":"aws/stepfunctions/#features-of-step-functions","title":"Features of Step functions","text":"<ul> <li>Step Functions is based on state machines and tasks.</li> <li>In Step Functions, a <code>workflow</code> is called a state machine, which is a series of event-driven steps.</li> <li>Each <code>step</code> in a workflow is called a state.</li> <li>A <code>Task state</code> represents a unit of work that another AWS service, such as AWS Lambda, performs. A Task state can call any AWS service or API.</li> <li>Using Step Functions, you define your workflows as state machines, which transform complex code into easy to understand statements and diagrams. </li> <li>Step Functions provides ready-made steps for your workflow called states that implement basic service primitives for you, which means you can remove that logic from your application.</li> <li>Using Step Functions service tasks, you can configure your Step Functions workflow to call other AWS services. </li> <li>Step Functions can coordinate any application that can make an HTTPS connection, regardless of where it is hosted\u2014Amazon EC2 instances, mobile devices, or on-premises servers. </li> <li>AWS Step Functions coordinates your existing Lambda functions and microservices and lets you modify them into new compositions. The tasks in your workflow can run anywhere, including on instances, containers, functions, and mobile devices. </li> <li>Nesting your Step Functions workflows allows you to build larger, more complex workflows out of smaller, simpler workflows. </li> <li>Step Functions keeps the logic of your application strictly separated from the implementation of your application. You can add, move, swap, and reorder steps without having to make changes to your business logic. </li> <li>Step Functions maintains the state of your application during execution, including tracking what step of execution it is in, and storing data that is moving between the steps of your workflow. You won\u2019t have to manage state yourself with data stores or by building complex state management into all of your tasks. </li> <li>Step Functions automatically handles errors and exceptions with built-in try/catch and retry, whether the task takes seconds or months to complete. You can automatically retry failed or timed-out tasks, respond differently to different types of errors, and recover gracefully by falling back to the designated cleanup and recovery code. </li> <li>Step Functions has built-in fault tolerance and maintains service capacity across multiple Availability Zones in each region, ensuring high availability for both the service itself and for the application workflow it operates. </li> <li>Step Functions automatically scales the operations and underlying compute to run the steps of your application for you in response to changing workloads. </li> <li>AWS Step Functions has a 99.9% SLA. </li> <li>AWS Step Functions enables access to metadata about workflow executions so that you can easily identify related resources. For example, your workflow can retrieve its execution ID or a timestamp indicating when a task started. This makes it easier to correlate logs for faster debugging and to measure workflow performance data. </li> <li>It also supports callback patterns. Callback patterns automate workflows for applications with human activities and custom integrations with third-party services.</li> <li><code>Monitoring</code>: AWS Step Functions supports workflow execution events, which makes it faster and easier to build and monitor event-driven, serverless workflows. Execution event notifications can be automatically delivered when a workflow starts or completes through CloudWatch Events, reaching targets like AWS Lambda, Amazon SNS, Amazon Kinesis, or AWS Step Functions for automated response to the event.</li> </ul>"},{"location":"aws/stepfunctions/#workflows","title":"Workflows","text":"<p>For long-running workflows you can use Standard Workflows with the AWS Fargate integration to orchestrate applications running in containers. For short-duration, high-volume workflows that require an immediate response, Synchronous Express Workflows are ideal. </p> <p>These can be used for <code>web-based</code> or <code>mobile applications</code>, which often have workflows of short duration, and require the completion of a series of steps before they return a response</p> <p>Step Functions has 2 workflow types.</p>"},{"location":"aws/stepfunctions/#standard-workflows","title":"Standard workflows","text":"<ul> <li>They have exactly-once workflow execution and can run for up to 1 year. This means that each step in a Standard workflow will execute exactly once. </li> <li>Designed for <code>long-running business processes</code> that require durable state storage</li> <li>Provides automatic retries, error handling, and human approval steps. </li> <li>Offers many advanced features, including <code>execution history tracking</code> for up to 90 days and timeouts. </li> <li>Suitable for complex and fault-tolerant workflows that require coordination across multiple services.</li> </ul>"},{"location":"aws/stepfunctions/#express-workflows","title":"Express workflows","text":"<ul> <li>AWS Step Functions has Express Workflows to support event rates greater than 100,000 per second, so you can build high volume, short duration workflows that has to be completed within <code>5 mins max</code>. </li> <li>Provides no durable state storage, no automatic retries, no error handling, and no human approval steps. </li> <li>Optimized for low-latency and high-throughput use cases, such as event-driven serverless applications. </li> <li>Can integrate with AWS Step Functions service integrations, AWS Lambda, and HTTP/HTTPS endpoints. </li> <li>Provides a simpler and more lightweight workflow execution environment with lower per-state costs.</li> </ul>"},{"location":"aws/stepfunctions/#concepts","title":"Concepts","text":""},{"location":"aws/stepfunctions/#asl-amazon-state-language","title":"ASL (Amazon State Language)","text":"<p>In <code>AWS Step Functions</code>, you define your workflows in the <code>Amazon States Language</code>. The Amazon States Language is a JSON-based, structured language used to define your state machine. Using Amazon States Language, you create workflows. </p>"},{"location":"aws/stepfunctions/#transitions","title":"Transitions","text":"<p>Transitions link states together, defining the control flow for the state machine. When a state machine is invoked, the system begins with the state referenced in the top-level \"StartAt\" field. This field is a string value that must match the name of one of the states exactly. It is case sensitive. </p>"},{"location":"aws/stepfunctions/#states","title":"States","text":"<p> State in Step Function <p></p> <p>There are 8 types of states:</p> <ul> <li><code>Task state</code> \u2013 represents a single unit of work done by your state machine. It could be making API calls to AWS services, such as running an Amazon Athena query, writing to a DynamoDB table, or invoking a Lambda function to perform some custom processes.</li> <li>Choice state \u2013 Make a choice between branches of execution </li> <li><code>Fail state</code> \u2013 Stops execution and marks it as failure </li> <li><code>Succeed state</code> \u2013 Stops execution and marks it as a success </li> <li><code>Pass state</code> \u2013 Simply pass its input to its output or inject some fixed data </li> <li><code>Wait state</code> \u2013 Provide a delay for a certain amount of time or until a specified time/date </li> <li><code>Parallel state</code> \u2013 Begin parallel branches of execution </li> <li><code>Map state</code> \u2013 Adds a for-each loop condition</li> </ul>"},{"location":"aws/stepfunctions/#pass","title":"Pass","text":"<p>A Pass state (\"Type\": \"Pass\") passes its input to its output, without performing work. Pass states are useful when constructing and debugging state machines. You can also use a Pass state to transform JSON state input using filters, and then pass the transformed data to the next state in your workflows. </p> <p>Pass State Example</p> <p>Here is an example of a Pass state that injects some fixed data into the state machine, probably for testing purposes.</p> <pre><code>\"No-op\": {\n\"Type\": \"Pass\",\n\"Result\": {\n\"x-datum\": 0.381018,\n\"y-datum\": 622.2269926397355\n},\n\"ResultPath\": \"$.coords\",\n\"End\": true\n}\n</code></pre> <p>Suppose the input to this state is the following.</p> <pre><code>{\n\"georefOf\": \"Home\"\n}\n</code></pre> <p>Then the output would be this.</p> <pre><code>{\n\"georefOf\": \"Home\",\n\"coords\": {\n\"x-datum\": 0.381018,\n\"y-datum\": 622.2269926397355\n}\n}\n</code></pre>"},{"location":"aws/stepfunctions/#choice","title":"Choice","text":"<p>A customer requests a credit limit increase. Using a <code>Choice state</code>, you can have Step Functions make decisions based on the Choice state\u2019s input. If the request is more than your customer\u2019s pre-approved credit limit, you can have Step Functions send your customer's request to a manager for sign-off. If the request is less than your customer\u2019s pre-approved credit limit, you can have Step Functions approve the request automatically.</p>"},{"location":"aws/stepfunctions/#parallel","title":"Parallel","text":"<p>A customer converts a video file into five different display resolutions, so viewers can watch the video on multiple devices. Using a Parallel state, Step Functions inputs the video file, so Lambda can process it into the five display resolutions at the same time.</p> <p>A Parallel state provides each branch with a copy of its own input data (subject to modification by the InputPath field). It generates output that is an array with one element for each branch, containing the output from that branch. There is no requirement that all elements be of the same </p>"},{"location":"aws/stepfunctions/#map","title":"Map","text":"<p>Use the Map state to run a set of workflow steps for each item in a dataset. The Map state's iterations run in parallel, which makes it possible to process a dataset quickly. Map states can use a variety of input types, including a JSON array, a list of Amazon S3 objects, or a CSV file.</p>"},{"location":"aws/stepfunctions/#wait","title":"Wait","text":"<p>A Wait state (\"Type\": \"Wait\") delays the state machine from continuing for a specified time. You can choose either a relative time, specified in seconds from when the state begins, or an absolute end time, specified as a timestamp.</p>"},{"location":"aws/stepfunctions/#workflow-studio","title":"Workflow Studio","text":"<p>Workflow Studio for AWS Step Functions is a low-code visual workflow designer for Step Functions that lets you create serverless workflows by orchestrating AWS services.</p> <p> </p>"},{"location":"aws/stepfunctions/#states-browser","title":"States browser","text":"<p>The States browser is where you select states to drag and drop into your workflow graph. The Actions tab provides a list of AWS APIs, and the Flow tab provides a list of flow states. While the Patterns tab provides several ready-to-use, reusable building blocks that you can use for a variety of use cases. You can search all states in the States Browser using the search field at the top.</p> <p> </p>"},{"location":"aws/stepfunctions/#quotas","title":"Quotas","text":"<p>AWS Step Functions places quotas on the sizes of certain state machine parameters, such as the number of API actions during a certain time period or the number of state machines that you can define. Although these quotas are designed to prevent a misconfigured state machine from consuming all of the resources of the system, many aren't hard quotas</p>"},{"location":"aws/storage/","title":"Storage","text":""},{"location":"aws/storage/#diff-storage-types","title":"Diff storage types","text":"Block Storage (EBS)       File Storage (EFS)       Object Storage (S3)     <p><code>Block storage</code> is raw storage in which the hardware storage device or drive is a disk or volume that is formatted and attached to the compute system for use. The storage is formatted into predefined continuous segments on the storage device. These segments are called blocks. The blocks are the basic fixed storage units used to store data on the device.</p> <p>Storage devices can be <code>hard disk drives</code> (HDDs), <code>solid state drives</code> (SSDs), or newer types of storage devices, such as <code>Non-Volatile Memory Express</code> (NVMe). In addition to individual storage devices, you can deploy block storage on <code>Storage Area Network</code> (SAN) systems.</p> <p><code>File storage</code> is built on top of block storage, typically serving as a file share or file server. File storage is created using an operating system that formats and manages the reading and writing of data to the block storage devices. The name file storage comes from the primary use of storing data as files typically in a directory tree hierarchy.</p> <p>The two most common storage protocols for file storage are <code>Server Message Block</code> (SMB) and <code>Network File System</code> (NFS). You can use the network protocols to communicate with remote computers and servers. You can also use server resources or share, open, and edit files.</p> <p><code>Object storage</code> is also built on top of block storage. Object storage is created using an operating system that formats and manages the reading and writing of data to the block storage devices. </p> <p>The name object storage comes from the primary use of storing the data within a binary object. Unlike file storage, object storage does not differentiate between types of data. The type of data or the file type becomes part of the data's metadata.</p> <p>An object is made up of a larger set of blocks organized by using a predetermined size. For example, one object storage system uses binary object sizes of 128 megabytes (MB). Smaller files or data are stored at a binary level within the object. Larger data files are stored by spreading the data across multiple objects.</p> <p>Difference between Object storage and Block Storage with an example</p> <p>Let's imagine you have an 80-gigabyte video files shooted by Amar on his 14 Pro Max that you're making edit corrections on. To know the best storage class here, we need to understand the difference between object storage and block storage.</p> <p>Object storage treats any file as a complete, discreet object. Now this is great for documents, and images, and video files that get uploaded and consumed as entire objects, but every time there's a change to the object, you must re-upload the entire file. There are no delta updates.</p> <p>Block storage breaks those files down to small component parts or blocks. This means, for that 80-gigabyte file, when you make an edit to one scene in the film and save that change, the engine only updates the blocks where those bits live. </p> <p>If you're making a bunch of micro edits, using EBS, elastic block storage, is the perfect use case. If you were using S3, every time you saved the changes, the system would have to upload all 80 gigabytes, the whole thing, every time.</p> <p>This means, if you are using complete objects or only occasional changes, S3 is victorious. If you are doing complex read, write, change functions, then, absolutely, EBS is your knockout winner. Your winner depends on your individual workload.  </p> <p> Different types of storages </p>"},{"location":"aws/storage/#overview-of-storages","title":"Overview of Storages","text":"<p> Various offerings <p></p>"},{"location":"aws/storagegw/","title":"Storage Gateway","text":"<ul> <li>AWS Storage Gateway is a hybrid-cloud storage service that gives you on-premises access to virtually unlimited cloud storage.</li> <li>Customers use <code>Storage Gateway</code> to simplify storage management and reduce costs for key <code>hybrid cloud storage use cases</code>.</li> <li>Storage Gateway provides a standard set of storage protocols such as <code>iSCSI</code>, <code>SMB</code>, and <code>NFS</code>, which allow you to use AWS storage without rewriting your existing applications.</li> <li>It provides low-latency performance by caching frequently accessed data on premises, while storing data securely and durably in Amazon cloud storage services.</li> <li>Storage Gateway optimizes data transfer to AWS by sending only changed data and compressing data</li> </ul>"},{"location":"aws/storagegw/#types-based-on-installation","title":"Types based on installation","text":"<p>They are of two types:</p> <ol> <li>It can be installed as VM on a <code>hypervisor</code>.</li> <li>It comes as a physical box.</li> </ol>"},{"location":"aws/storagegw/#sharing-protocols","title":"Sharing Protocols","text":"<p>Two types of file sharing protocols are:</p> <ul> <li><code>SMB</code>: Made by IBM originally and then taken by Microsoft. It is best used for Windows.</li> <li><code>NFS</code>: Made by Sun. It can be used both with SMB and NFS.</li> </ul>"},{"location":"aws/storagegw/#types-of-storage-gws","title":"Types of Storage GW's","text":"<p>3 different types of fo storage gateways are:</p> <ul> <li> <p><code>File gateway (using NFS)</code>: The File Gateway presents a file interface that enables you to store files as objects in Amazon S3 using the industry-standard NFS and SMB file protocols, and access those files via NFS and SMB from your datacenter or Amazon EC2, or access those files as objects with the S3 API.</p> </li> <li> <p><code>Volume Gateway</code>: The Volume Gateway presents your applications block storage volumes using the iSCSI protocol. Data written to these volumes can be asynchronously backed up as point-in-time snapshots of your volumes, and stored in the cloud as Amazon EBS snapshots. Tape Gateway: Used to backup data into the glacier.</p> </li> <li> <p><code>Cached volumes</code>: Entire dataset is stored on S3 and the most frequently used data is cached on-prem</p> </li> </ul>"},{"location":"aws/storagegw/#use-cases","title":"Use cases","text":"<p>Storage Gateway supports four key hybrid cloud use cases </p> <ol> <li>Move backups and archives to the cloud.</li> <li>Reduce on-premises storage with cloud-backed file shares.</li> <li>Provide on-premises applications low-latency access to data stored in AWS.</li> <li>Data lake access for pre and post processing workflows.</li> </ol>"},{"location":"aws/storagegw/#notes","title":"Notes","text":"<ul> <li>Snapshots are incremental backups (delta) which will only capture the changed blocks. </li> <li>In the case of stored volume all the data is stored on the data center (on-prem) and  the data is replicated  to the backend server in an <code>Async</code> fashion</li> <li>VM is used to replicate the data.</li> </ul>"},{"location":"aws/subnet/","title":"Subnet","text":"<p><code>Public subnets</code> contain resources that need to be accessible by the public, such as an online store\u2019s website.</p> <p><code>Private subnets</code> contain resources that should be accessible only through your private network, such as a database that contains customers\u2019 personal information and order histories. </p> <p>In a VPC, subnets can communicate with each other. For example, you might have an application that involves Amazon EC2 instances in a public subnet communicating with databases that are located in a private subnet.</p> <ul> <li>AWS reserves both the <code>first 4</code> and <code>last 1</code> IP addresses in each subnet\u2019s CIDR block. In total, AWS reserves 5 IP\u2019s for your subnet.</li> <li>We can have only 1 IG per subnet.</li> <li>When we create a custom VPC, no subnet or IGW is created.</li> </ul>"},{"location":"aws/systems-manager/","title":"Systems Manager","text":"<p>AWS Systems Manager is the operations hub for your AWS applications and resources and a secure end-to-end management solution for hybrid and multicloud environments that enables secure operations at scale. </p> <p> </p> <p>The above  diagram describes how some Systems Manager capabilities perform actions on your resources. The diagram doesn't cover all capabilities. Each enumerated interaction is described before the diagram.</p> <ol> <li> <p>Access Systems Manager \u2013 Use one of the available options for accessing Systems Manager.</p> </li> <li> <p>Choose a Systems Manager capability \u2013 Determine which capability can help you perform the action you want to perform on your resources. The diagram shows only a few of the capabilities that IT administrators and DevOps personnel use to manage their applications and resources.</p> </li> <li> <p>Verification and processing \u2013 Systems Manager verifies that your user, group, or role has the required AWS Identity and Access Management (IAM) permissions to perform the action you specified. If the target of your action is a managed node, the Systems Manager Agent (SSM Agent) running on the node performs the action. For other types of resources, Systems Manager performs the specified action or communicates with other AWS services to perform the action on behalf of Systems Manager.</p> </li> <li> <p>Reporting \u2013 Systems Manager, SSM Agent, and other AWS services that performed an action on behalf of Systems Manager report status. Systems Manager can send status details to other AWS services, if configured.</p> </li> <li> <p>Systems Manager operations management capabilities \u2013 If enabled, Systems Manager operations management capabilities such as Explorer, OpsCenter, and Incident Manager aggregate operations data or create artifacts in response to events or errors with your resources. These artifacts include operational work items (OpsItems) and incidents. Systems Manager operations management capabilities provide operational insight into your applications and resources and automated remediation solutions to help troubleshoot problems</p> </li> </ol>"},{"location":"aws/transit-gateway/","title":"Transit Gateway","text":"<p>A transit gateway is a network transit hub that you can use to interconnect your virtual private clouds (VPCs) and on-premises networks.</p> <p>As your cloud infrastructure expands globally, inter-Region peering connects transit gateways together using the AWS Global Infrastructure.</p> <p>All network traffic between <code>AWS data centers</code> is automatically encrypted at the physical layer.</p> <p> VPC Peering and Transit Gateway difference <p></p>"},{"location":"aws/transit-gateway/#use-cases","title":"Use Cases \ud83c\udfb9","text":""},{"location":"aws/transit-gateway/#centraized-router","title":"Centraized Router \ud83d\ude8f","text":"<p>The following diagram shows the key components of the configuration for this scenario. In this scenario, there are <code>three VPC attachments</code> and one <code>Site-to-Site VPN</code> attachment to the transit gateway. </p> <p>Packets from the subnets in <code>VPC A</code>, <code>VPC B</code>, and <code>VPC C</code> that are destined for a subnet in another VPC or for the VPN connection first route through the transit gateway.</p> <p> Centraized Router Use Case <p></p> <p>`</p>"},{"location":"aws/transit-gateway/#peered-transit-gateways","title":"Peered transit gateways \u26e9\ufe0f","text":"<p>The following diagram shows the key components of the configuration for this scenario. Transit gateway 1 has two VPC attachments, and transit gateway 2 has one Site-to-Site VPN attachment. Packets from the subnets in VPC A and VPC B that have the internet as a destination first route through transit gateway 1, then transit gateway 2, and then route to the VPN connection.</p> <p> Peered transit gateways use-case <p></p>"},{"location":"aws/transit-gateway/#centralized-outbound-route","title":"Centralized outbound-route \ud83d\udede","text":"<p>You have applications in <code>VPC A</code> and <code>VPC B</code> that need outbound only internet access. You configure <code>VPC C</code> with a <code>public NAT gateway</code> and an <code>internet gateway</code>, and a private subnet for the VPC attachment.</p> <p>Connect all VPCs to a <code>transit gateway</code>. Configure routing so that outbound internet traffic from <code>VPC A</code> and <code>VPC B</code> traverses the transit gateway to <code>VPC C</code>.</p> <p>The <code>NAT gateway</code> in <code>VPC C</code> routes the traffic to the <code>internet gateway</code>.</p> <p> Centrailzed outbound routing use-case <p></p>"},{"location":"aws/trusted-advisor/","title":"Trusted Advisor","text":"<p><code>AWS Trusted Advisor</code> is a web service that inspects your AWS environment and provides real-time recommendations in accordance with AWS best practices.</p> <p>Trusted Advisor compares its findings to AWS best practices in 5 categories: </p> <ul> <li>Cost optimization</li> <li>Performance</li> <li>Security</li> <li>Fault tolerance</li> <li>Service limits</li> </ul> <p> </p>"},{"location":"aws/vpc/","title":"VPC","text":"<p>A networking service that you can use to establish boundaries around your AWS resources is Amazon <code>Virtual Private Cloud (VPC)</code> </p> <p> VPC Example <p></p> <p>Virtual Private Gateway is shown below</p> <p> Private VPC Example <p></p> <p>The <code>Virtual Private Gateway (VPG)</code> is the component that allows protected internet traffic to enter into the VPC. </p> <p>Key Takeaway</p> <p>A <code>virtual private gateway</code> enables you to establish a <code>virtual private network (VPN)</code> connection between your VPC and a private network, such as an on-premises data center or internal corporate network.</p> <p>A virtual private gateway allows traffic into the VPC only if it is coming from an approved network.</p> <p>Here is an exmaple of 3 tier web applicaiton</p> <p> 3 Tier HA application in AWS <p></p>"},{"location":"aws/vpc/#elements-in-vpc","title":"Elements in VPC \ud83c\udf10","text":""},{"location":"aws/vpc/#elastic-network-interface","title":"Elastic Network Interface \ud83d\udd17","text":"<p>It is a logical networking component in a VPC that represents a virtual network card.</p>"},{"location":"aws/vpc/#subnet","title":"Subnet \ud83e\udd45","text":"<ul> <li>A range of IP addresses in your VPC. You can add AWS resources to a specified subnet.</li> <li>Use a <code>public subnet</code> for resources that must connect to the internet, and a <code>private subnet</code> for resources that don't connect to the internet.</li> </ul>"},{"location":"aws/vpc/#security-group","title":"Security group \ud83d\ude94","text":"<p>A security group controls the traffic that is allowed to reach and leave the resources that it is associated with. For example, after you associate a security group with an EC2 instance, it controls the inbound and outbound traffic for the instance.</p> <p>SG are stateful</p> <p>Security groups are <code>stateful</code>. </p> <p>For example, if you send a request from an instance, the response traffic for that request is allowed to reach the instance regardless of the inbound security group rules. Responses to allowed inbound traffic are allowed to leave the instance, regardless of the outbound rules.</p> <p>Security Group Rules</p> <ul> <li> <p>You can specify allow rules, but not deny rules \u274c</p> </li> <li> <p>When you first create a security group, it has no inbound rules. Therefore, no inbound traffic is allowed until you add inbound rules to the security group.</p> </li> <li> <p>When you first create a security group, it has an outbound rule that allows all outbound traffic from the resource. You can remove the rule and add outbound rules that allow specific outbound traffic only. If your security group has no outbound rules, no outbound traffic is allowed.</p> </li> <li> <p>When you associate multiple security groups with a resource, the rules from each security group are aggregated to form a single set of rules that are used to determine whether to allow access.</p> </li> <li> <p>When you add, update, or remove rules, your changes are automatically applied to all resources associated with the security group.</p> </li> <li> <p>The effect of some rule changes can depend on how the traffic is tracked.</p> </li> <li> <p>When you create a security group rule, AWS assigns a <code>unique ID</code> to the rule. You can use the ID of a rule when you use the API or CLI to modify or delete the rule</p> </li> </ul> <p>When you create a VPC, it comes with a default security group. You can create additional security groups for a VPC, each with their own inbound and outbound rules. You can specify the source, port range, and protocol for each inbound rule. You can specify the destination, port range, and protocol for each outbound rule.</p> <p>Security Group naming rules</p> <ul> <li> <p>You can assign a security group only to resources created in the same VPC as the security group. You can assign multiple security groups to a resource.</p> </li> <li> <p>When you create a security group, you must provide it with a name and a description. The following rules apply:</p> <ol> <li>A security group name must be unique within the VPC.</li> <li>Names and descriptions can be up to 255 characters in length.</li> <li>Names and descriptions are limited to the following characters: a-z, A-Z, 0-9, spaces, and ._-:/()#,@[]+=&amp;;{}!$*.</li> <li>When the name contains trailing spaces, we trim the space at the end of the name. For example, if you enter \"Test Security Group \" for the name, we store it as \"Test Security Group\".</li> <li>A security group name cannot start with sg-.</li> <li>Security groups are stateful. For example, if you send a request from an instance, the response traffic for that request is allowed to reach the instance regardless of the inbound security group rules. Responses to allowed inbound traffic are allowed to leave the instance, regardless of the outbound rules.</li> </ol> </li> </ul>"},{"location":"aws/vpc/#nacl","title":"NACL \ud83d\udccb","text":"<p>A network access control list (ACL) allows or denies specific inbound or outbound traffic at the subnet level. </p> <p>You can use the <code>default network ACL</code> for your VPC, or you can create a <code>custom network ACL</code> for your VPC with rules that are similar to the rules for your security groups in order to add an additional layer of security to your VPC.</p> <p>Example</p> <p> NACL Example </p> <p>The following diagram shows a VPC with two subnets. Each subnet has a network ACL. When traffic enters the VPC (for example, from a peered VPC, VPN connection, or the internet), the router sends the traffic to its destination. </p> <p>Network ACL A determines which traffic destined for <code>subnet 1</code> is allowed to enter <code>subnet 1</code>, and which traffic destined for a location outside <code>subnet 1</code> is allowed to leave <code>subnet 1</code>.</p> <p>Similarly, network ACL B determines which traffic is allowed to enter and leave <code>subnet 2</code></p> <ul> <li> <p>A subnet can only be associated with a single NACL at a time.</p> </li> <li> <p>The default subnet ACL allows all inbound and outbound traffic.</p> </li> <li> <p>NACL rules have</p> <ul> <li>Rule number</li> <li>Protocol</li> <li>Choice of ALLOW or DENY</li> <li><code>CIDR range</code> </li> <li><code>Porst</code> or <code>port range</code> for inbound and outbound traffic</li> </ul> </li> </ul> <p>Few notes about NACL</p> <p>The following are the basic things that you need to know about network ACLs:</p> <ul> <li> <p>Your VPC automatically comes with a modifiable default network ACL. By default, it allows all inbound and outbound IPv4 traffic and, if applicable, IPv6 traffic.</p> </li> <li> <p>You can create a <code>custom network ACL</code> and associate it with a subnet to allow or deny specific inbound or outbound traffic at the subnet level.</p> </li> <li> <p>Each subnet in your VPC must be associated with a network ACL. If you don't explicitly associate a subnet with a network ACL, the subnet is automatically associated with the default network ACL.</p> </li> <li> <p>You can associate a network ACL with multiple subnets. However, a subnet can be associated with only one network ACL at a time. When you associate a network ACL with a subnet, the previous association is removed.</p> </li> <li> <p>A network ACL has inbound rules and outbound rules. Each rule can either allow or deny traffic. Each rule has a number from 1 to 32766. We evaluate the rules in order, starting with the lowest numbered rule, when deciding whether allow or deny traffic. If the traffic matches a rule, the rule is applied and we do not evaluate any additional rules. We recommend that you start by creating rules in increments (for example, increments of 10 or 100) so that you can insert new rules later on, if needed.</p> </li> <li> <p>We evaluate the network ACL rules when traffic enters and leaves the subnet, not as it is routed within a subnet.</p> </li> <li> <p>NACLs are <code>stateless</code>, which means that information about previously sent or received traffic is not saved. If, for example, you create a NACL rule to allow specific inbound traffic to a subnet, responses to that traffic are not automatically allowed. This is in contrast to how security groups work. Security groups are stateful, which means that information about previously sent or received traffic is saved. If, for example, a security group allows inbound traffic to an EC2 instance, responses are automatically allowed regardless of outbound security group rules.</p> </li> </ul>"},{"location":"aws/vpc/#route-table","title":"Route table \ud83d\udcd1","text":"<ul> <li> <p>It contains a set of routes that AWS uses to direct the network traffic for your VPC.</p> </li> <li> <p>You can explicitly associate a subnet with a particular route table. </p> </li> </ul> <p>Main route table and subnet association</p> <p>By default, the subnet is associated with the <code>main route table</code></p>"},{"location":"aws/vpc/#route","title":"Route \ud83d\udee3\ufe0f","text":"<p>Each route in a <code>route table</code> specifies a range of IP addresses and the destination where Lambda sends the traffic for that range. The route also specifies a target, which is the gateway, network interface, or connection through which to send the traffic.</p>"},{"location":"aws/vpc/#nat-gateway","title":"NAT gateway \ud83c\udf09","text":"<p>An AWS <code>Network Address Translation (NAT)</code> service that controls access from a <code>private VPC</code> private subnet to the Internet.</p>"},{"location":"aws/vpc/#vpc-endpoints","title":"VPC endpoints \ud83d\udd87\ufe0f","text":"<p>You can use an Amazon VPC endpoint to create private connectivity to services hosted in AWS, without requiring access over the internet or through a NAT device, VPN connection, or AWS Direct Connect connection.</p>"},{"location":"aws/vpc/#vpc-flow-logs","title":"VPC flow logs \ud83d\udcd5","text":"<p>VPC Flow Logs is a feature that enables you to capture information about the IP traffic going to and from network interfaces in your VPC.</p> <p>We can use them for troubleshooting to see if there is some attack on the VPC. Also, they can let us know why the communication between the VPC\u2019s is not working out.</p> <p>You can create a flow log for a VPC, a subnet, or a network interface. If you create a flow log for a subnet or VPC, each network interface in that subnet or VPC is monitored.</p>"},{"location":"aws/vpc/#vpc-peering","title":"VPC Peering \ud83e\ude99","text":"<p>A VPC peering connection is a networking connection between two VPCs that enables you to route traffic between them using private <code>IPv4 addresses</code> or <code>IPv6 addresses</code>.</p> <p>Instances in either VPC can communicate with each other as if they are within the same network. You can create a VPC peering connection between your own VPCs, or with a VPC in another AWS account.</p> <p>The VPCs can be in different Regions (also known as an inter-Region VPC peering connection).</p> <p> VPC Peering <p></p> <p>No IP Overlap</p> <p>AWS uses the existing infrastructure of a VPC to create a VPC peering connection; it is neither a gateway nor a VPN connection, and does not rely on a separate piece of physical hardware. There is no single point of failure for communication or a bandwidth bottleneck. </p> <p> VPC Peering Example wihh no IP overlap </p>"},{"location":"aws/vpc/#vpc-sharing","title":"VPC Sharing \ud83d\udd0d","text":"<p>Why is VPC sharing requried?</p> <p>Sharing VPCs is useful when network isolation between teams does not need to be strictly managed by the VPC owner, but the account level users and permissions must be. </p> <p>With Shared VPC, multiple AWS accounts create their application resources (such as Amazon EC2 instances) in shared, centrally managed Amazon VPCs.</p> <p>In this model, the account that owns the VPC (owner) shares one or more subnets with other accounts (participants). After a subnet is shared, the participants can view, create, modify, and delete their application resources in the subnets shared with them.</p> <p>Participants cannot view, modify, or delete resources that belong to other participants or the VPC owner. Security between resources in shared VPCs is managed using security groups, network access control lists (NACLs), or through a firewall between the subnets.</p> <p>VPC sharing allows multiple AWS accounts to create their application resources, such as Amazon EC2 instances, Amazon Relational Database Service (RDS) databases, Amazon Redshift clusters, and AWS Lambda functions, into shared, centrally-managed virtual private clouds (VPCs). </p> <p> VPC Sharing with Owner and participants <p></p> <p>In this model, the account that owns the VPC (<code>owner</code>) shares one or more subnets with other accounts (<code>participants</code>) that belong to the same organization from AWS Organizations.</p> <p>After a subnet is shared, the participants can view, create, modify, and delete their application resources in the subnets shared with them. Participants cannot view, modify, or delete resources that belong to other participants or the VPC owner.</p>"},{"location":"aws/vpc/#vpc-endpoints_1","title":"VPC Endpoints \u26a1\ufe0f","text":"<p>A VPC endpoint enables customers to privately connect to supported AWS services and VPC endpoint services powered by AWS PrivateLink. Amazon VPC instances do not require public IP addresses to communicate with resources of the service. Traffic between an Amazon VPC and a service does not leave the Amazon network.</p> <p>VPC endpoints are virtual devices. They are horizontally scaled, redundant, and highly available Amazon VPC components that allow communication between instances in an Amazon VPC and services without imposing availability risks or bandwidth constraints on network traffic. There are two types of VPC endpoints:</p> <ol> <li> <p>Interface endpoints:  Interface endpoints enable connectivity to services over AWS PrivateLink. These services include some <code>AWS managed services</code>, services hosted by other <code>AWS customers</code> and partners in their own Amazon VPCs (referred to as endpoint services), and supported AWS Marketplace partner services. The owner of a service is a service provider. The principal creating the interface endpoint and using that service is a service consumer. </p> </li> <li> <p>Gateway endpoints:  A gateway endpoint targets specific IP routes in an Amazon VPC route table, in the form of a prefix-list, used for traffic destined to <code>Amazon DynamoDB</code> or <code>Amazon S3</code>. Gateway endpoints do not enable AWS PrivateLink. </p> </li> </ol> <p>Remember</p> <p>Interface Endpoints use PrivateLink to establish a private and secure connection, while Gateway Endpoints are used within route tables to reach supported services. </p> <p> VPC Gateway Endpoints <p></p>"},{"location":"aws/vpc/#dynamic-routing","title":"Dynamic Routing \ud83d\ude8f","text":"<p>The type of routing that you select can depend on the make and model of your customer gateway device. If your customer gateway device supports <code>Border Gateway Protocol (BGP)</code>, specify dynamic routing when you configure your <code>Site-to-Site VPN</code> connection. </p> <p>If your customer gateway device does NOT support BGP, specify static routing.</p> <p> Dynamic Routing support <p></p>"},{"location":"aws/vpc/#customer-managed-s2s-vpn","title":"Customer Managed S2S VPN \ud83c\udf10","text":"<p>One advantage of customer-managed site-to-site VPNs is that you can control the protocols that your connection uses for authentication, integrity checks, and encryption. When using VPC peering, you have no control of security protocols use to secure traffic. </p>"},{"location":"aws/vpc/#notes","title":"Notes","text":"<p>Some notes on VPC are</p> <ul> <li> <p>NACL applies to all instances in an associated subnet.</p> </li> <li> <p>NACLs are always evaluated first because they exist at the border of a subnet. As <code>security groups</code> are attached to instances, they are not processed until traffic passes through the NACL and into the instance\u2019s subnet.</p> </li> <li> <p>NACLs are associated with subnets, not VPC.</p> </li> <li> <p>Each rule in an NACL has a number, and those rules are evaluated using those numbers, moving from low to high.</p> </li> <li> <p>Each subnet in your VPC must be associated with an NACL.</p> </li> <li> <p>An NACL is associated with a subnet, not an instance or VPC. It can be associated with a single subnet or multiple subnets.</p> </li> <li> <p>A VPC spans all the availability zones in a region.</p> </li> <li> <p>You must always select a region to create a VPC, and you must always provide a CIDR block. VPCs span all the AZs in a region, so that is not required.</p> </li> <li> <p>While you can add secondary IPv4 CIDR blocks, you cannot add additional CIDR blocks for IPv6 at this time.</p> </li> <li> <p>When creating a VPC, you can specify an option name (no description needed), a required IPv4 CIDR block, and an optional IPv6 CIDR block.</p> </li> <li> <p>A VPN-only subnet routes traffic through a virtual private gateway rather than an internet gateway</p> </li> <li> <p>At a minimum, a VPC-only subnet must have a routing table routing traffic and a virtual private gateway to which traffic is routed.</p> </li> <li> <p>You can only create 5 VPCs per region by default. Creating more requires a request to AWS.</p> </li> <li> <p>You can create 200 subnets per VPC.</p> </li> <li> <p>You\u2019re allowed 5 elastic IP addresses per region unless you have the default limits raised by AWS.</p> </li> <li> <p>A security group denies all traffic unless explicitly allowed. This means it functions as a whitelist.</p> </li> <li> <p>A VPC endpoint is a connection to an AWS service and explicitly does not use internet gateways or VPN connections.</p> </li> <li> <p>Internet gateways scale horizontally, not vertically. They are also redundant and highly available automatically.</p> </li> <li> <p>Default VPC has an internet gateway automatically attached.</p> </li> <li> <p>Load Balancers</p> <ul> <li> <p><code>Application load balancers</code> operate at the Application layer, which is layer 7 of the OSI model. </p> </li> <li> <p><code>ELBs</code> (classic load balancers) operate at the Transport layer, layer 4, as well as layer 7.</p> </li> <li> <p><code>Network load balancers</code> operate at layer 4 as well</p> </li> </ul> </li> <li> <p>A NAT device\u2014network address translation\u2014provides routing for instances to an internet gateway but can prevent undesired inbound traffic.</p> </li> <li> <p>A site-to-site connection is going to require a private subnet on the AWS side with private instances within it.</p> </li> <li> <p>An <code>egress-only gateway</code> is for use with IPv6 traffic and only allows outbound traffic. They are stateful.</p> </li> <li> <p>The default VPC has an internet gateway attached by default while custom VPC do not</p> </li> <li> <p>The default VPC has a CIDR block of <code>/16</code>, but the default subnet in each AZ is a <code>/20</code>.</p> </li> <li> <p>Default VPC does get a subnet automatically (as well as an internet gateway).</p> </li> <li> <p>While the default VPC automatically creates a subnet, while custom VPCs do not create a subnet. </p> </li> <li> <p>With custom VPC, You do automatically get a security group, route table, and NACL.</p> </li> <li> <p>A VPC endpoint can connect to S3 and DynamoDB, as well as a host of additional AWS services. It does not require an internet gateway or a VPN connection and does not route traffic over the public Internet.</p> </li> <li> <p>A VPC endpoint is a virtual device that provides redundancy via AWS (and automatically).</p> </li> <li> <p>A VPC endpoint provides a connection over the Amazon network between your VPC and service, such as S3. This avoids leaving the network and routing over the public Internet, which inherently provides greater security for the traffic involved</p> </li> <li> <p>VPCs can peer with other VPCs, in the same account or different ones</p> </li> </ul>"},{"location":"aws/vpg/","title":"Virtual Private Gateway (VGW)","text":"<p>A virtual private gateway establishes a connection to the cloud while creating a <code>VPN tunnel</code>.</p> <p>The AWS virtual private gateways come into the picture when we want to establish a VPN connection to Amazon VPC. </p> <p>Where is VPG hosted?</p> <p>The gateway on the customer(our) side is known as <code>Customer Gateway</code>, and the gateway on the AWS side of the VPN tunnel is named <code>Virtual Private Gateway</code>.</p> <p> VPG Example <p></p> <p>AWS Virtual Private Gateway establishes a secure connection between your on-premises server and cloud-hosted VPC. On establishing this connection, you get access to all the resources of your AWS VPC using its private IP address from your on-premises data center.</p> <p> VPG Example in each of the VPC <p></p> <p>Use transit gateway in case of multiple VPC's</p> <p>A transit gateway is a transit hub that you can use to interconnect your VPCs and your on-premises networks. For more information, see Amazon VPC Transit Gateways. You can create a Site-to-Site VPN connection as an attachment on a transit gateway.</p> <p>The following diagram shows a VPN connection between multiple VPCs and your on-premises network using a transit gateway. The transit gateway has three VPC attachments and a VPN attachment.</p> <p> Transit Gateway Example </p>"},{"location":"aws/waf/","title":"WAF","text":"<ul> <li>AWS WAF is a web application firewall that lets you monitor network requests that come into your <code>web applications</code>.</li> <li>AWS WAF works together with Amazon CloudFront and an Application Load Balancer.</li> </ul>"},{"location":"aws/well-architected/","title":"Well Architected","text":"<p>The Well-Architected Framework is based on six pillars: </p> <ul> <li><code>Operational excellence</code>: It is the ability to run and monitor systems to deliver business value and to continually improve supporting processes and procedures.  </li> <li><code>Security</code>: It is the ability to protect information, systems, and assets while delivering business value through risk assessments and mitigation strategies. </li> <li><code>Reliability</code>:</li> <li><code>Performance efficiency</code>: It is the ability to use computing resources efficiently to meet system requirements and to maintain that efficiency as demand changes and technologies evolve. </li> <li><code>Cost optimization</code>: It is the ability to run systems to deliver business value at the lowest price point. </li> <li><code>Sustainability</code>: Sustainability is the ability to continually improve sustainability impacts by reducing energy consumption and increasing efficiency across all components of a workload by maximizing the benefits from the provisioned resources and minimizing the total resources required.</li> </ul>"},{"location":"aws/well-architected/#generic-patterns","title":"Generic patterns","text":"<p> Generic SPA example <p> </p>"},{"location":"aws/xray/","title":"X-Ray","text":"<ul> <li>AWS X-Ray is a service that collects data about requests that your application serves, and provides tools that you can use to view, filter, and gain insights into that data to identify issues and opportunities for optimization.  to downstream AWS resources, microservices, databases, and web APIs.</li> <li>Its great to create the <code>service maps</code> for micro-services</li> </ul>"},{"location":"azure/aad/","title":"Azure Active Directory","text":"<ul> <li>It is global IAM service for Azure</li> <li>We can use it for B2B and B2C (for support)</li> <li>We can do monitoring with Azure AD. Example: check if user is logging in from another location based on the distance.</li> </ul> <p>AAD vs Windows server AD</p> <ul> <li>You must use <code>Windows Server Active Directory</code> to update the identity, contact info, or job info for users whose source of authority is <code>Windows Server Active Directory</code>.</li> <li><code>Usage location</code> is an Azure property that can only be modified from Azure AD (for all users including Windows Server AD users synced via Azure AD Connect).</li> </ul>"},{"location":"azure/aad/#aad-forest","title":"AAD Forest","text":"<p>As you may know, Active Directory (AD) is a database and set of services that help users get their work done in a Microsoft IT environment. The database (or directory) contains critical information about your environment, including what users and computers there are and who\u2019s allowed to do what. The services control much of the activity that goes on; in particular, they make sure each person is who they claim to be (authentication) and allow them to access only the data they\u2019re allowed to use (authorization).</p> <p>Active Directory has three main tiers: domains, domain trees and forests:</p> <ul> <li> <p><code>Domain</code> is a group of users, computers and other Active Directory objects that share the same <code>AD database</code>, such as all the AD objects for your company\u2019s Chicago office.</p> </li> <li> <p><code>Domain tree</code> is a collection of one or multiple domains grouped together in a hierarchical parent-child structure with a contiguous namespace and transitive trust relationships between the domains. For example, the domains Quest.com and <code>Sales.Quest.com</code> would be considered part of the <code>Quest.com</code> domain tree</p> </li> <li> <p><code>Forest</code> is a group of multiple trees with shared directory schemas, catalogs, application information and domain configurations. The schema defines an object\u2019s class and attributes in a forest, and the global catalog servers provide a listing of all the objects in an AD forest.</p> </li> </ul> <p>Difference between AAD Tenant and AAD Forest</p> <p>Beyond the structural differences, it\u2019s important to understand the differences in purpose between an Active Directory domain and an Active Directory forest:</p> <ul> <li> <p>The domain is a management boundary \u2014 The objects for a particular single domain are stored in a single Active Directory database and can be managed together.</p> </li> <li> <p>The forest is a security boundary \u2014 Objects in different Active Directory forests are not able to interact with each other unless the administrators of each forest create a trust between them.</p> </li> </ul>"},{"location":"azure/aad/#security-basics","title":"Security basics","text":"<ul> <li><code>User Principal</code>: The one accessing the account</li> <li><code>Service Principal</code>: Identity created by system to access applications</li> <li><code>Audience</code>: The Audince is the applicaton id for API which your App has permissions on</li> <li> <p><code>Role and Scope</code>: Scopes are typically associated with API access. An API defines what scopes are available (what services it provides). For example a user account management API might define scopes like <code>read:user, create:user, update:user</code>. These are the capabilities the API provides, but not necessarily what any given user can do. </p> <p>In the \u201cRole &amp; Scope\u201d model, Roles are defined, and users are given a Role. Individual Scopes are associated with a given Role, combining all these elements together. For example, you might have:</p> <pre><code>Role: Audit, API: user_manager, Scopes: read:user\nRole: Access Control, API:  user_manager, Scopes: create:user, update:user\n</code></pre> <p>And maybe Amar has the Audit role, while Bob has the Access Control role.</p> <p>When you request a scope, the Authorization Server (Auth0/AAD) will decide whether you get that scope or not. You can request anything you like, but the <code>Authorization Server</code> sends back a token with only the scopes it has decided you are allowed to have.</p> Role defination example<pre><code>{\n    \"value\": [\n        {\n            \"properties\": {\n                \"roleName\": \"Billing Reader Plus\",\n                \"type\": \"CustomRole\",\n                \"description\": \"Read billing data and download invoices\",\n                \"assignableScopes\": [\n                    \"/subscriptions/{subscriptionId1}\"\n                ],\n                \"permissions\": [\n                    {\n                        \"actions\": [\n                            \"Microsoft.Authorization/*/read\",\n                            \"Microsoft.Billing/*/read\",\n                            \"Microsoft.Commerce/*/read\",\n                            \"Microsoft.Consumption/*/read\",\n                            \"Microsoft.Management/managementGroups/read\",\n                            \"Microsoft.CostManagement/*/read\",\n                            \"Microsoft.Billing/invoices/download/action\",\n                            \"Microsoft.CostManagement/exports/*\"\n                        ],\n                        \"notActions\": [\n                            \"Microsoft.CostManagement/exports/delete\"\n                        ],\n                        \"dataActions\": [],\n                        \"notDataActions\": []\n                    }\n                ],\n                \"createdOn\": \"2021-05-22T21:57:23.5764138Z\",\n                \"updatedOn\": \"2021-05-22T21:57:23.5764138Z\",\n                \"createdBy\": \"{createdByObjectId1}\",\n                \"updatedBy\": \"{updatedByObjectId1}\"\n            },\n            \"id\": \"/subscriptions/{subscriptionId1}/providers/Microsoft.Authorization/roleDefinitions/{roleDefinitionId1}\",\n            \"type\": \"Microsoft.Authorization/roleDefinitions\",\n            \"name\": \"{roleDefinitionId1}\"\n        }\n    ]\n}\n</code></pre> </li> </ul>"},{"location":"azure/aad/#what-is-azure-ad-tenant","title":"What is Azure AD tenant?","text":"<p>Tldr</p> <p>Tenant is an instance of AAD</p> <p>An Azure AD tenant is a <code>reserved Azure AD service instance</code> that an organization receives and owns once it signs up for a Microsoft cloud service such as Azure, Microsoft Intune, or Microsoft 365.</p> <p> </p> <p>Remember</p> <p>Each tenant represents an organization, and is distinct and separate from other Azure AD tenants.</p>"},{"location":"azure/aad/#scp-service-connection-point","title":"SCP (Service Connection Point)","text":"<p>The Active Directory Schema defines a <code>Service Connection Point (SCP)</code> object class to make it easy for a service to publish service-specific data in the directory. Clients of the service use the data in an SCP to locate, connect to, and authenticate an instance of your service.</p>"},{"location":"azure/aad/#service-principal","title":"Service Principal","text":"<p>An <code>Azure service principal</code> is an identity created for use with applications, hosted services, and automated tools to access Azure resources. This access is restricted by the <code>roles</code> assigned to the service principal, giving you control over which resources can be accessed and at which level.</p> <p> </p> <p>For security reasons, it's always recommended to use service principals with automated tools rather than allowing them to log in with a user identity.</p> <p>There are two types of authentication available for service principals: </p> <ol> <li>Password-based authentication</li> <li>Certificate-based authentication.</li> </ol>"},{"location":"azure/aad/#sp-vs-managed-identity","title":"SP VS Managed Identity","text":"SP and Managed Identity <p>The difference between managed identities and service principals is subtle. Service principals are no longer recommended for services that support managed identities. Managed identities are effectively \u201cmanaged service principals\u201d and remove the need for storing credentials in the application\u2019s configuration and instead inject certificates into the resource that the managed identity is created into.</p>"},{"location":"azure/aad/#active-directory-vs-azure-ad","title":"Active Directory vs Azure AD","text":"<p>They are not competing services. Think of them as 2 separate services.</p> Acrive Directory Azure AD Organizational Units Administrative Units Kerberos, LDAP SAML, OAuth Hierarchial Flat structure On Premise Cloud based and Global"},{"location":"azure/aad/#default-tenant","title":"Default Tenant","text":"<p>The default tenant is created when we create first subscription. Later, we can create another tenant and will have option to switch between various tenants. Below you can see how we can create a new tenant.</p> <p> </p>"},{"location":"azure/aad/#users","title":"Users","text":"<ul> <li>Users are set of JSON properties</li> </ul> <p>Types of users:</p> <ol> <li>Admins</li> <li>Members:</li> <li><code>Assigned</code>: Specifically assign a user to a group</li> <li><code>Dynamic user</code>: Rules are created which  automate group membership via user attributes</li> <li>Guests</li> </ol> <p> </p>"},{"location":"azure/aad/#communication-bw-diff-tenants","title":"Communication bw diff tenants","text":"<p>The associated service principal in tenant 1 will be used to authenticate to resources within the service's own subscription. A separate associated service principal which resides in tenant 2 will be used to authenticate to resources in subscriptions 2 and 3.</p>"},{"location":"azure/aad/#core-concepts","title":"Core Concepts","text":""},{"location":"azure/aad/#groups","title":"Groups","text":"<p>Type of groups:</p> <ol> <li>Security Group: Azure AD Security Groups are analogous to Security Groups in on-prem Windows Active Directory. They are Security Principals, which means they can be used to secure objects in Azure AD. They can be created natively in Azure AD, or synced from Windows AD with Azure AD Connect. Their membership can be static, or it can be generated dynamically with rules.</li> <li>Microsoft 365: Microsoft 365 Groups are a membership object in Microsoft 365 that eases the task of ensuring a group of people have consistent permissions to a group of related resources. You may see them referred to as Microsoft 365 Groups or Unified Groups. </li> </ol>"},{"location":"azure/aad/#dynamic-groups","title":"Dynamic groups","text":"<p><code>Dynamic group membership</code> adds and removes group members automatically using membership rules based on member attributes.  You can set up a rule for dynamic membership on security groups or Microsoft 365 groups.</p> <p>Remember</p> <p>When the attributes of a user or a device change, the system evaluates all dynamic group rules in a directory to see if the change would trigger any group adds or removes. If a user or device satisfies a rule on a group, they're added as a member of that group. If they no longer satisfy the rule, they're removed. You can't manually add or remove a member of a dynamic group.</p>"},{"location":"azure/aad/#rule-builder","title":"Rule Builder","text":"<p>Azure AD provides a rule builder to create and update your important rules more quickly. The rule builder supports the construction of up to five expressions. The rule builder makes it easier to form a rule with a few simple expressions, however, it can't be used to reproduce every rule. If the rule builder doesn't support the rule you want to create, you can use the text box.</p>"},{"location":"azure/aad/#administrative-units","title":"Administrative Units","text":"<p>An administrative unit is an <code>Azure AD resource</code> that can be a container for other Azure AD resources. An administrative unit can contain only users, groups, or devices.</p> <p>Use of Admin Units</p> <p>Administrative units restrict permissions in a role to any portion of your organization that you define. You could, for example, use administrative units to delegate the Helpdesk Administrator role to regional support specialists, so they can manage users only in the region that they support.</p> <p> </p> <p>A use case scenario</p> <p>It can be useful to restrict administrative scope by using administrative units in organizations that are made up of independent divisions of any kind. Consider the example of a large university that's made up of many autonomous schools (School of Business, School of Engineering, and so on). Each school has a team of IT admins who control access, manage users, and set policies for their school.</p> <p>A central administrator could:</p> <ul> <li>Create an AU for the School of Business.</li> <li>Populate the AU with only students and staff within the School of Business.</li> <li>Create a role with administrative permissions over only Azure AD users in the School of Business AU.</li> <li>Add the business school IT team to the role, along with its scope.</li> </ul> <p> </p>"},{"location":"azure/aad/#aad-connect","title":"AAD Connect","text":"<p>Azure AD Connect is an on-premises Microsoft application that's designed to meet and accomplish your hybrid identity goals. If you're evaluating how to best meet your goals, you should also consider the cloud-managed solution Azure AD Connect cloud sync.</p> <p> </p> <p>What is AAD Connect Health?</p> <p>Azure Active Directory (Azure AD) Connect Health provides robust monitoring of your on-premises identity infrastructure. It enables you to maintain a reliable connection to Microsoft 365 and Microsoft Online Services.</p>"},{"location":"azure/aad/#azure-ad-connect-sync","title":"Azure AD Connect sync","text":"<p><code>Azure AD Connect sync</code> synchronize changes occurring in your <code>on-premises directory</code> using a scheduler. There are two scheduler processes:     - one for password sync      - another for object/attribute sync and maintenance tasks</p> <p>When to Sync?</p> <p>By default every <code>30 minutes</code> a synchronization cycle is run. If you have modified the synchronization cycle you will need to make sure that a synchronization cycle is run at least once every 7 days.</p> <p>Tip</p> <p>Best way is either a Synchronization being executed through the <code>Azure AD Connect</code>, in the Portal or using the command <code>Start-ADSyncSyncCycle -PolicyType Delta</code>. Running a full sync cycle can be very time-consuming, so if you need to replicate the user information to Azure AD immediately then run <code>Start-ADSyncSyncCycle -PolicyType Delta</code>.</p>"},{"location":"azure/aad/#licence","title":"Licence","text":"<p>Many AAD services require you to license each of your users or groups (and associated members) for that service. Only users with <code>active licenses</code> will be able to access and use the <code>licensed Azure AD services</code> for which that's true. Licenses are applied per tenant and don't transfer to other tenants.</p>"},{"location":"azure/aad/#configuring-sspr-self-service-password-reset","title":"Configuring SSPR (Self Service Password Reset)","text":"<p>Azure Active Directory (Azure AD) self-service password reset (SSPR) gives users the ability to change or reset their password, with no administrator or help desk involvement. If Azure AD locks a user's account, or they forget their password, they can follow prompts to unblock themselves and get back to work. This ability reduces help desk calls and loss of productivity when a user can't sign in to their device or an application.</p> <p>SSPR can be set for </p> <ul> <li>All users</li> <li>None</li> <li>Selected Users</li> </ul> <p>Remember</p> <ul> <li>Security questions as an authentication method is not available for admins</li> <li>Admins are always enabled for SSPR and need to have 2 security methods to reset their password.</li> </ul> <p>Various options for SSPR are</p> <ol> <li>Mobile app notification</li> <li>Mobile app code</li> <li>Email</li> <li>Mobile phone call</li> <li>Office phone call</li> <li>Security questions (least recommended)</li> </ol>"},{"location":"azure/aad/#device-management","title":"Device Management","text":"<ul> <li>We can register the devices which are not in our organization</li> </ul> <p>Azure AD registered devices are signed in to using a local account like a Microsoft account on a Windows 10 or newer device. These devices have an Azure AD account for access to organizational resources. Access to resources in the organization can be limited based on that Azure AD account and Conditional Access policies applied to the device identity.</p> <p>MDM</p> <p>Administrators can secure and further control these Azure AD registered devices using Mobile Device Management (MDM) tools like Microsoft Intune. MDM provides a means to enforce organization-required configurations like requiring storage to be encrypted, password complexity, and security software kept updated.</p>"},{"location":"azure/aad/#registration-options","title":"Registration options","text":"<p>We have 3 options: - Azure AD registered: This is the least restrictive and allows BYOD. Also supportgs iOS and macOS - Azure AD joined: Azure AD joined devices are signed in to using an organizational Azure AD account. In this case the device is owned by organization. - Hybrid Azure AD joined: Joined to on-premises AD and Azure AD requiring organizational account to sign in to the device</p>"},{"location":"azure/aad/#conditional-access-policy","title":"Conditional Access Policy","text":"<p>Info</p> <p>Organizations who have deployed Microsoft Intune can use the information returned from their devices to identify devices that meet compliance requirements such as:</p> <pre><code>Requiring a PIN to unlock\nRequiring device encryption\nRequiring a minimum or maximum operating system version\nRequiring a device isn't jailbroken or rooted\n</code></pre>"},{"location":"azure/aad/#rbac-and-aad-roles","title":"RBAC and AAD roles","text":"<p>Roles have descending effect as shown in below figure</p> <p> </p> <p>RBAC Vs Azure AD?</p> <p>Azure (RBAC) and Azure AD roles are independent. AD roles do not grant access to resources and Azure roles do not grant access to Azure AD. However, a Global Administrator in AD can elevate access to all subscriptions and will be User Access Administrator in Azure root scope.</p> <p> </p>"},{"location":"azure/aad/#elevated-access-for-global-admin","title":"Elevated access for global admin","text":"<p>When you set the toggle to <code>Yes</code>, you are assigned the <code>User Access Administrator role</code> in Azure RBAC at root scope <code>/</code>. This grants you permission to assign roles in all Azure subscriptions and management groups associated with this Azure AD directory. This toggle is only available to users who are assigned the Global Administrator role in Azure AD.</p> <p>When you set the toggle to <code>No</code>, the <code>User Access Administrator role</code> in Azure RBAC is removed from your user account. You can no longer assign roles in all Azure subscriptions and management groups that are associated with this Azure AD directory. You can view and manage only the Azure subscriptions and management groups to which you have been granted access.</p>"},{"location":"azure/aad/#azure-rbac-roles","title":"Azure RBAC roles","text":"<ul> <li><code>Owner</code>: Grants full access to manage all resources, including the ability to assign roles in Azure RBAC.</li> <li><code>Contributor</code>: Grants full access to manage all resources, but does NOT allow you to assign roles in Azure RBAC. (you cannot add users or changes their rights)</li> <li><code>User Access Administrator</code>: Lets you manage user access to Azure resources. </li> <li><code>Reader</code>:  View all resources, but does not allow you to make any changes. </li> <li><code>Security Admin</code>: View and update permissions for Security Center. Same permissions as the Security Reader role and can also update the security policy and dismiss alerts and recommendations. </li> <li><code>Network Contributor</code>: Lets you manage networks, but not access to them. (so you can add VNET, subnet, etc)</li> </ul>"},{"location":"azure/aad/#azure-ad-roles","title":"Azure AD roles","text":"<ul> <li><code>Global admin role</code>: Can manage azure AD resources</li> <li><code>Billing admin role</code>: Can perform billing tasks</li> <li><code>User admin role</code>: Can manage users and groups</li> <li><code>Helpdesk admin</code>: Can reset passwords</li> </ul> <p>Scope of roles</p> <p>Scope of AD roles is at tenant level whereas RBAC roles scope can be at different levels</p>"},{"location":"azure/aad/#mfa-provider","title":"MFA Provider","text":"<p>Info</p> <p>Authentication providers can be found in the <code>Azure portal</code> &gt; <code>Azure Active Directory</code> &gt; <code>Security</code> &gt; <code>MFA</code> &gt; <code>Providers</code></p> <p>Two-step verification is available by default for Global Administrators who have Azure Active Directory, and Microsoft 365 users. However, if you wish to take advantage of advanced features then you should purchase the full version of Azure AD Multi-Factor Authentication (MFA).</p> <p>An Azure AD Multi-Factor Auth Provider is used to take advantage of features provided by Azure AD Multi-Factor Authentication for users who do not have licenses.</p> <p>Warning</p> <p>You can't change the usage model (per enabled user or per authentication) after an MFA provider is created.</p>"},{"location":"azure/aad/#per-user","title":"Per-user","text":"<p>The per-user option calculates the number of users who are eligible to perform MFA, which is all users in Azure AD, and all enabled users in MFA Server. This option is best if some users have licenses, but you need to extend MFA to more users beyond your licensing limits.</p>"},{"location":"azure/aad/#per-authentication","title":"Per authentication","text":"<p>The per-authentication option calculates the number of authentications performed against your tenant in a month. This option is best if some users authenticate only occasionally</p>"},{"location":"azure/aad/#3rd-party-access","title":"3<sup>rd</sup> party access","text":""},{"location":"azure/aad/#catalog","title":"Catalog","text":"<p>A catalog is a container of resources and access packages. You create a catalog when you want to group related resources and access packages. An administrator can create a catalog</p> <p>Create a new catalog using <code>Azure Active Directory --&gt; Identity Governance</code></p>"},{"location":"azure/aad/#access-package","title":"Access package","text":"<p>An access package enables you to do a one-time setup of resources and policies that automatically administers access for the life of the access package</p> <p>All access packages must be put in a container called a catalog. A catalog defines what resources you can add to your access package. If you don't specify a catalog, your access package will be put into the general catalog. Currently, you can't move an existing access package to a different catalog.</p>"},{"location":"azure/aad/#access-package-assignments","title":"Access package assignments","text":"<p>In entitlement management, you can see who has been assigned to access packages, their policy, and status.</p>"},{"location":"azure/aad/#application-proxy","title":"Application Proxy","text":"<p>Application Proxy is a feature of Azure AD that enables users to access on-premises web applications from a remote client. Application Proxy includes both the Application Proxy service which runs in the cloud, and the Application Proxy connector which runs on an on-premises server. Azure AD, the Application Proxy service, and the Application Proxy connector work together to securely pass the user sign-on token from Azure AD to the web application.</p>"},{"location":"azure/aad/#access-reviews","title":"Access reviews","text":"<p>Access reviews in Azure Active Directory (Azure AD), part of Microsoft Entra, enable organizations to efficiently manage group memberships, access to enterprise applications, and role assignments. User's access can be reviewed regularly to make sure only the right people have continued access.</p> <p>Why are access reviews important?</p> <p>Azure AD enables you to collaborate with users from inside your organization and with external users. Users can join groups, invite guests, connect to cloud apps, and work remotely from their work or personal devices. The convenience of using self-service has led to a need for better access management capabilities.</p> <ul> <li>As new employees join, how do you ensure they have the access they need to be productive?</li> <li>As people move teams or leave the company, how do you make sure that their old access is removed?</li> <li>Excessive access rights can lead to compromises.</li> <li>Excessive access right may also lead audit findings as they indicate a lack of control over access.</li> <li>You have to proactively engage with resource owners to ensure they regularly review who has access to their resources.</li> </ul>"},{"location":"azure/aad/#privileged-identity-management","title":"Privileged Identity Management","text":"<p><code>Privileged Identity Management</code> provides time-based and approval-based role activation to mitigate the risks of excessive, unnecessary, or misused access permissions on resources that you care about. Here are some of the key features of Privileged Identity Management:</p> <ul> <li>Provide <code>just-in-time privileged access</code> to Azure AD and Azure resources</li> <li>Assign time-bound access to resources using start and end dates</li> <li>Require approval to activate privileged roles</li> <li>Enforce multi-factor authentication to activate any role</li> <li>Use justification to understand why users activate</li> <li>Get notifications when privileged roles are activated</li> <li>Conduct access reviews to ensure users still need roles</li> <li>Download audit history for internal or external audit</li> <li>Prevents removal of the last active Global Administrator and Privileged Role Administrator role assignments</li> </ul>"},{"location":"azure/aci/","title":"Azure Container Instance","text":"<p>Azure Container Instances (ACI) is a managed service that allows you to run containers directly on the Microsoft Azure public cloud, without requiring the use of virtual machines (VMs).</p> <p><code>Azure Container Instances</code> offers the fastest and simplest way to run a container in Azure, without having to manage any virtual machines and without having to adopt a higher-level service such as AKS.</p> <p> </p> <p>When to use AKS?</p> <p><code>Azure Container Instances</code> is a great solution for any scenario that can operate in isolated containers, including simple applications, task automation, and build jobs. For scenarios where you need full container orchestration, including service discovery across multiple containers, automatic scaling, and coordinated application upgrades, Azure Kubernetes Service (AKS) is recommended</p>"},{"location":"azure/aci/#how-to-aci","title":"How to ACI?","text":"<p><code>Azure Container Instances</code> enables exposing your container groups directly to the internet with an IP address and a <code>fully qualified domain name</code> (FQDN). When you create a container instance, you can specify a <code>custom DNS name label</code> so your application is reachable at <code>customlabel.azureregion.azurecontainer.io</code></p>"},{"location":"azure/aci/#features-of-aci","title":"Features of ACI","text":"<ul> <li>Support for both Linux and Windows containers</li> <li>Ability to launch new containers through the <code>Azure portal</code> or command line interface (CLI)</li> <li>Support for <code>standard Docker images</code> and the use of public container registries, such as <code>Docker Hub</code>, as well as <code>Azure Container Registry</code></li> <li>Ability to provide access to containers over Internet using a <code>fully qualified domain name</code> and IP address</li> <li>Ability to specify the number of CPU cores and memory required for container instances</li> <li>Support for persistent storage by mounting <code>Azure file shares</code> to the container.</li> </ul>"},{"location":"azure/aci/#container-grouppod","title":"Container Group/Pod","text":"<p>A <code>container group</code> is a collection of containers that get scheduled on the same host machine. The containers in a container group share a lifecycle, resources, local network, and storage volumes. It's similar in concept to a pod in Kubernetes.</p>"},{"location":"azure/aci/#no-autoscaling","title":"No Autoscaling","text":"<p>No autoscaling is available in ACI and manual redeployment of container group is required. </p>"},{"location":"azure/aci/#restart-policy","title":"Restart Policy","text":"<p>It can be</p> <ul> <li>Always</li> <li>Never</li> <li>OnFailure</li> </ul>"},{"location":"azure/aci/#persistent-storage","title":"Persistent storage","text":"<p>Azure Files are used for this purpose.</p>"},{"location":"azure/aci/#linux-and-windows-containers","title":"Linux and Windows containers","text":"<p>Azure Container Instances can schedule both Windows and Linux containers with the same API. Simply specify the OS type when you create your container groups.</p>"},{"location":"azure/acr/","title":"ACR","text":"<p>Azure Container Registry is a managed registry service based on the open-source Docker Registry 2.0. Create and maintain Azure container registries to store and manage your container images and related artifacts.</p> <p>Use Azure container registries with your existing container development and deployment pipelines, or use Azure Container Registry Tasks to build container images in Azure. Build on demand, or fully automate builds with triggers such as source code commits and base image updates.</p>"},{"location":"azure/adf/","title":"ADF","text":"<p>Tldr</p> <p>Azure Data Factory is Azure's cloud ETL service for scale-out serverless data integration and data transformation.It offers a code-free UI for intuitive authoring and single-pane-of-glass monitoring and management.</p>"},{"location":"azure/adf/#concepts","title":"Concepts","text":""},{"location":"azure/adf/#integration-runtime-ir","title":"Integration Runtime (IR)","text":"<p>This is a configuration object stored in the ADF metastore that defines the location and type of compute that you\u2019ll use for parts of your pipeline that require computation. This can mean VMs for copying data, executing SSIS (SQL Server Integration Services) packages, or cluster size and type for Mapping Data Flows.</p>"},{"location":"azure/adf/#self-hosted-ir","title":"Self-Hosted IR","text":"<p>Another approach to executing ADF pipeline activities in a private network or to connect to on-premises data is by using the self-hosted integration runtime or SHIR.</p>"},{"location":"azure/adf/#pipeline","title":"Pipeline \ud83d\udeb0","text":"<p>The primary unit of work in ADF is pipelines. Pipelines drive all of the actions that your data integration and ETL jobs perform. A pipeline is essentially a collection of activities that you connect together in a meaningful pattern to create a workflow. All actions in ADF are scheduled through a pipeline execution, including the <code>Mapping Data Flows</code>.</p> <p>You deploy and schedule the pipeline instead of the activities independently.</p>"},{"location":"azure/adf/#pipeline-run","title":"Pipeline run","text":"<p>A pipeline run in Azure Data Factory defines an instance of a pipeline execution. </p> <p><code>For example</code>, say you have a pipeline that executes at 8:00 AM, 9:00 AM, and 10:00 AM. In this case, there are three separate runs of the pipeline or pipeline runs. Each pipeline run has a unique pipeline run ID. A run ID is a GUID that uniquely defines that particular pipeline run.</p>"},{"location":"azure/adf/#on-demand-execution","title":"On-demand execution","text":"<p>The manual execution of a pipeline is also referred to as on-demand execution.</p>"},{"location":"azure/adf/#activities","title":"Activities","text":"<p>Pipelines are constructed from individual activities. Activities define the individual action that you wish to perform. There are many activities that you can use to compose a pipeline. Examples of activities include copying data (Copy activity), transforming data (Mapping Data Flows), \u201cFor Each,\u201d \u201cIf Then,\u201d and other control flow activities. </p> <p>How to call external activities?</p> <p>You can also call out to external compute activities like Databricks Notebook and Azure Functions to execute custom code.</p>"},{"location":"azure/adf/#copy-activity","title":"Copy activity","text":"<p><code>Copy Activity</code> in Data Factory copies data from a source data store to a sink data store. </p>"},{"location":"azure/adf/#data-flow","title":"Data Flow \u23f3","text":"<p><code>Mapping Data Flows</code> provide a way to transform data at scale without any coding required. You can design a data transformation job in the <code>data flow designer</code> by constructing a series of transformations. Start with any number of source transformations followed by data transformation steps. Then, complete your data flow with sink to land your results in a destination.</p>"},{"location":"azure/adf/#triggers","title":"Triggers","text":"<p>Triggers allow you to set the conditions for your pipeline to execute. You can create schedule triggers, tumbling window, storage events, and custom events. </p> <p>Triggers can be shared across pipelines inside your factory, and there is a separate monitoring view organized by trigger</p>"},{"location":"azure/adf/#schedule-triggers","title":"Schedule Triggers","text":"<p>They allow you to set the execute frequency and times for your pipeline. </p>"},{"location":"azure/adf/#tumbling-windows","title":"Tumbling windows","text":"<p>Tumbling window allows for time intervals. <code>ADF</code> will establish windows of time for the recurrence that you choose starting on the date that you choose. </p>"},{"location":"azure/adf/#storage-events","title":"Storage Events","text":"<p>Storage events will allow you to trigger your pipeline when a file arrives or is deleted from a storage account. </p>"},{"location":"azure/adf/#custom-events","title":"Custom Events","text":"<p>You can create custom topics in Azure Event Grid and then subscribe to those events. When a specific event is received by your custom event trigger, your pipeline will be triggered automatically.</p>"},{"location":"azure/adf/#linked-service","title":"Linked Service","text":"<p>Tldr</p> <p>They are used to store credentials, location, and authentication mechanisms to connect to your data. Linked services are used by datasets and activities in ADF pipelines so that it can be determined where and how to connect to your data. You can share linked service definitions across objects in your factory.</p> <p>Linked services are much like connection strings, which define the connection information needed for the service to connect to external resources.</p> <p>Think of it this way; the dataset represents the structure of the data within the linked data stores, and the linked service defines the connection to the data source. <code>For example</code>, an Azure Storage linked service links a storage account to the service. An Azure Blob dataset represents the blob container and the folder within that Azure Storage account that contains the input blobs to be processed</p>"},{"location":"azure/adf/#datasets","title":"Datasets \ud83d\udcc0","text":"<p>Datasets define the shape of your data. In ADF, datasets do not contain or hold any data. Instead, they point to the data and provide ADF information about the schema for your data.</p> <p>In ADF, your data does not require schema. You can work with data in a schema-less manner. When you build ETL jobs using schema-less datasets, you will build data flows that are known as \u201clate binding\u201d and working with \u201cschema drift.\u201d</p>"},{"location":"azure/adf/#data-drift","title":"Data Drift","text":"<p>Similar to <code>metadata schema drift</code>, <code>data drift</code> occurs when values inside of existing columns begin to arrive outside of a set domain or boundaries. In ADF, you can establish <code>Assert expectations</code> that define data ranges. When those domains or ranges of metadata rules are breached in the data, you can fail the job or tag the rows as data quality errors and make downstream decisions on how to handle those errors.</p>"},{"location":"azure/adsl/","title":"ADLS-Gen2","text":"<p>A fundamental part of Data Lake Storage Gen2 is the addition of a hierarchical namespace to Blob storage. The hierarchical namespace organizes objects/files into a hierarchy of directories for efficient data access. A common object store naming convention uses slashes in the name to mimic a hierarchical directory structure. This structure becomes real with Data Lake Storage Gen2. </p>"},{"location":"azure/adsl/#key-features","title":"Key features","text":"<ul> <li>Can be used with Hadoop, Spark, Databricks</li> <li>Allow us to use POSIX and ACL permissions</li> <li></li> </ul> <p>Data Lake Storage Gen2 is very cost effective because it's built on top of the low-cost Azure Blob Storage. The extra features further lower the total cost of ownership for running big data analytics on Azure.</p>"},{"location":"azure/adsl/#lifecycle-policy","title":"Lifecycle policy","text":"<p>Lifecycle management offers a rich, rule-based policy for general purpose v2 and blob storage accounts. Use the policy to transition your data to the appropriate access tiers or expire at the end of the data's lifecycle.</p>"},{"location":"azure/aks/","title":"AKS","text":"<p>Azure Kubernetes Service (AKS) simplifies deploying a <code>managed Kubernetes cluster</code> in Azure by offloading the operational overhead to Azure. As a hosted Kubernetes service, Azure handles critical tasks, like health monitoring and maintenance.</p> <p>Who manages control plane?</p> <p>When you create an AKS cluster, a <code>control plane</code> is automatically created and configured by Microsoft and you pay for only the number of nodes you add to the cluster</p>"},{"location":"azure/aks/#core-components","title":"Core components","text":"<p>Core Kubernetes infrastructure components:</p> <ul> <li>Control plane; <code>Azure-managed nodes</code> are the masters of the cluster, and they\u2019re responsible for managing the cluster. When you create an AKS cluster, this node is automatically provisioned and configured with all the management-layer components. As this layer is managed by Azure, customers will not be able to access or make configuration changes to this one.</li> <li>Nodes: They are the VMs in which the containerized applications are running.</li> </ul> <p> </p> <ul> <li>Node pools: Nodes of the same configuration are grouped together into <code>node pools</code>. A Kubernetes cluster contains at least one node pool. The initial number of nodes and size are defined when you create an AKS cluster, which creates a <code>default node pool</code>. This default node pool in AKS contains the underlying VMs that run your <code>agent nodes</code>. </li> </ul> <p>Example</p> <p>You could create a pool with Windows VMs for running Windows containers and a pool with Linux VM for running Linux containers</p> <p>windows support</p> <p>To run an AKS cluster that supports node pools for <code>Windows Server containers</code>, your cluster needs to use a network policy that uses <code>Azure CNI</code> (advanced) network plugin. Also, <code>Windows Containers</code> need their own Node pool as default AKS configuration is for <code>Linux containers</code>.</p> <ul> <li>Pods: It represent the smallest execution unit in Kubernetes. A pod encapsulates one or more containers</li> </ul>"},{"location":"azure/aks/#aks-features","title":"AKS Features","text":"<ol> <li>IAM : You can authenticate, authorize, secure, and control access to Kubernetes clusters in 2 ways:</li> <li> <p>Using <code>Kubernetes role-based access control</code> (Kubernetes RBAC), you can grant users, groups, and service accounts access to only the resources they need. Here you can use:</p> <ul> <li>Roles</li> <li>ClusterRoles</li> <li>RoleBindings</li> <li>ClusterRoleBindings</li> <li>Service Accounts</li> <li><code>Azure Active Directory</code> and <code>Azure RBAC</code>.</li> <li>Registry support</li> <li>Autoscaling of Node pools</li> <li>Networking using CNI plugin</li> </ul> </li> </ol>"},{"location":"azure/aks/#aks-namespaces","title":"AKS Namespaces","text":"<p>When you create an AKS cluster, the following namespaces are available:</p> <ul> <li> <p>Default:  Where <code>pods</code> and <code>deployments</code> are created by default when none is provided. When you interact with the Kubernetes API, such as with kubectl get pods, the default namespace is used when none is specified.</p> </li> <li> <p>Kube-system:  Where core resources exist, such as network features like <code>DNS and proxy</code>, or the <code>Kubernetes dashboard</code>. You typically don't deploy your own applications into this namespace</p> </li> <li> <p>Kube-public:  Typically not used, but can be used for resources to be visible across the whole cluster, and can be viewed by any user.</p> </li> </ul>"},{"location":"azure/aks/#network-plugins","title":"Network Plugins","text":""},{"location":"azure/aks/#kubenet","title":"Kubenet","text":"<p>NAT is used in Kubenet but not on CNI</p> <p><code>Kubenet</code> is a very basic, simple network plugin, on <code>Linux only</code>. It does not, of itself, implement more advanced features like cross-node networking or network policy. It is typically used together with a cloud provider that sets up routing rules for communication between nodes, or in single-node environments.</p> <ul> <li>Nodes receive an IP address from the Azure virtual network subnet. </li> <li>Pods receive an IP address from a logically different address space than the nodes' Azure virtual network subnet. </li> <li><code>Network address translation</code> (NAT) is then configured so that the pods can reach resources on the Azure virtual network. </li> <li>The <code>source IP address</code> of the traffic is translated to the node's primary IP address.</li> </ul>"},{"location":"azure/aks/#cni","title":"CNI","text":"<p>With Azure <code>Container Networking Interface</code> (CNI), every pod gets an IP address from the subnet and can be accessed directly. These IP addresses must be planned in advance and unique across your network space. </p> <p>Carefully plan for the IP's</p> <p>Each node has a configuration parameter for the <code>maximum number of pods</code> it supports. The equivalent number of IP addresses per node are then reserved up front. This approach can lead to <code>IP address exhaustion</code> or the need to rebuild clusters in a larger subnet as your application demands grow, so it's important to plan properly.</p>"},{"location":"azure/aks/#network-policy","title":"Network Policy","text":"<p>If you want to control traffic flow at the IP address or port level (OSI layer 3 or 4), then you might consider using Kubernetes NetworkPolicies for particular applications in your cluster.</p> <p>Dependency on Network Plugin</p> <p><code>Network policies</code> are implemented by the <code>network plugin</code>. To use network policies, you must be using a networking solution which supports NetworkPolicy. Creating a NetworkPolicy resource without a controller that implements it will have no effect.</p>"},{"location":"azure/aks/#volumes","title":"Volumes","text":"<p>In AKS, data volumes can be created using <code>Azure Files</code> or <code>Azure Disks</code></p>"},{"location":"azure/aks/#disks","title":"Disks","text":"<p>Azure Disks can be referenced in the YAML file as a DataDisk resource. Both Azure Standard storage (HDD) and Premium storage (SSD) is supported. Since there is premium storage with high-performance SSD, this is ideal for production workloads that demand high IOPS. </p> <p>Warning</p> <p>Azure Disk is mounted as <code>ReadWriteOnce</code>, so it will be available to a single node. If you are planning to implement shared storage, then Azure Files is the right choice</p>"},{"location":"azure/aks/#files","title":"Files","text":"<p>Azure Files uses SMB 3.0 and lets you mount the same storage to multiple nodes and pods. In Azure Files also you have support for Standard storage and Premium storage. These files can be accessed by multiple nodes and pods at the same time, making it ideal for shared storage scenarios.</p>"},{"location":"azure/aks/#auto-scaling","title":"Auto-Scaling","text":""},{"location":"azure/aks/#cluster-autoscaling","title":"Cluster Autoscaling","text":"<p><code>Cluster autoscaler</code> is typically used alongside the horizontal pod autoscaler. When combined, the horizontal pod autoscaler increases or decreases the number of pods based on application demand, and the cluster autoscaler adjusts the number of nodes as needed to run those additional pods accordingly.</p>"},{"location":"azure/aks/#hpa","title":"HPA","text":"<p>Kubernetes uses the <code>horizontal pod autoscaler</code> (HPA) to monitor the resource demand and automatically scale the number of replicas. </p> <p>By default, the horizontal pod autoscaler checks the Metrics API every 15 seconds for any required changes in replica count, but the Metrics API retrieves data from the Kubelet every 60 seconds. Effectively, the HPA is updated every 60 seconds. </p> <p>When changes are required, the number of replicas is increased or decreased accordingly</p> <p>Configuration required</p> <p>When you configure the HPA, you can decide on the minimum number of instances,the maximum number of instances, and the metrics that need to be monitored.</p>"},{"location":"azure/api_gateway/","title":"APIM","text":"<p>Tldr</p> <p>API gateway is the entry point for clients. Instead of calling services directly, clients call the API gateway, which forwards the call to the appropriate services on the back end.</p> <p>Features</p> <ul> <li>Cross cutting: The API Gateway can perform other cross-cutting functions such as authentication, logging, SSL termination, and load balancing.</li> <li>Caching</li> <li>Inbound and outbound policies</li> <li>API testing</li> <li>API observability</li> <li>Consolidate various API's to one endpoint</li> </ul>"},{"location":"azure/api_gateway/#apim-components","title":"APIM Components","text":"<p>APIM includes </p> <ol> <li>API Gateway: AKA data plane/runtime</li> <li><code>Managed</code>: Default setting</li> <li><code>Self Hosted</code>: Optional and containerized version for on-prem</li> <li>Management Portal/Plane</li> <li>Developer portal: The open-source developer portal.</li> </ol> <p> </p> <p>URL's for APIM endpoints?</p> <p>We can have 3 URLs - Gateway URL - Management URL - Developer Portal URL</p>"},{"location":"azure/api_gateway/#kinds-of-apim","title":"Kinds of APIM","text":"<ul> <li>On cloud</li> <li>Serverless</li> <li>Self hosted and Federated</li> </ul>"},{"location":"azure/api_gateway/#api-gateway","title":"API Gateway","text":"<ul> <li>Facade: Accepts API calls and routes them to your backend APIs.</li> <li>AuthT and AuthZ: Verifies <code>API keys, JWT tokens, certificates</code>, and other credentials.</li> <li>API Throttling: Enforces usage <code>quotas and rate limits</code> by Denial of Service (DOS)</li> <li>Caching: <code>Caches</code> backend responses where set up.</li> <li>Logging:<code>Logs</code> for monitoring and reporting.</li> <li>Policy enforcement:<code>Transforms</code> your API on the fly using <code>policy statements</code></li> </ul>"},{"location":"azure/api_gateway/#management-plane","title":"Management plane","text":"<p><code>API providers</code> interact with the service through the management plane, which provides full access to the <code>API Management</code> service capabilities.</p> <p><code>Customers</code> interact with the management plane through Azure tools including the Azure portal, Azure PowerShell, Azure CLI, a Visual Studio Code extension, or client SDKs in several popular programming languages.</p> <p>Use of Management portal?</p> <p>Use the management plane to:</p> <ul> <li>Provision and configure <code>API Management</code> service settings</li> <li>Define or import API schemas from a wide range of sources, including OpenAPI specifications, - Azure compute services, or WebSocket or GraphQL backends</li> <li>Package APIs into products</li> <li>Set up policies like quotas or transformations on the APIs</li> <li>Get insights from analytics</li> <li>Manage users</li> </ul>"},{"location":"azure/api_gateway/#developer-portal","title":"Developer Portal","text":"<p>API providers can customize the look and feel of the developer portal by adding custom content, customizing styles, and adding their branding. Extend the developer portal further by self-hosting.</p> <p>Using the developer portal, developers can:</p> <ul> <li>Read API documentation</li> <li>Call an API via the interactive console</li> <li>Create an account and subscribe to get API keys</li> <li>Access analytics on their own usage</li> <li>Download API definitions</li> <li>Manage API keys</li> </ul>"},{"location":"azure/api_gateway/#core-concepts","title":"Core concepts","text":""},{"location":"azure/api_gateway/#policy","title":"Policy","text":"<p>With policies, an API publisher can change the behavior of an API through configuration. Policies are a collection of statements that are executed sequentially on the request or response of an API. Popular statements include format conversion from <code>XML to JSON</code> and <code>call-rate limiting</code> to restrict the number of incoming calls from a developer.</p> <p>The policy XML configuration is divided into inbound, backend, outbound, and on-error sections. This series of specified policy statements is executed in order for a request and a response.</p> Policy definaiton<pre><code>&lt;policies&gt;\n  &lt;inbound&gt;\n    &lt;!-- statements to be applied to the request go here --&gt;\n  &lt;/inbound&gt;\n  &lt;backend&gt;\n    &lt;!-- statements to be applied before the request is forwarded to \n         the backend service go here --&gt;\n  &lt;/backend&gt;\n  &lt;outbound&gt;\n    &lt;!-- statements to be applied to the response go here --&gt;\n  &lt;/outbound&gt;\n  &lt;on-error&gt;\n    &lt;!-- statements to be applied if there is an error condition go here --&gt;\n  &lt;/on-error&gt;\n&lt;/policies&gt;\n</code></pre> <p>The policies can be applied at various scopes, which determine the affected APIs or operations and dynamically configured using policy expressions</p> <p> </p>"},{"location":"azure/api_gateway/#backend-api","title":"Backend API","text":"<p>A service, most commonly HTTP-based, that implements an API and its operations. Sometimes backend APIs are referred to simply as backends. For more information, see Backends.</p>"},{"location":"azure/api_gateway/#frontend-api","title":"Frontend API","text":"<p><code>API Management</code> serves as mediation layer over the backend APIs. Frontend API is an API that is exposed to API consumers from <code>API Management</code>. You can customize the shape and behavior of a frontend API in <code>API Management</code> without making changes to the backend API(s) that it represents</p>"},{"location":"azure/api_gateway/#products","title":"Products","text":"<p>Products are how APIs are surfaced to developers. Products in <code>API Management</code> have one or more APIs, and can be open or protected. Protected products require a subscription key, while open products can be consumed freely.</p>"},{"location":"azure/api_gateway/#version","title":"Version","text":"<p>A version is a distinct variant of existing frontend API that differs in shape or behavior from the original. Versions give customers a choice of sticking with the original API or upgrading to a new version at the time of their choosing. Versions are a mechanism for releasing breaking changes without impacting API consumers.</p>"},{"location":"azure/api_gateway/#apim-with-aks","title":"APIM with AKS","text":"<p>APIM can be used with AKS in various use cases</p> <p>Should we connect to pods?</p> <p>In a Kubernetes cluster, containers are deployed in Pods, which are ephemeral and have a lifecycle. When a worker node dies, the Pods running on the node are lost. Therefore, the IP address of a Pod can change anytime. We cannot rely on it to communicate with the pod.</p> <p>To solve this problem, Kubernetes introduced the concept of Services. A Kubernetes Service is an abstraction layer which defines a <code>logic group of Pods</code> and enables external traffic exposure, load balancing and service discovery for those Pods.</p>"},{"location":"azure/api_gateway/#direct","title":"Direct","text":"<p>Services in an AKS cluster can be exposed publicly using Service types of NodePort, LoadBalancer, or ExternalName. In this case, Services are accessible directly from public internet. </p> <p>After deploying API Management in front of the cluster, we need to ensure all inbound traffic goes through API Management by applying authentication in the microservices.</p> <p> </p>"},{"location":"azure/api_gateway/#using-ingress","title":"Using Ingress","text":"<p>If an <code>API Management</code> instance does not reside in the cluster VNet, Mutual TLS authentication (mTLS) is a robust way of ensuring the traffic is secure and trusted in both directions between an <code>API Management</code> instance and an AKS cluster.</p> <p>Mutual TLS authentication is natively supported by API Management and can be enabled in Kubernetes by installing an <code>Ingress Controller</code>. As a result, <code>authentication</code> will be performed in the <code>Ingress Controller</code>, which simplifies the microservices. Additionally, you can add the IP addresses of <code>API Management</code> to the allowed list by Ingress to make sure only <code>API Management</code> has access to the cluster.</p> <p> </p>"},{"location":"azure/api_gateway/#revision-and-version","title":"Revision and Version","text":"<p>Version: They are for breaking changes</p> <p>Revision: They are for non breaking changes </p>"},{"location":"azure/app-gw/","title":"App Gateway (L7 LB + WAF)","text":"<p>Tldr</p> <p><code>Azure Load Balancer</code> could load balance any <code>TCP/UDP traffic</code> to the backend servers; however, <code>Azure Application Gateway</code> is designed to distribute the incoming web requests to a web application (using HTTP). Unlike Azure Load Balancer, which operates at layer 4, or the Transport layer, Application Gateway uses layer 7, or the Application layer, routing to route the traffic to the backend web applications.</p> <p> OSI Layer AWS Azure L4 Network LB Azure LB L7 Application LB App Gateway <p></p> <p>Since Application Gateway is operating at layer 7, the IP addresses of the backend servers are not considered; rather, hostnames and paths are used for routing.</p> <p>Unlike typical load balancers that function at Layer 4 and route traffic based on the source IP address and port, Azure Application Gateway makes routing choices based on additional parameters of an HTTP request, such as URI path or host headers. It is a very helpful and valuable tool for web traffic managers, and it operates similarly to the <code>AWS Application Gateway</code>.</p> <p>Using sticky sessions</p> <p>If you would like to implement <code>session stickiness</code>, Application Gateway supports that as well. Using session stickiness, you can override the default <code>round-robin fashion</code>, and client requests in the same session will be routed to the same backend server.</p>"},{"location":"azure/app-gw/#features","title":"Features","text":"<ul> <li><code>Supported protocols</code>: It supports HTTP, HTTPS, HTTP/2, and WebSocket.</li> <li><code>WAF support</code>: Web Application Firewall can be incorporated with Application Gateway to protect web applications.</li> <li><code>Encryption</code>: It supports end-to-end request encryption.</li> <li><code>Autoscaling</code>: You can dynamically scale Application Gateway to handle traffic spikes.</li> <li><code>Redirection</code>: Traffic can be redirected to another site or from HTTP to HTTPS.</li> <li><code>Rewrite HTTP headers</code>: It allows passing additional information with the request or response.</li> </ul>"},{"location":"azure/app-gw/#components","title":"Components","text":"<ul> <li>Frontend IP</li> <li>HTTP/HTTPs Listner</li> <li>Routing Rules<ul> <li>HTTP Settings</li> </ul> </li> <li>Backend Pool<ul> <li>Health Probes</li> </ul> </li> </ul>"},{"location":"azure/app-gw/#frontend-ip","title":"Frontend IP","text":"<p>The IP address connected to an application gateway is known as the \u201cfrontend IP address.\u201d You can set up an application gateway to have a private IP address, a public IP address, or both. An application gateway supports one private or one public IP address.</p>"},{"location":"azure/app-gw/#listner","title":"Listner","text":"<p>There can be multiple listeners linked to an application gateway, and they can be utilized for the same protocol. When a listener detects incoming client requests, the application gateway directs them to members of the backend pool specified in the rule.</p> <p>They are used for port, protocol and certificate configurations</p>"},{"location":"azure/app-gw/#routing-rules","title":"Routing rules","text":"<p>When a listener receives a request, the request routing rule either passes it to the backend or redirects it to another location. If the request is sent to the backend, the request routing rule specifies which backend server pool it should be routed to. The request routing rule also specifies whether the headers of the request are to be modified. One listener can be assigned to one rule.</p>"},{"location":"azure/app-gw/#backend-pool","title":"Backend Pool","text":"<p>A backend pool directs requests to backend servers, who serve the requests. Backend pools can contain</p> <ol> <li>VMSS</li> <li>VM</li> <li>App Service</li> </ol>"},{"location":"azure/app-gw/#health-probes","title":"Health Probes","text":"<p><code>Application gateway</code> checks the health of all resources in its backend pool and eliminates unhealthy ones automatically. When sick instances become available, it monitors them and adds them back to the healthy backend pool, as well as responding to health probes.</p>"},{"location":"azure/app-gw/#routing-methods","title":"Routing methods","text":""},{"location":"azure/app-gw/#pathurl-based","title":"Path/URL based","text":"<p>As shown in the below image, for path-based routing, the <code>Application Gateway</code> inspects the URL paths and then  routes the traffic to the different backend pools. For example, you can direct the requests with the path <code>/images/*</code> to the backend pool containing the image documents. Similarly, all URLs containing the path <code>/videos/*</code> can be routed to the backend servers optimized for video streaming.</p> <p> </p>"},{"location":"azure/app-gw/#re-write-http-headers-url","title":"Re-write HTTP headers + URL","text":"<p>Application Gateway allows you to rewrite selected content of requests and responses. With this feature, you can translate URLs, query string parameters as well as modify request and response headers. It also allows you to add conditions to ensure that the URL or the specified headers are rewritten only when certain conditions are met. </p>"},{"location":"azure/app-gw/#multi-site-hosting","title":"Multi-site hosting","text":"<p>Multiple site hosting enables you to configure more than one web application on the same port of application gateways using public-facing listeners. It allows you to configure a more efficient topology for your deployments by adding up to 100+ websites to one application gateway. Each website can be directed to its own backend pool. For example, three domains, contoso.com, fabrikam.com, and adatum.com, point to the IP address of the application gateway. You'd create three multi-site listeners and configure each listener for the respective port and protocol setting.</p> <p> </p>"},{"location":"azure/appservice/","title":"Azure App Service","text":"<p>Azure App Service enables you to build and host web apps, mobile back-ends, and RESTful APIs in the programming language of your choice without managing infrastructure. It offers auto-scaling and high availability, supports both Windows and Linux, and enables automated deployments from GitHub, Azure DevOps, or any Git repo.</p> <p>Azure App Service is Microsoft\u2019s <code>PaaS</code> offering. Azure App Service comes in four flavors.</p> <ul> <li>Azure Web Apps/WebJobs</li> <li>Azure App Service Web App for Containers</li> <li>Azure Mobile Apps</li> <li>Azure API Apps</li> </ul>"},{"location":"azure/appservice/#app-service-plan","title":"App Service Plan","text":"<p>An <code>App Service plan</code> defines a set of compute resources for a web app to run. An <code>app service</code> always runs in an <code>App Service plan</code>. In addition, <code>Azure Functions</code> also has the option of running in an App Service plan. </p> <p>Contents of Azure app service</p> <ul> <li>Operating System (Windows, Linux)</li> <li>Region (West US, East US, etc.)</li> <li>Number of VM instances</li> <li>Size of VM instances (Small, Medium, Large)</li> <li>Pricing tier </li> </ul> <p>App Service plans are created at a regional scope. When you create an <code>App Service plan</code> in a certain region (for example, West Europe), a set of compute resources is created for that plan in that region. Whatever apps you put into this App Service plan run on these compute resources as defined by your App Service plan.</p> <p>Moving webapp in App service</p> <p>You can move an app to another App Service plan, as long as the source plan and the target plan are in the same resource group and geographical region. The region in which your app runs is the region of the App Service plan it's in. However, you cannot change an App Service plan's region.</p>"},{"location":"azure/appservice/#pricing-tier","title":"Pricing tier","text":"<p>The pricing tier of an App Service plan determines what App Service features you get and how much you pay for the plan. </p> <ul> <li>Shared \ud83d\udcb0 :  It allocate <code>CPU quotas</code> to each app that runs on the <code>shared VM</code>, and the resources cannot scale out.</li> <li>Dedicated \ud83d\udcb0\ud83d\udcb0:  Only apps in the same <code>App Service plan</code> share the same compute resources on <code>dedicated VM</code>. The higher the tier, the more VM instances are available to you for scale-out.</li> <li>Isolated \ud83d\udcb0\ud83d\udcb0\ud83d\udcb0\ud83d\udcb0: Your apps run <code>dedicated Azure VMs</code> on <code>dedicated Azure VNets</code>. It provides network isolation on top of compute isolation to your apps. It provides the maximum scale-out capabilities.</li> </ul>"},{"location":"azure/appservice/#webapps","title":"WebApps","text":"<ul> <li>Containerization: You can publish using code or docker container</li> <li>Multiple runtimes:We can choose runtime stack as .NET, Java, Node.js, PHP, and Python on Windows/Linux.</li> </ul>"},{"location":"azure/appservice/#backup-app-service","title":"Backup App service","text":"<p>There are two types of backups in App Service.</p> <ul> <li><code>Automatic backups</code> made for your app regularly as long as it's in a supported pricing tier.</li> <li><code>Custom backups</code> require initial configuration, and can be made on-demand or on a schedule.</li> </ul> <p>How backups are stored?</p> <p>After you have made one or more backups for your app, the backups are visible on the <code>Containers</code> page of your <code>storage account</code>, and your app. In the storage account, each backup consists of a <code>.zip</code> file that contains the backup data and an <code>.xml</code> file that contains a manifest of the <code>.zip</code> file contents. You can unzip and browse these files if you want to access your backups without actually performing an app restore.</p> <p>Remember difference in different backups</p> <ul> <li> <p>You need a <code>Recovery Service Vault</code> if you want to backup VMs, File Shares, SAP HANA in a VM or SQL Server in a VM.</p> </li> <li> <p>You need a <code>Backup Vault</code> if you want to backup Azure Disks, Azure Blobs or Azure Database for PostgreSQL Server.</p> </li> <li> <p>To backup <code>App Service</code>,you need a storage account. Choose your backup destination by selecting a Storage Account and Container. The storage account must belong to the same subscription as the app you want to back up. If you wish, you can create a new storage account or a new container in the respective pages.</p> </li> </ul>"},{"location":"azure/appservice/#partial-backup","title":"Partial backup","text":"<p>Partial backups are supported for <code>custom backups</code> (not for <code>automatic backups</code>). Sometimes you don't want to back up everything on your app.</p> <p>To exclude folders and files from being stored in your future backups, create a <code>_backup.filter</code> file in the <code>%HOME%\\site\\wwwroot</code> folder of your app. Specify the list of files and folders you want to exclude in this file.</p>"},{"location":"azure/arm/","title":"Azure Resource Manager","text":"<p>For if you are creating  a <code>virtual machine</code>, you will send our inputs to the <code>Azure Resource Manager</code>, and ARM will forward the request to <code>Microsoft.Compute</code> Compute resource provider after validation. The resource provider will deploy the resource and update you with  the status via ARM. Similarly, when you create a networking resource like a virtual network, the resource provisioning is done by the <code>Microsoft.Network</code> resource provider.</p> <p>Note</p> <ul> <li>Resource groups is a feature of ARM</li> <li>ARM is not only responsible for the deployment of the resources, but also responsible for the management of resources.</li> </ul>"},{"location":"azure/arm/#template-file","title":"Template File","text":"<p>In its simplest structure, a template has the following elements:</p> <pre><code>{\n  \"$schema\": \"https://schema.management.azure.com/schemas/2019-04-01/deploymentTemplate.json#\",\n  \"contentVersion\": \"\",\n  \"apiProfile\": \"\",\n  \"parameters\": {  },\n  \"variables\": {  },\n  \"functions\": [  ],\n  \"resources\": [  ],\n  \"outputs\": {  }\n}\n</code></pre> <p>Required values are </p> <ul> <li>$Schema</li> <li>contentVersion</li> <li>resources</li> </ul> <p>The template has the following sections:</p> <p>Template sections</p> <ul> <li><code>$schema</code>: Location of the JSON schema file that describes the version of the template language. The version number you use depends on the scope of the deployment and your JSON editor.</li> <li><code>contentVersion</code>: Version of the template (such as 1.0.0.0). You can provide any value for this element. Use this value to document significant changes in your template. When deploying resources using the template, this value can be used to make sure that the right template is being used.</li> <li><code>apiProfile</code>; An API version that serves as a collection of API versions for resource types. Use this value to avoid having to specify API versions for each resource in the template. When you specify an API profile version and don't specify an API version for the resource type, Resource Manager uses the API version for that resource type that is defined in the profile.</li> <li><code>Parameters</code> - Provide values during deployment/runtime that allow the same template to be used with different environments. </li> <li><code>Variables</code> - Define values that are reused in your templates. They can be constructed from parameter values. </li> <li><code>User-defined functions</code> - Create customized functions that simplify your template. </li> <li><code>Resources</code> - Specify the resources to deploy. </li> <li><code>Outputs</code> - Return values from the deployed resources.</li> </ul>"},{"location":"azure/arm/#deployment-mode","title":"Deployment mode","text":"<ul> <li>Complete mode: In this mode, RM deletes resources that exist in the resource group but aren't specified in the template.</li> </ul> <p>Always use the <code>what-if operation</code> before deploying a template in <code>complete mode</code>. What-if shows you which resources will be created, deleted, or modified. Use what-if to avoid unintentionally deleting resources.</p> <ul> <li>Incremental mode: In this mode the RM leaves unchanged resources that exist in the resource group but aren't specified in the template. Resources in the template are added to the resource group.</li> </ul> <p>Example</p> <p>To illustrate the difference between incremental and complete modes, consider the following scenario.</p> <pre><code>Resource Group contains:\n    -  Resource A\n    -  Resource B\n    -  Resource C\n\nTemplate contains:\n    - Resource A\n    - Resource B\n    - Resource D\n\nWhen deployed in incremental mode, the resource group has:\n    - Resource A\n    - Resource B\n    - Resource C\n    - Resource D\n\nWhen deployed in complete mode, Resource C is deleted. The resource group has:\n    - Resource A\n    - Resource B\n    - Resource D\n</code></pre>"},{"location":"azure/arm/#deployment-scope","title":"Deployment Scope","text":"<p>Scope of deployment can be</p> <ol> <li>Management Group</li> <li>Subscription</li> <li>Resource Group</li> <li>Tenant</li> <li>Extension resource: An extension resource is a resource that modifies another resource. For example, you can assign a role to a resource. The role assignment is an extension resource type.</li> </ol> <p>You can target your deployment to a resource group, subscription, management group, or tenant. Depending on the scope of the deployment, you use different commands. </p> <p>There is no resource level scope</p> create various resources using powershell<pre><code># To create an Azure resource, such as a website, Azure SQL Database server, or Azure SQL Database, in a resource group use:\nNew-AzResource \n\n# To add a deployment to an existing resource group, use:\nNew-AzResourceGroupDeployment \n\n# The New-AzDeployment cmdlet adds a deployment at the current subscription scope. This includes the resources that the deployment requires.\nNew-AzDeployment \n</code></pre>"},{"location":"azure/arm/#what-ifs","title":"What-ifs","text":"<p>ARM provides the <code>what-if</code> operation to let you see how resources will change if you deploy the template. The <code>what-if</code> operation doesn't make any changes to existing resources. Instead, it predicts the changes if the specified template is deployed.</p>"},{"location":"azure/arm/#resource-dependency","title":"Resource Dependency","text":"<p>When deploying resources, you may need to make sure some resources exist before other resources. For example, you need a <code>logical SQL server</code> before deploying a <code>database</code>. You establish this relationship by marking one resource as dependent on the other resource. Use the <code>dependsOn</code> element to define an explicit dependency. Use the reference or list functions to define an implicit dependency.</p> NIC dependency a VNET, NSG, and public IP<pre><code>{\n    \"type\": \"Microsoft.Network/networkInterfaces\",\n    \"apiVersion\": \"2020-06-01\",\n    \"name\": \"[variables('networkInterfaceName')]\",\n    \"location\": \"[parameters('location')]\",\n    \"dependsOn\": [\n      \"[resourceId('Microsoft.Network/networkSecurityGroups/', parameters('networkSecurityGroupName'))]\",\n      \"[resourceId('Microsoft.Network/virtualNetworks/', parameters('virtualNetworkName'))]\",\n      \"[resourceId('Microsoft.Network/publicIpAddresses/', variables('publicIpAddressName'))]\"\n    ],\n    ...\n}\n</code></pre>"},{"location":"azure/atm/","title":"Azure Traffic Manager","text":"<p><code>Azure Traffic Manager</code> is a DNS resolver, which helps in distributing the traffic to different endpoints globally based on the routing method you configure. </p> <p>In other solutions, the request hits the front end and gets routed to the backend based on the rules you configure. However, in the case of Azure Traffic Manager, the requests hit the Azure Traffic Manager, and Azure Traffic Manager returns the endpoint address to the client as a DNS response. As the client is now aware of the endpoint, the client directly reaches out to the endpoint. </p> <p>Tldr</p> <p>The traffic doesn\u2019t transit through the <code>Azure Traffic Manager</code>; instead, it returns the endpoint address to the client.</p>"},{"location":"azure/atm/#resolution-process","title":"Resolution process","text":"<ol> <li>The request from the client reaches the recursive DNS servers, and the request gets forwarded to the <code>Azure Traffic Manager</code>. For example, if you have a website <code>www.amarjitdhillon.com</code>, you will map the www record as a <code>CNAME</code> to the <code>DNS name</code> of the <code>Azure Traffic Manager</code>.</li> <li>A recursive DNS server will forward the request to the <code>Azure Traffic Manager</code>, and based on the routing method you have configured, Traffic Manager determines the best endpoint.</li> <li>Once the endpoint is determined, Traffic Manager will return the <code>IP address of the backend server</code> as a DNS response to the DNS resolver</li> <li>DNS resolver to return endpoint to the client. </li> <li>Since the client knows the IP address to the endpoint, it will directly communicate with the endpoint returned by the Traffic Manager.</li> </ol>"},{"location":"azure/atm/#routing-methods","title":"Routing methods","text":"<p>Various routing methods in Traffic Manager are:</p>"},{"location":"azure/atm/#priority","title":"Priority","text":"<p>You can set the priority for each endpoint, and the requests will be served by the endpoint that has the highest priority. If the endpoint with high priority is not healthy, then the endpoint with the second-highest priority serves the requests.</p>"},{"location":"azure/atm/#weighted","title":"Weighted","text":"<p>Select Weighted routing when you want to distribute traffic across a set of endpoints based on their weight. Set the weight the same to distribute evenly across all endpoints.</p>"},{"location":"azure/atm/#performance","title":"Performance","text":"<p>Requests are routed to endpoints with <code>lowest network latency</code>.</p>"},{"location":"azure/atm/#geographic","title":"Geographic","text":"<p>Requests are routed to endpoints based on the location of the client. This is ideal for serving websites in local language for customers accessing from a country.</p>"},{"location":"azure/atm/#multi-value","title":"Multi-value","text":"<p>This is used when we can have only IPv4/IPv6 addresses as endpoints. When a query is received for this profile, all healthy endpoints are returned.</p>"},{"location":"azure/atm/#subnet","title":"Subnet","text":"<p>This is used to map the end-user IP address ranges to a specific endpoint.</p>"},{"location":"azure/az-monitor/","title":"Azure Monitor","text":"<p>It delivers a comprehensive solution for collecting, analyzing, and acting on telemetry from your cloud and on-premises environments. This information helps you understand how your applications are performing and proactively identify issues that affect them and the resources they depend on.</p> <p> </p> <p>States: - Alert State: set by user such as admin - Monitor state: Set by system</p>"},{"location":"azure/az-monitor/#log-analytics-workspace","title":"Log Analytics Workspace","text":"<p>A <code>Log Analytics workspace</code> is a unique environment for log data from <code>Azure Monitor</code> and other Azure services, such as <code>Microsoft Sentinel</code> and <code>Microsoft Defender</code> for Cloud. The connected sources, configuration, and the repository are managed per workspace.</p> <p> </p>"},{"location":"azure/az-monitor/#logs","title":"Logs","text":"<ul> <li>Its an event-based data. Example: Syslog in Linux is an example of a log, as the log data is not consistent and the format may vary from source to source.</li> <li>Free form or structured</li> <li>Stored in <code>logs analytics workspace</code></li> <li>Analysis via <code>Kusto Query Language (KQL)</code>: The first step in writing queries is to understand which table contains the information you need. Some examples include Event, Syslog, Heartbeat, and Alert.</li> </ul> View error events from a table named Event using 3 methods<pre><code>search in (Event) \"error\"\n\nEvent | search \"error\"\n\nEvent | where EventType == \"error\"\n</code></pre> query perf table<pre><code>Perf \n| summarize AggregatedValue = count() by CounterPath\n</code></pre> Count of agent nodes that are sending a heartbeat each day in the last month<pre><code>Heartbeat \n| where TimeGenerated &gt; startofday(ago(31d))\n| summarize nodes = dcount(Computer) by bin(TimeGenerated, 1d)    \n| render timechart\n</code></pre>"},{"location":"azure/az-monitor/#log-analytics","title":"Log Analytics","text":"<p>A service for aggregating the log data in a single pane so that it can be analyzed, visualized and queried via <code>KQL</code></p> <p> </p>"},{"location":"azure/az-monitor/#agents","title":"Agents","text":"<p>Azure Monitor Agent (AMA) collects monitoring data from the guest operating system of Azure and hybrid virtual machines and delivers it to Azure Monitor for use</p> <ul> <li>Windows agents</li> <li>Linux agents</li> </ul> <p>Cost control using agents</p> <p>With the help of the <code>agents configuration</code>, you will be able to declare what logs you want to collect using the agents and what level of logging information you need. In this way, you will have granular control over what is getting ingested into your workspace.</p> <p>Various logs used are:</p> <ul> <li><code>Windows Event Logs</code>: This helps you to select which event log items you want to ingest to the workspace </li> <li><code>Windows Performance Counters</code>: You can select performance counters of Windows servers and the sample rate. </li> <li><code>Linux Performance Counters</code>: These are performance counters for Linux servers and their sample rate. </li> <li><code>Syslog</code>: Control which facilities in Syslog you want to ingest. </li> <li><code>IIS Logs</code>: This enables collection of <code>W3C format log files</code> from IIS server.</li> </ul>"},{"location":"azure/az-monitor/#dcr","title":"DCR","text":"<p><code>Azure Monitor Agent</code> uses <code>data collection rules (DCR)</code>, where you define which data you want each agent to collect. Data collection rules let you manage data collection settings at scale and define unique, scoped configurations for subsets of machines. You can define a rule to send data from multiple machines to multiple destinations across regions and tenants.</p>"},{"location":"azure/az-monitor/#data-retention","title":"Data retention","text":"<p>Data in each table in a <code>Log Analytics workspace</code> is retained for a specified period of time after which it's either removed or archived with a reduced retention fee.</p>"},{"location":"azure/az-monitor/#activity-log","title":"Activity log","text":"<p>With the help of an activity log, you can get insights into different operations occurring at the subscription level. </p>"},{"location":"azure/az-monitor/#categories","title":"Categories","text":"<p>As Azure Activity Log is a subscription-wide logging system, Azure has divided the logs ingested into different categories.</p> <ul> <li><code>Administrative</code>: Contains the record of all create, update, delete, and action operations performed through Resource Manager. Examples of Administrative events include create virtual machine and delete network security group.</li> <li><code>Service Health</code>: Contains the record of any service health incidents that have occurred in Azure. An example of a Service Health event SQL Azure in East US is experiencing downtime.</li> <li><code>Resource Health</code>: Contains the record of any resource health events that have occurred to your Azure resources. An example of a Resource Health event is Virtual Machine health status changed to unavailable.</li> <li><code>Alert</code>: Contains the record of activations for Azure alerts. An example of an Alert event is CPU % on myVM has been over 80 for the past 5 minutes.</li> <li><code>Autoscale</code>: Contains the record of any events related to the operation of the autoscale engine based on any autoscale settings you have defined in your subscription. An example of an Autoscale event is Autoscale scale up action failed.</li> <li><code>Recommendation</code>: From advisor</li> <li><code>Security</code>: This includes any security alerts generated by Azure Defender for Servers.  </li> <li><code>Policy</code>: Whenever Azure Policy is evaluated, the effect action will be logged in this category. </li> </ul>"},{"location":"azure/az-monitor/#metrics","title":"Metrics","text":"<p>Metrics are <code>numerical values</code> that are ingested from Azure resources used to represent the state of the system at a particular point in time.</p> <p>Metric Example</p> <p>For a virtual machine, the metrics available will be <code>CPU Percentage</code>, <code>Network In</code>, <code>Network Out</code>, <code>Memory</code>, etc. On the other hand, if you take a <code>storage account</code>, the available metrics will be <code>Number Of Requests</code>, <code>Number Of Failed Requests</code>, <code>Number Of API Calls</code>, etc.</p> <ul> <li>Short time based data. </li> <li>Frequently updated</li> <li>Near real-time data</li> <li>Visualizations via <code>Metrics Explorer</code></li> </ul>"},{"location":"azure/az-monitor/#distributed-tracing","title":"Distributed Tracing","text":"<p>In <code>monolithic architectures</code>, we've gotten used to debugging with call stacks. Call stacks are brilliant tools for showing the flow of execution (<code>Method A</code> called <code>Method B</code>, which called <code>Method C</code>), along with details and parameters about each of those calls. This technique is great for monoliths or services running on a single process. But how do we debug when the call is across a process boundary, not simply a reference on the local stack?</p> <p>That's where distributed tracing comes in.</p> <p>!!! info ''     <code>Distributed tracing</code> is the equivalent of call stacks for modern cloud and <code>microservices architectures</code>, with the addition of a simplistic performance profiler thrown in.</p>"},{"location":"azure/az-monitor/#diagnostic-settings","title":"Diagnostic Settings","text":"<p>They are used to define where the logs and metrics will be stored.</p>"},{"location":"azure/az-monitor/#action-groups","title":"Action groups","text":"<p>What are action groups?</p> <p>An action group is a collection of notification preferences that can be reused in multiple alerts. The</p> <p>notifications and actions that you define inside the action group will be executed when the alert is fired. You can create multiple action groups with different notification preferences, and these can be used across your alerts.</p> <p>They can be found in <code>Monitor</code> \u2192 <code>Alerts</code> \u2192 <code>Action Groups</code></p> <p>Action groups consist of two parts: <code>notifications</code> and <code>actions</code></p>"},{"location":"azure/az-monitor/#notification-types","title":"Notification types","text":"<ul> <li><code>Email/SMS</code>: These will work even if Azure is down while other services needs the Azure to be running.</li> <li><code>Email Azure Resource Manager</code>; Role You can send email notifications to Azure RBAC roles like Owner, Contributor, Reader, Monitoring Contributor, and Monitoring Reader that are assigned at the subscription scope. All <code>user principals</code> assigned with any of the aforementioned roles will be notified when the alert is triggered. <code>Azure AD group</code> and <code>service principals</code> are excluded from the email notification.</li> </ul>"},{"location":"azure/az-monitor/#action-types","title":"Action types","text":"<ul> <li><code>Azure app push notification</code>:</li> <li><code>Azure Function</code>: Using serverless compute, you can run small chunks of code when the alert is fired.</li> <li><code>Logic App</code>: run a business process</li> <li><code>Secure Webhook/WebHook</code>: This is the HTTPS or HTTP endpoint for an external application to communicate.</li> <li><code>ITSM</code>: You can integrate your ITSM tools like <code>ServiceNow</code> so that whenever an alert is triggered, the corresponding ticket will be created in the ITSM tool.</li> <li><code>Automation runbook</code>:</li> <li><code>Event Hub</code>: Ingest event to other systems.</li> </ul>"},{"location":"azure/az-monitor/#insights","title":"Insights","text":"<p>This is service specific monitoring feature in Azure</p>"},{"location":"azure/az-monitor/#vm-insights","title":"VM Insights","text":"<ul> <li>Used to monitor VM and VMSS. </li> <li>This is also called as Azure monitor for VMs</li> <li>Require <code>Log Analytics Agent</code> to be installed</li> </ul>"},{"location":"azure/az-monitor/#network-insights","title":"Network Insights","text":"<ul> <li>No agent installation required for this.</li> </ul>"},{"location":"azure/az-monitor/#container-insights","title":"Container Insights","text":"<ul> <li>Used to monitor containers</li> </ul>"},{"location":"azure/az-monitor/#app-insights-for-devs","title":"App Insights (for Devs)","text":"<p><code>Application Insights</code> is an extension of <code>Azure Monitor</code> and provides Application Performance Monitoring (also known as \u201cAPM\u201d) features. APM tools are useful to monitor applications from development, through test, and into production.</p> <p>What to use for logging?</p> <p>In addition to collecting <code>Metrics</code> and <code>application Telemetry data</code>, which describe application activities and health, Application Insights can also be used to collect and store application <code>trace logging data</code>.</p>"},{"location":"azure/az-monitor/#features","title":"Features","text":"<ul> <li>Metrics and alerts</li> <li>Application Map</li> <li>Profiler</li> <li>Usage Analytics</li> </ul>"},{"location":"azure/az-monitor/#instrumentation","title":"Instrumentation","text":"<p>At a basic level, <code>instrumenting</code> is simply enabling an application to capture telemetry.</p> <p>There are two methods to instrument your application:</p> <ul> <li>Manual instrumentation </li> <li>Automatic instrumentation (auto-instrumentation)</li> </ul> <p>It can be:</p> <ul> <li>Runtime instrumentation</li> <li>Build-time instrumentation</li> </ul> <p>Instrumentation key</p> <p>Key of implementing Instrumentation in application and is stored in <code>app insights</code> resource.</p>"},{"location":"azure/az-monitor/#auto-instrumentation","title":"Auto-instrumentation","text":"<p>It's an agent for App insights. <code>Auto-instrumentation</code> enables telemetry collection through configuration without touching the application's code. Although it's more convenient, it tends to be less configurable. It's also not available in all languages</p> <p>The <code>Application Insights agent</code> or <code>SDK</code> pre-processes telemetry and metrics before sending the data to Azure where it's ingested and processed further before being stored in <code>Azure Monitor Logs</code>.</p>"},{"location":"azure/az-monitor/#network-watcher","title":"Network Watcher","text":"<p>It is a <code>regional service</code> which is used to monitor networks. It can monitor <code>IaaS</code> but not <code>PaaS</code>.</p> <p>Azure <code>Network Watcher</code> provides tools to monitor, diagnose, view metrics, and enable or disable logs for resources in an Azure virtual network. Network Watcher is designed to monitor and repair the network health of IaaS (Infrastructure-as-a-Service) products including Virtual Machines (VM), Virtual Networks, Application Gateways, Load balancers, etc.</p> <p>NSG flow logs</p> <p>NSG <code>flow logs</code> is a feature of Azure Network Watcher that allows you to log information about IP traffic flowing through an NSG. These logs will show inbound and outbound flows on a <code>per-rule basis</code>.</p>"},{"location":"azure/az-monitor/#monitoring-tools","title":"Monitoring Tools","text":""},{"location":"azure/az-monitor/#topology-map","title":"Topology map","text":"<p>As resources are added to a virtual network, it can become difficult to understand what resources are in a virtual network and how they relate to each other. The topology capability enables you to generate a visual diagram of the resources in a virtual network and the relationships between the resources</p>"},{"location":"azure/az-monitor/#connection-monitor","title":"Connection Monitor","text":"<p>Monitor connectivity between Azure resources on Network.</p>"},{"location":"azure/az-monitor/#network-performance-monitor","title":"Network Performance Monitor","text":"<p>Monitor Network performance, connectivity between Vnets, ExpressRoute etc.</p>"},{"location":"azure/az-monitor/#diagnostic-tools","title":"Diagnostic Tools","text":""},{"location":"azure/az-monitor/#next-hop","title":"Next Hop","text":"<p><code>Next Hop</code> is used to ensure if the traffic is getting routed to the expected destination. Ideally, this will be useful in scenarios where you will be using <code>user-defined routes</code> (UDRs) to verify if the routing rules are working</p> <p>By using Next Hop, you can easily find which route table is used for routing the traffic from a source to destination</p>"},{"location":"azure/az-monitor/#ip-flow-verify","title":"IP Flow verify","text":"<p><code>IP Flow Verify</code> can be used to quickly troubleshoot connectivity issues from or to a remote IP address from a local IP address. </p> <p>Example</p> <p>When you create a VM, there will be a default NSG that will be assigned to the VM. Let\u2019s assume that even after opening the ports you are not able to connect to the VM remotely via RDP.  To understand which rule is  blocking the connectivity from the remote IP to the VM, you can use IP Flow Verify</p>"},{"location":"azure/az-monitor/#effective-security-rules","title":"Effective security rules","text":"<p>As you know, you can apply an NSG at the subnet level and at the NIC level. Sometimes this can get complicated and with the help of effective security rules will be capable of finding the effective rules applied on the traffic.</p>"},{"location":"azure/az-monitor/#packet-capture","title":"Packet Capture","text":"<p>Capture packets from VM for analysis when a condition is met.</p>"},{"location":"azure/az-monitor/#connection-troubleshoot","title":"Connection Troubleshoot","text":"<p>Using Connection Troubleshoot, you can check the connectivity from a virtual machine to another VM, FQDN, URI, or IPv4 address.</p>"},{"location":"azure/az-monitor/#vpn-diagnostic","title":"VPN Diagnostic","text":"<p>Used to diagnose/troubleshoot VPN related issues.</p>"},{"location":"azure/az-monitor/#itsmc","title":"ITSMC","text":"<p><code>IT Service Management Connector</code> (ITSMC) allows you to connect Azure to a supported IT Service Management (ITSM) product or service. Azure services like <code>Azure Log Analytics</code> and <code>Azure Monitor</code> provide tools to detect, analyze, and troubleshoot problems with your Azure and non-Azure resources. But the work items related to an issue typically reside in an ITSM product or service. ITSMC provides a bidirectional connection between Azure and ITSM tools to help you resolve issues faster. </p> <p>supported ITSMC's</p> <p>ITSMC supports connections with the following ITSM tools: - ServiceNow - System Center Service Manager - Provance - Cherwell.</p>"},{"location":"azure/az-monitor/#performance-counters","title":"Performance Counters","text":"<p><code>Performance counters</code> in Windows and Linux provide insight into the performance of hardware components, OS, and applications. <code>Azure Monitor</code> can collect performance counters from <code>Log Analytics agents</code> such as <code>waagent (Azure Linux Agent)</code> at frequent intervals for <code>Near Real Time (NRT) analysis</code> in addition to aggregating performance data for longer term analysis and reporting.</p>"},{"location":"azure/az-monitor/#misc-notes","title":"Misc Notes","text":"<ul> <li>Heartbeat table can tell which systems have not sent heartbeat in last x minutes.</li> <li>Auth event is part of syslog table.</li> <li>The <code>Network Watcher connection troubleshoot</code> provides the capability to check a direct TCP connection from a virtual machine to a virtual machine (VM), fully qualified domain name (FQDN), URI, or IPv4 address.</li> <li>Activity log saves data for 90 days.</li> <li>Performance counters are stored in <code>perf</code> table.</li> <li>Usage Table; it stores <code>Hourly usage data</code> for each table in the workspace.</li> <li><code>Log analytics</code> is billed for data ingestion and data retention.</li> <li>31 days of data retention is free in <code>Log analytics</code></li> <li>NSG flow logs are stored in JSON format.</li> </ul>"},{"location":"azure/az_commands/","title":"Azure Commands","text":""},{"location":"azure/az_commands/#vm-realted","title":"VM realted","text":"<pre><code># Get a list of subscriptions for the logged in account. (autogenerated)\naz account list \n\naz vm list\n\naz group list --query \"[?location=='eastus2']\"  \n</code></pre>"},{"location":"azure/az_commands/#nsg","title":"NSG","text":"<pre><code># get the nsg associated with an RG\naz network nsg list \\\n  --resource-group XXX \\\n  --query '[].name' \\\n  --output tsv\n</code></pre>"},{"location":"azure/az_commands/#get-nsg-rules","title":"Get NSG rules","text":"<pre><code>az network nsg rule list \\\n  --resource-group XXX \\\n  --nsg-name my-vmNSG\n</code></pre>"},{"location":"azure/az_commands/#create-a-nsg-rule","title":"Create a NSG rule","text":"<p>Here we are creaating a rule to allow the http access on port 80</p> <pre><code>az network nsg rule create \\\n  --resource-group XXX \\\n  --nsg-name my-vmNSG \\\n  --name allow-http \\\n  --protocol tcp \\\n  --priority 100 \\\n  --destination-port-range 80 \\\n  --access Allow\n</code></pre>"},{"location":"azure/az_commands/#check-if-rule-is-added","title":"Check if rule is added","text":"<pre><code>az network nsg rule list \\\n    --resource-group XXX \\\n    --nsg-name my-vmNSG  \\\n    --query '[].{Name:name, Priority:priority, Port:destinationPortRange, Access:access}'  \\\n    --output table\n\n\nName               Priority    Port    Access\n-----------------  ----------  ------  --------\ndefault-allow-ssh  1000        22      Allow\nallow-http         100         80      Allow\n</code></pre>"},{"location":"azure/azlb/","title":"Az Load Balancer (L4)","text":"<p>In AWS terms, its a <code>Network Load Balancer (NLB)</code> as Azure Load Balancer operates at layer 4 of the Open Systems Interconnection (OSI) model. It's the single point of contact for clients. </p> <p> OSI Layer AWS Azure L4 Network LB Azure LB L7 Application LB App Gateway <p></p> <p>Load balancer distributes inbound flows that arrive at the load balancer's front end to <code>backend pool</code> instances. These flows are according to configured load-balancing rules and health probes. The <code>backend pool</code> instances can be <code>Azure Virtual Machines (VM)</code> or instances in a <code>Virtual Machine Scale Set (VMSS)</code>.</p>"},{"location":"azure/azlb/#types-of-lb","title":"Types of LB","text":"<p>The purpose of a load balancer is to distribute the traffic to the servers after verifying the <code>health of the backend server</code>. Depending upon the placement of the load balancer in the architecture, the load balancer can be categorized as:</p> <p> </p> <ol> <li> <p><code>Public/External LB</code>: As the name suggests, a public load balancer will have a <code>public IP address</code>, and it will be Internet facing. In a public load balancer, the <code>public IP address</code> and a <code>port number</code> are mapped to the <code>private IP address</code> and <code>port number</code> of the VMs that are part of the backend pool.</p> </li> <li> <p><code>Private/Internal LB</code>: There will be scenarios where you want to load balance the requests between resources that are deployed inside a virtual network without exposing any Internet endpoint. For example, this could be a set of <code>database servers</code> that will distribute the database requests coming from the <code>front-end servers</code>. Since the backend database servers cannot be exposed to the Internet, you need to make sure that the load balancer has no public endpoint. Internal load balancers are deployed to distribute the traffic to your backend servers that cannot be exposed to the Internet. The internal load balancer will not have a <code>public IP address</code> and will be using the private IP address for all communication. This private IP address can be reached by the resources within the same virtual network, within peered networks, or from on-premises over VPN.</p> </li> </ol>"},{"location":"azure/azlb/#backend-pool","title":"Backend pool","text":"<p>The <code>backend pool</code> is a critical component of the load balancer. The <code>backend pool</code> defines the group of resources that will serve traffic for a given load-balancing rule. The backend pool contains the IP address of the network interface cards that are attached to the set of virtual machines or virtual machine scale set</p> <p>In the Standard SKU, you can have up to 1,000 instances, and the Basic SKU can have up to 100 instances in the backend pool.</p> <p>There are two ways of configuring a <code>backend pool</code>:</p> <ul> <li>Network Interface Card (NIC)</li> <li>IP address</li> </ul>"},{"location":"azure/azlb/#floating-ip","title":"Floating IP","text":"<p>Why floating IP is required?</p> <p>If you want to reuse the backend port across multiple rules, you must enable Floating IP in the load balancing rule definition.</p> <p>Floating IP is Azure's terminology for a portion of what is known as Direct Server Return (DSR). DSR consists of two parts: a flow topology and an IP address mapping scheme. At a platform level, Azure Load Balancer always operates in a DSR flow topology regardless of whether Floating IP is enabled or not. This means that the outbound part of a flow is always correctly rewritten to flow directly back to the origin.</p> <p> </p> <p>When Floating IP is enabled, Azure changes the IP address mapping to the Frontend IP address (FIP) of the Load Balancer frontend instead of backend instance's IP (BIP). Without Floating IP, Azure exposes the VM instances' IP. Enabling Floating IP changes the IP address mapping to the Frontend IP of the load Balancer to allow for more flexibility.</p>"},{"location":"azure/azlb/#health-probes","title":"Health Probes","text":"<p>Azure Load Balancer rules require a <code>health probe</code> to detect the endpoint status. The configuration of the <code>health probe</code> and probe responses determines which backend pool instances will receive new connections. </p> <p>Why health probe is required?</p> <p>The purpose of a health probe is to let the load balancer know the status of the application. The health probe will be constantly checking the status of the application using an HTTP or TCP probe. If the application is not responding after a set of consecutive failures, the load balancer will mark the virtual machine as unhealthy. Incoming requests will not be routed to the unhealthy virtual machines. The load balancer will start routing the traffic once the health probe is able to identify that the application is working and is responding to the probe.</p> <p>How health probe works for HTTP?</p> <p>In the HTTP probe, the endpoint will be probed every 15 seconds (default value), and if the response is <code>HTTP 200</code>, then it means that the application is healthy. If the application is returning a <code>non-2xx response</code> within the timeout period, the virtual machine will be marked unhealthy.</p>"},{"location":"azure/azlb/#hashing-lb-rules","title":"Hashing/ LB Rules","text":"<p>Without the load balancer rules, the traffic that hits the front end will never reach the backend pool</p> <p>The hash is used to route traffic to healthy backend instances within the <code>backend pool</code>. The algorithm provides stickiness only within a transport session. When the client starts a new session from the same source IP, the source port changes and causes the traffic to go to a different backend instance.</p> <p>key differences</p> <ul> <li>5 tuple (default): Traffic from the same client IP routed to any healthy instance in the <code>backend pool</code> </li> <li>3 tuple: Traffic from the same client IP and protocol is routed to the same backend instance. Its also called <code>source IP affinity</code></li> <li>2 tuple: Traffic from the same client IP is routed to the same backend instance   </li> </ul>"},{"location":"azure/azlb/#5-tuple-hash","title":"5 tuple Hash","text":"<p>Azure Load Balancer uses a five tuple hash based distribution mode by default.</p> <p>The five tuple consists of:</p> 5 tuple hash<pre><code>Source IP\nSource port\n\nDestination IP\nDestination port\n\nProtocol type\n</code></pre>"},{"location":"azure/azlb/#3-tuple-hash","title":"3 tuple hash","text":"<p>Client IP and protocol (3-tuple) - Specifies that successive requests from the same client IP address and protocol combination will be handled by the same backend instance.</p> 3 tuple hash<pre><code>Source IP\nDestination IP\nProtocol type\n</code></pre>"},{"location":"azure/azlb/#2-tuple-hash","title":"2 tuple hash","text":"<p>Client IP (2-tuple) - Specifies that successive requests from the same client IP address will be handled by the same backend instance.</p> 2 tuple hash<pre><code>Source IP\nDestination IP\n</code></pre>"},{"location":"azure/azure_function/","title":"Azure Functions :azure-compute-function-apps:","text":"<p>A function contains two important pieces</p> <ul> <li><code>Code</code>: which can be written in a variety of languages </li> <li><code>Config</code>: the function.json file</li> </ul> <p>For compiled languages, this config file is generated automatically from annotations in your code. For scripting languages, you must provide the config file yourself.</p>"},{"location":"azure/azure_function/#function-app","title":"Function app","text":"<p>In <code>Azure Functions</code>, a function app provides the execution context for your individual functions. Function app behaviors apply to all functions hosted by a given function app. All functions in a function app must be of the same language.</p> <p>Scaling of functions in Function app</p> <p>Individual functions in a <code>function app</code> are deployed together and are scaled together. All functions in the same function app share resources, per instance, as the function app scales.</p>"},{"location":"azure/azure_function/#data-sharing-between-app-service","title":"Data sharing between App Service","text":"<p>Connection strings, environment variables, and other application settings are defined separately for each function app. Any data that must be shared between function apps should be stored externally in a persisted store.</p>"},{"location":"azure/azure_function/#durable-azure-functions","title":"Durable Azure Functions","text":"<p><code>Durable Functions</code> is an extension of Azure Functions that lets you write stateful functions in a serverless compute environment. The extension lets you define  - Stateful workflows by writing <code>orchestrator functions</code>  - Stateful entities by writing <code>entity functions</code>  - State-less entities by writing <code>activity functions</code></p> <p>Note</p> <p>Behind the scenes, the extension manages state, checkpoints, and restarts for you, allowing you to focus on your business logic.</p>"},{"location":"azure/azure_function/#app-patterns","title":"App Patterns","text":""},{"location":"azure/azure_function/#example-of-durable-azure-function","title":"Example of Durable Azure function","text":"<p>To begin with, let\u2019s do a brief explanation about a few key concepts around Azure Durable Functions.</p>"},{"location":"azure/azure_function/#azure-durable-functions-basics","title":"Azure Durable Functions basics","text":"<p>For those unaware of Azure Durable Functions, it is an extension of Azure Functions that lets you write stateful functions in a serverless compute environment.    The extension lets you define stateful workflows by writing orchestrator functions and stateful entities.</p> <p>The 4 main concepts you should know about are:</p> <ol> <li> <p>Client functions:The client function is responsible for starting, stopping and monitoring the orchestrator functions.</p> </li> <li> <p>Orchestrator functions: This function is the heart when building a durable function solution. In this function you write your workflow in code. The workflow can consist of code statements calling other functions like activity functions, other orchestration functions or even wait for other events to occur. An orchestration is started by a client function, a function that on its turn can be triggered by a message in a queue, an HTTP request, or any other trigger mechanism you are familiar with. Every instance of an orchestration function will have an instance identifier, which can be auto-generated or user-generated.</p> </li> <li> <p>Activity functions:These activity functions are the basic unit of work in a durable function solution. Each activity function executes one task and can be anything you want.</p> </li> <li> <p>Entity functions: <code>Entity functions</code> define operations for reading and updating small pieces of state. We often refer to these stateful entities as durable entities. Like orchestrator functions, entity functions are functions with a special trigger type, entity trigger. They can also be invoked from client functions or from orchestrator functions. Unlike orchestrator functions, entity functions do not have any specific code constraints. Entity functions also manage state explicitly rather than implicitly representing state via control flow.</p> </li> </ol>"},{"location":"azure/azure_function/#problem-statement","title":"Problem Statement","text":"<p>In this section I\u2019ll like to talk a little bit about what I was trying to solve using Durable functions and why Durable functions was a good fit. One of my clients has an HTTP API that uses an Azure Function as its backend, the function takes a few parameters via <code>querystring</code>, runs a query against an Azure Storage Table and returns a result. It\u2019s a really simple setup.</p> <p>Info</p> <p>The problem with this approach lies in the fact that the query takes between 3 and 7 minutes to complete, so most of the time the HTTP function times out because 230 seconds is the maximum amount of time that an HTTP triggered function can take to respond to a request. This is because of the idle timeout of Azure Load Balancer. For longer processing times, I needed to consider using an async pattern or defer the actual work and return an immediate response. Azure Durable Functions provides built-in support for the async http api pattern. </p>"},{"location":"azure/azure_function/#async-http-api-pattern","title":"Async HTTP API Pattern","text":"<p>The async HTTP API pattern addresses the problem of coordinating the state of long-running operations with external clients.</p> <p>A common way to implement this pattern is by having an HTTP endpoint trigger the long-running action. Then, redirect the client to a status endpoint that the client polls to learn when the operation is finished.</p>"},{"location":"azure/azure_function/#scenario","title":"Scenario","text":"<p>The idea behind what we\u2019re going to build is the following one:</p> <ul> <li>The customer submits a job by calling an Azure Function that has an HTTP endpoint trigger. This is the Client Function. </li> <li>The submitted job is going to be picked up by the Orchestrator Function and it will call the Activity Function. </li> <li>The Activity Function is going to run the query against the Azure Storage Table and return the result. </li> <li>There is going to be an extra Azure Function that queries the orchestrator function and retrieves the status and the result of a given job.</li> </ul> <p>Here\u2019s a diagram:</p> <ul> <li><code>client-function</code>: submits the job that needs to be executed.</li> <li><code>get-status-function</code>: it is used to retrieve the status and the result of a given job.</li> <li><code>orchestrator-function</code>: Unwraps the parameters from the submitted job and calls the activity function.</li> <li><code>query-storage-account-activity-function</code>: Runs a custom query in an Azure Storage Table.</li> </ul>"},{"location":"azure/azure_function/#implementation","title":"Implementation","text":"<p>The components of the solution are described in the previous section. Now let\u2019s start building them.</p>"},{"location":"azure/azure_function/#client-function","title":"client-function","text":"<p>This is an Http triggered function.</p> <p>The function receives 2 parameters via QueryString (those parameters are needed to build the query against the Storage Table) and validates both of them.</p> <p>Afterwards, it creates a <code>DurableOrchestrationClient</code> and starts a new orchestration instance using the start_new method.</p> <p>The start_new method takes 3 parameters:</p> <ul> <li>Name: The name of the orchestrator function to schedule. -InstanceId: (Optional) The unique ID of the instance. If you don\u2019t specify this parameter, the method uses a random ID.</li> <li>Input: Any JSON-serializable data that should be passed as the input to the orchestrator function.</li> </ul> <p>And lastly, it builds the URL from where the client can retrieve the status and the result of the submitted job.</p> <pre><code>import azure.functions as func\nimport azure.durable_functions as df\nimport json\nimport dateutil.parser\nfrom urllib.parse import urlparse\n\nasync def main(req: func.HttpRequest, starter: str) -&gt; func.HttpResponse:\n\n    response = { }\n    headers = { \"Content-Type\": \"application/json\" }\n\n    start_date = req.params.get('s')\n    end_date = req.params.get('e')\n\n    if start_date:\n        try:\n          start_date = dateutil.parser.parse(start_date, dayfirst=True)\n        except:\n          response['error'] = \"Invalid start date format.\"\n          return func.HttpResponse(json.dumps(response) ,headers=headers, status_code=400 )\n    else:\n        response['error'] = \"Empty start date.\"\n        return func.HttpResponse(json.dumps(response) ,headers=headers, status_code=400 )\n\n    if end_date:\n        try:\n          end_date = dateutil.parser.parse(end_date, dayfirst=True)\n        except:\n          response['error'] = \"Invalid end date format.\"\n          return func.HttpResponse(json.dumps(response) ,headers=headers, status_code=400 )\n    else:\n        response['error'] = \"Empty end date.\"\n        return func.HttpResponse(json.dumps(response) ,headers=headers, status_code=400 )\n\n    delta = end_date - start_date\n    if delta.days &lt; 1:\n        response['error'] = \"Invalid date range.\"\n        return func.HttpResponse(json.dumps(response) ,headers=headers, status_code=400 )\n\n\n    client = df.DurableOrchestrationClient(starter)\n\n    parameters = {\n        \"start\": start_date.strftime(\"%Y-%m-%d\"),\n        \"end\": end_date.strftime(\"%Y-%m-%d\")\n    }\n\n    instance_id = await client.start_new('orchestrator-function', None, parameters)\n\n    status_uri = build_api_url(urlparse(req.url).scheme, req.headers.get(\"host\"), instance_id)\n    response[\"statusUri\"] = status_uri\n    return func.HttpResponse(json.dumps(response), headers=headers, status_code=200 )\n</code></pre> <p>def build_api_url(scheme, host, instance_id): return f\"{scheme}://{host}/api/status/{instance_id}\"</p>"},{"location":"azure/azure_function/#get-status-function","title":"get-status-function","text":"<p>Is this function really needed? Before discussing the implementation of this Azure Function, I think it\u2019s important to talk a bit about if this function is really needed or not.</p> <p>Most of the examples that you are going to find online don\u2019t use a custom function to retrieve the status of the jobs. Instead of that, they use the create_check_status_response method on the Client Function.</p> <p>That means that in the previous section I could have built the client function like this:</p> <pre><code>...\nclient = df.DurableOrchestrationClient(starter)\n\nparameters = {\n    \"start\": start_date.strftime(\"%Y-%m-%d\"),\n    \"end\": end_date.strftime(\"%Y-%m-%d\")\n}\n\ninstance_id = await client.start_new('orchestrator-function', None, parameters)\nreturn client.create_check_status_response(req, instance_id)\n</code></pre> <p>And that\u2019s how the function would have responded:</p> <pre><code>{\n\"id\": \"b78244c9e19f43e89e7b1578f711940d\",\n\"statusQueryGetUri\": \"http://localhost:7071/runtime/webhooks/durabletask/instances/b78244c9e19f43e89e7b1578f711940d?taskHub=TestHubName&amp;connection=Storage&amp;code=V/xX4rraZaheGNwbMePOhhGtyHxGsgks4Q36WKBdS3WfiOjD5Sb5lw==\",\n\"sendEventPostUri\": \"http://localhost:7071/runtime/webhooks/durabletask/instances/b78244c9e19f43e89e7b1578f711940d/raiseEvent/{eventName}?taskHub=TestHubName&amp;connection=Storage&amp;code=V/xX4rraZaheGNwbMePOhhGtyHxGsgks4Q36WKBdS3WfiOjD5Sb5lw==\",\n\"terminatePostUri\": \"http://localhost:7071/runtime/webhooks/durabletask/instances/b78244c9e19f43e89e7b1578f711940d/terminate?reason={text}&amp;taskHub=TestHubName&amp;connection=Storage&amp;code=V/xX4rraZaheGNwbMePOhhGtyHxGsgks4Q36WKBdS3WfiOjD5Sb5lw==\",\n\"rewindPostUri\": \"http://localhost:7071/runtime/webhooks/durabletask/instances/b78244c9e19f43e89e7b1578f711940d/rewind?reason={text}&amp;taskHub=TestHubName&amp;connection=Storage&amp;code=V/xX4rraZaheGNwbMePOhhGtyHxGsgks4Q36WKBdS3WfiOjD5Sb5lw==\",\n\"purgeHistoryDeleteUri\": \"http://localhost:7071/runtime/webhooks/durabletask/instances/b78244c9e19f43e89e7b1578f711940d?taskHub=TestHubName&amp;connection=Storage&amp;code=V/xX4rraZaheGNwbMePOhhGtyHxGsgks4Q36WKBdS3WfiOjD5Sb5lw==\",\n\"restartPostUri\": \"http://localhost:7071/runtime/webhooks/durabletask/instances/b78244c9e19f43e89e7b1578f711940d/restart?taskHub=TestHubName&amp;connection=Storage&amp;code=V/xX4rraZaheGNwbMePOhhGtyHxGsgks4Q36WKBdS3WfiOjD5Sb5lw==\"\n}\n</code></pre> <p>As you can see the <code>create_check_status_response</code> method returns a response that contains links to the various management operations that can be invoked on an orchestration instance. These operations include querying the orchestration status, raising events or terminating the orchestration.</p> <p>So, why I\u2019m building an extra Azure Function to retrieve the status of a job instead of directly using the <code>create_check_status_response</code> method?</p> <p>This is because the general public shouldn\u2019t be able to query the Orchestration status endpoint directly. As you can see the response payload includes the orchestrator management endpoints with their corresponding keys, by providing that, you would allow the clients not only to get the status of a running instance, but also to get the execution history, send external events to the orchestration instance or terminate that instance.</p> <p>Basically if you don\u2019t want your clients messing around with the heart of the function, you need to expose another azure function that returns only the minimum information required.</p>"},{"location":"azure/azure_function/#implementation-of-get-status-function","title":"Implementation of get-status-function","text":"<p>The get-status-function is an Http triggered function.</p> <p>This function needs the <code>instance_id</code> that the Client function generated when submitting the job to the orchestrator function.</p> <p>To retrieve the status and the result of a submitted job you can use the <code>get_status</code> method. This method queries the orchestrator function to obtain the status of the job.</p> <p>The <code>get_status</code> returns an object with the following properties:</p> <pre><code>Name: The name of the orchestrator function.\nInstanceId: The instance ID of the orchestration (should be the same as the instanceId input).\nCreatedTime: The time at which the orchestrator function started running.\nLastUpdatedTime: The time at which the orchestration last checkpointed.\nInput: The input of the function as a JSON value. This field isn\u2019t populated if showInput is false.\nCustomStatus: Custom orchestration status in JSON format.\nOutput: The output of the function as a JSON value (if the function has completed). If the orchestrator function failed, this property includes the failure details. If the orchestrator function was terminated, this property includes the reason for the termination (if any).\nRuntimeStatus: One of the following values:\nPending: The instance has been scheduled but has not yet started running.\nRunning: The instance has started running.\nCompleted: The instance has completed normally.\nContinuedAsNew: The instance has restarted itself with a new history. This state is a transient state.\nFailed: The instance failed with an error.\nTerminated: The instance was stopped abruptly.\nHistory: The execution history of the orchestration. This field is only populated if showHistory is set to true.\n</code></pre> <pre><code>import azure.functions as func\nimport azure.durable_functions as df\nimport json\n\n\nasync def main(req: func.HttpRequest, starter: str) -&gt; func.HttpResponse:\nclient = df.DurableOrchestrationClient(starter)\n\n    instance_id = req.route_params[\"id\"]\n\n    response = await client.get_status(instance_id)\n\n    if response.instance_id is None:\n        return func.HttpResponse(\"Job not found\", status_code=404)\n\n    return func.HttpResponse(json.dumps({\n        \"id\": response.instance_id,\n        \"status\": response.runtime_status.value,\n        \"result\": response.output\n    }))\n</code></pre>"},{"location":"azure/azure_function/#orchestrator-function","title":"orchestrator-function","text":"<p>Nothing fancy here. The orchestrator function in this application is really simple because there is no need to orchestrate multiple activies nor build a complex workflow.</p> <p>The function does the following steps:</p> <p>Retrieves the inputs send from the Client Function (These parameters are needed to build the query against the Azure Table Storage) Calls the activity function passing the retrieved inputs. Waits for the activity function to end and returns the result.</p> <pre><code>import azure.durable_functions as df\n\ndef orchestrator_function(context: df.DurableOrchestrationContext):\ninput = context.get_input()\nresult = yield context.call_activity('query-storage-account-activity-function', {'start': input['start'], 'end': input['end']})\nreturn result\n\nmain = df.Orchestrator.create(orchestrator_function)\n</code></pre>"},{"location":"azure/azure_function/#query-storage-account-activity-function","title":"query-storage-account-activity-function","text":"<p>This Activity Function is responsible to run the query against the Azure Table Storage.</p> <p>This function retrieves the Storage Table connection string from an App Configuration with a Key Vault reference (https://docs.microsoft.com/en-us/azure/azure-app-configuration/overview), runs the query and returns the result.</p> <p>As you can see this function is a single unit of work and does not contain any reference to the durable-functions library.</p> <pre><code>import json\nimport os\nfrom azure.appconfiguration import AzureAppConfigurationClient\nfrom azure.identity import DefaultAzureCredential\nfrom azure.keyvault.secrets import SecretClient\nfrom azure.data.tables import TableClient\nfrom pathlib import Path\n\ndef main(request: dict) -&gt; int:\nstart = request['start']\nend   = request['end']\n\n    table_conn_str = get_azure_table_connection_string()\n    response = run_query(start, end, table_conn_str)\n\n    return response\n\ndef run_query(start, end, table_conn_str) -&gt; int:\n\n    items = []\n    client =  TableClient.from_connection_string(conn_str=table_conn_str, table_name=\"audit\") \n\n    parameters = {\n        \"start\": start,\n        \"end\": end\n    }\n\n    query_filter = \"PartitionKey ge @start and PartitionKey le @end and CertNumber ne ''\"\n    entities = client.query_entities(query_filter, parameters=parameters, select='identificationNumber')\n\n    for entity in entities:\n        items.append(entity['identificationNumber'])\n\n    return len(set(items))\n</code></pre> <pre><code>def get_azure_table_connection_string() -&gt; str:\n    defaultAzureCredential = DefaultAzureCredential()\n\n    app_config_base_url = os.getenv('AppConfigEndpoint')\n    app_config_client = AzureAppConfigurationClient(base_url=app_config_base_url, credential=defaultAzureCredential)\n\n    keyvault_value = app_config_client.get_configuration_setting(key=\"storage-account-connection-string\", label=\"async-http-api\")\n    url_parts = Path(json.loads(keyvault_value.value)[\"uri\"]).parts\n    vault_url = \"//\".join(url_parts[:2])\n    kv_secret = url_parts[-1]\n    kv_client = SecretClient(vault_url, defaultAzureCredential)\n    secret_val = kv_client.get_secret(kv_secret).value\n\n    return secret_val\n</code></pre>"},{"location":"azure/azure_function/#test","title":"Test","text":"<p>Everything is put in place, now let\u2019s test it.</p> <p>If I try to submit a new job with a few invalid parameters, it responds with an error:</p> <pre><code>curl \"https://func-sa-table-durable-dev.azurewebsites.net/api/submit/query\"\n{\"error\": \"Empty start date.\"}\ncurl \"https://func-sa-table-durable-dev.azurewebsites.net/api/submit/query?s=10/08/2021&amp;e=abcd\"\n{\"error\": \"Invalid end date format.\"}\ncurl \"https://func-sa-table-durable-dev.azurewebsites.net/api/submit/query?s=10/08/2021&amp;e=05/08/2021\"\n{\"error\": \"Invalid date range.\"}\nCopy\nIf I try to submit a new job with valid parameters, the client function responds with the status endpoint.\n\ncurl \"https://func-sa-table-durable-dev.azurewebsites.net/api/submit/query?s=10/10/2021&amp;e=10/12/2021\"\n{\"statusUri\": \"https://func-sa-table-durable-dev.azurewebsites.net/api/status/433ebcfe85ec4012abe94dcda2aa6b00\"}\nCopy\nIf I query the get-status function right away, it returns that the job is still being executed.\n\ncurl \"https://func-sa-table-durable-dev.azurewebsites.net/api/status/433ebcfe85ec4012abe94dcda2aa6b00\"\n{\"id\": \"433ebcfe85ec4012abe94dcda2aa6b00\", \"status\": \"Running\", \"result\": null}\nCopy\nIf I query the get-status function after a few minutes, the job has completed and we can see the result.\n\ncurl \"https://func-sa-table-durable-dev.azurewebsites.net/api/status/433ebcfe85ec4012abe94dcda2aa6b00\"\n{\"id\": \"433ebcfe85ec4012abe94dcda2aa6b00\", \"status\": \"Completed\", \"result\": 119}\n</code></pre>"},{"location":"azure/azure_function/#deployment-to-azure","title":"Deployment to Azure","text":"<p>I didn\u2019t plan to write about how to deploy these functions to Azure, but it might be useful to someone.</p> <p>Here\u2019s how you can deploy them using:</p> <p>Azure DevOps pipelines - Github Actions - Using Azure Pipelines</p> <pre><code>trigger: none\n\npool:\nvmImage: 'ubuntu-latest'\n\nvariables:\n- name: azureSubscription\n  value: 'cpons-demos-dev'\n- name: functionAppName\n  value: 'func-staccount-report-query-dev'\n\nsteps:\n- task: UsePythonVersion@0\n  inputs:\n  versionSpec: '3.8'\n  displayName: 'Use Python 3.8'\n\n- script: |\n  python -m pip install --upgrade pip\n  pip install --target=\"./.python_packages/lib/site-packages\" -r ./requirements.txt    \n  displayName: 'Install dependencies'\n\n- task: CopyFiles@2\n  inputs:\n  SourceFolder: '$(Build.SourcesDirectory)'\n  Contents: '**'\n  TargetFolder: '$(Build.ArtifactStagingDirectory)'\n\n- task: ArchiveFiles@2\n  inputs:\n  rootFolderOrFile: '$(Build.ArtifactStagingDirectory)'\n  includeRootFolder: false\n  archiveType: 'zip'\n  archiveFile: '$(Build.ArtifactStagingDirectory)/$(Build.BuildId).zip'\n  replaceExistingArchive: true\n\n- task: AzureFunctionApp@1\n  inputs:\n  azureSubscription: '$(azureSubscription)'\n  appType: 'functionAppLinux'\n  appName: '$(functionAppName)'\n  package: '$(Build.ArtifactStagingDirectory)/$(Build.BuildId).zip'\n  runtimeStack: 'PYTHON|3.8'\n  Copy\n  Using Github Action\n  name: Deploy Durable Functions to Azure Function App\n</code></pre>"},{"location":"azure/azure_function/#securing-durable-functions","title":"Securing durable functions","text":"<ul> <li><code>Access keys</code> - Functions lets you use keys to make it harder to access your HTTP function endpoints during development. Unless the HTTP access level on an HTTP triggered function is set to anonymous, requests must include an API access key in the request. Keys can be </li> <li><code>host keys</code> </li> <li><code>function keys</code></li> <li><code>System keys</code>: Specific extensions may require a system-managed key to access webhook endpoints. System keys are designed for extension-specific function endpoints that called by internal components. For example, the Event Grid trigger requires that the subscription use a system key when calling the trigger endpoint.</li> <li></li> </ul>"},{"location":"azure/azvm/","title":"Azure VM","text":""},{"location":"azure/azvm/#disk-types","title":"Disk types","text":"<p>VM will use 3 disks; of which 2 comes by default</p> <ul> <li>OS disk: The comes with a VM and store OS files. Its <code>C:</code> drive in windows and <code>/dev/sda</code> for Unix-like systems.</li> <li> <p>Temporary disks: They also come by default and store <code>non-persistent data</code> like swap and page files. During a planned maintenance event, redeployment, or host change, the data stored in the <code>temporary disk</code> will be erased. Data will be available during normal reboots, though.</p> <p>Any data on the drive should not be critical to you as it is prone to loss. In Windows, the temporary disk will be labeled as the <code>D: drive</code>, and in Linux, the disk will be labeled as <code>/dev/sdb</code></p> </li> <li> <p>Data disk: These are additional disks for <code>persistent</code>stores like data files. These disks are VHDs (Virtual Hard Disks). They are page blobs in the blob service.</p> <p>These disks can be labeled with any letter in Windows, and in Linux data disks are labeled from <code>/dev/sdc</code> onward</p> </li> </ul> <p> </p>"},{"location":"azure/azvm/#unmanaged-disks","title":"(Un)/managed disks","text":"<p>Microsoft has two offerings in the case of storage disks </p> <ol> <li>Managed disk: An ARM managed resources with <code>Azure managed Storage Accounts</code>. Since you don\u2019t manage the storage account, the <code>storage account</code> limits don\u2019t apply.</li> <li>Unmanaged disk: In Unmanaged Disk storage, you must create a storage account in resources to hold the disks (VHD files) for your Virtual Machines.</li> </ol>"},{"location":"azure/azvm/#custom-data","title":"Custom data","text":"<p>Why do we need it?</p> <p>You might need to inject a script or other metadata into a Microsoft Azure virtual machine (VM) at provisioning time. In other clouds, this concept is often called <code>user data</code>. Microsoft Azure has a similar feature called <code>custom data</code>.</p> <p>Custom data is made available to the VM during the first startup or setup, which is called provisioning. Provisioning is the process where VM creation parameters (for example, hostname, username, password, certificates, custom data, and keys) are made available to the VM. A provisioning agent, such as the Linux Agent or cloud-init, processes those parameters.</p> <p>Custom data is sent to the VM along with the other provisioning configuration information such as the new hostname, username, password, certificates, and keys, etc. This data is passed to the Azure API as <code>base64-encoded data</code>.  On Windows, this data ends up in <code>%SYSTEMDRIVE%\\AzureData\\CustomData.bin</code> as a binary file.  On Linux, this data is passed to the VM via the <code>ovf-env.xml</code> file, which is copied to the <code>/var/lib/waagent</code> directory during provisioning.</p> <p>How to pass data</p> <p>To use custom data, you must Base64-encode the contents before passing the data to the API--unless you're using a CLI tool that does the conversion for you, such as the Azure CLI. The size can't exceed 64 KB. In the CLI, you can pass your custom data as a file, as the following example shows. The file will be converted to Base64.</p> <pre><code>az vm create \\\n    --resource-group myResourceGroup \\\n    --name centos74 \\\n    --image OpenLogic:CentOS-CI:7-CI:latest \\\n    --custom-data cloud-init.txt \\\n    --generate-ssh-keys\n</code></pre>"},{"location":"azure/azvm/#proximity-placement-groups","title":"Proximity Placement groups","text":"<p>Placing VMs in a <code>single region</code> reduces the physical distance between the instances. Placing them within a single availability zone will also bring them physically closer together. However, as the Azure footprint grows, a single availability zone may span multiple physical data centers, which may result in a network latency impacting your application.</p> <p>Why to use proximity placement groups?</p> <p>To get VMs as close as possible, achieving the lowest possible latency, you should deploy them within a proximity placement group.</p> <p>A proximity placement group is a logical grouping used to make sure that Azure compute resources are physically located close to each other. Proximity placement groups are useful for workloads where low latency is a requirement.</p>"},{"location":"azure/azvm/#how-to-use-placement-group","title":"How to use placement group?","text":"<p>A <code>proximity placement group</code> is a resource in Azure. You need to create one before using it with other resources. Once created, it could be used with virtual machines, availability sets, or virtual machine scale sets. You specify a proximity placement group when creating compute resources providing the <code>proximity placement group ID</code>.</p> <p>How to move existing resources?</p> <p>You can also move an existing resource into a proximity placement group. When moving a resource into a proximity placement group, you should stop (deallocate) the asset first since it will be redeployed potentially into a different data center in the region to satisfy the <code>co-location constraint</code>.</p>"},{"location":"azure/azvm/#cloud-init","title":"Cloud Init","text":"<p>Tldr</p> <p>By default, this agent will process custom data. It accepts multiple formats of custom data, such as cloud-init configuration and scripts.</p> <p>Similar to the Linux Agent, if errors happen during the execution of the configuration processing or scripts when cloud-init is processing the custom data, that's not a fatal provisioning failure. You'll need to create a notification path to alert you of the completion state of the script.</p> <p>Currently, only the Ubuntu images in the Microsoft Azure Gallery have cloud-init preinstalled and configured to act on the custom data sent during provisioning.  This means that for Ubuntu you can use custom data to provision a VM using a cloud-init configuration file, or just simply send a script that will be executed by cloud-init during provisioning.</p>"},{"location":"azure/azvm/#ha-solutions-in-vm","title":"HA solutions in VM","text":"<p>Various HA solutions are:</p> <ol> <li>VMSS</li> <li>Availability Set</li> <li>Availability Zone</li> </ol>"},{"location":"azure/azvm/#vmss-scale-sets","title":"VMSS (Scale Sets)","text":"<p>Azure <code>Virtual Machine Scale Sets (VMSS)</code> provide the management capabilities for applications that run across many VMs, automatic scaling of resources, and load balancing of traffic. With scale sets, you create a virtual machine configuration model, automatically add or remove additional instances based on CPU or memory load, and automatically upgrade to the latest OS version</p> <p>Why to use VMSS?</p> <ul> <li>With <code>scale sets</code>, all VM instances are created from the <code>same base OS image</code> and configuration. This approach lets you easily manage hundreds of VMs without extra configuration tasks or network management.</li> <li><code>Scale sets</code> are used to run multiple instances of your application. If one of these VM instances has a problem, customers continue to access your application through one of the other VM instances with minimal interruption.</li> <li><code>Scale sets</code> are backed with Azure load balancer for <code>auto-scaling</code></li> </ul> <p>How many VMs can I have in a scale set?</p> <p>A scale set can have 0 to 1,000 virtual machines (VMs) based on platform images or 0 to 600 VMs based on custom images.</p>"},{"location":"azure/azvm/#availability-set","title":"Availability Set","text":"<p>In short, an <code>availability set</code> is a logical grouping of the VMs hosting our application. Since there are multiple instances, you are eliminating the <code>Single Point Of Failure (SPOF)</code></p> <p>Availability set consists of</p> <ol> <li>Fault domains (FD)</li> <li>Update domain (UD)</li> </ol> <p> </p> <p>How many FD and UD per set?</p> <p>Every <code>virtual machine</code> that you create will be associated with a UD and FD. You can configure up to 3 FDs and 20 UDs in an availability set.</p>"},{"location":"azure/azvm/#fault-domain-fd","title":"Fault domain (FD)","text":"<p><code>Fault domains</code> represent a set of virtual machines that share a common network switch, power, and air conditioning. FDs can be configured up to a maximum of 3, and this is the default value while setting up availability sets.</p> <p>They vary by region</p> <p>The number of managed disk fault domains <code>varies by region</code> - either 2 or 3 managed disk fault domains per region.</p>"},{"location":"azure/azvm/#update-domain-ud","title":"Update domain (UD)","text":"<p>UDs represent a group of VMs and the underlying host that can be updated and rebooted at the same time. This will ensure that only one UD is rebooted at a time during planned maintenance events such as patching, firmware updates, etc.</p> <p>Updating VMs in UD</p> <p>The default number of UDs is 5, and if you are creating more than 5 VMs, the 6<sup>th</sup> VM will be placed on the 1<sup>st</sup> UD, the 7<sup>th</sup> will be on the 2<sup>nd</sup>, and so 4<sup>th</sup> depending upon the number of instances. While a UD is getting rebooted, it\u2019s given 30 minutes to recover before the maintenance task is started on a different domain</p>"},{"location":"azure/azvm/#as-vs-az","title":"AS vs AZ","text":"<p>Why do we need AS in addition to AZ?</p> <p>An <code>availability set</code> avoids service downtime caused by hardware failure or planned maintenance. However, the protection offered is limited to the datacenter level. What if the entire datacenter is unavailable due to a power outage, natural disaster, or any other reasons? Since the datacenter is down, your workloads will not be available and that is why we need AZ</p> <p>You can use AS and AZ at the same time while creating a VM.</p>"},{"location":"azure/azvm/#availability-zone-for-ha","title":"Availability Zone (for HA)","text":"<p>Within Azure regions, you have unique physical locations named zones. Each zone comprises one or more datacenters. Also, each zone has independent cooling, power, and networking. As these zones are physically isolated and located in different parts of the region, you can deploy our applications to availability zones to protect from datacenter failures.</p> <p>Azure availability zones are connected by a high-performance network with a <code>round-trip latency</code> of less than 2ms. They help your data stay synchronized and accessible when things go wrong. </p> <p>In case of AZ failure</p> <p>AZs are designed so that if one zone is affected, regional services, capacity, and high availability are supported by the remaining two zones.</p>"},{"location":"azure/azvm/#regions","title":"Regions","text":"<p>In Azure, every region comprises a set of datacenters that are interconnected with a regional low-latency network</p>"},{"location":"azure/azvm/#auto-scaling","title":"Auto-scaling","text":""},{"location":"azure/azvm/#vmss","title":"VMSS","text":"<p>When you have many VMs that run your application, it\u2019s important to maintain a consistent configuration across your environment. For the reliable performance of your application, the VM size, disk configuration, and application installs should match across all VMs.</p> <p>Virtual Machines Scale Sets provide a logical grouping of platform-managed virtual machines. With scale sets, you create a virtual machine configuration model, automatically add or remove additional instances based on CPU or memory load, and automatically upgrade to the latest OS version.</p> <p>We can set scale-out (increase VM count by 1) and scale-in (decrease VM count by 1 for example) configuration in VMSS. Also, the scale-in policy can be defined as <code>oldest VM</code> or <code>newest VM</code>, or by <code>instance id</code>, etc to make sure which VM gets deleted when the load decreases.</p>"},{"location":"azure/azvm/#flexible","title":"Flexible","text":"<p>With <code>Flexible orchestration</code>, Azure provides a unified experience across the Azure VM ecosystem. Flexible orchestration offers high availability guarantees (up to 1000 VMs) by spreading VMs across fault domains in a region or within an Availability Zone. This enables you to scale out your application while maintaining fault domain isolation that is essential to run quorum-based or stateful workloads</p>"},{"location":"azure/azvm/#uniform","title":"Uniform","text":"<p>With <code>uniform orchestration</code>, virtual machine scale sets use a virtual machine profile or template to scale up to desired capacity. While there is some ability to manage or customize individual virtual machine instances, uniform uses identical VM instances.</p>"},{"location":"azure/azvm/#automate-vm-deployment","title":"Automate VM deployment","text":"<p>This can be done using IaC (Infra as Code) - <code>ARM</code> templates: We can download a template from a running VM - Terraform scripts - Bicep</p> <p>We can use the concept of <code>Golden Image</code> for this where A golden image is a template for a virtual machine, virtual desktop, server, or hard disk drive. Golden images are also known as ghost images, clones, master images, or base images. To create a golden image, an administrator first sets up the computing environment with the exact specifications needed and then saves the disk image as a pattern for future copies. </p>"},{"location":"azure/azvm/#azure-bastion-jump-box","title":"Azure Bastion (Jump box)","text":"<p>Hardening the <code>jumpbox VM</code> and keeping it away from vulnerabilities are always tedious tasks for the administrator. Azure offers a life-saver service called <code>Azure Bastion</code> to solve this problem.</p> <p>Tldr</p> <p>With Azure Bastion, you can connect to Azure virtual machines without the need to have public IP addresses. The catch here is Azure Bastion, which is a PaaS solution; this means you don\u2019t have to worry about hardening the infrastructure, as this will be handled by <code>Microsoft Azure</code></p> <p> </p> <ol> <li>The <code>Bastion host</code> is deployed to the virtual network in a separate subnet.</li> <li>The user connects to the <code>Azure portal</code> using any HTML5 browser.</li> <li>The user navigates to the Azure virtual machine to RDP/SSH.</li> <li>Connect Integration - Single-click RDP/SSH session inside the browser</li> <li>No public IP is required on the Azure VM</li> </ol> <p>Some useful tips on bastion host</p> <ul> <li>It's a fully managed PaaS in Azure that provides RDP/SSH connectivity over <code>TLS/SSL</code></li> <li>It makes sure that no public IP is exposed. </li> <li>The VMs does not need to have a public IP. The Bastion service will open the <code>RDP/SSH</code> session/connection to your virtual machine over the <code>private IP</code> of your virtual machine, within your virtual network.</li> <li>NSG is not needed because the bastion is hardened internally (they are optional)</li> <li>It is deployed per VNET, earlier it was deployed for each VM and now the process is made more streamlined by Microsoft. <code>Bastion</code> provides secure RDP and SSH connectivity to all the VMs in the virtual network in which it is provisioned</li> <li>It only supports IPV4 addresses</li> <li>There is a separate <code>bastion host subnet</code> to host the jump box.</li> <li>There is a <code>concurrent connection limit</code> with the bastion host (25 for RDP and 50 for SSH)</li> <li>The minimum AzureBastionSubnet size is <code>/26</code> or larger (<code>/25, /24</code>, etc.)</li> <li>Bastion connectivity to <code>Azure Virtual Desktop (WVD)</code> isn't supported.</li> </ul>"},{"location":"azure/azvm/#vm-extensions","title":"VM Extensions","text":"<p>The post-deployment configuration and automation tasks on Azure VMs can be accomplished using small applications that are called <code>vm extensions</code>. Two main extensions are: CSE and DSC</p>"},{"location":"azure/azvm/#cse","title":"CSE","text":"<p><code>Custom Script Extension for Linux</code> (CSE): It is used to automatically invoke scripts and run them on virtual machines post-deployment.</p>"},{"location":"azure/azvm/#dsc","title":"DSC","text":"<p><code>Desired State Configuration</code>(DSC) : In the case of CSE, you cannot deal with complex installation procedures such as reboots. Desired State Configuration helps you overcome this crisis and define a state for your virtual machines instead of writing scripts.</p> <p>You can define the state of a machine and enforce the state using the DSC extension handler. You could store these configuration files in Azure Storage, in internal storage, or even in source control. The handler is responsible for pulling the configuration and implementing the desired state on the virtual machine. Even if there are installations that require reboots, DSC will continue the execution of the remaining scripts after reboot</p>"},{"location":"azure/azvm/#spot-instance","title":"Spot instance","text":"<p>These are low-priority VMs that are allocated from Azure\u2019s excess capacity in the region. Using spot instances can reduce the cost of instances rather than running them as regular instances.</p>"},{"location":"azure/azvm/#windows-setup-scripts","title":"Windows Setup Scripts","text":"<p><code>Setupcomplete.cmd</code> and <code>ErrorHandler.cmd</code> are custom scripts that run during or after the Windows Setup process. They can be used to install applications or run other tasks by using <code>cscript/wscript</code> scripts.</p> <ul> <li><code>%WINDIR%\\Setup\\Scripts\\SetupComplete.cmd</code>: This script runs with local system permissions and starts immediately after the user sees the desktop. </li> <li><code>%WINDIR%\\Setup\\Scripts\\ErrorHandler.cmd</code>: This script runs automatically when Setup encounters a fatal error. It runs with local system permission.</li> </ul>"},{"location":"azure/azvm/#notes","title":"Notes","text":"<ul> <li>For a <code>Windows VM</code>, it can be up to 15 characters, and <code>Linux VMs</code> can support up to 64 characters.</li> <li>VM name can\u2019t be changed once the VM is deployed, except re-deployment.</li> <li>If you shut down the VM from the <code>operating system</code>\u2014for example, by clicking the Windows button and select Shutdown, then you will be charged for the VM as the hardware allocated is not released.</li> <li>You can use your existing licenses purchased from Software Assurance to cut down this cost. This reduction method is called <code>Azure Hybrid Benefit (AHUB)</code>. AHUB can be used for Windows, Linux (RHEL and SUSE), and SQL virtual machine licenses.</li> <li>You can connect to Windows machines using two options, namely, <code>Remote Desktop Protocol</code>   (RDP) for GUI and <code>Windows Remote Management</code> (WinRM) for CLI</li> <li>The public key will end with the <code>extension .pub</code>, and the private key will not have any extension.</li> <li>ssh to vm using <code>ssh -i &lt;path-to-private-key&gt; username@publicIP</code></li> <li>An <code>availability set</code> is free of cost, and you pay only for the instances that you are   deploying.</li> <li>Uploads a virtual hard disk from an on-premises machine to Azure (managed disk or blob) using <code>Add-AzVhd</code></li> <li>If you make a change to the topology of your network and have Windows VPN clients, the VPN client package for Windows clients must be downloaded and installed again in order for the changes to be applied to the client.]</li> <li>The <code>Set-AzureStaticVNetIP</code> cmdlet sets the static virtual network (VNet) IP address information for a virtual machine object.</li> <li><code>New-AzureADMSInvitation</code> is used to invite a new external user to your directory.</li> </ul>"},{"location":"azure/blob/","title":"Blob storage","text":""},{"location":"azure/blob/#what-is-blob","title":"What is Blob?","text":"<p>Azure Blob Storage is Microsoft's object storage solution for the cloud. Blob Storage is optimized for storing massive amounts of <code>unstructured data</code>. Unstructured data is data that doesn't adhere to a particular data model or definition, such as text or binary data.</p> <p>How to access Blob?</p> <p>Users or client applications can access objects in Blob Storage via HTTP/HTTPS, from anywhere in the world.</p> <p>3 kinds of blob storage</p> <ul> <li><code>Block</code>: Variable size blocks, optimized for large blobs</li> <li><code>Page</code>: fixed size blobs, 512 bytes for example. Optimized for random read/write</li> <li><code>Append</code>: used for append operations</li> </ul> <p>Tip</p> <p>Blob Storage supports Azure Data Lake Storage Gen2, Microsoft's enterprise big data analytics solution for the cloud. </p> <p> </p>"},{"location":"azure/blob/#point-in-time-restore","title":"Point-in-time restore","text":"<ul> <li>Point-in-time restore provides protection against accidental deletion or corruption by enabling you to restore block blob data to an earlier state</li> <li>Point-in-time restore is useful in scenarios where a user or application accidentally deletes data or where an application error corrupts data</li> <li>Point-in-time restore also enables testing scenarios that require reverting a data set to a known state before running further tests.</li> </ul> <p>Point-in-time restore requires that the following Azure Storage features be enabled before you can enable point-in-time restore:</p> <pre><code>1. Soft delete\n2. Change feed\n3. Blob versioning\n</code></pre>"},{"location":"azure/blob/#versioning","title":"Versioning","text":"<p>You can enable Blob storage versioning to automatically maintain previous versions of an object. When blob versioning is enabled, you can access earlier versions of a blob to recover your data if it's modified or deleted.</p> <p>Each blob version is identified by a unique version ID. The value of the version ID is the timestamp at which the blob was updated. The version ID is assigned at the time that the version is created.</p>"},{"location":"azure/concepts/","title":"Azure Concepts","text":""},{"location":"azure/concepts/#private-endpoint","title":"Private endpoint","text":"<p>A <code>private endpoint</code> is a network interface that uses a private IP address from your virtual network. This network interface connects you privately and securely to a service that's powered by <code>Azure Private Link</code>. By enabling a private endpoint, you're bringing the service into your virtual network.</p> <p>The service could be an Azure service such as:</p> <pre><code>- Azure Storage\n- Azure Cosmos DB\n- Azure SQL Database\n</code></pre>"},{"location":"azure/concepts/#redundancy-in-azure","title":"Redundancy in Azure","text":"<p>These are the redundancy options in Azure </p> <ul> <li> <p>LRS (Locally Redundant Storage): In LRS, you will have 3 copies of data within a single data center in different fault domains. The copies will always be up-to-date, and all changes are written to the storage account <code>synchronously</code>.</p> </li> <li> <p>ZRS (Zonal Redundant Storage): It saves one copy of an instance in 3 AZs of a region.</p> </li> <li> <p>GRS (Geo Redundant Storage):  Data is replicated to a secondary region which is selected by Azure based on the <code>regional pairs</code>. This isolation ensures that if the primary is affected due to any calamity, it shouldn\u2019t affect the secondary region.</p> <p>Caution to read data in GRS </p> <p>This method replicates the data to the <code>secondary region</code>; however, the data in the secondary is not readable. The data can be read only if a failover is initiated to the secondary region. The failover can be initiated by the customer manually or by Microsoft in the case of a regional outage.</p> </li> <li> <p>Read-Access Geo Redundant Storage (RA-GRS) : In RA-GRS, the replication works in the same way as GRS. The key difference here is that you have the provision to read from the secondary region regardless of whether a failover is initiated. The secondary region is always available for read requests.</p> <p>More expensive?</p> <p>It is more expensive compared to GRS.</p> </li> <li> <p>GZRS (Geo Zone Redundant Storage): In GZRS you replicate the data synchronously in the primary region using ZRS, and then this data is replicated to the secondary region (as 3 copies in the same AZ).</p> </li> </ul> <p> </p>"},{"location":"azure/concepts/#sku","title":"SKU","text":"<p>SKU is short for <code>Stock-keeping-Unit</code>.  Each SKU is catered toward a specific scenario and has differences in scale, features, and pricing. In layman's language, SKU refers to an Item ready for sale. </p> <p>Azure supports both <code>Basic and Standard SKU</code>. For creating an AKS cluster, Azure uses Standard SKU. By utilizing the Standard SKU Load Balancer, Azure offers services like availability zones and larger backend pool sizes. </p>"},{"location":"azure/concepts/#region-pairs","title":"Region Pairs","text":"<p>Each Azure region is paired with another region within the same geography (such as US, Europe, or Asia). This approach allows for the replication of resources, such as VM storage, across geography that should reduce the likelihood of natural disasters, etc affecting both regions at once. </p>"},{"location":"azure/concepts/#authorizing-az-services","title":"Authorizing AZ Services","text":""},{"location":"azure/concepts/#spn-auth","title":"SPN Auth","text":"<ul> <li>The name by which a client uniquely identifies an instance of a service. If you install multiple instances of a service on computers throughout a forest, each instance must have its own SPN. A given service instance can have multiple SPNs if there are multiple names that clients might use for authentication</li> <li>This allows a client application to request that the service authenticate an account even if the client does not have the account name.</li> <li>If you install multiple instances of a service on computers throughout a forest, each instance must have its own SPN. A given service instance can have multiple SPNs if there are multiple names that clients might use for authentication.</li> </ul> <p>Did you registered your SPN?</p> <p>Before the <code>Kerberos authentication service</code> can use an SPN to authenticate a service, the SPN must be registered on the account object that the service instance uses to log on. A given SPN can be registered on only one account</p>"},{"location":"azure/concepts/#sas-uri","title":"SAS URI","text":"<p>SAS is a secure way to grant limited access to the resources in your storage account to the external world (clients, apps), without compromising your account keys. Shared Access Signature (SAS) is used for controlling access to blob, file, table, and queue storage containers. </p> <p>Never share the access key</p> <p>You would not want to share an access key since it is like a root password to all the containers existing within the Azure Storage account. A shared access signature (SAS) provides secure delegated access to resources in your storage account.</p> <p>It gives you granular control over the type of access you grant to clients, which includes -</p> <ul> <li>Interval \u2013 You can specify how long the SAS token should be valid by mentioning the start time and the expiry time.</li> <li>Permission \u2013 You can specify the permission at the granular level, for example, your clients just want to read the blob so grant them only read permission.</li> <li>IP Address \u2013 If you want <code>Azure Storage Account</code> to be accessed from a particular IP or range of IPs then you can specify an optional IP Address or range of IP addresses in your <code>SAS token</code>.</li> <li>Protocol \u2013 If you want the <code>Azure Storage account</code> to be accessed by either <code>HTTPS</code> or <code>HTTP &amp; HTTPS</code>, you can specify the same in the SAS token.</li> </ul>"},{"location":"azure/concepts/#types-of-shared-access-signatures","title":"Types of shared access signatures","text":"<p>Azure Storage supports three types of <code>shared access signatures</code> as shown below:</p>"},{"location":"azure/concepts/#user-delegation-sas-recommended","title":"User delegation SAS (Recommended)","text":"<p>A user delegation SAS is secured with Azure Active Directory (Azure AD) credentials and also by the permissions specified for the SAS. A user delegation SAS applies to Blob storage only.</p> <p>Recommended approach</p> <p>A user delegation SAS provides superior security to a service SAS or an account SAS. A user delegation SAS is secured with Azure AD credentials so that you do not need to store your account key with your code.</p>"},{"location":"azure/concepts/#service-sas","title":"Service SAS","text":"<p>A service SAS is secured with the <code>storage account key</code>. A service SAS delegates access to a resource in only one of the Azure Storage services:</p> <ul> <li><code>Blob</code> </li> <li><code>Queue</code></li> <li><code>Table</code></li> <li><code>Files</code></li> </ul>"},{"location":"azure/concepts/#account-sas","title":"Account SAS","text":"<p>An account SAS is secured with the <code>storage account key</code>. An account SAS delegates access to resources in one or more of the storage services. All the operations available via a service or user delegation SAS are also available via an account SAS.</p> <p>You can also delegate access to the following:</p> <ul> <li>Service-level operations (For example, the Get/Set Service Properties and Get Service Stats operations).</li> <li>Read, write, and delete operations that aren't permitted with a service SAS.</li> </ul> <p>Do you have access to the Account Key?</p> <p>Both a service SAS and an account SAS are signed with the storage account key. To create a SAS that is signed with the account key, an application must have access to the account key.</p>"},{"location":"azure/concepts/#sas-token","title":"SAS token","text":"<p>What is SAS Token?</p> <p>The SAS token is a string that you generate on the client side, for example by using one of the Azure Storage client libraries. The SAS token is not tracked by Azure Storage in any way. You can create an unlimited number of SAS tokens on the client side. After you create a SAS, you can distribute it to client applications that require access to resources in your storage account.</p> <p>Client applications provide the SAS URI to Azure Storage as part of a request. Then, the service checks the SAS parameters and the signature to verify that it is valid. If the service verifies that the signature is valid, then the request is authorized. Otherwise, the request is declined with error code 403 (Forbidden).</p> <p> </p>"},{"location":"azure/concepts/#acu","title":"ACU","text":"<p>The <code>Azure Compute Unit</code> (ACU) is used to help understand the relative compute performance between different Azure series and size VMs. It is based on the <code>A0</code> (extra small) having a value of 50. A VM with an ACU of 100 has twice the compute of a VM with an ACU of 50. </p>"},{"location":"azure/concepts/#dr","title":"DR","text":"<p>Various methods of DR are</p> <ul> <li>Backup: Its copy of business critical data</li> <li>Cold site: Little or no infra deployed</li> <li>Hot site: Exact copy of infra </li> </ul> <p>Difference between DR, HA and Backup</p> <p> </p>"},{"location":"azure/concepts/#rpo","title":"RPO","text":"<p>A <code>recovery point objective (RPO)</code> is the maximum length of time permitted that data can be restored from, which may or may not mean data loss. It is the age of the files or data in backup storage required to resume normal operations if a computer system or network failure occurs.</p> <p>Tldr</p> <p>RPO is the time from the last data backup until an incident occurred [that may have caused data loss] and RTO is the time that you set to recover the lost data.</p>"},{"location":"azure/concepts/#rto","title":"RTO","text":"<p>The <code>recovery time objective (RTO)</code> is the targeted duration of time between the event of failure and the point where operations resume.</p>"},{"location":"azure/concepts/#sla","title":"SLA","text":"<p>It is amount of time a service will be available. We need to use composite SLA as we have multiples services with different SLA's.</p>"},{"location":"azure/concepts/#mttr","title":"MTTR","text":"<p>It is the mean time a service will take to recover.</p>"},{"location":"azure/concepts/#always-on-availability-groups","title":"Always On availability groups","text":"<p>The <code>Always On availability groups</code> feature is a high-availability and disaster-recovery solution that provides an enterprise-level alternative to database mirroring. Introduced in <code>SQL Server 2012</code> (11.x), Always On availability groups maximizes the availability of a set of user databases for an enterprise.</p> <p>An availability group supports a failover environment for a discrete set of user databases, known as availability databases, that fail over together.</p> <p>Number of availability replicas</p> <p>Each set of availability database is hosted by an availability replica. Two types of availability replicas exist: <code>a single primary replica</code>, which hosts the primary databases, and <code>1 to 8 secondary replicas</code>, each of which hosts a set of secondary databases and serves as a potential failover targets for the availability group.</p>"},{"location":"azure/concepts/#azure-backup","title":"Azure backup","text":"<p>It is a managed service for backing up and recovering workloads.</p> <p><code>Azure File Backup</code> can be used to automate the backup using a backup policy. <code>Azure Snapshots</code> can be used to take manual snapshots of your file share.</p>"},{"location":"azure/concepts/#azure-recovery-service-vault","title":"Azure recovery service vault","text":"<p>A traditional backup solution requires infrastructure to be deployed to host backup services; however, with the help of a <code>lightweight agent</code>, you will be able to back up your servers securely to the cloud. Regardless of whether the server is in Azure, <code>on-premises</code>, or in any other cloud provider, you can back up and save the data in <code>Azure Recovery Services Vault</code>.</p>"},{"location":"azure/concepts/#backup-types","title":"Backup types","text":"<p>Various types of backups are:</p> <ol> <li>Full</li> <li>Differential</li> <li>Incremental</li> </ol> <p> </p> Image credits: Microsoft Learn"},{"location":"azure/concepts/#full-backup","title":"Full Backup","text":"<p>A full backup contains the entire data source. Takes more network bandwidth than differential or incremental backups.</p> <p>It is used for initial backup</p>"},{"location":"azure/concepts/#differential-backup","title":"Differential Backup","text":"<p>A differential backup stores the blocks that changed since the initial full backup. Uses a smaller amount of network and storage, and doesn't keep redundant copies of unchanged data.</p> <p>It is not used by Azure Backup as its inefficient.</p>"},{"location":"azure/concepts/#incremental-backup","title":"Incremental Backup","text":"<p>An incremental backup stores only the blocks of data that changed since the previous backup. High storage and network efficiency.</p>"},{"location":"azure/concepts/#backup-process","title":"Backup Process","text":"<p>Backup for on-prem and Cloud is done as shown below:</p> <p> </p>"},{"location":"azure/concepts/#on-premises-machines","title":"on-premises machines:","text":"<p>You can back up <code>on-premises Windows machines</code> directly to Azure by using the<code>Microsoft Azure Recovery Services (MARS)</code> agent. Linux machines aren't supported.</p> <p>You can back up on-premises machines to a backup server - either System Center <code>Data Protection Manager (DPM)</code> or <code>Microsoft Azure Backup Server (MABS)</code>. You can then back up the backup server to a <code>Recovery Services vault</code> in Azure.</p>"},{"location":"azure/concepts/#azure-vms","title":"Azure VMs:","text":"<p>You can back up Azure VMs directly. Azure Backup installs a <code>backup extension</code> to the Azure VM agent that's running on the VM. This extension backs up the entire VM.</p> <p>How to backup specific files in VM</p> <p>You can back up specific files and folders on the Azure VM by running the MARS agent. Default replication used in <code>Microsoft Azure Recovery Services</code> (MARS) is ZRS</p>"},{"location":"azure/concepts/#mars-agent","title":"MARS Agent","text":"<ul> <li>It can run on individual <code>on-premises Windows Server</code> machines to back up files, folders, and the system state. </li> <li>Runs on <code>Azure VMs</code> to back up files, folders, and the system state. </li> <li>Runs on <code>DPM/MABS servers</code> to back up the DPM/MABS local storage disk to Azure.</li> </ul>"},{"location":"azure/concepts/#azure-site-recovery","title":"Azure Site recovery","text":"<p><code>Azure Site Recovery (ASR)</code> It is a disaster recovery solution that can be used to replicate your VMs to a secondary region and fire them up during a regional outage. You can fail over from the Azure portal, and the infrastructure will be created in a secondary region within a few minutes.</p> <p> </p>"},{"location":"azure/concepts/#snapshots","title":"Snapshots","text":"<p>When it comes to development and testing, <code>managed disk snapshots</code> offer a reliable and simple option to back up your VMs. Snapshots are independent read-only copies of the managed disk and can be used to create new managed disks.</p>"},{"location":"azure/concepts/#image","title":"Image","text":"<p>Images will contain the OS disk and all the data disks that were part of the VM. Using this custom image, you can create hundreds of VMs</p> <p>Snapshot and Image difference?</p> <p>Snapshot applies to a <code>single disk</code>; if you have a VM with multiple disks, then you need to create separate snapshots for each of these disks. In the case of images, all disks are taken into consideration while creating the image.</p>"},{"location":"azure/cosmos/","title":"Cosmos DB","text":"<ul> <li>Azure Cosmos DB's API for <code>PostgreSQL</code> provides full support for SQL queries, geo-replication, and allows you to store and access data relationally. It offers automatic and instant scalability, global distribution, and effortless replication of data across Azure regions, fulfilling all of your mentioned requirements.</li> <li>Apache Cassandra is a NoSQL database that does not natively support SQL queries. While it does offer some SQL-like capabilities, it is not a fully relational database.</li> </ul>"},{"location":"azure/devops/","title":"Azure Devops","text":"<p>Azure Pipelines supports continuous integration (CI) and continuous delivery (CD) to continuously test, build, and deploy your code. You accomplish this by defining a pipeline.</p> <p>What is CI/CD?</p> <p><code>Continuous integration (CI)</code> automates tests and builds for your project. CI helps to catch bugs or issues early in the development cycle, when they're easier and faster to fix. Items known as <code>artifacts</code> are produced from CI systems. They're used by the continuous delivery release pipelines to drive automatic deployments.</p> <p><code>Continuous delivery</code> automatically deploys and tests code in multiple stages to help drive quality. </p>"},{"location":"azure/devops/#pipelines-in-az","title":"Pipelines in Az","text":"<p>The Azure cloud provides several types of pipeline, each with a different purpose. The following table lists the different pipelines and what they're used for:</p> <ul> <li>AML Pipelines: Used for Model orchestration (Machine learning) by DS</li> <li>ADF Pipelines: Used for Data orchestration (Data prep) by DE</li> <li>Azure Pipelines: Used for Code &amp; app orchestration by develper</li> </ul>"},{"location":"azure/devops/#concepts","title":"Concepts","text":""},{"location":"azure/devops/#trigger","title":"Trigger","text":"<p>A <code>trigger</code> tells a Pipeline to run. Examples: build triggers and release triggers </p>"},{"location":"azure/devops/#pipeline","title":"Pipeline","text":"<p>A <code>pipeline</code> is made up of one or more stages. A pipeline can deploy to one or more environments.</p>"},{"location":"azure/devops/#stage","title":"Stage","text":"<p>A <code>stage</code> is a way of organizing jobs in a pipeline and each stage can have one or more jobs.</p> <p>Tldr</p> <p>A stage is a logical boundary in the pipeline. It can be used to mark separation of concerns (for example, Build, QA, and production). Each stage contains one or more jobs. When you define multiple stages in a pipeline, by default, they run one after the other. You can specify the conditions for when a stage runs.</p>"},{"location":"azure/devops/#job","title":"Job","text":"<p>Each <code>job</code> runs on one agent. A job can also be agentless.</p> <p>Tip</p> <p>Each job runs on an agent. A job represents an execution boundary of a set of steps. All of the steps run together on the same agent. Jobs are most useful when you want to run a series of steps in different environments</p>"},{"location":"azure/devops/#agent","title":"Agent","text":"<p>Each <code>agent</code> runs a job that contains one or more steps. An agent is computing infrastructure with installed agent software that runs one job at a time. For example, your job could run on a Microsoft-hosted Ubuntu agent.</p> <p>Remember</p> <p>The agent communicates with Azure Pipelines or Azure DevOps Server to determine which job it needs to run, and to report the logs and job status. This communication is always initiated by the agent. </p> <p>There are 2 types of Agents as explained below:</p>"},{"location":"azure/devops/#microsoft-hosted-agents","title":"Microsoft Hosted agents","text":"<ul> <li>If your pipelines are in Azure Pipelines, then you've got a convenient option to run your jobs using a Microsoft-hosted agent.</li> </ul> <p>Managed solution</p> <p>With Microsoft-hosted agents, maintenance and upgrades are taken care of for you. Each time you run a pipeline, you get a fresh virtual machine for each job in the pipeline. The virtual machine is discarded after one job (which means any change that a job makes to the virtual machine file system, such as checking out code, will be unavailable to the next job). Microsoft-hosted agents can run jobs directly on the VM or in a container.</p>"},{"location":"azure/devops/#self-hosted-agent","title":"Self hosted agent","text":"<ul> <li>An agent that you set up and manage on your own to run jobs is a self-hosted agent.  </li> <li>Self-hosted agents give you more control to install dependent software needed for your builds and deployments</li> <li>Also, machine-level caches and configuration persist from run to run, which can boost speed.</li> </ul> <p>When to use self hosted agents?</p> <p>Below are the scenarios in which you should look at configuring Self-Hosted Agents</p> <ol> <li>If 10GB free space in the Virtual Machine (Agent) is not sufficient for your build needs.</li> <li>When you want a <code>Virtual Machine</code>, whose capacity is greater than of <code>Standard DS2V2</code></li> <li>When you would like to use a Software that is not available in the <code>Microsoft hosted Build Agents</code></li> </ol>"},{"location":"azure/devops/#agent-modes","title":"Agent modes","text":"<p>You can run your self-hosted agent as either </p> <ol> <li>Service </li> <li>Interactive process.</li> </ol> <p>After you've configured the agent, we recommend you first try it in <code>interactive mode</code> to make sure it works. </p>"},{"location":"azure/devops/#step","title":"Step","text":"<p>A <code>step</code> can be a task or script and is the <code>smallest building block of a pipeline</code>.</p>"},{"location":"azure/devops/#task","title":"Task","text":"<p>A <code>task</code> is a pre-packaged script that performs an action, such as invoking a REST API or publishing a build - artifact.</p>"},{"location":"azure/devops/#artifact","title":"Artifact","text":"<p>An <code>artifact</code> is a collection of files or packages published by a run.</p>"},{"location":"azure/devops/#run","title":"Run","text":"<p>A run represents one execution of a pipeline. It collects the logs associated with running the steps and the results of running tests. During a run, Azure Pipelines will first process the pipeline and then send the run to one or more agents. Each agent will run jobs.</p>"},{"location":"azure/devops/#agent-pools","title":"Agent pools","text":"<p>An agent pool is a collection of agents. Instead of managing each agent individually, you organize agents into agent pools. When you configure an agent, it is registered with a single pool, and when you create a pipeline, you specify the pool in which the pipeline runs. When you run the pipeline, it runs on an agent from that pool that meets the demands of the pipeline.</p>"},{"location":"azure/devops/#pat-personal-access-token","title":"PAT (Personal Access Token)","text":"<p>Generate and use a <code>PAT</code> to connect an agent with Azure Pipelines. PAT is the only scheme that works with Azure Pipelines. The PAT must have Agent Pools (read, manage) scope (for a deployment group agent, the PAT must have Deployment group (read, manage) scope), and while a single PAT can be used for registering multiple agents, the PAT is used only at the time of registering the agent, and not for subsequent communication. </p>"},{"location":"azure/dns/","title":"DNS","text":"<p>In Azure, Azure DNS is used to <code>host DNS zones</code> for providing <code>name resolution</code>. By using Azure DNS, we will be able to manage zone and records in the same way we used to do in on-premises; however, the only difference is that everything is managed from the Azure portal</p> <p>How it ensures relability?</p> <p>DNS domains in Azure DNS are hosted on Azure's global network of DNS name servers, providing resiliency and HA. Azure DNS uses <code>anycast networking</code>, so each DNS query is answered by the closest available DNS server.</p> <p>Another most asked question is:</p> <p>Can I use my own domain name?</p> <p>Azure DNS also supports <code>private DNS domains</code> so that you can use your own custom domain names rather than being stuck with the <code>Azure-provided names</code>.</p>"},{"location":"azure/dns/#dns-query-resolve","title":"DNS query resolve","text":"<p>This is done in 4 steps as shown below</p> <ol> <li><code>DNS recursor/resolver</code> - The DNS recursor is a server designed to receive queries from client machines through applications such as web browsers. Typically the recursor is then responsible for making additional requests in order to satisfy the client\u2019s DNS query.</li> </ol> <p>After receiving a <code>DNS query</code> from a web client, a <code>recursive resolver</code> will either respond with cached data, OR send a request to a root nameserver, followed by another request to a TLD nameserver, and then one last request to an authoritative nameserver.</p> <ol> <li> <p><code>Root nameserver</code> - A root server accepts a recursive resolver's query which includes a domain name, and the root nameserver responds by directing the recursive resolver to a TLD nameserver, based on the extension of that domain (<code>.com, .net, .org, etc</code>.).</p> </li> <li> <p><code>TLD nameserver</code> - The top level domain server hosts the last portion of a hostname (In example.com, the TLD server is \u201ccom\u201d).</p> </li> <li> <p><code>Authoritative nameserver</code> -  The authoritative nameserver is the last stop in the nameserver query. If the authoritative name server has access to the requested record, it will return the <code>IP address</code> for the requested hostname back to the <code>DNS Recursor/Resolver</code> that made the initial request.</p> </li> </ol> <p> </p> <p>Various steps taken to resovle the DNS quey are shown below</p> <p> </p>"},{"location":"azure/dns/#basics","title":"Basics","text":""},{"location":"azure/dns/#dns-zones","title":"DNS zones","text":"<p>A DNS zone is used to host the <code>DNS records</code> for a particular domain. To start hosting your domain in Azure DNS, you need to create a DNS zone for that domain name. Each DNS record for your domain is then created inside this DNS zone.</p> <p>How zone is managed?</p> <p>These zones differentiate between distinctly managed areas in the DNS namespace. A DNS zone is a portion of the DNS namespace that is managed by a <code>specific organization or administrator</code>.</p> <p>Zone files must always start with a <code>Start of Authority (SOA) record</code>, which contains important information including contact information for the zone administrator.</p> <p>These are of 2 types: public and private</p>"},{"location":"azure/dns/#public-zone","title":"Public Zone","text":"<p>DNS resolver can be accessed from public internet.</p>"},{"location":"azure/dns/#private-zone","title":"Private Zone","text":"<p>The records contained in a private DNS zone aren't resolvable from the Internet. DNS resolution against a private DNS zone works only from virtual networks that are linked to it using <code>private network link</code></p> <p>Auto registration in private DNS</p> <p>When you link a virtual network with a <code>private DNS zone</code> with <code>auto registration setting</code> enabled, a DNS record gets created for each virtual machine deployed in the virtual network.</p>"},{"location":"azure/dns/#ttl","title":"TTL","text":"<p>The time to live, or TTL, specifies how long each record is cached by clients before being requeried. In the below example, the TTL is <code>3600 seconds or 1 hour</code>.</p> record set example<pre><code>www.amarjitdhillon.com    3600    IN    A    134.170.185.46\n</code></pre>"},{"location":"azure/dns/#record-set","title":"Record set","text":"<p><code>Azure DNS</code> manages all DNS records using record sets. A <code>record set</code> (also known as a resource record set) is the collection of DNS records in a zone that have the same name and are of the same type. Most record sets contain a single record.</p> record set example<pre><code>www.amarjitdhillon.com    3600    IN    A    134.170.185.46\nwww.amarjitdhillon.com    3600    IN    A    134.170.188.221\n</code></pre> <p> </p> <p>Exception in record set</p> <p>The <code>SOA</code> and <code>CNAME</code> record types are exceptions. The DNS standards don't permit multiple records with the same name for these types, therefore these record sets can only contain a single record.</p>"},{"location":"azure/dns/#dns-records","title":"DNS Records","text":"<p>Azure DNS supports all common DNS record types including A, AAAA, MX, CAA, CNAME, PTR, SOA, SRV, and TXT records</p>"},{"location":"azure/dns/#cname","title":"CNAME","text":""},{"location":"azure/dns/#what-is-cname","title":"What is CNAME?","text":"<p>When the \"www\" subdomain is set to be an alias for the root domain name, a subdomain like www.samplesite.webname.com will have a CNAME record that points to the root domain webname.com.</p>"},{"location":"azure/dns/#cname-example","title":"CNAME Example","text":"<p>Suppose <code>blog.adhillon.com</code> has a CNAME record with a value of <code>adhillon.com</code> (without the \u2018blog\u2019). This means when a DNS server hits the DNS records for <code>blog.adhillon.com</code>, it actually triggers another DNS lookup to <code>adhillon.com</code>, returning adhillon.com\u2019s IP address via its A record. In this case we would say that adhillon.com is the canonical name (or true name) of <code>blog.adhillon.com</code></p> <p>IP update scenario</p> <p>Oftentimes, when sites have subdomains such as <code>blog.adhillon.com</code> or <code>test.adhillon.com</code>, those subdomains will have CNAME records that point to a root domain (<code>adhillon.com</code>). This way if the IP address of the host changes, only the DNS <code>A record</code> for the root domain needs to be updated and all the <code>CNAME records</code> will follow along with whatever changes are made to the root.</p> <p>Here are some of the cases in which CNAME has to be used:</p> <p>Uses of CNAME Records</p> <ul> <li>To send visitors from several websites owned by the same person or group to the <code>main website</code></li> <li>To give each network service, such as File Transfer Protocol (FTP) or email, its own hostname and point it to the root domain</li> <li>To give each <code>customer a subdomain on the domain</code> of a single service provider and use CNAME to point the subdomain to the customer's root domain</li> <li>To register the <code>same domain in more than one country</code> and point the versions for each country to the main domain.</li> </ul>"},{"location":"azure/dns/#caa-records","title":"CAA records","text":"<p>CAA records allow domain owners to specify which Certificate Authorities (CAs) are authorized to issue certificates for their domain. This record allows CAs to avoid mis-issuing certificates in some circumstances</p> CAA record for AWS<pre><code>$ dig CAA aws.amazon.com\n\n; &lt;&lt;&gt;&gt; DiG 9.10.6 &lt;&lt;&gt;&gt; CAA aws.amazon.com\n;; global options: +cmd\n;; Got answer:\n;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 33510\n;; flags: qr rd ra; QUERY: 1, ANSWER: 2, AUTHORITY: 1, ADDITIONAL: 1\n\n;; OPT PSEUDOSECTION:\n; EDNS: version: 0, flags:; udp: 512\n;; QUESTION SECTION:\n;aws.amazon.com.            IN  CAA\n\n;; ANSWER SECTION:\naws.amazon.com.     31  IN  CNAME   tp.8e49140c2-frontier.amazon.com.\ntp.8e49140c2-frontier.amazon.com. 60 IN CNAME   dr49lng3n1n2s.cloudfront.net.\n\n;; AUTHORITY SECTION:\ndr49lng3n1n2s.cloudfront.net. 60 IN SOA ns-905.awsdns-49.net. awsdns-hostmaster.amazon.com. 1 7200 900 1209600 86400\n\n;; Query time: 46 msec\n;; SERVER: 8.8.8.8#53(8.8.8.8)\n;; WHEN: Thu Dec 29 00:39:57 EST 2022\n;; MSG SIZE  rcvd: 192\n</code></pre>"},{"location":"azure/dns/#mx-record","title":"MX record","text":"<p>A DNS <code>mail exchange</code> (MX) record directs email to a mail server. The <code>MX record</code> indicates how email messages should be routed in accordance with the Simple Mail Transfer Protocol (SMTP, the standard protocol for all email). </p> check MX record for Google<pre><code>$ dig MX google.com\n\n; &lt;&lt;&gt;&gt; DiG 9.10.6 &lt;&lt;&gt;&gt; MX google.com\n;; global options: +cmd\n;; Got answer:\n;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 1277\n;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1\n\n;; OPT PSEUDOSECTION:\n; EDNS: version: 0, flags:; udp: 512\n;; QUESTION SECTION:\n;google.com.            IN  MX\n\n;; ANSWER SECTION:\ngoogle.com.     222 IN  MX  10 smtp.google.com.\n\n;; Query time: 40 msec\n;; SERVER: 8.8.8.8#53(8.8.8.8)\n;; WHEN: Thu Dec 29 00:37:31 EST 2022\n;; MSG SIZE  rcvd: 60\n</code></pre>"},{"location":"azure/dns/#ns-record","title":"NS record","text":"<p>NS stands for \u2018nameserver,\u2019 and the nameserver record indicates which DNS server is authoritative for that domain (i.e. which server contains the actual DNS records). Basically, NS records tell the Internet where to go to find out a domain's IP address. A domain often has multiple NS records which can indicate primary and secondary nameservers for that domain.</p> <p>Do you need NS record for your website?</p> <p>Without properly configured NS records, users will be unable to load a website or application.</p>"},{"location":"azure/dns/#a-record","title":"A record","text":"<p>The \"A\" stands for \"address\" and this is the most fundamental type of DNS record: it indicates the IP address of a given domain. It allows you to use memonic names, such as <code>www.amarjitdhillon.com</code>, in place of IP addresses like <code>162.0.232.222</code></p> <p>A records only hold <code>IPv4 addresses</code>. If a website has an IPv6 address, it will instead use an <code>AAAA record</code>.</p> <p>Use dig command to find A record like shown below</p> using dig to find dns records<pre><code>$ dig amarjitdhillon.com\n\n; &lt;&lt;&gt;&gt; DiG 9.10.6 &lt;&lt;&gt;&gt; amarjitdhillon.com\n;; global options: +cmd\n;; Got answer:\n;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 25476\n;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1\n\n;; OPT PSEUDOSECTION:\n; EDNS: version: 0, flags:; udp: 512\n;; QUESTION SECTION:\n;amarjitdhillon.com.        IN  A\n\n;; ANSWER SECTION:\namarjitdhillon.com. 1200    IN  A   162.0.232.222\n\n;; Query time: 48 msec\n;; SERVER: 8.8.8.8#53(8.8.8.8)\n;; WHEN: Thu Dec 29 00:33:13 EST 2022\n;; MSG SIZE  rcvd: 63\n</code></pre> <p>We can also get the IP using the nslookup as shown below</p> <pre><code>$ nslookup amarjitdhillon.com\nServer:     8.8.8.8\nAddress:    8.8.8.8#53\n\nNon-authoritative answer:\nName:   amarjitdhillon.com\nAddress: 162.0.232.222\n</code></pre>"},{"location":"azure/dns/#aaaa-record","title":"AAAA record","text":"<p>DNS <code>AAAA records</code> match a domain name to an IPv6 address. DNS <code>AAAA records</code> are exactly like DNS <code>A records</code>, except that they store a domain's IPv6 address instead of its IPv4 address.</p>"},{"location":"azure/dns/#txt-record","title":"TXT record","text":"<p>This record is used to associate text with a domain.</p>"},{"location":"azure/dns/#soa-start-of-authority","title":"SOA (Start of authority)","text":"<p>The DNS \u2018start of authority\u2019 (SOA) record stores important information about a domain or zone such as the email address of the administrator, when the domain was last updated, and how long the server should wait between refreshes.</p> <pre><code>$ nslookup -type=SOA contoso.com\nServer:     8.8.8.8\nAddress:    8.8.8.8#53\n\nNon-authoritative answer:\ncontoso.com\n    origin = ns1-205.azure-dns.com\n    mail addr = azuredns-hostmaster.microsoft.com\n    serial = 1\n    refresh = 3600\n    retry = 300\n    expire = 2419200\n    minimum = 300\n</code></pre>"},{"location":"azure/dns/#cert-certificate-record","title":"CERT (Certificate record)","text":"<p>It stores public key certificates.</p>"},{"location":"azure/dns/#srv-service-location-record","title":"SRV (Service Location record)","text":"<p>It is used to specify a port for specific services.</p>"},{"location":"azure/event_grid/","title":"Event Grid","text":"<p>Why event grid is required?</p> <p>The world we live in is <code>reactive</code>. Events take place, and we respond to those events. The reactive nature of Azure services is no exception to this paradigm. </p> <p>Example: A newly updated blob to Storage service needs to be processed to import data into CosmosDB. A provisioned resource needs to be tagged. Ability to notify subscribers scaled-out between multiple regions with non-ubiquitous receiving mechanisms (webhooks, queues, streams, etc). Reacting to the events taking place on a cloud-scale is not what the messaging services covered earlier were built for. That\u2019s what Event Grid was created for. </p> <ul> <li>Event Grid is to build reactive architecture. </li> </ul>"},{"location":"azure/event_grid/#integrations","title":"Integrations","text":"<ul> <li><code>Custom WebHooks</code>: Use webhooks for customizable endpoints that respond to events. </li> <li><code>Azure Functions</code>: Use Azure Functions for serverless response to events. </li> <li><code>Logic Apps</code>: Use Logic Apps to automate business processes for responding to events. </li> <li><code>Storage Queues</code>: Use Queue storage to receive events that need to be pulled. By sending events to Queue storage, the app can pull and process events on its own schedule. </li> <li><code>Event Hubs</code>: Use Event Hubs when your solution gets events faster than it can process the events. </li> <li><code>Azure Automation</code>: Use Azure Automation to process events with automated runbooks. </li> <li><code>Hybrid Connections</code>: Use Azure Relay Hybrid Connections to send events to applications that are within an enterprise network and don\u2019t have a publicly accessible endpoint</li> </ul>"},{"location":"azure/event_hub/","title":"Event Hub (Pull based)","text":""},{"location":"azure/event_hub/#event-grid-vs-event-hub","title":"Event Grid Vs Event Hub","text":"<p>A key difference between <code>Event Grid</code> and <code>Event Hubs</code> is in the way event data is made available to the subscribers. Event Grid <code>pushes</code> the ingested data to the subscribers whereas Event Hub makes the data available in a <code>pull model</code>. </p> <p>What is Azure Event Hubs?</p> <ul> <li>Platform-as-a-Service Event Stream Broker Use the Apache Kafka\u00ae API, but with far lower cost and better performance.</li> <li>Fully managed: You use the features, Azure deals with everything else</li> <li>AMQP 1.0 standards compliant, Apache Kafka\u00ae wire-compatible</li> <li>Polyglot Azure SDK and cross-platform client support</li> </ul>"},{"location":"azure/event_hub/#event-hub-vs-kafka","title":"Event Hub Vs Kafka \u2696\ufe0f","text":""},{"location":"azure/event_hub/#what-event-hub-is-not","title":"What Event hub is not?","text":""},{"location":"azure/event_hub/#event-hub-architecture","title":"Event-hub Architecture \ud83c\udfdb\ufe0f","text":""},{"location":"azure/event_hub/#concepts","title":"Concepts \ud83d\udcd5","text":"<p>Azure Event Hubs is part of a fleet of services, which also includes <code>Azure Service Bus</code>, and <code>Azure Event Grid</code>.</p>"},{"location":"azure/event_hub/#namespaces","title":"Namespaces \ud83d\udcc7","text":""},{"location":"azure/event_hub/#event-hub-capture","title":"Event hub capture","text":"<p>Usign the AVRO schemas as shown below  </p>"},{"location":"azure/event_hub/#stream-processing","title":"Stream Processing \ud83c\udfb0","text":"<p>When it comes to stream processing, there are generally two approaches to working through the infinite stream of input data (or tuples): </p> <ul> <li><code>Tuple at a time</code> :You can process one tuple at a time with downstream processing applications, </li> <li><code>Micro batching</code> : You can create small batches (consisting of a few hundred or a few thousand tuples) and process these micro-batches with your downstream applications.</li> </ul> <p> </p>"},{"location":"azure/event_hub/#kafka-and-event-hub-mapping","title":"Kafka and Event hub mapping","text":"Kafka Event Hub Cluster Namespace Topic Event hub Partition Partition Consumer Group Consumer Group Offset Offset"},{"location":"azure/event_hub/#pubsub","title":"Pubsub \ud83d\ude84","text":"<p>The producer (known as the publisher in this context) has no expectations that the events will result in any action.</p> <p>Interested <code>consumer(s)</code>, can subscribe, listen for events, and take actions depending on their consumption scenario. <code>Events</code> can have multiple <code>subscribers</code> or no subscribers at all. Two different subscribers can react to an event with different actions and not be aware of one another.</p> <p>The producer and consumer are <code>loosely coupled</code> and managed independently. The consumer isn't expected to acknowledge the event back to the producer. A consumer that is no longer interested in the events, can unsubscribe. The consumer is removed from the pipeline without affecting the producer or the overall functionality of the system.</p> <p>How loose coupling is ensured?</p> <p>A <code>message broker</code> provides temporal decoupling. The producer and consumer don't have to run concurrently. A producer can send a message to the message broker regardless of the availability of the consumer. Conversely, the consumer isn't restricted by the producer's availability.</p>"},{"location":"azure/event_hub/#event-publishers","title":"Event publishers","text":"<p>Any entity that sends data to an event hub is an <code>event publisher</code> (synonymously used with <code>event producer</code>). Event publishers can publish events using <code>HTTPS</code> or <code>AMQP 1.0</code> or the <code>Kafka protocol</code>. Event publishers use <code>AAD</code> based authorization with <code>OAuth2-issued JWT tokens</code> or an <code>Event Hub-specific Shared Access Signature (SAS) token</code> to gain publishing access.</p> <p>HTTPS or AMQP</p> <p>AMQP requires the establishment of a persistent bidirectional socket in + transport level security (TLS). AMQP has higher network costs when initializing the session, however HTTPS requires extra TLS overhead for every request. AMQP has higher performance for frequent publishers and can achieve much lower latencies when used with asynchronous publishing code.</p>"},{"location":"azure/event_hub/#consumer-groups","title":"Consumer Groups \ud83e\uddd1\u200d\ud83c\udfeb","text":"<p>The logical group of consumers that receive messages from each Event Hub partition is called a <code>consumer group</code>. The intention of a consumer group is to represent a single downstream processing application, where that application consists of multiple parallel processes, each consuming and processing messages from a partition.</p> <p>All consumers must belong to a consumer group. The consumer group also acts to limit concurrent access to a given partition by multiple consumers, which is desired for most applications, because two consumers could mean data is being redundantly processed by downstream components and could have unintended consequences.</p> <p>A diagam of non-competing consumers is shown below:  </p> <p>Whos responsibility to state management?</p> <p>In competing consumers, the queue system itself keeps track of the delivery state of every message. In Event Hubs, no such state is tracked, so managing the state of progress through the queue becomes the responsibility of the individual consumer.</p>"},{"location":"azure/event_hub/#event-retention","title":"Event retention \u2b07\ufe0f","text":"<p>Published events are removed from an event hub based on a configurable, timed-based retention policy. Here are a few important points:</p> <ul> <li>The default value and shortest possible retention period is 1 day (24 hours). </li> <li>For Event Hubs Standard, the maximum retention period is 7 days.</li> <li>For Event Hubs Premium and Dedicated, the maximum retention period is 90 days.</li> <li>If you change the retention period, it applies to all events including events that are already in the event hub</li> </ul> <p>I need to hold my events</p> <p>If you need to archive events beyond the allowed retention period, you can have them automatically stored in Azure Storage or Azure Data Lake by turning on the <code>Event Hubs Capture feature</code>.</p>"},{"location":"azure/event_hub/#publisher-policy","title":"Publisher policy \ud83e\uddd1\u200d\ud83d\udcbc","text":"<p>Event Hubs enables granular control over event publishers through publisher policies. Publisher policies are run-time features designed to facilitate large numbers of independent event publishers</p>"},{"location":"azure/event_hub/#event-hubs-capture","title":"Event Hubs Capture \ud83c\udf8f","text":"<p><code>Event Hubs Capture</code> enables you to automatically capture the streaming data in Event Hubs and save it to your choice of either a Blob storage account, or an Azure Data Lake Storage account. You can enable capture from the Azure portal, and specify a minimum size and time window to perform the capture. </p> <p>Using Event Hubs Capture, you specify your own Azure Blob Storage account and container, or Azure Data Lake Storage account, one of which is used to store the captured data. Captured data is written in the <code>Apache Avro format</code>.</p> <p>Event Hubs Capture uses Avro format for the data it captures. Avro is a row-oriented format that is suitable for various data types, it's compact, fast, binary, and enables efficient and fast serialization of data. </p>"},{"location":"azure/event_hub/#partitions","title":"Partitions \ud83d\udce5","text":"<p>Event Hubs organizes sequences of events sent to an event hub into one or more partitions. As newer events arrive, they're added to the end of this sequence.</p> <p> </p> <p>You can't change the partition count for an event hub after its creation except for the event hub in a dedicated cluster and premium tier</p> <p>A partition can be thought of as a \"commit log\". Partitions hold event data that contains body of the event, a user-defined property bag describing the event, metadata such as its offset in the partition, its number in the stream sequence, and service-side timestamp at which it was accepted.</p>"},{"location":"azure/event_hub/#mapping-of-events-to-partitions","title":"Mapping of events to partitions","text":"<p>You can use a <code>partition key</code> to map incoming event data into specific partitions for the purpose of data organization. The <code>partition key</code> is a sender-supplied value passed into an event hub. It is processed through a <code>static hashing function</code>, which creates the partition assignment. If you don't specify a partition key when publishing an event, a <code>round-robin assignment</code> is used.</p> <p>The event publisher is only aware of its partition key, not the partition to which the events are published. This decoupling of key and partition insulates the sender from needing to know too much about the downstream processing.</p>"},{"location":"azure/event_hub/#throughput-units","title":"Throughput units \ud83d\udce6","text":"<p>Tldr</p> <p>The throughput capacity of Event Hubs is controlled by throughput units. Throughput units are pre-purchased units of capacity. A single throughput unit lets you:</p> <ul> <li><code>Ingress</code>: Up to 1 MB per second or 1000 events per second (whichever comes first).</li> <li><code>Egress</code>: Up to 2 MB per second or 4096 events per second.</li> </ul> <p>Beyond the capacity of the purchased throughput units, ingress is throttled and a <code>ServerBusyException</code> is returned</p>"},{"location":"azure/event_hub/#dead-letter-queue-dlq","title":"Dead-letter queue (DLQ) \ud83d\udc80","text":"<p>A <code>Service Bus queue</code> has a default subqueue, called the dead-letter queue (DLQ) to hold messages that couldn't be delivered or processed. Service Bus or the message processing logic in the consumer can add messages to the DLQ. The DLQ keeps the messages until they are retrieved from the queue.</p>"},{"location":"azure/event_hub/#qos","title":"QOS","text":"<ul> <li>At least once</li> <li>At most once</li> <li>Exactly once</li> </ul>"},{"location":"azure/front-door/","title":"Front Door","text":"<p>While both <code>Front Door</code> and <code>Application Gateway</code> are layer 7 (HTTP/HTTPS) load balancers, the primary difference is that Front Door is a non-regional service whereas Application Gateway is a regional service.</p> <p>The following are the features offered by Azure Front Door: - <code>Enhanced performance</code>: Users will be connected to the nearest point of presence (POP) using the split TCP anycast protocol.  - <code>Heath probes</code>: Using smart health probes in Azure Front Door you can monitor the latency and availability of the web application. Also, instance failover to another region can be triggered if the backend is found unhealthy.  - <code>URL based routing</code>: You can route based on the path in the URL.    - Multiple-site routing: This helps in hosting multiple sites behind the same Front Door. - <code>Session affinity</code>: You can route the users to the same backend targets using the cookie-based session affinity feature.  - <code>TLS termination</code>: It supports TLS termination so that you can reduce the load on the backend servers.</p>"},{"location":"azure/frontdoor/","title":"Front Door","text":""},{"location":"azure/grid-hub-bus/","title":"Grid/Hub vs Service Bus","text":"<p>This blog is to compare 3 Async messaging systems in Azure as there are lot of overlapping features among these 3</p>"},{"location":"azure/grid-hub-bus/#queueing-patterns","title":"Queueing patterns","text":"<p>Let's first see what is the difference in terms of</p> <ul> <li>Push/Pull model</li> <li>Queue/Topics</li> <li>Non partitioned/Partitioned topics</li> </ul> <p>The below list shows the services in various cloud providers such as AWS, GCP and Azure  </p> <p>Specific patterns on Azure are shown below</p> <p> </p> <ul> <li>Its worth noting that Event Grid is <code>push based</code> while other 2 are <code>pull based</code></li> <li><code>Event Hub</code> is for high thoughput scenario as it uses multiple partitions. Event Hubs also has the unique ability to ingest massive volume of data (1 million messages per second) in an unmatchable speed.</li> <li><code>Service Bus</code> connects any devices, application or services running in the cloud to any other applications or services and transfers data between them. The data being transferred can be in XML, JSON, or text format.</li> <li>Azure Event Hubs focuses more on event streaming whereas Azure Service Bus is more focused on high-value enterprise messaging, which means the later is focused on messages rather than events.</li> </ul>"},{"location":"azure/grid-hub-bus/#when-to-use-what","title":"When to use what?","text":"<ul> <li>If you are implementing a pattern which uses a Request/Reply message, then use <code>Azure Service Bus</code>.</li> <li><code>Azure Event Hubs</code> is more likely to be used if you\u2019re implementing patterns with Event Messages. If you want a reliable event ingestor which can deal massive scale of event messages, then preferably you should go for Event Hubs.</li> <li>Use <code>Event Grid</code> is generally used for building reactive architecture.</li> </ul>"},{"location":"azure/grid-hub-bus/#event-grids-vs-event-hubs","title":"Event Grids VS Event Hubs","text":"<ul> <li>Event Grids doesn\u2019t guarantee the order of the events, but Event Hubs use partitions which are ordered sequences, so it can maintain the order of the events in the same partition. </li> <li>Event Hubs are accepting only endpoints for the ingestion of data and they don\u2019t provide a mechanism for sending data back to publishers. On the other hand, Event Grids sends HTTP requests to notify events that happen in publishers. </li> <li>Event Grid can trigger an Azure Function. In the case of Event Hubs, the Azure Function needs to pull and process an event. </li> <li>Event Grids is a distribution system, not a queueing mechanism. If an event is pushed in, it gets pushed out immediately and if it doesn\u2019t get handled, it\u2019s gone forever. Unless we send the undelivered events to a storage account. This process is known as dead-lettering. </li> <li>In Event Hubs the data can be kept for up to seven days and then replayed. This gives us the ability to resume from a certain point or to restart from an older point in time and reprocess events when we need it.</li> </ul>"},{"location":"azure/grid-hub-bus/#event-hubs-vs-service-bus","title":"Event Hubs vs Service Bus","text":"<p>To the external publisher or the receiver Service Bus and Event Hubs can look very similar and this is what makes it difficult to understand the differences between the two and when to use what.</p> <ul> <li>Event Hubs focuses on <code>event streaming</code> where Service Bus is more of a <code>traditional messaging broker</code>. </li> <li>Service Bus is used as the backbone to connects applications running in the cloud to other applications or services and transfers data between them whereas Event Hubs is more concerned about receiving massive volume of data with high throughout and low latency. </li> <li>Event Hubs decouples multiple event-producers from event-receivers whereas Service Bus aims to decouple applications. </li> <li>Service Bus messaging supports a message property \u2018Time to Live\u2019 whereas Event Hubs has a default retention period of 7 days. </li> <li>Service Bus has the concept of message session. It allows relating messages based on their session-id property whereas Event Hubs does not. </li> <li>Service Bus the messages are <code>pulled out</code> by the receiver &amp; cannot be processed again whereas Event Hubs message can be ingested by multiple receivers. </li> <li>Service Bus uses the terminology of <code>queues</code> and <code>topics</code> whereas Event Hubs <code>partitions</code> terminology is used.</li> </ul>"},{"location":"azure/grid-hub-bus/#patterns","title":"Patterns","text":""},{"location":"azure/grid-hub-bus/#event-grid","title":"Event Grid \ud83d\uddf3\ufe0f","text":""},{"location":"azure/grid-hub-bus/#event-hub","title":"Event-hub \ud83d\udca1","text":""},{"location":"azure/grid-hub-bus/#service-bus","title":"Service bus \ud83d\ude8c","text":""},{"location":"azure/keyVault/","title":"Azure Key Vault","text":"<p>Azure Key Vault is one of several key management solutions in Azure, and helps solve the following problems:</p> <ul> <li><code>Secrets Management</code> - Azure Key Vault can be used to Securely store and tightly control access to tokens, passwords, certificates, API keys, and other secrets </li> <li><code>Key Management</code> - Azure Key Vault can be used as a Key Management solution. Azure Key Vault makes it easy to create and control the encryption keys used to encrypt your data.</li> <li><code>Certificate Management</code> - Azure Key Vault lets you easily provision, manage, and deploy public and private <code>TLS/SSL certificates</code> for use with Azure and your internal connected resources.</li> </ul> <p>To access any item stored in a Key Vault, you must first be authenticated.</p> <p>Azure Key Vault supports the following authentication capabilities, which have been previously discussed: 1. <code>Service principals</code> 2. <code>Managed identities</code></p>"},{"location":"azure/logicApp/","title":"Logic App","text":""},{"location":"azure/logicApp/#what-is-logic-app","title":"What is Logic App?","text":"<p>This service defines the <code>workflow</code> which can consuming a range of APIs exposed as connectors. These <code>Logic App connectors</code> will perform the sequence of actions defined in the workflow whenever the trigger gets fired.</p>"},{"location":"azure/logicApp/#roles","title":"Roles","text":"<ul> <li><code>Logic App Operator</code> - Lets you read, enable, and disable logic apps, but not edit or update them.</li> <li><code>Logic App Contributor</code> - Lets you create, manage logic apps, but not access to them.</li> <li></li> </ul>"},{"location":"azure/networkingInfra/","title":"Azure Networking Infrastructure","text":""},{"location":"azure/networkingInfra/#locking-infra","title":"Locking Infra","text":"<p>As an <code>administrator</code>, you can lock an Azure subscription, resource group, or resource to protect them from accidental user deletions and modifications. The lock overrides any user permissions.</p> <p>Lock Inheritance</p> <p>When you apply a lock at a parent scope, all resources within that scope inherit the same lock. Even resources you add later inherit the same parent lock. The most restrictive lock in the inheritance takes precedence.</p>"},{"location":"azure/networkingInfra/#tags","title":"Tags","text":"<p>You can apply tags to your Azure resources, resource groups, and subscriptions.</p> <p>Tags are metadata elements that you apply to your Azure resources. They're key-value pairs that help you identify resources based on settings that are relevant to your organization</p> <p>Are tags inherited?</p> <p>Resources don't inherit the tags you apply to a resource group or a subscription</p>"},{"location":"azure/networkingInfra/#region","title":"Region","text":"<p>Each Azure region features datacenters deployed within a latency-defined perimeter. They're connected through a dedicated regional low-latency network. </p>"},{"location":"azure/networkingInfra/#region-pairs","title":"Region pairs","text":"<p>Most Azure regions are paired with another region within the same geography (such as US, Europe, or Asia) at least 300 miles away. For example, if a region in a pair was affected by a natural disaster, services would automatically fail over to the other region in its region pair.</p>"},{"location":"azure/networkingInfra/#sovereign-regions","title":"Sovereign Regions","text":"<p>In addition to <code>regular regions</code>, Azure also has <code>sovereign regions</code>. Sovereign regions are instances of Azure that are isolated from the main instance of Azure. You may need to use a sovereign region for compliance or legal purposes.</p>"},{"location":"azure/networkingInfra/#az","title":"AZ","text":"<p>Azure az's are physically separate locations within each Azure region that are tolerant to local failures.</p> <ul> <li>To ensure resiliency, a minimum of three separate availability zones are present in all <code>AZ-enabled regions</code>.</li> <li>AZ's are connected by a high-performance network with a round-trip latency of less than <code>2ms</code>. </li> <li>AZ's are designed so that if one zone is affected, then services are supported by the remaining 2 zones.</li> </ul>"},{"location":"azure/networkingInfra/#management-groups","title":"Management groups","text":""},{"location":"azure/networkingInfra/#why-do-we-need-it","title":"Why do we need it?","text":"<p>If you have many subscriptions, you might need a way to efficiently manage access, policies, and compliance for those subscriptions.</p> <p>Access to MG's</p> <pre><code>No one is given default access to the root management group. _Azure AD Global Administrators_ are the only users that can elevate themselves to gain access. Once they have access to the root management group, the global administrators can assign any Azure role to other users to manage it.\n</code></pre> <p>Note</p> <ul> <li>All subscriptions and management groups fold up to the one root management group within the directory.</li> <li>The root management group can't be moved or deleted, unlike other management groups.</li> <li>All Azure customers can see the root management group, but not all customers have access to manage that root management group.</li> <li>By default, the root management group's display name is Tenant root group and operates itself as a management group. The ID is the same value as the Azure Active Directory (Azure AD) tenant ID.</li> </ul>"},{"location":"azure/networkingInfra/#solution","title":"Solution","text":"<p>Azure management groups provide a level of scope above subscriptions. You organize subscriptions into containers called management groups and apply governance conditions to the management groups. All subscriptions within a management group automatically inherit the conditions applied to the management group, the same way that resource groups inherit settings from subscriptions and resources inherit from resource groups.</p>"},{"location":"azure/networkingInfra/#azure-subscription","title":"Azure Subscription","text":"<p>In Azure, subscriptions are a unit of management, billing, and scale. Similar to how resource groups are a way to logically organize resources, subscriptions allow you to logically organize your resource groups and facilitate billing.</p> <p>An account can have multiple subscriptions, but it\u2019s only required to have one. In a multi-subscription account, you can use the subscriptions to configure different billing models and apply different access-management policies.</p>"},{"location":"azure/networkingInfra/#resource-groups","title":"Resource groups","text":"<p>Resource groups are simply groupings of resources. When you create a resource, you\u2019re required to place it into a <code>resource group</code>. While a resource group can contain many resources, a single resource can only be in one resource group at a time.</p> <p>Note</p> <p>Some resources may be moved between resource groups, but when you move a resource to a new group, it will no longer be associated with the former group. Additionally, resource groups can't be nested, meaning you can\u2019t put resource group B inside of resource group A.</p>"},{"location":"azure/networkingInfra/#benefit-of-using-rg","title":"Benefit of using RG","text":"<p>When you <code>apply an action</code> to a resource group, that action will apply to all the resources within the resource group. If you <code>delete a resource group</code>, all the resources will be deleted. If you <code>grant</code> or <code>deny access</code> to a resource group, you\u2019ve granted or denied access to all the resources within the resource group.</p>"},{"location":"azure/networkingInfra/#virtual-machine-scale-sets","title":"Virtual machine scale sets","text":"<p>Virtual machine scale sets let you create and manage a group of identical, load-balanced VMs.  Scale sets allow you to centrally manage, configure, and update a large number of VMs in minutes. The number of VM instances can automatically increase or decrease in response to demand, or you can set it to scale based on a defined schedule.</p> <p>Tip</p> <p>Virtual machine scale sets also automatically deploy a load balancer to make sure that your resources are being used efficiently</p>"},{"location":"azure/networkingInfra/#azure-container-instances","title":"Azure Container Instances","text":"<p>Azure Container Instances offer the fastest and simplest way to run a container in Azure; without having to manage any virtual machines or adopt any additional services. Azure Container Instances are a <code>platform as a service (PaaS)</code> offering. Azure Container Instances allow you to upload your containers and then the service will run the containers for you.</p>"},{"location":"azure/networkingInfra/#azure-functions","title":"Azure Functions","text":"<p>Azure Functions is an event-driven, serverless compute option that doesn\u2019t require maintaining virtual machines or containers. If you build an app using VMs or containers, those resources have to be \u201crunning\u201d in order for your app to function. With Azure Functions, an event wakes the function, alleviating the need to keep resources provisioned when there are no events.</p> <p>Azure Functions runs your code when it's triggered and automatically deallocates resources when the function is finished. In this model, you're only charged for the CPU time used while your function runs.</p> <p>Remember</p> <p>Functions can be either stateless or stateful. When they're stateless (the default), they behave as if they're restarted every time they respond to an event. When they're stateful (called Durable Functions), a context is passed through the function to track prior activity.</p>"},{"location":"azure/networkingInfra/#connecting-with-on-premise","title":"Connecting with On-Premise","text":""},{"location":"azure/networkingInfra/#expressroute","title":"ExpressRoute","text":"<p>An <code>ExpressRoute</code> instance is the only connection between an <code>Azure VNet</code> and an <code>on-premise network</code> that does not cross into the internet. The connection is private and managed directly by an <code>ExpressRoute partner</code>.</p> <p>Azure ExpressRoute lets you extend your <code>on-premises</code> networks into the Microsoft cloud over a private connection, with the help of a <code>connectivity provider</code>. This connection is called an <code>ExpressRoute Circuit</code>. With ExpressRoute, you can establish connections to Microsoft cloud services, such as Microsoft Azure and Microsoft 365. This allows you to connect offices, datacenters, or other facilities to the Microsoft cloud. Each location would have its own ExpressRoute circuit.</p> <p>How does ExpressRoute work on high level?</p> <p>ExpressRoute uses the <code>BGP</code>. BGP is used to exchange routes between on-premises networks and resources running in Azure. This protocol enables dynamic routing between your on-premises network and services running in the Microsoft cloud.</p>"},{"location":"azure/networkingInfra/#point-to-site-connection-vpn","title":"Point-to-Site Connection (VPN)","text":"<p>A point-to-site (P2S) connection, on the other hand, is an inexpensive solution for connecting any supported Azure resource to one existing in another network.</p>"},{"location":"azure/networkingInfra/#site-to-site-connection-vpn","title":"Site-to-Site Connection (VPN)","text":"<p>Unlike P2S, traffic passing through a <code>site-to-site (S2S)</code> connection is sent within an <code>encrypted tunnel</code> across the internet. </p> <p>The primary difference is that the connectivity happens between Azure VPN Gateway and an on-premise VPN device.</p>"},{"location":"azure/networkingInfra/#nsg-and-asg","title":"NSG and ASG","text":"<p>NSG's (Network Security Group) &amp; ASG's (Application Security Group) are the main Azure Resources that are used to administrate and control network traffic within a virtual network (vNET).</p>"},{"location":"azure/networkingInfra/#network-security-group-nsg","title":"Network Security Group (NSG)","text":"<p>NSG's control access by permitting or denying network traffic in a number of ways, whether it be:-</p> <ol> <li>Communication between different workloads on a vNET</li> <li>Network connectivity from on-site environment into Azure</li> <li>Direct internet connection</li> </ol> <p>Theoretically speaking, it is just a group of Access Control List rules that either allow or deny network traffic to a specific destination located on your vNET. </p> <p>Remember</p> <p>NSG's can be applied either on a virtual machine or subnet (one NSG can be applied to multiple subnets or virtual machines</p>"},{"location":"azure/networkingInfra/#application-security-group","title":"Application Security Group","text":"<p>An ASG provides the ability to group a set of Azure VMs and define NSGs specifically for that group. An application security group is a logical collection of virtual machines (NICs). You join virtual machines to the application security group, and then use the application security group as a source or destination in NSG rules.</p> <p>Remember</p> <p>A Virtual Machine can be attached to more than one Application Security Group. This helps in cases of multi-application servers.</p> <p> </p> <p>As per the rules defined in NSG, the WebServer and AppServer ASG are able to communicate with one another while the  WebServer and DbServer ASG are not.</p> <p>Tldr</p> <p><code>Application Security Groups</code> helps to manage the security of Virtual Machines by grouping them according the applications that runs on them. It is a feature that allows the application-centric use of Network Security Groups.</p> <p>ASG allow us to define fine-grained network security policies based on applications instead of explicit IP addresses.</p> <p>Using alias/monikers</p> <p>We can define ASG by providing a alias/moniker that fits our needs.</p> <p>NSGs and ASGs are not omnipresent</p> <p>When NSGs and ASGs are discussed by IT professionals working with Azure, those conversations generally occur within the context of Azure resources such as Azure VNets and Azure VMs. This is because you cannot currently configure NSGs or ASGs for Azure resources that exist outside a VNet. There are no Azure-wide capabilities to configure NSGs or ASGs for Azure App Service, Azure Storage, or Azure Event Hub</p>"},{"location":"azure/networkingInfra/#networking","title":"Networking","text":""},{"location":"azure/networkingInfra/#home-address","title":"Home address","text":"<p>What is special about <code>127.0.0.1</code> ?</p> <p>The <code>127.0.0.1</code> IPv4 address is commonly referred to as a <code>loopback address</code>. The IP address is typically mapped to the host name <code>localhost</code> and can be referred to as home in networking. This is useful when accessing a website hosted on the machine that you are currently logged into. For example, <code>http://localhost</code> accesses a website hosted on the currently logged into machine. The IPv6 address for localhost is <code>::1</code></p>"},{"location":"azure/networkingInfra/#nat","title":"NAT","text":"<p>AT simplifies outbound Internet connectivity for virtual networks. When configured on a subnet, all outbound connectivity uses the Virtual Network NAT's static public IP addresses.</p> <p>Example</p> <p>As shown in the below diagram the address for VMs in the VNET are <code>10.0.0.*</code>. As we know that this address are not accessible on the internet (they are private). A NAT translates the <code>private IP address</code> into a <code>public IP address</code>. In other words, the NAT device has both a <code>private IP</code> address as well as a <code>public IP</code> </p> <p> </p> <p>How does the router know the response from browser that has the destination address that is now 155.35.25.26 needs to be directed to <code>10.0.0.4</code>? The answer is found by digging into a socket.</p> <p>A <code>socket</code> is the combination of an <code>IP address:port number</code>. More specifically, it has a source IP address, source port, destination IP address, and destination port    </p>"},{"location":"azure/networkingInfra/#network-peering","title":"Network Peering","text":"<p>Peering allows two virtual networks to connect directly to each other. Network traffic between peered networks is private, and travels on the Microsoft backbone network, never entering the public internet. Peering enables resources in each virtual network to communicate with each other. These virtual networks can be in separate regions, which allows you to create a global interconnected network through Azure.</p>"},{"location":"azure/networkingInfra/#udr","title":"UDR","text":"<p>User-defined routes (UDR) allow you to control the routing tables between subnets within a virtual network or between virtual networks. This allows for greater control over network traffic flow.</p>"},{"location":"azure/office365/","title":"Office 365","text":"<p>Microsoft 365 is a multi-tenant, cloud-based, Software-as-a-Service (SaaS) subscription offering from Microsoft. Various components of office 365 are:</p> <ul> <li>Outlook</li> <li>Teams</li> <li>Sharepoint</li> <li>Onedrive</li> <li>Word,PowerPoint and Excel</li> </ul>"},{"location":"azure/office365/#microsoft-365-groups","title":"Microsoft 365 Groups","text":"<p><code>Microsoft 365 Groups</code> is the foundational membership service that drives all teamwork across Microsoft 365. With Microsoft 365 Groups, you can give a group of people access to a collection of shared resources. These resources include:</p> <ul> <li>A shared Outlook inbox</li> <li>A shared calendar</li> <li>A SharePoint document library</li> <li>A Planner</li> <li>A OneNote notebook</li> <li>Power BI</li> </ul> <p>Expiration Policy</p> <p>With the increase in usage of <code>Microsoft 365 groups</code> and <code>Microsoft Teams</code>, administrators and users need a way to clean up unused groups and teams. A Microsoft 365 groups <code>expiration policy</code> can help remove inactive groups from the system and make things cleaner.</p>"},{"location":"azure/redis-azure/","title":"Redis Cache Azure","text":""},{"location":"azure/redis-azure/#concepts","title":"Concepts","text":""},{"location":"azure/redis-azure/#task-queuing","title":"Task Queuing","text":"<p>Any time an application receives a request, there's a chance that the operations that are associated with that request might take additional time to execute. A common way to deal with these types of longer-running operations is to add them to a queue, which is then processed later and maybe even by a different server altogether. This type of deferment strategy is called task queuing and Azure Cache for Redis serves this purpose well by acting as a distributed queue.</p>"},{"location":"azure/redis-azure/#purge-content","title":"Purge Content","text":"<p>There will be times when content is updated on a website when we may wish to purge the cached content from our CDN. So by purging the CDN cache, we allow it to recache the new content from the source website.</p>"},{"location":"azure/redis-azure/#transactions","title":"Transactions","text":"<p>Azure Cache for Redis provides support for executing a batch of commands in a single operation. It does so in the form of transactions.</p>"},{"location":"azure/redis-azure/#cache-aside","title":"Cache-aside","text":"<p>Because databases can be quite large, they should never be loaded in their entirety into a cache. That said, a common strategy would be to use the cache-aside pattern to load data items into the cache only as needed.</p>"},{"location":"azure/security/","title":"Azure Security","text":""},{"location":"azure/security/#handle-pii-data","title":"Handle PII data","text":"<p>Dynamic Data Masking: Masks sensitive data in the result set of a query, limiting data exposure without changing the data in the database.</p> <p>Always Encrypted: Ensures sensitive data is encrypted both at rest and in motion, and can only be decrypted by the client application that has the encryption keys</p>"},{"location":"azure/serviceFabric/","title":"Service Fabric","text":"<ul> <li>Service Fabric is Microsoft's container orchestrator for deploying and managing microservices across a cluster of machines.</li> <li>Service Fabric provides a sophisticated, lightweight runtime that supports stateless and stateful microservices</li> </ul>"},{"location":"azure/service_bus/","title":"Service Bus","text":"<p>Tldr</p> <ul> <li>Azure Service Bus is a <code>fully-managed enterprise message broker</code> with message queues and publish-subscribe topics (in a namespace).</li> <li>Azure Service Bus' primary protocol is AMQP 1.0 and it can be used from any AMQP 1.0 compliant protocol client.</li> <li>JMS 2.0 compliant</li> <li>Polyglot Azure SDK and cross-platform client support</li> </ul>"},{"location":"azure/service_bus/#demos","title":"Demo's","text":""},{"location":"azure/service_bus/#adding-message-to-the-queue","title":"Adding message to the queue \ud83c\udfa5","text":""},{"location":"azure/service_bus/#architecture","title":"Architecture \ud83c\udfdb\ufe0f","text":""},{"location":"azure/service_bus/#authorization","title":"Authorization \ud83d\udd10","text":""},{"location":"azure/service_bus/#concepts","title":"Concepts","text":""},{"location":"azure/service_bus/#messages","title":"Messages \u2709\ufe0f","text":"<p>Data is transferred between different applications and services using messages. Messages can be in <code>JSON</code>, <code>XML</code>, <code>Apache Avro</code>, <code>Plain Text</code> format.</p>"},{"location":"azure/service_bus/#queues","title":"Queues \u4dc4","text":"<ul> <li>Messages are sent to and received from queues.</li> <li>Queues store messages until the receiving application is available to receive and process them.</li> <li>Once the broker accepts the message, the message is always held durably in triple-redundant storage, spread across availability zones if the namespace is zone-enabled.</li> <li>Messages are delivered in pull mode, only delivering messages when requested. </li> </ul> <p>Dead letter queue and rejected  queues are shown below  </p> <p> </p>"},{"location":"azure/service_bus/#topics","title":"Topics \ud83d\udccb","text":"<ul> <li>While a queue is often used for <code>point-to-point communication</code>, topics are useful in <code>publish/subscribe scenarios</code>.</li> </ul>"},{"location":"azure/service_bus/#subscriptions","title":"Subscriptions \u2705","text":"<ul> <li>Subscriptions are <code>named entities</code>. Subscriptions are durable by default, but can be configured to expire and then be automatically deleted.</li> </ul> <p>Define Filters and rules</p> <p>You can define <code>rules</code> on a subscription. A subscription rule has a <code>filter</code> to define a condition for the message to be copied into the subscription and an optional action that can modify message metadata.</p> <p>Subscribers can define which messages they want to receive from a topic. These messages are specified in the form of one or more named subscription rules. Each rule consists of a filter condition that selects particular messages, and optionally contains an action that annotates the selected message.</p>"},{"location":"azure/service_bus/#namespaces","title":"Namespaces \ud83e\ude90","text":"<ul> <li>A Service Bus namespace is your own capacity slice of a large cluster made up of dozens of all-active virtual machines.</li> <li>Multiple <code>queues</code> and <code>topics</code> can be in a single namespace, and namespaces often serve as application containers.</li> </ul>"},{"location":"azure/service_bus/#partitioned","title":"Partitioned \ud83d\uddc2\ufe0f","text":"<p>Service Bus partitions enable queues and topics, or messaging entities, to be partitioned across multiple message brokers and messaging stores. Partitioning means that the overall throughput of a partitioned entity is no longer limited by the performance of a single message broker or messaging store. In addition, a temporary outage of a messaging store does not render a partitioned queue or topic unavailable</p>"},{"location":"azure/service_bus/#message-sessions","title":"Message sessions \ud83d\udce8","text":"<p>To realize a <code>first-in, first-out</code> (FIFO) guarantee in processing messages in Service Bus queue or subscriptions, use sessions. Sessions can also be used in implementing request-response patterns. The request-response pattern enables the sender application to send a request and provides a way for the receiver to correctly send a response back to the sender application.</p> <p> </p>"},{"location":"azure/service_bus/#auto-forwarding","title":"Auto-forwarding \ud83d\udce4","text":"<p>The Auto-forwarding feature enables you to chain a queue or subscription to another queue or topic that is part of the same namespace. When auto-forwarding is enabled, Service Bus automatically removes messages that are placed in the first queue or subscription (source) and puts them in the second queue or topic (destination).</p>"},{"location":"azure/service_bus/#dead-lettering","title":"Dead-lettering \ud83d\udc80","text":"<p>Service Bus queues and topic subscriptions provide a secondary subqueue, called a <code>dead-letter queue (DLQ)</code>. The dead letter queue holds messages that can't be delivered to any receiver, or messages that can't be processed. You can then remove messages from the DLQ and inspect them.</p>"},{"location":"azure/service_bus/#auto-delete-on-idle","title":"Auto-delete on idle \u274c","text":"<p>Auto-delete on idle enables you to specify an idle interval after which the queue is automatically deleted. The interval is reset when there's traffic on the queue. The minimum duration is 5 minutes.</p>"},{"location":"azure/service_bus/#scheduled-delivery","title":"Scheduled delivery \ud83d\ude9a","text":"<p>You can submit messages to a queue or topic for delayed processing; for example, to schedule a job to become available for processing by a system at a certain time</p>"},{"location":"azure/sqldb/","title":"SQL DB","text":"<ul> <li>Azure SQL Managed Instance is a fully managed, Azure-based version of SQL Server that can be used to host your on-premises SQL Server database in the cloud. </li> <li>It supports the deployment of multiple secondary, read-only replicas and can automatically replicate data between primary and secondary replicas. </li> <li>It also supports failover between primary and secondary replicas within a 15-minute RTO, which meets the requirement for the solution to have a recovery time objective of 15 minutes.</li> </ul>"},{"location":"azure/sqldb/#always-on-ag","title":"Always On AG","text":"<p>The Always On availability groups feature is a high-availability and disaster-recovery solution that provides an enterprise-level alternative to database mirroring. </p> <p>Introduced in SQL Server 2012 (11.x), Always On availability groups maximizes the availability of a set of user databases for an enterprise. An availability group supports a failover environment for a discrete set of user databases, known as availability databases, that fail over together. An availability group supports a set of read-write primary databases and one to eight sets of corresponding secondary databases. </p> <p>Optionally, secondary databases can be made available for read-only access and/or some backup operations.</p>"},{"location":"azure/storage/","title":"Storage Accounts","text":"<p>Data objects in Azure Storage are accessible from anywhere in the world over HTTP or HTTPS via a REST API.The Azure Storage platform includes the following data services:</p> <ul> <li>Azure Blobs: A massively scalable <code>object store</code>. Also includes support for big data analytics through Data Lake Storage Gen2.</li> <li>Azure Files: Cloud based file sharing service.This file share can be mounted to multiple VMs or on-premises machines, which is ideal for sharing files across machines.</li> <li>Azure Queues:  Allows for <code>asynchronous message queueing</code> between application components.Messages can be stored and retrieved using queues. These stored messages can be up to 64 KB in size and can be accessed from anywhere in the world over <code>HTTP</code> or <code>HTTPS</code>.</li> <li>Azure Tables: A <code>NoSQL store</code> for schemaless storage of semi-structured data. Tables is a <code>NoSQL datastore</code> that is now part of <code>Azure Cosmos DB</code>. Besides the Table Storage, Cosmos DB offers a new Table API with additional features such as turnkey failover, global distribution, automatic secondary indexes, and throughput optimized tables.</li> <li>Azure Disk: Azure Disks provides persistent storage to Azure Vm, Azure VMSS etc.</li> </ul> <p> </p> <p>Naming in Storage accounts</p> <p>The name for the storage account is unique across Azure. Each object stored in the storage account is represented using a <code>unique URL</code>. During the creation of the storage account, you need to pass the name of the storage account to the Azure Resource Manager. Using this storage account name, endpoints are created.</p> <p>For example, if the name of your storage account is <code>amar_blog_storage</code>, then the default endpoints will be as follows:</p> <ul> <li>Blobs https://amar_blog_storage.blob.core.windows.net</li> <li>Tables https://amar_blog_storage.table.core.windows.net</li> <li>Files https://amar_blog_storage.file.core.windows.net</li> <li>Queues https://amar_blog_storage.queue.core.windows.net</li> </ul> <p>Things to consider when architecting?</p> <p>These are things to consider for Azure blob store when doing architecture</p> <pre><code>    - Account type: v1 or v2\n    - Performance tier: standard or performance\n    - Replication: LRS, ZRS, ZLRS, GRS\n    - Access Tier: Hold, cold or archived.\n</code></pre>"},{"location":"azure/storage/#blobdiscfile-diff","title":"Blob/Disc/File diff","text":"<ul> <li> <p><code>Azure File Storage</code> may be mounted as an SMB volume (so that all instances of your app can work with it). Note: This is not something readily supported with Web Apps currently - you'd only be able to write to the file share via API, not via attached disk. Azure File Storage volumes support up to 5TB each, and throughput is max. 60MB/sec across the share. It's backed by Azure blob storage (so, just as durable as blobs).</p> </li> <li> <p><code>Azure Disks</code> are again blob-backed (page blobs), up to 1TB each. Each disk is mountable to a single VM. Throughput is higher than File Storage. Cannot be shared across VMs without your own solution to sync data. Once mounted and formatted, accessible just like any other local file (e.g. no modifications to your app)</p> </li> <li> <p><code>Azure blobs</code>: they can be accessed via REST API/SDK and are not mountable as a disk/drive. Without modifying your app, you'd need to make sure blob contents were copied to local disk to perform operations on the content (you can't just open a blob as a file and modify it).</p> </li> </ul>"},{"location":"azure/storage/#storage-account-types","title":"Storage Account types","text":"<ul> <li>General-Purpose v2: This is recommended for most cases. This storage account type provides the blob, file, queue and table service. <code>General-purpose v2 accounts</code> deliver the lowest per-gigabyte capacity prices for Azure Storage, as well as industry-competitive transaction prices.</li> <li>General-purpose v1: this also provides the blob, file, queue and table service but is older version of this account type. General Purpose v1 (GPv1) accounts do not support tiering.</li> <li>BlockBlobStorage : this is specifically when you want premium performance for storing block or appending blobs </li> <li>FileStorage : This is specifically when you want premium performance for file-ONLY storage </li> <li>BlobStorage : This is legacy storage account. Use General-purpose v2 account as much as possible.</li> </ul>"},{"location":"azure/storage/#sa-access-types","title":"SA access types","text":"<ol> <li><code>Access keys</code>: These are not preferred as you give whole access.<ol> <li>They provide unlimited access </li> <li>They are auto generated</li> </ol> </li> <li><code>Identity based</code>: Using on-prem AD or Azure AD. They use RBAC instead of keys. </li> <li><code>SAS tokens</code>: We can provide more granular level details such as access level, dates and IP values.</li> </ol>"},{"location":"azure/storage/#azure-blob","title":"Azure blob","text":"<p>As Azure Blob Storage is for unstructured data, you can store any type of text or binary data. Blob storage is also referred to as object storage</p> <p>Tip</p> <p><code>$logs</code> is the system container that you should not mess with</p> <p>Blob structure is shown below</p> <p> </p> <p>Blob Storage comprises three resources.</p> <ul> <li><code>Storage account</code>: Used to provide complete access. They are not recommended and should be stored in <code>Key Vault</code>.</li> <li><code>Container</code> in the storage account</li> <li><code>Blobs/objects</code> stored in the container</li> </ul> <p>Default access</p> <p>When you create a container, you need to provide the <code>public access level</code>, which specifies whether you want to expose the data stored in the container publicly. By default, the contents are <code>private</code></p>"},{"location":"azure/storage/#types-of-blobs","title":"Types of blobs","text":""},{"location":"azure/storage/#block","title":"Block","text":"<p>For audio and video files</p>"},{"location":"azure/storage/#append-blobs","title":"Append blobs","text":"<p>For log files</p>"},{"location":"azure/storage/#page-blob","title":"Page blob","text":"<p>Used as VM disks</p>"},{"location":"azure/storage/#containerblob-visibility","title":"Container/blob visibility","text":"<p>We have the following options to control the visibility of container/blob:</p> <ul> <li>Private: This is the default option; no anonymous access is allowed to <code>containers and blobs</code>.</li> <li>Blob (public): This will grant anonymous public read access to the <code>blobs alone</code>.</li> <li>Container (public): This will grant anonymous public read and list access to the <code>container and all the blobs stored inside the container</code>.</li> </ul>"},{"location":"azure/storage/#storage-lifecycle","title":"Storage Lifecycle","text":""},{"location":"azure/storage/#blob-access-tiers","title":"Blob Access Tiers","text":"<p>The access tiers are defined based on the usage pattern or frequency of access. When you create a file, it will be accessed frequently; gradually the frequency of access will reduce, and eventually you will not be accessing the file. However, you would still like to keep the file because of the data retention policies and for auditing purposes. Choosing the right access tier will help you optimize the cost of data storage and data access.</p>"},{"location":"azure/storage/#hot-online","title":"Hot (Online)","text":"<p>Optimized for frequent access of objects. From a cost perspective, accessing data in the Hot tier is the least expensive compared to the other tiers; however, the data storage costs are higher.When you create a new storage account, this is the default tier.</p>"},{"location":"azure/storage/#cool-online","title":"Cool (Online)","text":"<p>Optimized for storing data that is not accessed very frequently and is stored for at least 30 days. Storing data in the Cool tier is cheaper than the Hot tier; however, accessing data in the Cool tier is more expensive than Hot tier.</p>"},{"location":"azure/storage/#archive-offline","title":"Archive (Offline)","text":"<p>Optimized for storing data that can tolerate hours of retrieval latency and will remain in the Archive tier for at least 180 days. When it comes to storing data, the Archive tier is the most cost-effective tier.</p> <p>How to access data from Archive tier?</p> <p>While a blob is in the archive tier, it can't be read or modified. To read or download a blob in the archive tier, you must first <code>rehydrate</code> it to an online tier, either hot or cool. Data in the archive tier can take up to 15 hours to rehydrate, depending on the priority you specify for the <code>rehydration operation</code>.</p> <p>Only storage accounts that are configured for LRS, GRS, or RA-GRS support moving blobs to the archive tier. The archive tier isn't supported for ZRS, GZRS, or RA-GZRS accounts.</p> <p> </p> <p>Be careful while choosing access tier</p> <p>If you are setting up the access tier from the storage account level, you will have only two choices: <code>Hot</code> and <code>Cool</code>. This access tier will be inherited by all objects stored in the storage account. The <code>Archive tier</code> can be set at the individual object level only.</p>"},{"location":"azure/storage/#tiering-policy","title":"Tiering Policy","text":"<p>Azure Storage lifecycle management offers a rule-based policy that you can use to transition blob data to the appropriate access tiers or to expire data at the end of the data lifecycle.</p> <p>More than 1 policy on same blob</p> <p>If you define more than one action on the same blob, lifecycle management applies the <code>least expensive</code> action to the blob. For example, action <code>delete</code> is cheaper than action <code>tierToArchive</code>. Action <code>tierToArchive</code> is cheaper than action <code>tierToCool</code>.</p>"},{"location":"azure/storage/#object-replication","title":"Object replication","text":"<p>Object replication asynchronously copies block blobs in a container according to rules that you configure. The contents of the blob, any versions associated with the blob, and the blob's metadata and properties are all copied from the source container to the destination container.</p> <p>Requirements for replication</p> <p>Object replication requires that the following <code>Azure Storage features</code> to be enabled:</p> <ul> <li>Change feed: Must be enabled on the source account.</li> <li>Blob versioning: Must be enabled on both the source and destination accounts. </li> </ul> <p> </p> <p>The replicaiton process between 2 SA's is shown below</p> <p> </p>"},{"location":"azure/storage/#immutable-blob-worm","title":"Immutable blob (WORM)","text":"<p>Immutable storage for Azure Blob Storage enables users to store business-critical data in a <code>WORM (Write Once, Read Many)</code> state. While in a WORM state, data cannot be modified or deleted for a user-specified interval. By configuring immutability policies for blob data, you can protect your data from overwrites and deletes.</p> <p><code>Immutable storage</code> for Azure Blob Storage supports 2 types of immutability policies:</p> <ul> <li>Time-based retention policies: With a time-based retention policy, users can set policies to store data for a specified interval. When a time-based retention policy is set, objects can be created and read, but not modified or deleted. After the retention period has expired, objects can be deleted but not overwritten. </li> <li>Legal hold policies: A legal hold stores immutable data until the legal hold is explicitly cleared. When a legal hold is set, objects can be created and read, but not modified or deleted. </li> </ul>"},{"location":"azure/storage/#encryption-scopes","title":"Encryption scopes","text":"<p>Encryption scopes enable you to manage encryption with a key that is scoped to a container or an individual blob. You can use encryption scopes to create secure boundaries between data that resides in the same storage account but belongs to different customers.</p> <p>How encryption scopes work?</p> <p>By default, a storage account is encrypted with a key that is scoped to the <code>entire storage account</code>. When you define an encryption scope, you specify a key that may be <code>scoped to a container or an individual blob</code>.</p> <p>When the encryption scope is applied to a blob, the blob is encrypted with that key. When the encryption scope is applied to a container, it serves as the default scope for blobs in that container, so that all blobs that are uploaded to that container may be encrypted with the same key.</p>"},{"location":"azure/storage/#azure-file","title":"Azure File","text":"<p>It has hierarchical file structure</p> Generic file share structure<pre><code>## Generic structure\n&lt;storageAccountName&gt;.file.core.windows.net\\&lt;fileShareName&gt;\n\n# if storageAccountName == amarTestStorage and fileShareName == dataFile01, then URI is\n\namarTestStorage.file.core.windows.net\\dataFile01\n</code></pre> <p>Difference between Azure File and Azure Blob?</p> <ul> <li>Azure Blobs uses a <code>flat namespace</code> that includes containers and objects. Azure Files uses <code>directory objects</code> as you have seen with our traditional file shares.</li> <li>Azure Blobs is <code>accessed via containers</code>, and Azure Files is <code>accessed through file shares</code>.</li> <li>Azure Blobs is accessed via an HTTP/HTTPS connection, and Azure Files is <code>accessed via the SMB/NFS protocol</code> when mounted to a virtual machine.</li> <li>Azure Blobs doesn\u2019t need to be mounted and can be accessed directly from any client that supports HTTP calls. Azure Files needs to be mounted to virtual machines before working with the data. On a side note, you can still manage the files in Azure Files via tools like the Azure portal and Azure Storage Explorer without the need to mount it.</li> </ul>"},{"location":"azure/storage/#azure-file-share","title":"Azure File Share","text":"<p>File shares are not something new; you still have on-premises file shares that serve your enterprise needs and requirements. <code>Azure File Share</code> is a cloud-based file share that enables you to access the file share from any computer anywhere in the world.</p>"},{"location":"azure/storage/#azure-file-sync","title":"Azure File Sync","text":"<p>With Azure File Sync, you will be using Azure Files as a centralized location for storing files. In simple words, you can synchronize the files you have on-premises with <code>Azure File Share</code>.</p> <p> </p> <p>An <code>Azure File Sync</code> deployment has these components:</p> <ul> <li>Azure file share: An Azure file share is a serverless cloud file share, which provides the <code>cloud endpoint</code> of an Azure File Sync relationship.</li> <li>Server endpoint: The path on the <code>Windows Server</code> that is being synced to an Azure file share. This can be a specific folder on a volume or the root of the volume. Multiple server endpoints can exist on the same volume if their namespaces do not overlap (as shown in the figure)</li> <li>Sync group: The object that defines the sync relationship between a cloud endpoint, or Azure file share, and a server endpoint. Endpoints within a sync group are kept in sync with each other. If for example, you have two distinct sets of files that you want to manage with Azure File Sync, you would create two sync groups and add different endpoints to each sync group.</li> </ul>"},{"location":"azure/storage/#file-share-steps","title":"File Share steps","text":"<p>Steps to synchronize the files in the file share named data to an on-premises server named Server1</p> <ul> <li> <p>Step 1: <code>Install the Azure File Sync agent on Server1</code> - The Azure File Sync agent is a downloadable package that enables Windows Server to be synced with an Azure file share.</p> </li> <li> <p>Step 2: <code>Register Server1</code> - Register Windows Server with Storage Sync Service. Registering your Windows Server with a Storage Sync Service establishes a trust relationship between your server and the Storage Sync Service.</p> </li> <li> <p>Step 3: <code>Create a sync group and a cloud endpoint</code> - A sync group defines the sync topology for a set of files. Endpoints within a sync group are kept in sync with each other. A sync group must contain one cloud, which represents an Azure file share and one or more server endpoints. A server endpoint represents a path on registered server.</p> </li> </ul>"},{"location":"azure/storage/#storage-security","title":"Storage Security","text":"<p>Remember</p> <p>You can control <code>fine-grained access</code> to data objects using SAS token instead of using access keys as you can define <code>time-bound access</code>.</p>"},{"location":"azure/storage/#authorization-options","title":"Authorization options","text":"<p>The following are the authorization options available to Azure Storage:</p> <ol> <li><code>Azure AD</code>: Using Azure AD:  you can authorize access to Azure Storage via role-based access control (RBAC). With RBAC, you can assign fine-grained access to users, groups, or applications.</li> <li><code>Shared Key</code>: Every storage account has two keys: primary and secondary. The access keys of the storage account will be used in the Authorization header of the API calls. <code>Shared Access Signatures</code> (SAS), you can limit access to services with specified permissions and over a specified timeframe.</li> <li><code>Anonymous Access to Containers and Blobs</code>: If you set the access level to blob or container, you can have public access to the objects without the need to pass keys or grant authorization. </li> </ol>"},{"location":"azure/storage/#azcopy","title":"AzCopy","text":"<p><code>AzCopy</code> is a next-generation command-line tool for copying data from or to Azure Blob and Azure Files. </p> <p>Behind the scenes, Azure Storage Explorer uses <code>AzCopy</code> to accomplish all the data transfer operations. </p> <p>With <code>AzCopy</code> you can copy data in the following scenarios: </p> <ul> <li>Copy data from a local machine to Azure Blobs or Azure Files </li> <li>Copy data from Azure Blobs or Azure Files to a local machine </li> <li>Copy data between storage accounts</li> </ul>"},{"location":"azure/storage/#custom-domain-configuration","title":"Custom Domain Configuration","text":"<p>Accessing the service using the default domain may be hectic, and customers would like to use their own <code>custom domains</code> to represent their storage space.</p>"},{"location":"azure/synapse/","title":"Synapse","text":"<p>Azure Synapse Analytics (formerly SQL Data Warehouse) is a <code>Massively Parallel Processing (MPP)</code>  data warehouse that can handle large amounts of data and provides a scalable solution for analytics.</p>"},{"location":"azure/synapse/#synapse-link","title":"Synapse Link","text":"<p>Azure Synapse Link for Azure Cosmos DB creates a tight integration between Azure Cosmos DB and Azure Synapse Analytics, allowing you to run near real-time analytics over operational data in Azure Cosmos DB. It creates a \"no-ETL\" (Extract, Transform, Load) environment that allows you to analyze data directly without affecting the performance of the transactional workload, which is exactly what is required in this scenario.</p> <p>What is PolyBase?</p> <p>PolyBase is a technology used to <code>query data from different data sources</code> through SQL Server. In this way, it is possible to combine data in different formats and different sources at a single point.</p>"},{"location":"azure/synapse/#data-share","title":"Data Share","text":"<p><code>Azure Data Share</code> enables organizations to securely share data with multiple customers and partners. Data providers are always in control of the data that they've shared and Azure Data Share makes it simple to manage and monitor what data was shared, when and by whom.</p>"},{"location":"azure/vnet/","title":"Virtual Network","text":"<ul> <li> <p>VNet enables many types of Azure resources, such as Azure Virtual Machines (VM), to securely communicate with each other.</p> </li> <li> <p>Virtual networks and subnets span all availability zones in a region. You don't need to divide them by availability zones to accommodate zonal resources</p> </li> </ul> <p>Default communication to Internet</p> <p>All resources in a VNet can communicate outbound to the internet, by default. You can communicate inbound to a resource by assigning a public IP address or a public Load Balancer.</p>"},{"location":"azure/vnet/#vnet-components","title":"VNet components","text":""},{"location":"azure/vnet/#address-space","title":"Address space","text":"<p>When creating a VNet, you must specify a <code>custom private IP address</code> space using public and private addresses. Azure assigns resources in a virtual network a private IP address from the address space that you assign. For example, if you deploy a VM in a VNet with address space, <code>10.0.0.0/16</code>, the VM will be assigned a private IP like <code>10.0.0.4</code>.</p> <p>Address range</p> <p>Largest is <code>x.x.x.x/8</code> and smallest is <code>x.x.x.x/29</code></p>"},{"location":"azure/vnet/#subnets","title":"Subnets:","text":"<p>Subnets enable you to segment the virtual network into one or more subnetworks and allocate a portion of the virtual network\u2019s address space to each subnet. Reserved IP addresses are <code>x.x.x.0-3</code> and <code>x.x.x.255</code>. </p>"},{"location":"azure/vnet/#some-tips-for-vnets","title":"Some tips for VNets","text":"<ul> <li>Private IP addresses are supported using in-built <code>DHCP server</code>. </li> <li>Vnet supportes <code>IPv4 and IPV6 public IPs</code></li> <li>VNET Subnet: They are used as <code>Gateway subnets</code></li> <li>We can have a <code>default subnet</code></li> <li>Vnet has <code>Azure provided DNS</code>, we can also use the <code>custom DNS</code> too.</li> <li>At the subnet level, you can configure <code>Network Security Groups (NSGs)</code> to secure your workloads</li> </ul>"},{"location":"azure/vnet/#a-dmz-subnet-use-case","title":"A DMZ subnet use case","text":"<p>In this typical use case, our goal is to make sure that all traffic coming from the public subnet should be routed to the DMZ before it gets routed to the private subnet.</p> <p> </p> <p>The reason for this is that a public subnet contains workloads that are Internet facing, and a <code>private subnet</code> contains workloads that are not exposed to the Internet, such as <code>databases, application logic, backend servers, etc</code>. By default, the communication from the public subnet is allowed to the private subnet; however, we need to make sure that the traffic is filtered before it reaches the private subnet. This will help us to protect the private workloads, as all traffic is matched against the rules that you have set up in the <code>NVA (WAF in this case)</code>. If there is any attack or malicious traffic, <code>NVA</code> will take care of it. </p> <p>Now, the question is, how can we implement this? As you can see in above diagram, we have used the routing table with User Defined Route (UDR) to accomplish this task. By default, the traffic from the public subnet to the private subnet is allowed, but using UDR, you are forcing the packets to go through the <code>DMZ subnet</code>.</p>"},{"location":"azure/vnet/#regions","title":"Regions","text":"<p>VNet is scoped to a single region/location; however, multiple virtual networks from different regions can be connected together using peering.</p> <p>Choose region with care</p> <p>When you create a virtual network in Azure, you will get an option to choose the region. Depending on the region you choose, the virtual network will be deployed to the respective region, and the virtual machines deployed to the virtual network will also fall under the same region. </p>"},{"location":"azure/vnet/#subscription","title":"Subscription","text":"<p>VNet is scoped to a subscription. You can implement multiple virtual networks within each Azure subscription and Azure region.</p>"},{"location":"azure/vnet/#vnet-peering","title":"VNET peering","text":"<p><code>Virtual network peering</code> enables you to seamlessly connect two or more <code>Virtual Networks</code> in Azure. The virtual networks appear as one for connectivity purposes. The traffic between virtual machines in peered virtual networks uses the Microsoft backbone infrastructure. Like traffic between virtual machines in the same network, traffic is routed through Microsoft's private network only.</p> <p>VNET non-reciprocal and non-trasitive connection</p> <p>If we setup a connection between 2 VNets, that connection is non-reciprocal as shown in the diagram below. That is why we have to setup 2 connections in this case.  </p> <p>The connction between different VNETS is non-trasitive as shown below.  </p> <p><code>Active-passive</code> and <code>active-active</code> peering are possible. Lets take an example of on-site and cloud peering with <code>active-passive</code> mode as shown below</p> <p> </p> <p>Azure supports the following types of peering:</p> <ol> <li><code>Virtual network peering</code>: Connecting virtual networks within the same Azure region.</li> <li><code>Global virtual network peering</code>: Connecting virtual networks across Azure regions.</li> </ol>"},{"location":"azure/vnet/#express-route","title":"Express Route","text":"<p><code>ExpressRoute</code> lets you extend your on-premises networks into the Microsoft cloud over a private connection with the help of a connectivity provider. With <code>ExpressRoute</code>, you can establish connections to Microsoft cloud services, such as <code>Microsoft Azure</code> and <code>Microsoft 365</code>.</p> <p>Remember</p> <p><code>ExpressRoute</code> connections don't go over the public Internet. This allows <code>ExpressRoute</code> connections to offer more reliability, faster speeds</p> <p>We have 2 options in Express routes</p> <ol> <li>ExpressRoute Direct: It does not require connectivity provider (service provider), it directly connects to Microsoft Edge Routers</li> </ol> <p>What is FastPath?</p> <p>FastPath is an additional feature that can be used with <code>ExpressRoute</code> Direc. When enabled, FastPath sends network traffic directly to virtual machines in the virtual network, bypassing the gateway (<code>ExpressRoute</code> gateway is still needed though)</p> <ol> <li>ExpresRoute Circuit: It uses a <code>service provider</code>.   </li> </ol> <p> </p> <p>Points to remember</p> <ul> <li> <p>Allocate two /30 subnets that are not used anywhere else in your network topology. One subnet will be used for the <code>primary connction</code>; the other will be used for the <code>secondary connction</code> as shown in the diagram above. From each of these subnets, you will assign the first usable IP address to your router because Microsoft uses the second usable IP for its router.</p> </li> <li> <p>A valid <code>VLAN ID</code> is required to establish this peering on. Ensure that no other peering in the circuit uses the same VLAN ID. For both primary and secondary links you must use the same VLAN ID. This information is provided from your provider.</p> </li> </ul>"},{"location":"azure/vnet/#circuits-and-peering","title":"Circuits and Peering","text":""},{"location":"azure/vnet/#circuit","title":"Circuit","text":"<ul> <li><code>ExpressRoute</code> circuit is a logical connection between on-premises infrastructure and Microsoft cloud services through a connectivity provider</li> <li><code>ExpressRoute</code> circuits do not map to any physical entities</li> <li>A circuit is uniquely identified by a standard GUID called as a service key (s-key)</li> <li>There is a 1:1 mapping between an <code>ExpressRoute</code> circuit and the s-key</li> <li>The service key is the only piece of information exchanged between Microsoft, the connectivity provider, and you (s-key is not a secret for security purposes)</li> <li>A circuit consists of 2 connections:<ul> <li><code>Primary connection</code>: main</li> <li><code>Secondary connection</code>: redundent for HA</li> </ul> </li> </ul>"},{"location":"azure/vnet/#peering","title":"Peering","text":"<p>A peering is the interconnection between on-premise network and Microsoft cloud services (Azure, Microsoft 365). <code>ExpressRoute</code> circuits can include two independent peerings: - <code>Private peering</code>: Azure VNet - <code>Microsoft peering</code>: Microsoft 365, Dynamics 365 etc.</p>"},{"location":"azure/vnet/#basics","title":"Basics","text":""},{"location":"azure/vnet/#nic","title":"NIC","text":"<p><code>Virtual machines</code> have one or more <code>network interface cards (NICs)</code> that exist in the same region as the virtual network. You assign an IP address (either statically or dynamically) from a subnet in the virtual network. This action allows the virtual machine to communicate with other resources in the virtual network (and any peered network).</p> <p> </p>"},{"location":"azure/vnet/#public-ips","title":"Public IPs","text":"<p>Public IP addresses are associated with a virtual machine NIC, public load balancer, VPN gateways, application gateways, and any other resource that can be accessed from the Internet. </p> <p>Here also we can choose the allocation method to be <code>static or dynamic</code>. However, the availability of allocation methods depends on which SKU of public IP address we are using. The SKU is more like a pricing tier, where you will find different prices based on which SKU you are selecting</p> <ul> <li>Basic SKU: Static or Dynamic IP that is <code>accessible by default</code> and needs NSG to configure</li> <li>Standard SKU: Static PIP, that is <code>not accessible by defult</code> and needs NSG to allow traffic</li> </ul> <p> </p>"},{"location":"azure/vnet/#private-endpoint","title":"Private Endpoint","text":"<p>Before defining private endpoint, lets clear some basics terms:</p> <p> Some basics: revised</p> <ol> <li><code>Azure Private Link</code> enables you to access Azure PaaS Services (for example, Azure Storage and SQL Database) and Azure hosted customer-owned/partner services over a Private Endpoint in your virtual network. Traffic between your virtual network and the service traverses over the Microsoft backbone network, eliminating exposure from the public Internet.</li> <li><code>Azure Private Link service</code> is the reference to your own service that is powered by Azure Private Link.</li> <li>A <code>private-link resource</code> is the destination target of a specified private endpoint.</li> <li>A <code>private endpoint</code> is a <code>network interface</code> that uses a private IP address from your virtual network. This network interface connects you privately and securely to a service that's powered by Azure Private Link. By enabling a private endpoint, you're bringing the service into your virtual network. Private endpoint has NIC attached to it.</li> </ol> <p> </p> <ul> <li>The value of the <code>private IP address</code> remains unchanged for the entire lifecycle of the private endpoint.</li> <li>The <code>private endpoint</code> must be deployed in the same region and subscription as the virtual network.</li> <li>The <code>private-link resource</code> can be deployed in a different region than the one for the virtual network and private endpoint.</li> </ul> <p>Private Endpoint Example</p> <p>Imgine that there is a virtual network using the address range 10.10.0.0/16. Inside this virtual network are two subnets, SubnetA (<code>10.10.5.0/24</code>) and SubnetB (<code>10.10.6.0/24</code>).</p> <p>One or more virtual machines reside in <code>SubnetA</code>. A storage account has a private endpoint with an IP address of <code>10.10.6.5</code> inside <code>SubnetB</code>. The virtual machines inside <code>SubnetA</code> access the storage account through the storage account\u2019s private endpoint IP address located in <code>SubnetB</code>.</p> <p>Resources in other networks can access the <code>storage account</code> through the private endpoint IP address.</p> <p> </p> <p>Using private endpoints we can drill down to a sub-resource as shown below. This can't be done using <code>Service Endpoint</code> as it will allow access to the resource level instead of sub-resource level.  </p> <p>When to use Private endpoint?</p> <p>Private endpoints enable connectivity between the customers from the same:</p> <ul> <li>Virtual network</li> <li>Regionally peered virtual networks</li> <li>Globally peered virtual networks</li> <li>On-premises environments that use VPN or Express Route</li> <li>Services that are powered by Private Link</li> </ul>"},{"location":"azure/vnet/#private-link-service","title":"Private Link Service","text":"<p>Azure <code>Private Link service</code> is the reference to your own service that is powered by <code>Azure Private Link</code>. Your service that is running behind Azure Standard Load Balancer can be enabled for Private Link access so that consumers to your service can access it privately from their own VNets. Your customers can create a <code>private endpoint</code> inside their virtual network and map it to this service. </p> <p> </p>"},{"location":"azure/vnet/#service-endpoint","title":"Service Endpoint","text":"<ul> <li>Service endpoints direct VNet traffic <code>off the public Internet</code> and to the <code>Azure backbone network</code>. </li> <li>They are enabled per subnet using policies.</li> <li>This service does not provide private IPs to the services.</li> <li>Service Endpoints enables private IP addresses in the their VNet to reach the endpoint of an Azure service without needing a public IP address on the their VNet. These VM's will connect to public IP of the service though.</li> <li><code>Service endpoints</code> apply to all instances of the Azure resource, not just the ones you create. If you want to limit virtual network traffic to specific instances or regions of a resource, you need a service endpoint policy.</li> </ul> <p>Example of service endpoint</p> <p>Review the image below representing service endpoints in action. In this example, you have enabled a service endpoint for the <code>Microsoft.SQL service</code> on <code>SubnetA</code>. A virtual machine in <code>SubnetA</code> uses its private IP address to access a SQL server hosting several databases. The virtual machine connects using the public IP addresses associated with the <code>Microsoft.SQL service</code></p> <p>Notice that Subnet B does not have service endpoints enabled for its subnet for the <code>Microsoft.SQL service</code>. Compute resources in this subnet use the public IP address for the virtual network to connect to the public IP address of the Azure service.</p> <p> </p>"},{"location":"azure/vnet/#service-endpoint-policies","title":"Service Endpoint Policies","text":"<p>Virtual Network (VNet) <code>service endpoint policies</code> allow you to filter egress virtual network traffic to Azure <code>Storage accounts</code> over <code>service endpoint</code>, and allow data exfiltration to only specific Azure Storage accounts. Endpoint policies provide granular access control for virtual network traffic to Azure Storage when connecting over service endpoint.</p> <p> </p>"},{"location":"azure/vnet/#private-vs-service-endpoints","title":"Private Vs Service Endpoints","text":"<p>Public and Private IPs in service/private endpoints</p> <p>With <code>service endpoints</code>, you're still connecting to the target resource's public endpoint. This effectively extends the identity of the VNet to the target resource. With <code>private endpoints</code>, you're assigning the target resource a private IP address from the VNet, essentially bringing it into the network. The target resource's public IP address doesn't go away, but you can lock it down so all traffic from the internet is denied.</p> <p>Here are some tips on when to use which endpint</p> <p>When to use which endpoint?</p> <ol> <li>If you want to be able to block all internet traffic to a target resource, use a private endpoint.</li> <li>If you're dealing with traffic from on-premises, use a private endpoint.</li> <li>If you want to secure a specific sub-resource to your VNet resources, use a private endpoint.</li> <li>If you want to secure a <code>specific storage account</code> to your VNet resources, you can use a private endpoint, or a service endpoint with a <code>service endpoint policy</code>.</li> <li>If you don't need a private IP address at the destination, service endpoints are considerably easier to create and maintain, and they don't require special DNS configuration.</li> <li>if cost is a concern, note that service endpoints are free.</li> </ol>"},{"location":"azure/vnet/#network-routing-types","title":"Network Routing types","text":"<p><code>Network routes</code> or <code>route tables</code> have existed in traditional networks for an exceptionally long time. The routes that are part of the route table decide how to direct a packet to the destination.</p>"},{"location":"azure/vnet/#system-routes","title":"System routes","text":"<p>They are built in routes which <code>cannot be modified</code></p> <p>VM's default communicaiton to internet</p> <p>Whenever we create a VM in Azure, the VM will be able to communicate with the Internet without setting up any routes. In AWS, we need to create different gateways like NAT Gateway or Internet Gateway to facilitate the connection from a VM to the Internet. However, in Azure this is enabled by default with the help of <code>system routes</code></p>"},{"location":"azure/vnet/#custom-user-defined-routes-udr","title":"Custom/ User defined routes (UDR)","text":"<p>User defined routes (UDR) or BGP routes which can <code>override the system routes</code>.</p> Precenence order of the routes<pre><code>Custom routes &gt; BGP routes &gt; System routes\n</code></pre> <p>Block access to public internet</p> <p>We can block access to public internet by adding a custom route from <code>0.0.0.0/0</code> to <code>None</code> as shown below</p> <p> </p>"},{"location":"azure/vnet/#nsg-l3-and-l4","title":"NSG (L3 and L4)","text":"<p>Tldr</p> <p>They are used to control the flow of traffic inside the VNET and are similar to Firewalls</p> <p>An NSG is a collection of security rules that can be used to allow or deny inbound or outbound traffic.</p> <p>Is NSG associted with subnet?</p> <p>As shown in the diagram, NSG can be associated with a <code>subnet</code>, or a <code>NIC</code> based on your requirement. NSGs are reusable, which means you can have multiple NICs or subnet associations to a single NSG. </p> <p>NSG as no effect unless its associated with either NIC or a Subnet.</p> <p> </p> <p>Priorities are used in NSG to evaluate; lower the number, higher the priority.</p>"},{"location":"azure/vnet/#nsg-effective-rules","title":"NSG Effective rules","text":"<p>When you apply NSG to both subnet and NIC, then effective rules come into the picture. If you are using NSGs at both the subnet and NIC level, the rules will be evaluated at both levels. </p> <p>Rules evaluation order</p> <p><code>Inbound Traffic</code>: Each security rule will be evaluated independently when the NSG is applied to both the subnet and NIC levels. Incoming traffic will be first evaluated against the rules applied on the subnet level; if there is an allow rule, then the packet will be sent to the NIC. The rules set at the NIC level will be evaluated now, and only if there is an allow rule then the packet is allowed; otherwise, it will be dropped. In short, first at Subnet then NIC</p> <p><code>Outbound Traffic</code>: all outgoing traffic will be first evaluated against the NIC rules; if there is an allow rule, then the traffic is evaluated at the subnet level. In short, first at NIC then Subnet</p>"},{"location":"azure/vnet/#nva","title":"NVA","text":"<p>Network Virtual Appliance  <code>NVA</code>s are VMs that can optimize our networks with routing and firewall capabilities.</p>"},{"location":"azure/vnet/#azure-firewall-l7-and-l4","title":"Azure Firewall (L7 and L4)","text":"<p>Using Azure Firewall, we can create, enforce, and manage <code>network policies</code> across virtual networks and subscriptions. Azure Firewall provides more features then NSG. Also, Azure firewall is different than WAF</p> <p>Setting for Azure firewall is shown below</p> <p> </p> <p>Do we need WAF in addition to NSG?</p> <p>Yes, we need both. The main difference is that <code>NSG</code> operates at layers 3 and 4 of the OSI layer; on the other hand, Azure Firewall works at layers 7 and 4.</p> <p>It comes in 3 flavours</p> <ul> <li>Standard</li> <li>Premium</li> <li>Basic</li> </ul>"},{"location":"azure/vnet/#fqdn-tags","title":"FQDN Tags","text":"<p>We can use FQDN tag in the Azure firewall rules. An FQDN tag represents a group of fully qualified domain names (FQDNs) associated with well known Microsoft services. </p>"},{"location":"azure/vnet/#service-tags","title":"Service tags","text":"<p>A <code>service tag</code> represents a <code>group of IP address prefixes</code> to help minimize complexity for security rule creation. You can\u2019t create your own service tag, nor specify which IP addresses are included within a tag. Microsoft manages the address prefixes encompassed by the service tag, and automatically updates the service tag as addresses change.</p> <p>How to use service tags?</p> <p>You can use service tags to define network access controls on <code>network security groups</code>, <code>Azure Firewall</code>, and <code>user-defined routes</code>. Use <code>service tags</code> in place of specific IP addresses when you create security rules and routes. </p> <p>By specifying the service tag name, such as <code>Storage</code>, in the appropriate source or destination field of a security rule, you can allow or deny the traffic for the corresponding service. 2 tags are used in the below diagram in the destination fields </p> <p> </p>"},{"location":"azure/vnet/#firewall-policyrules","title":"Firewall Policy/Rules","text":"<ul> <li><code>Rule collection group</code>: It is used to group rule collections. They're the first unit to be processed by the Azure Firewall and they follow a priority order based on values</li> <li><code>Rule collection</code>: it belongs to a rule collection group, and it contains one or multiple rules. They're the second unit processed by the firewall and they follow a priority order based on values. Rule collections must have a defined action (allow or deny) and a priority value</li> </ul> <p>Rules used in Firewall are of 3 types</p> <ol> <li>NAT </li> <li>Network (L4)</li> <li>Application (L7)</li> </ol> Priority of Firewall rules<pre><code>NAT rule &gt; Network rule &gt; Application rule\n</code></pre>"},{"location":"azure/vnet/#nat-rules","title":"NAT rules","text":"<p>Using a NAT rule collection, you will be able to translate a public IP and port to a private IP and port</p> <p>NAT example using VM</p> <p>Assume you have a <code>Ubuntu server</code> with private IP address, say, <code>10.0.0.2</code>, and the SSH server is available on port 22. Instead of exposing this port and IP to the Internet, you can create a NAT rule on the firewall. Using the public IP of the firewall and a random port, you can set up a NAT rule. Let\u2019s assume that the public IP address of the firewall is <code>66.142.43.02</code>; using NAT rule you can redirect all traffic hitting port 5000 of the firewall to be translated to <code>10.0.0.2</code> and port 22. In short, translate <code>66.142.43.02:5000</code> to <code>10.0.0.2:22</code>. In this way, you can translate the public IP address and port to a private IP address and port.</p> <p> </p>"},{"location":"azure/vnet/#inbound-dnat","title":"Inbound DNAT","text":"<p>Inbound Internet network traffic to your firewall public IP address is translated (Destination Network Address Translation) and filtered to the private IP addresses on your virtual networks.</p>"},{"location":"azure/vnet/#outbound-snat","title":"Outbound SNAT","text":"<p>All outbound virtual network traffic IP addresses are translated to the <code>Azure Firewall public IP</code> (Source Network Address Translation). You can identify and allow traffic originating from your virtual network to remote Internet destinations.</p>"},{"location":"azure/vnet/#network-rules","title":"Network rules","text":"<p>A network rule should be in place for any <code>non-HTTP/S</code> traffic to be allowed through the firewall. For a source to communicate with a destination deployed behind a firewall, you need to have a network rule configured from the source to the destination. If there is no rule that specifically calls out this incoming traffic, then it will be dropped.</p> <p>Tldr</p> <p>You can use a network rule when you want to filter traffic based on IP addresses, any ports, and any protocols.</p>"},{"location":"azure/vnet/#application-rules","title":"Application rules","text":"<p>Application rules allow or deny outbound and east-west traffic based on the application layer (L7). You can use an application rule when you want to filter traffic based on fully qualified domain names (FQDNs), URLs, and HTTP/HTTPS protocols.</p> <p>if you configure network rules and application rules, then network rules are applied in priority order before application rules. The rules are terminating. So, if a match is found in a network rule, no other rules are processed.</p>"},{"location":"azure/vnet/#asg","title":"ASG","text":"<p><code>Application security groups</code> enable you to configure network security as a natural extension of an application's structure, allowing you to group virtual machines and define network security policies based on those groups. You can reuse your security policy at scale without manual maintenance of explicit IP addresses. </p>"},{"location":"azure/vpn_gw/","title":"VPN Gateways","text":"<p>A VPN gateway is a type of virtual network gateway. Azure VPN Gateway instances are deployed in a dedicated subnet of the virtual network and enable the following connectivity:</p> <ul> <li>Connect on-premises datacenters to virtual networks through a <code>site-to-site</code> connection.</li> <li>Connect individual devices to virtual networks through a <code>point-to-site</code> connection.</li> <li>Connect virtual networks to other virtual networks through a <code>network-to-network</code> connection.</li> </ul> <p>How many VPN's can be deployed?</p> <p>You can deploy only one VPN gateway in each virtual network. However, you can use one gateway to connect to multiple locations, which includes other virtual networks or on-premises datacenters.</p>"},{"location":"azure/vpn_gw/#gateway-subnet","title":"Gateway Subnet","text":"<p>Before you deploy the VPN gateway, you need to add a dedicated subnet to the vnet you want to connect. This subnet is for hosting the VPN gateway instances, and a CIDR block of <code>/27</code> or <code>/28</code> would suffice. The name of the subnet should be <code>GatewaySubnet</code>.</p> <p> </p>"},{"location":"azure/vpn_gw/#ha-modes","title":"HA modes","text":""},{"location":"azure/vpn_gw/#active-active","title":"Active-Active","text":""},{"location":"azure/vpn_gw/#active-standby","title":"Active-StandBy","text":""},{"location":"azure/vpn_gw/#types-of-vpn-gw","title":"Types of VPN GW","text":"<p>There are 2 types of VPN gateways:</p> <ol> <li> <p><code>Policy-based VPN gateways</code>: They specify statically the IP address of packets that should be encrypted through each tunnel. This type of device evaluates every data packet against those sets of IP addresses to choose the tunnel where that packet is going to be sent through.</p> </li> <li> <p><code>Route-based gateways</code>: IPSec tunnels are modeled as a network interface or virtual tunnel interface. IP routing (either static routes or dynamic routing protocols) decides which one of these tunnel interfaces to use when sending each packet. Route-based VPNs are the preferred connection method for on-premises devices. They're more resilient to topology changes such as the creation of new subnets.</p> </li> </ol>"},{"location":"azure/vpn_gw/#azure-to-on-premises","title":"Azure to On-Premises","text":"<p>There are 2 ways to connect on-premises to Azure 1. VPN gateway  2. ExpressRoute</p>"},{"location":"azure/vpn_gw/#vpn-gateway-connection","title":"VPN Gateway connection","text":"<p>The <code>local network gateway</code> refers to the on-premises location. You create a reference resource called a <code>local network gateway</code> in Azure to specify your <code>on-premises site</code>. While creating the local network gateway, you will specify the address prefixes that are there in the on-premises network.</p> <p>You can use either an IP address or an FQDN to specify your on-premises VPN device. Azure VPN gateway will be establishing connectivity to this device. Other than the IP address and address prefix, you will be asked to choose the subscription, resource group, and location for the resource.</p> <p>Using a P2S connection, you can create a secure connection from an individual computer to an Azure virtual network over VPN. The P2S connections are always initiated from the client machine. All clients need to download the VPN profile and install it on their device to establish the P2S connection. Before Azure accepts P2S requests from the clients, authentication should be done first.</p>"},{"location":"azure/waf/","title":"WAF","text":"<p>Azure <code>Web Application Firewall</code> is a cloud-native service that protects web apps from common web-hacking techniques such as SQL injection and security vulnerabilities such as cross-site scripting</p> <p>WAF VS AppGateway</p> <p>Application gateways provide more capabilities than a web application firewall (WAF)</p> <p>WAF can be deployed with the below services</p> <ol> <li>Azure Application Gateway</li> <li>Azure Front Door</li> <li>Azure Content Delivery Network (CDN)</li> </ol> <p>WAF on Application Gateway is based on the <code>Core Rule Set</code> (CRS) from the Open Web Application Security Project (OWASP).</p> <p>using pre-configured rules</p> <p>Protect your web applications in just a few minutes with the latest <code>managed and preconfigured rule sets</code>. The Azure Web Application Firewall detection engine combined with updated rule sets increases security, reduces false positives, and improves performance.</p>"},{"location":"blog/","title":"Blog","text":""},{"location":"blog/2024/08/15/6rs-of-cloud-migration-/","title":"6R's of Cloud Migration \ud83e\udded","text":"<p>Here are the 6 R's of cloud migration</p> <p> </p>"},{"location":"blog/2024/08/15/6rs-of-cloud-migration-/#6rs","title":"6R's","text":""},{"location":"blog/2024/08/15/6rs-of-cloud-migration-/#retire","title":"Retire \ud83d\udc74","text":"<p>Decommissioning of the unnecessary workloads</p>"},{"location":"blog/2024/08/15/6rs-of-cloud-migration-/#retaining","title":"Retaining \ud83d\udcdd","text":"<p>Don\u2019t move something to the cloud while move some pieces</p>"},{"location":"blog/2024/08/15/6rs-of-cloud-migration-/#rehost-lift-and-shift-using-iaas","title":"Rehost (lift and shift using IaaS) \ud83c\udfda\ufe0f","text":"<p>No re-factoring or code changes needed</p> <p>Migrate your applications first using the rehosting approach (\"lift-and-shift\"). With rehosting, you move an existing application to the cloud as-is and modernize it later.</p> <p> Re-host example <p></p> <p>Rehosting has four major benefits:</p> <ul> <li><code>Immediate sustainability</code>: The lift-and-shift approach is the fastest way to reduce your data center footprint. </li> <li><code>Immediate cost savings</code>: Using comparable cloud solutions will let you trade capital expenses with operational expenses. Pay-as-you-go and only pay for what you use. </li> <li><code>IaaS solutions</code>: IaaS virtual machines (VMs) provide immediate compatibility with existing on-premises applications. Migrate your workloads to Azure Virtual Machines and modernize while in the cloud. Some on-premises applications can move to an application platform with minimal effort. We recommend Azure App Service as a first option with IaaS solutions able to host all applications. </li> <li><code>Immediate cloud-readiness test</code>: Test your migration to ensure your organization has the people and processes in place to adopt the cloud. Migrating a minimum viable product is a great approach to test the cloud readiness of your organization.</li> </ul>"},{"location":"blog/2024/08/15/6rs-of-cloud-migration-/#re-purchasing-saas","title":"Re-purchasing (SaaS) \ud83d\udcb0","text":"<p>To buy SaaS alternatives.Most organizations replace about 15% of their applications with software-as-a-service (SaaS) and low-code solutions. They see the value in moving \"from\" technologies with management overhead (\"control\") and moving \"to\" solutions that let them focus on achieving their objectives (\"productivity\").</p> <p> Re-purchase example <p></p>"},{"location":"blog/2024/08/15/6rs-of-cloud-migration-/#re-platforming-paas","title":"Re-platforming (PaaS) \ud83d\udce6","text":"<p>It means lift and shift + some tuning. Replatforming, also known as \u201clift, tinker, and shift,\u201d involves making a few cloud optimizations to realize a tangible benefit. Optimization is achieved without changing the core architecture of the application.</p> <p> Re-platform example <p></p> <p>Modernize or re-platform your applications first. In this approach, you change parts of an application during the migration process.</p>"},{"location":"blog/2024/08/15/6rs-of-cloud-migration-/#refactoring","title":"Refactoring \ud83c\udfed","text":"<p>Rebuilding the apps from scratch. it's very expensive but being able to use all max benefits of the cloud</p> <p> Re-factor example <p></p>"},{"location":"blog/2024/08/15/6rs-of-cloud-migration-/#retire_1","title":"Retire","text":"<p>We recommend retiring any workloads your organization doesn't need. You'll need to do some discovery and inventory to find applications and environments that aren't worth the investment to keep. The goal of retiring is to be cost and time efficient. Shrinking your portfolio before you move to the cloud allows your team to focus on the most important assets.</p> <p> Retire example <p></p>"},{"location":"blog/2024/08/15/6rs-of-cloud-migration-/#aws-migration-evaluator","title":"AWS Migration Evaluator \ud83e\udd14","text":"<p> Migration Evaluator <p></p>"},{"location":"blog/2024/08/15/6rs-of-cloud-migration-/#migration-hub","title":"Migration Hub \ud83c\udfdb\ufe0f","text":"<p> Migration Evaluator <p></p> <p>AWS <code>Migration Hub</code> provides a single place to discover your existing servers, plan migrations, and track the status of each application migration. Before migrating you can discover information about your <code>on-premises server</code> and application resources to help you build a business case for migrating or to build a migration plan.</p> <p>Discovering your servers first is an optional starting point for migrations, gathering detailed server information, and then grouping the discovered servers into applications to be migrated and tracked. Migration Hub also gives you the choice to start migrating right away and to group servers during migration.</p> <p>Partners get exclusive tools \ud83d\udda5\ufe0f</p> <p>Using <code>Migration Hub</code> allows you to choose the AWS and partner migration tools that best fit your needs, while providing visibility into the status of migrations across your application portfolio.</p> <p>You get the data about your servers and applications into the <code>AWS Migration Hub console</code> by using the following discovery tools.</p> <ul> <li>Application Discovery Service Agentless Collector \u2013 Agentless Collector is an <code>on-premises application</code> that collects information through agentless methods about your on-premises environment, including server profile information (for example, OS, number of CPUs, amount of RAM), database metadata (for example, version, edition, numbers of tables and schemas), and server utilization metrics. </li> </ul> <p> Agentless <p></p> <p>You install the Agentless Collector as a virtual machine (VM) in your VMware vCenter Server environment using an Open Virtualization Archive (OVA) file. </p> <ul> <li>AWS Application Discovery Agent \u2013 The Discovery Agent is AWS software that you install on your on-premises servers and VMs to capture system configuration, system performance, running processes, and details of the network connections between systems.</li> </ul> <p> Agent Based <p></p> <p>Agents support most Linux and Windows operating systems, and you can deploy them on physical on-premises servers, Amazon EC2 instances, and virtual machines. </p> <ul> <li> <p>Migration Evaluator Collector \u2013 Migration Evaluator is a migration assessment service that helps you create a directional business case for AWS cloud planning and migration. The information that the Migration Evaluator collects includes server profile information (for example, OS, number of CPUs, amount of RAM), SQL Server metadata (for example, version and edition), utilization metrics, and network connections. </p> </li> <li> <p>Migration Hub import \u2013 With Migration Hub import, you can import information about your on-premises servers and applications into Migration Hub, including server specifications and utilization data. You can also use this data to track the status of application migrations. </p> </li> </ul>"},{"location":"blog/2023/06/07/feature-engineering-using-databricks-/","title":"Feature Engineering using Databricks \ud83e\uddf1","text":"<p>The Databricks Runtime includes additional optimizations and proprietary features that build upon and extend Apache Spark, including Photon which is an optimized version of Apache Spark rewritten in C++ using vectorized query processing.</p> <p>Spark Context </p> <p>You don\u2019t need to worry about configuring or initializing a Spark context or Spark session, as these are managed for you by Databricks.</p>"},{"location":"blog/2023/06/07/feature-engineering-using-databricks-/#architecture","title":"Architecture \ud83c\udfdb\ufe0f","text":"<p>Databricks operates out of a control plane and a data plane.</p> <p> </p> Image credits: Microsoft Learn"},{"location":"blog/2023/06/07/feature-engineering-using-databricks-/#control-plane","title":"Control Plane \ud83e\uddd1\u200d\u2708\ufe0f","text":"<p>The control plane includes the backend services that Azure Databricks manages in its own Azure account. Notebook commands and many other workspace configurations are stored in the control plane and encrypted at rest.</p>"},{"location":"blog/2023/06/07/feature-engineering-using-databricks-/#data-plane","title":"Data Plane \ud83d\udc77","text":"<p>Your Azure account manages the data plane, and is where your data resides. This is also where data is processed</p> <ul> <li>Job results reside in storage in your account.</li> <li><code>Interactive notebook results</code> are stored in a combination of the control plane (partial results for presentation in the UI) and your Azure storage. If you want interactive notebook results stored only in your cloud account storage, you can ask your Databricks representative to enable interactive notebook results in the customer account for your workspace.</li> </ul>"},{"location":"blog/2023/06/07/feature-engineering-using-databricks-/#spark-concepts","title":"Spark Concepts","text":""},{"location":"blog/2023/06/07/feature-engineering-using-databricks-/#dataframe-and-rdd","title":"DataFrame and RDD \ud83e\uddee","text":"<p>Tldr</p> <p>A DataFrame is a <code>two-dimensional labeled data structure</code> with columns of potentially different types. You can think of a DataFrame like a spreadsheet, a SQL table, or a dictionary of series objects. Apache Spark DataFrames provide a rich set of functions (select columns, filter, join, aggregate) that allow you to solve common data analysis problems efficiently.</p> <p>Apache Spark DataFrames are an abstraction built on top of Resilient Distributed Datasets (RDDs).</p> <p>Use of Lazy loading in Spark Dataframe instead of Pandas</p> <p>One of the key differences between <code>Pandas</code> and <code>Spark dataframes</code> is eager versus lazy execution. In PySpark, operations are delayed until a result is actually requested in the pipeline. For example, you can specify operations for loading a data set from Amazon S3 and applying a number of transformations to the dataframe, but these operations won\u2019t be applied immediately. Instead, a graph of transformations is recorded, and once the data are actually needed, for example when writing the results back to S3, then the transformations are applied as a single pipeline operation. This approach is used to avoid pulling the full dataframe into memory, and enables more effective processing across a cluster of machines.</p>"},{"location":"blog/2023/06/07/feature-engineering-using-databricks-/#spark-sql","title":"Spark SQL \ud83c\udf10","text":"<p>The term Spark SQL technically applies to all operations that use Spark DataFrames. Spark SQL replaced the Spark RDD API in Spark 2.x, introducing support for SQL queries and the DataFrame API for Python, Scala, R, and Java.</p>"},{"location":"blog/2023/06/07/feature-engineering-using-databricks-/#pyspark","title":"PySpark \ud83d\udd25","text":"<p>PySpark is the Python API for Apache Spark, an open source, distributed computing framework and set of libraries for real-time, large-scale data processing. </p>"},{"location":"blog/2023/06/07/feature-engineering-using-databricks-/#databricks-concepts","title":"Databricks Concepts \ud83e\uddd1\u200d\ud83c\udfeb","text":""},{"location":"blog/2023/06/07/feature-engineering-using-databricks-/#databricks-file-system-dbfs","title":"Databricks File System (DBFS)","text":"<p>A filesystem abstraction layer over a blob store. It contains directories, which can contain files (data files, libraries, and images), and other directories. DBFS is automatically populated with some datasets that you can use to learn Azure Databricks.</p> <p>DBFS is an abstraction on top of scalable object storage that maps <code>Unix-like filesystem</code> calls to native cloud storage API calls.</p>"},{"location":"blog/2023/06/07/feature-engineering-using-databricks-/#mount-blob-to-dbfs","title":"Mount blob to DBFS \ud83d\udccd","text":"<p>Mounting object storage to DBFS allows you to access objects in object storage as if they were on the local file system. Mounts store Hadoop configurations necessary for accessing storage, so you do not need to specify these settings in code or during cluster configuration.</p>"},{"location":"blog/2023/06/07/feature-engineering-using-databricks-/#dbfs-root","title":"DBFS root \ud83c\udf34","text":"<p>The DBFS root is the default storage location for a Databricks workspace, provisioned as part of workspace creation in the cloud account containing the Databricks workspace</p> <p>It is important to differentiate that DBFS is a file system used for interacting with data in cloud object storage, and the DBFS root is a cloud object storage location.</p>"},{"location":"blog/2023/06/07/feature-engineering-using-databricks-/#auto-loader","title":"Auto Loader \ud83d\udefa","text":"<p>Tldr</p> <p>Auto Loader incrementally and efficiently processes new data files as they arrive in cloud storage without any additional setup.</p> <p>Auto Loader incrementally and efficiently processes new data files as they arrive in cloud storage. Auto Loader can load data files from</p> <ul> <li>AWS S3 <code>(s3://)</code></li> <li>Azure Data Lake Storage Gen2 (ADLS Gen2, <code>abfss://</code>)</li> <li>Google Cloud Storage (GCS, <code>gs://</code>)</li> <li>Azure Blob Storage (<code>wasbs://</code>)</li> <li>ADLS Gen1 (<code>adl://</code>) </li> <li>Databricks File System (DBFS, <code>dbfs:/</code>)</li> </ul> <p>Auto Loader can ingest JSON, CSV, PARQUET, AVRO, ORC, TEXT, and BINARYFILE file formats.</p> <p>How does Auto Loader track ingestion progress?</p> <p>As files are discovered, their metadata is persisted in a scalable key-value store (<code>RocksDB</code>) in the checkpoint location of your Auto Loader pipeline. This key-value store ensures that data is processed exactly once.</p>"},{"location":"blog/2023/06/07/feature-engineering-using-databricks-/#delta-lake","title":"Delta Lake \u26f4\ufe0f","text":"<p>Delta Lake is open source software that extends <code>Parquet data files</code> with a file-based transaction log for ACID transactions and scalable metadata handling. </p> <p>Delta Lake is fully compatible with Apache Spark APIs, and was developed for tight integration with Structured Streaming, allowing you to easily use a single copy of data for both batch and streaming operations and providing incremental processing at scale.</p> <p>Who created delta lake format?</p> <p>Delta Lake is the default storage format for all operations on Databricks. Unless otherwise specified, all tables on Databricks are Delta tables. Databricks originally developed the Delta Lake protocol and continues to actively contribute to the open source project.</p>"},{"location":"blog/2023/06/07/feature-engineering-using-databricks-/#connecting-to-blob-adls","title":"Connecting to blob/  ADLS \ud83d\udd17","text":"<p>We can use the <code>Azure Blob Filesystem driver</code> (ABFS) to connect to <code>Azure Blob Storage</code> and <code>Azure Data Lake Storage (ADLS) Gen2</code> from Databricks</p> <p>The connection can be scoped to either 1. Databricks cluster 2. Databricks Notebook</p> <p>ABFS vs WASB</p> <p>The legacy <code>Windows Azure Storage Blob</code>driver (WASB) has been deprecated. ABFS has numerous benefits over WASB.</p> <p>Credentials walkthrough</p> <p>When you enable Azure Data Lake Storage <code>credential passthrough</code> for your cluster, commands that you run on that cluster can read and write data in <code>Azure Data Lake Storage</code> without requiring you to configure <code>service principal credentials</code> for access to storage. Azure Data Lake Storage credential passthrough is supported with Azure Data Lake Storage Gen1 and Gen2 only. Azure Blob storage does not support credential passthrough.</p>"},{"location":"blog/2023/06/07/feature-engineering-using-databricks-/#delta-table","title":"Delta table \u0394","text":"<p>A Delta table stores data as a directory of files on cloud object storage and registers table metadata to the metastore within a catalog and schema.</p>"},{"location":"blog/2023/06/07/feature-engineering-using-databricks-/#hive-metastore","title":"Hive metastore \ud83d\udc1d","text":"<p>The component that stores all the structure information of the various tables and partitions in the data warehouse including column and column type information, the <code>serializers</code> and <code>deserializers</code> necessary to read and write data, and the corresponding files where the data is stored.</p>"},{"location":"blog/2023/06/07/feature-engineering-using-databricks-/#delta-live-tables","title":"Delta live tables \ud83d\udd96","text":"<p>Instead of defining your data pipelines using a series of separate Apache Spark tasks, <code>Delta Live Tables</code> manages how your data is transformed based on a target schema you define for each processing step.</p> <p>You can also enforce data quality with Delta Live Tables expectations. Expectations allow you to define expected data quality and specify how to handle records that fail those expectations.</p>"},{"location":"blog/2023/06/07/feature-engineering-using-databricks-/#authentication-and-authorization","title":"Authentication and authorization \ud83e\udeaa","text":""},{"location":"blog/2023/06/07/feature-engineering-using-databricks-/#user","title":"User \ud83e\uddd1\u200d\ud83e\uddb0","text":"<p>A unique individual who has access to the system. User identities are represented by email addresses.</p>"},{"location":"blog/2023/06/07/feature-engineering-using-databricks-/#service-principal","title":"Service principal \u2603\ufe0f","text":"<p>A service identity for use with jobs, automated tools, and systems such as scripts, apps, and CI/CD platforms. Service principals are represented by an application ID.</p>"},{"location":"blog/2023/06/07/feature-engineering-using-databricks-/#group","title":"Group \ud83c\udfe0","text":"<p>Groups simplify identity management, making it easier to assign access to workspaces, data, and other securable objects. All Databricks identities can be assigned as members of groups.</p>"},{"location":"blog/2023/06/07/feature-engineering-using-databricks-/#acl","title":"ACL \u26d4\ufe0f","text":"<p>A list of permissions attached to the workspace, cluster, job, table, or experiment. An ACL specifies which users or system processes are granted access to the objects, as well as what operations are allowed on the assets</p>"},{"location":"blog/2023/06/07/feature-engineering-using-databricks-/#pat","title":"PAT \ud83d\udcb3","text":"<p><code>An opaque string</code> is used to authenticate to the REST API and by tools in the Databricks integrations to connect to SQL warehouses.</p>"},{"location":"blog/2023/06/07/feature-engineering-using-databricks-/#ds-engineering-space","title":"DS &amp; Engineering Space \u2699\ufe0f","text":""},{"location":"blog/2023/06/07/feature-engineering-using-databricks-/#workspace","title":"Workspace \ud83e\ude90","text":"<p>A workspace is an environment for accessing all of your Azure Databricks assets. A workspace organizes objects (notebooks, libraries, dashboards, and experiments) into folders and provides access to data objects and computational resources.</p>"},{"location":"blog/2023/06/07/feature-engineering-using-databricks-/#notebook","title":"Notebook \ud83d\udd16","text":"<p>A web-based interface to documents that contain runnable commands, visualizations, and narrative text.</p>"},{"location":"blog/2023/06/07/feature-engineering-using-databricks-/#repo","title":"Repo \ud83d\udce6","text":"<p>A folder whose contents are co-versioned together by syncing them to a remote Git repository.</p>"},{"location":"blog/2023/06/07/feature-engineering-using-databricks-/#databricks-workflow","title":"Databricks Workflow \u23f3","text":"<p>Azure Databricks Workflows orchestrates data processing, machine learning, and analytics pipelines in the Azure Databricks Lakehouse Platform. </p> <p>Workflows has fully managed orchestration services integrated with the Azure Databricks platform, including Azure Databricks Jobs to run non-interactive code in your Azure Databricks workspace and Delta Live Tables to build reliable and maintainable ETL pipelines.</p>"},{"location":"blog/2023/06/07/feature-engineering-using-databricks-/#sccnpip","title":"SCC/NPIP \ud83c\udfad","text":"<p>Secure cluster connectivity is also known as No Public IP (NPIP).</p> <p>Tldr</p> <p>With secure cluster connectivity enabled, customer virtual networks have no open ports and Databricks Runtime cluster nodes in the classic compute plane have no public IP addresses.</p> <ul> <li> <p>At a network level, each cluster initiates a connection to the control plane secure cluster connectivity relay during cluster creation. The cluster establishes this connection using port 443 (HTTPS) and uses a different IP address than is used for the Web application and REST API.</p> </li> <li> <p>When the control plane logically starts new Databricks Runtime jobs or performs other cluster administration tasks, these requests are sent to the cluster through this tunnel.</p> </li> <li> <p>The compute plane (the VNet) has no open ports, and Databricks Runtime cluster nodes have no public IP addresses.</p> </li> </ul> <p> </p>"},{"location":"blog/2023/06/07/feature-engineering-using-databricks-/#delta-lake_1","title":"Delta Lake \ud83d\udc1f","text":"<ul> <li>Delta Lake is the optimized storage layer that provides the foundation for storing data and tables in the Databricks lakehouse.</li> <li>Delta Lake is <code>open source software</code> that extends <code>Parquet data files</code> with a file-based transaction log for ACID transactions and scalable metadata handling.</li> <li>Delta Lake is fully compatible with Apache Spark APIs, and was developed for tight integration with Structured Streaming, allowing you to easily use a single copy of data for both batch and streaming operations and providing incremental processing at scale.</li> </ul> <p>The default format</p> <p>Delta Lake is the <code>default storage format</code> for all operations on Azure Databricks. Unless otherwise specified, all tables on Azure Databricks are Delta tables. Databricks originally developed the Delta Lake protocol and continues to actively contribute to the open source project. </p>"},{"location":"blog/2023/06/07/feature-engineering-using-databricks-/#delta-table_1","title":"Delta Table \ud83e\udde9","text":"<p>A Delta table stores data as a directory of files on cloud object storage and registers table metadata to the metastore within a <code>catalog</code> and <code>schema</code>.</p>"},{"location":"blog/2023/06/07/feature-engineering-using-databricks-/#dbfs","title":"DBFS \ud83d\uddc4\ufe0f","text":"<p>The <code>Databricks File System (DBFS)</code> is a ==distributed file system= mounted into an Azure Databricks workspace and available on Azure Databricks clusters. DBFS is an abstraction on top of scalable <code>object storage</code> that maps Unix-like filesystem calls to native cloud storage API calls.</p> <p>So what is DBFS root?</p> <p>The DBFS root is the default storage location for an Azure Databricks workspace, provisioned as part of workspace creation in the cloud account containing the Azure Databricks workspace.  it is important to differentiate that DBFS is a file system used for interacting with data in cloud object storage, and the DBFS root is a <code>cloud object storage location</code></p>"},{"location":"blog/2023/06/07/feature-engineering-using-databricks-/#unity-catalog-metastore","title":"Unity Catalog Metastore \ud83e\udded","text":"<p>Unity Catalog provides centralized access control, auditing, lineage, and data discovery capabilities. You create <code>Unity Catalog metastores</code> at the Azure Databricks account level, and a single metastore can be used across multiple workspaces.</p>"},{"location":"blog/2023/06/07/feature-engineering-using-databricks-/#hive-metastore-legacy","title":"Hive Metastore (Legacy) \ud83d\udce6","text":"<p>Each Azure Databricks workspace includes a built-in Hive metastore as a managed service. An instance of the metastore deploys to each cluster and securely accesses metadata from a central repository for each customer workspace.</p> <p>The Hive metastore provides a less centralized data governance model than Unity Catalog. By default, a cluster allows all users to access all data managed by the workspace\u2019s built-in Hive metastore unless table access control is enabled for that cluster.</p>"},{"location":"blog/2023/06/07/feature-engineering-using-databricks-/#catalog","title":"Catalog \ud83d\udcd5","text":"<ul> <li>A <code>catalog</code> is the highest abstraction (or coarsest grain) in the Databricks lakehouse relational model. </li> <li>Every database will be associated with a catalog. </li> <li>Catalogs exist as objects within a metastore.</li> </ul> <p>Before the introduction of Unity Catalog, Azure Databricks used a two-tier namespace. Catalogs are the third tier in the Unity Catalog namespacing model: <pre><code>catalog_name.database_name.table_name\n</code></pre></p>"},{"location":"blog/2023/06/07/feature-engineering-using-databricks-/#scim","title":"SCIM \ud83d\udd10","text":"<p>SCIM (<code>System for Cross-domain Identity Management</code>) lets you use an identity provider (IdP) to create users in Azure Databricks, give them the proper level of access, and remove access (deprovision them) when they leave your organization or no longer need access to Azure Databricks.</p> <p>You can either configure one SCIM provisioning connector from Microsoft Entra ID (formerly Azure Active Directory) to your Azure Databricks account, using account-level SCIM provisioning, or configure separate SCIM provisioning connectors to each workspace, using workspace-level SCIM provisioning.</p> <p> </p> <p>Account-level SCIM provisioning: Azure Databricks recommends that you use account-level SCIM provisioning to create, update, and delete all users from the account. You manage the assignment of users and groups to workspaces within Databricks. Your workspaces must be enabled for identity federation to manage users\u2019 workspace assignments.</p> <p>Workspace-level SCIM provisioning (public preview): If none of your workspaces is enabled for identity federation, or if you have a mix of workspaces, some enabled for identity federation and others not, you must manage account-level and workspace-level SCIM provisioning in parallel. In a mixed scenario, you don\u2019t need workspace-level SCIM provisioning for any workspaces that are enabled for identity federation.</p>"},{"location":"blog/2023/06/07/feature-engineering-using-databricks-/#unity-catalog","title":"Unity Catalog \u269b\ufe0f","text":"<p>Unity Catalog provides centralized access control, auditing, lineage, and data discovery capabilities across Azure Databricks workspaces.</p> <p> </p> <p>In Unity Catalog, the hierarchy of primary data objects flows from metastore to table or volume:</p> <ul> <li>Metastore: The top-level container for metadata. Each metastore exposes a three-level namespace (catalog.schema.table) that organizes your data.</li> <li>Catalog: The first layer of the object hierarchy, used to organize your data assets.</li> <li>Schema: Also known as databases, schemas are the second layer of the object hierarchy and contain tables and views.</li> <li>Tables, views, and volumes: At the lowest level in the object hierarchy are tables, views, and volumes. Volumes provide governance for non-tabular data.</li> </ul> <p> Unity Catalog object model diagram <p> </p> <p>3 level namespace</p> <p>You reference all data in Unity Catalog using a three-level namespace: catalog.schema.asset, where asset can be a table, view, or volume.</p>"},{"location":"blog/2023/06/07/feature-engineering-using-databricks-/#metastores","title":"Metastores \ud83c\udfec","text":"<ul> <li>A metastore is the top-level container of objects in <code>Unity Catalog</code>.</li> <li>It registers metadata about data and AI assets and the permissions that govern access to them.</li> <li>Azure Databricks account admins should create one metastore for each region in which they operate and assign them to Azure Databricks workspaces in the same region.</li> <li>For a workspace to use Unity Catalog, it must have a Unity Catalog metastore attached.</li> </ul>"},{"location":"blog/2023/06/07/feature-engineering-using-databricks-/#external-tables","title":"External tables \u203c\ufe0f","text":"<p>External tables are tables whose data lifecycle and file layout are not managed by Unity Catalog. Use external tables to register large amounts of existing data in Unity Catalog, or if you require direct access to the data using tools outside of Azure Databricks clusters or Databricks SQL warehouses.</p> <p>Dropping an External Table</p> <p>When you <code>drop an external table</code>, Unity Catalog does not delete the underlying data. You can manage privileges on external tables and use them in queries in the same way as managed tables.</p>"},{"location":"blog/2023/06/07/feature-engineering-using-databricks-/#ip-access-lists","title":"IP access lists \u26a0\ufe0f","text":"<p>IP access lists enable you to restrict access to your Azure Databricks account and workspaces based on a user\u2019s IP address. For example, you can configure IP access lists to allow users to connect only through existing corporate networks with a secure perimeter. If the internal VPN network is authorized, users who are remote or traveling can use the VPN to connect to the corporate network. If a user attempts to connect to Azure Databricks from an insecure network, like from a coffee shop, access is blocked.</p> <p> IP Access list access check process <p> </p>"},{"location":"blog/2023/06/07/feature-engineering-using-databricks-/#udr-custom-route","title":"UDR/ Custom route \ud83d\ude97","text":"<p>If your Azure Databricks workspace is deployed to your own virtual network (VNet), you can use custom routes, also known as user-defined routes (UDR), to ensure that network traffic is routed correctly for your workspace. For example, if you connect the virtual network to your on-premises network, traffic may be routed through the on-premises network and unable to reach the Azure Databricks control plane. User-defined routes can solve that problem</p>"},{"location":"blog/2023/06/07/feature-engineering-using-databricks-/#private-link","title":"Private Link \ud83c\udf10","text":"<p>Private Link provides private connectivity from Azure VNets and on-premises networks to Azure services without exposing the traffic to the public network. Azure Databricks supports the following Private Link connection types:</p> <ol> <li> <p>Front-end Private Link (also known as user to workspace): A front-end Private Link connection allows users to connect to the Azure Databricks web application, REST API, and Databricks Connect API over a VNet interface endpoint. The front-end connection is also used by JDBC/ODBC and PowerBI integrations. The network traffic for a front-end Private Link connection between a transit VNet and the workspace control plane traverses over the Microsoft backbone network.</p> </li> <li> <p>Back-end Private Link (also known as compute plane to control plane): Databricks Runtime clusters in a customer-managed VNet (the compute plane) connect to an Azure Databricks workspace\u2019s core services (the control plane) in the Azure Databricks cloud account. This enables private connectivity from the clusters to the secure cluster connectivity relay endpoint and REST API endpoint.</p> </li> <li> <p>Browser authentication private endpoint: To support private front-end connections to the Azure Databricks web application for clients that have no public internet connectivity, you must add a browser authentication private endpoint to support single sign-on (SSO) login callbacks to the Azure Databricks web application from Microsoft Entra ID (formerly Azure Active Directory). If you allow connections from your network to the public internet, adding a browser authentication private endpoint is recommended but not required. A browser authentication private endpoint is a private connection with sub-resource type browser_authentication.</p> </li> </ol> <p> Unity Catalog object model diagram <p> </p>"},{"location":"blog/2023/06/07/feature-engineering-using-databricks-/#on-prem-connectivity","title":"On-Prem connectivity \ud83c\udfe2","text":"<p>Traffic is routed via a transit virtual network (VNet) to the on-premises network, using the following hub-and-spoke topology.</p> <p> On Prem connectivity diagram <p> </p>"},{"location":"blog/2023/06/07/feature-engineering-using-databricks-/#private-link_1","title":"Private Link \ud83d\udd17","text":"<p>The following diagram shows the network flow in a typical implementation of the Private Link simplified deployment:</p> <p> Private Link simplified deployment <p> </p> <p>The following diagram shows the network object architecture:</p> <p> Network object architecture <p> </p>"},{"location":"blog/2023/06/07/feature-engineering-using-databricks-/#workflows","title":"Workflows \u23f3","text":"<p><code>Azure Databricks Workflows</code> orchestrates data processing, machine learning, and analytics pipelines on the Databricks Data Intelligence Platform. Workflows has <code>fully managed orchestration services</code> integrated with the Databricks platform, including Azure Databricks Jobs to run non-interactive code in your Azure Databricks workspace and Delta Live Tables to build reliable and maintainable ETL pipelines.</p>"},{"location":"blog/2023/06/07/feature-engineering-using-databricks-/#jobs","title":"Jobs \ud83d\udc68\u200d\ud83c\udfa8","text":"<ul> <li> <p>An Azure Databricks job is a way to run your data processing and analysis applications in an Azure Databricks workspace.</p> </li> <li> <p>Your job can consist of a single task or can be a large, multi-task workflow with complex dependencies. </p> </li> <li> <p>Azure Databricks manages the task orchestration, cluster management, monitoring, and error reporting for all of your jobs.</p> </li> <li> <p>You can run your jobs immediately, periodically through an easy-to-use scheduling system, whenever new files arrive in an external location, or continuously to ensure an instance of the job is always running. </p> </li> <li> <p>You can also run jobs interactively in the notebook UI.</p> </li> </ul>"},{"location":"blog/2024/08/15/llm-as-a-judge-/","title":"LLM as a Judge \ud83e\uddd1\u200d\u2696\ufe0f","text":"<p>LLM-as-a-Judge is a powerful solution that uses LLMs to evaluate LLM responses based on any specific criteria of your choice, which means using LLMs to carry out LLM (system) evaluation.</p> <p>Potential issues with using LLM as a Judge?</p> <p>The non-deterministic nature of LLMs implies that even with controlled parameters, outputs may vary, raising concerns about the reliability of these judgments.</p> LLM Judge Prompt Example<pre><code>prompt = \"\"\"\nYou will be given 1 summary (LLM output) written for a news article published in Ottawa Daily. \nYour task is to rate the summary on how coherent it is to the original text (input). \n\nOriginal Text:\n{input}\n\nSummary:\n{llm_output}\n\nScore:\n\"\"\"\n</code></pre>"},{"location":"blog/2024/08/15/llm-as-a-judge-/#llm-metrics","title":"LLM Metrics \ud83d\udcca","text":"<ul> <li> <p><code>Recall@k</code>: It measures the proportion of all relevant documents retrieved in the top k results, and is crucial for ensuring the system captures a high percentage of pertinent information. </p> </li> <li> <p><code>Precision@k</code>: It complements this by measuring the proportion of retrieved documents that are relevant. </p> </li> <li> <p><code>Mean Average Precision (MAP)</code>: It provides an overall measure of retrieval quality across different recall levels. </p> </li> <li> <p><code>Normalized Discounted Cumulative Gain (NDCG)</code>: It is particularly valuable as it considers both the relevance and ranking of retrieved documents.</p> </li> </ul>"},{"location":"blog/2024/08/15/llm-as-a-judge-/#llm-metric-types","title":"LLM Metric Types \u2390","text":"<p>Metrics for LLM calls can be broken up into two categories</p> <ul> <li>Absolute</li> <li>Subjective</li> </ul>"},{"location":"blog/2024/08/15/llm-as-a-judge-/#absolute-metrics","title":"Absolute Metrics","text":"<p>These metrics like latency, throughput, etc are easier to calculate. </p>"},{"location":"blog/2024/08/15/llm-as-a-judge-/#subjective-metrics","title":"Subjective Metrics","text":"<p>They are more difficult to calculate. These subjective categories range from <code>truthfulness</code>, <code>faithfulness</code>, <code>answer relevancy</code>, to any custom metric your business cares about.</p> <p>How to find the relavancy for Subjective metrics?</p> <p>Typically, in all the subjective metrics, it requires a level of human reasoning to determine a numeric answer. Techniques used for evaluation are:</p>"},{"location":"blog/2024/08/15/llm-as-a-judge-/#1-human-evaluators","title":"1. Human Evaluators","text":"<p>This is a time intensive process although sometimes its considered as gold standard. It requires humans to go through and evaluate your answer. You need to select the humans carefully and make sure their instructions on how to grade are clear</p> <p>It\u2019s not unusual for a real-world LLM application to generate approximately 100,000 responses a month. I don\u2019t know about you, but it takes me about 60 seconds on average to read through a few paragraphs and make a judgment about it. That adds up to around 6 million seconds, or about 65 consecutive days each month\u200a\u2014\u200awithout taking lunch breaks\u200a\u2014\u200ato evaluate every single generated LLM responses.</p>"},{"location":"blog/2024/08/15/llm-as-a-judge-/#2-llms-as-a-judge","title":"2. LLM's as a Judge","text":"<p>To use LLM-as-a-judge, you have to iterate on a prompt until the human annotators generally agree with the LLMs grades. An evaluation dataset should be created and graded by a human.</p>"},{"location":"blog/2024/08/15/llm-as-a-judge-/#single-layer-judge","title":"Single Layer Judge \u00b7","text":"<p>The flow for single layer Judge is shown below</p> <p> </p>"},{"location":"blog/2024/08/15/llm-as-a-judge-/#muti-layered-judgements","title":"Muti Layered Judgements \u2d58","text":"<p>We can also use a master LLM judge to judge the judgement of First level Judge for getting better recall</p> <p> </p> <p>Why are we using Sampling?</p> <p>It is also worth noting that using a random sampling method for evaluation might be a good approach to save resources</p>"},{"location":"blog/2024/08/15/llm-as-a-judge-/#how-to-improve-llm-judgements","title":"How to improve LLM Judgements? \ud83d\udcc8","text":"<ul> <li>Use <code>Chain of Thought (CoT)</code> Prompting by asking the reasoning process</li> <li>Use <code>Few shot Prompting</code>: This approach can be more computationally expensive</li> <li>Provide a <code>reference guide for Judgements</code></li> <li>Evaluate based on <code>QAG (Question Answer Generation)</code></li> </ul>"},{"location":"blog/2024/10/08/quantization-in-llms-/","title":"Quantization in LLMs \ud83c\udf10","text":"<p>In the rapidly evolving world of artificial intelligence, large language models (LLMs) have become pivotal in various applications, from chatbots to recommendation systems. However, deploying these advanced models can be challenging due to high memory and computational requirements. </p> <p>This is where quantization comes into play! </p> <p>Do you know?</p> <p><code>GPT-3.5</code> has around 175 billion parameters, while the current state-of-the-art <code>GPT-4</code> has in excess of 1 trillion parameters. </p> <p>In this blog, let\u2019s explore how <code>quantization</code> can make LLMs more efficient, accessible, and ready for deployment on edge devices. \ud83c\udf0d</p>"},{"location":"blog/2024/10/08/quantization-in-llms-/#what-is-quantization","title":"What is Quantization? \ud83e\udd14","text":"<p>Quantization is a procedure that maps the range of high precision weight values, like <code>FP32</code>, into lower precision values such as <code>FP16</code> or even <code>INT8</code> (8-bit Integer) data types. By reducing the precision of the weights, we create a more compact version of the model without significantly losing accuracy. </p> <p>Tldr</p> <p>Quantization transforms high precision weights into lower precision formats to optimize resource usage without sacrificing performance.</p>"},{"location":"blog/2024/10/08/quantization-in-llms-/#why-quantize","title":"Why Quantize? \ud83c\udf1f","text":"<p>Here are a few compelling reasons to consider quantization:</p> <ol> <li> <p>Reduced Memory Footprint \ud83d\uddc4\ufe0f    Quantization dramatically lowers memory requirements, making it possible to deploy LLMs on lower-end machines and edge devices. This is particularly important as many edge devices only support integer data types for storage.</p> <p> </p> </li> <li> <p>Faster Inference \u26a1    Lower precision computations (such as integers) are inherently faster than higher precision (floats). By using quantized weights, mathematical operations during inference speed up significantly. Plus, modern CPUs and GPUs have specialized instructions designed for lower-precision computations, allowing you to take full advantage of hardware acceleration for even better performance!</p> </li> <li> <p>Reduced Energy Consumption \ud83d\udd0b    Many contemporary hardware accelerators are optimized for lower-precision operations, capable of performing more calculations per watt of energy when models are quantized. This is a win-win for efficiency and sustainability!</p> </li> </ol>"},{"location":"blog/2024/10/08/quantization-in-llms-/#linear-quantization","title":"Linear Quantization \ud83d\udccf","text":"<p>In linear quantization, we essentially perform scaling within a specified range. Here, the minimum value <code>(Rmin</code>) is mapped to its quantized minimum (<code>Qmin</code>), and the maximum (<code>Rmax</code>) to its quantized counterpart (<code>Qmax</code>). </p> <p>The zero in the actual range corresponds to a specific <code>zero_point</code> in the quantized range, allowing for efficient mapping and representation.</p> <p> </p> <p>To achieve quantization, we need to find the optimum way to project our range of FP32 weight values, which we\u2019ll label [min, max] to the INT4 space: one method of implementing this is called the affine quantization scheme, which is shown in the formula below:</p> <p>$$   x_q = round(x/S + Z)   $$</p> <p>where:</p> <ul> <li> <p>x_q: the quantized INT4 value that corresponds to the FP32 value x</p> </li> <li> <p>S: an FP32 scaling factor and is a positive float32</p> </li> <li> <p>Zthe zero-point: the INT4 value that corresponds to 0 in the FP32 space</p> </li> <li> <p>round: refers to the rounding of the resultant value to the closest integer</p> </li> </ul>"},{"location":"blog/2024/10/08/quantization-in-llms-/#types-of-quantization","title":"Types of Quantization","text":""},{"location":"blog/2024/10/08/quantization-in-llms-/#ptq","title":"PTQ \ud83d\udee0\ufe0f","text":"<p>As the name suggests, <code>Post Training Quantization (PTQ)</code> occurs after the LLM training phase. </p> <p>In this process, the model\u2019s weights are converted from higher precision formats to lower precision types, applicable to both weights and activations. While this enhances speed, memory efficiency, and power usage, it comes with an accuracy trade-off.</p> <p>Beware of Quantizaion Error</p> <p>During quantization, rounding or truncation introduces quantization error, which can affect the model\u2019s ability to capture fine details in weights.</p>"},{"location":"blog/2024/10/08/quantization-in-llms-/#qat","title":"QAT \u23f0","text":"<p><code>Quantization-Aware Training (QAT)</code> refers to methods of fine-tuning on data with quantization in mind. In contrast to PTQ techniques, QAT integrates the weight conversion process, i.e., calibration, range estimation, clipping, rounding, etc., during the training stage. This often results in superior model performance, but is more computationally demanding. </p> <p>Tip</p> <p><code>PTQ</code> is easier to implement than <code>QAT</code>, as it requires less training data and is faster. However, it can also result in reduced model accuracy from lost precision in the value of the weights. </p>"},{"location":"blog/2024/10/08/quantization-in-llms-/#final-thoughts","title":"Final Thoughts \ud83d\udcad","text":"<p>Quantization is not just a technical detail; it's a game-changer for making LLMs accessible and cost-effective. </p> <p>By leveraging this technique, developers can democratize AI technology and deploy sophisticated language models on everyday CPUs.</p> <p>So, whether you\u2019re building intelligent chatbots, personalized recommendation engines, or innovative code generators, don\u2019t forget to incorporate quantization into your toolkit\u2014it might just be your secret weapon! \ud83d\ude80</p> <p>Happy learning \ud83e\uddd1\u200d\ud83c\udfeb</p>"},{"location":"blog/2024/07/20/prompt-engineering/","title":"Prompt engineering","text":""},{"location":"blog/2024/07/20/prompt-engineering/#prompt-engineering-techniques","title":"Prompt Engineering Techniques","text":""},{"location":"blog/2024/07/20/prompt-engineering/#prompting-best-practices","title":"Prompting best practices","text":"<ul> <li> <p>Be precise in saying what to do (write, summarize, extract information).</p> </li> <li> <p>Avoid saying what <code>not to do</code> and say <code>what to do instead</code></p> </li> <li> <p>Be specific: instead of saying \u201cin a few sentences\u201d, say \u201cin 2\u20133 sentences\u201d.</p> </li> <li> <p>Add tags or delimiters to structurize the prompt.</p> </li> <li> <p>Ask for a structured output (JSON. HTML) if needed.</p> </li> <li> <p>Ask the model to verify whether the conditions are satisfied (e.g. \u201cif you do not know the answer. say \u201cNo information\u201d).</p> </li> <li> <p>Ask a model to first explain and then provide the answer (otherwise a model may try to justify an incorrect answer).</p> </li> </ul>"},{"location":"blog/2024/07/20/prompt-engineering/#prompting-types","title":"Prompting Types","text":""},{"location":"blog/2024/07/20/prompt-engineering/#zero-shot-learning","title":"Zero-Shot Learning","text":"<p>This involves giving the AI a task without any prior examples. You describe what you want in detail, assuming the AI has no prior knowledge of the task.</p>"},{"location":"blog/2024/07/20/prompt-engineering/#one-shot-learning","title":"One-Shot Learning","text":"<p>You provide one example along with your prompt. This helps the AI understand the context or format you\u2019re expecting.</p>"},{"location":"blog/2024/07/20/prompt-engineering/#few-shot-prompting","title":"Few-Shot Prompting","text":"<p>This involves providing a few examples (usually 2\u20135) to help the AI understand the pattern or style of the response you\u2019re looking for.</p> <p>It is definitely more computationally expensive as you\u2019ll be including more input tokens </p>"},{"location":"blog/2024/07/20/prompt-engineering/#chain-of-thought-prompting","title":"Chain of Thought Prompting","text":"<p><code>Chain-of-thought (CoT) prompting</code> is an approach where the model is prompted to articulate its reasoning process</p> <p>Tip</p> <p>In the context of using <code>CoTs</code> for <code>LLM judges</code>, it involves including detailed evaluation steps in the prompt instead of vague, high-level criteria to help a judge LLM perform more accurate and reliable evaluations.</p>"},{"location":"blog/2024/07/20/prompt-engineering/#iterative-prompting","title":"Iterative Prompting","text":"<p>This is a process where you refine your prompt based on the outputs you get, slowly guiding the AI to the desired answer or style of answer.</p>"},{"location":"blog/2024/07/20/prompt-engineering/#negative-prompting","title":"Negative Prompting","text":"<p>In this method, you tell the AI what not to do. For instance, you might specify that you don\u2019t want a certain type of content in the response.</p>"},{"location":"blog/2024/07/20/prompt-engineering/#hybrid-prompting","title":"Hybrid Prompting","text":"<p>Combining different methods, like few-shot with chain-of-thought, to get more precise or creative outputs.</p>"},{"location":"blog/2024/07/20/prompt-engineering/#prompt-chaining","title":"Prompt Chaining","text":"<p>Breaking down a complex task into smaller prompts and then chaining the outputs together to form a final response.</p>"},{"location":"blog/2024/07/20/prompt-engineering/#voting","title":"Voting","text":""},{"location":"blog/2024/07/20/prompt-engineering/#divide-and-conquer-prompting","title":"Divide and Conquer Prompting","text":"<p>With external tools</p>"},{"location":"blog/2024/07/20/prompt-engineering/#rag","title":"RAG","text":""},{"location":"blog/2024/07/20/prompt-engineering/#react","title":"ReAct","text":"<p>ReAct, short for \u201cReasoning and Acting,\u201d is a groundbreaking approach that combines reasoning traces and task-specific actions within LLMs. This integration allows the model to perform dynamic and context-aware decision-making, bridging the gap between reasoning and acting, which were traditionally studied as separate topics.</p>"},{"location":"blog/2024/06/10/rag-types/","title":"RAG Types","text":"<p>Large Language Models (LLMs) are trained on vast volumes of data and use billions of parameters to generate original output for tasks like answering questions, translating languages, and completing sentences. </p> <p>RAG extends the already powerful capabilities of LLMs to specific domains or an organization's <code>internal knowledge base</code>, all without the need to <code>retrain the model</code>.</p> <p>Why RAG was needed?</p> <p>Lets say we have a goal to create bots that can answer user questions in various contexts by cross-referencing authoritative knowledge sources. Unfortunately, the nature of LLM technology introduces unpredictability in LLM responses. Additionally, LLM training data is static and introduces a cut-off date on the knowledge it has.</p> <p>You can think of the Large Language Model as an over-enthusiastic new employee who refuses to stay informed with current events but will always answer every question with absolute confidence. Unfortunately, such an attitude can negatively impact user trust and is not something you want your chatbots to emulate!</p> <p>RAG is one approach to solving some of these challenges. It redirects the LLM to retrieve relevant information from authoritative, pre-determined knowledge sources.</p>"},{"location":"blog/2024/06/10/rag-types/#benefits-of-rag","title":"Benefits of RAG","text":"<ul> <li> <p>User Trust: RAG allows the LLM to present accurate information with source attribution. The output can include citations or references to sources. Users can also look up source documents themselves if they require further clarification or more detail. This can increase trust and confidence in your generative AI solution.</p> </li> <li> <p>Latest information: RAG allows developers to provide the latest research, statistics, or news to the generative models. They can use RAG to connect the LLM directly to live social media feeds, news sites, or other frequently-updated information sources. The LLM can then provide the latest information to the users.</p> </li> <li> <p>More control on output: With RAG, developers can test and improve their chat applications more efficiently. They can control and change the LLM's information sources to adapt to changing requirements or cross-functional usage. Developers can also restrict sensitive information retrieval to different authorization levels and ensure the LLM generates appropriate responses. </p> </li> </ul>"},{"location":"blog/2024/06/10/rag-types/#rag-steps","title":"RAG Steps","text":"<ul> <li>User input is converted to embedding vectors using an embedding model</li> <li>Embeddings are saved in Vector Database</li> <li>Vector Database runs a similarity search to find the related content</li> <li>Question + Context is our final prompt which is sent to LLM</li> </ul>"},{"location":"blog/2024/06/10/sagemaker-deployment-options/","title":"SageMaker Deployment Options","text":""},{"location":"blog/2024/06/10/sagemaker-deployment-options/#basics","title":"Basics","text":""},{"location":"blog/2024/06/10/sagemaker-deployment-options/#sagemaker-domain","title":"Sagemaker Domain","text":"<p>Amazon SageMaker uses domains to organize user profiles, applications, and their associated resources. An Amazon SageMaker domain consists of the following:</p> <ul> <li>An associated Amazon <code>Elastic File System</code> (Amazon EFS) volume.</li> <li>A list of authorized users.</li> <li>A variety of security, application, policy, and Amazon Virtual Private Cloud (Amazon VPC) configurations.</li> </ul>"},{"location":"blog/2024/06/10/sagemaker-deployment-options/#user-profile","title":"User Profile \ud83d\udc68\u200d\ud83e\uddb0","text":""},{"location":"blog/2024/06/10/sagemaker-deployment-options/#studio-vs-notebook-instance","title":"Studio vs Notebook Instance \ud83d\uddd2\ufe0f","text":"<ul> <li> <p>Notebooks are jupyter notebook where resource allocation are done by AWS. So they are fully managed</p> </li> <li> <p>Studio is IDE which is not fully managed</p> </li> </ul>"},{"location":"blog/2024/06/10/sagemaker-deployment-options/#features","title":"Features","text":""},{"location":"blog/2024/06/10/sagemaker-deployment-options/#prepare","title":"Prepare \ud83e\uddd1\u200d\ud83c\udf73","text":"<ul> <li>Ground Truth: Enable DS to label jobs, datasets and workforces.</li> <li>Clarify: Detect bias and understand model predictions.</li> <li>Data wrangler: Filtering, joining and other data preparation tasks. </li> </ul>"},{"location":"blog/2024/06/10/sagemaker-deployment-options/#build","title":"Build \ud83d\udc77","text":"<ul> <li>Autopilot: Automatically create models</li> <li>Built-in: </li> <li>JumpStart:</li> </ul>"},{"location":"blog/2024/06/10/sagemaker-deployment-options/#train","title":"Train \ud83d\ude8a","text":"<ul> <li>Experiments:</li> <li>Managed Train: Fully managed training by AWS to reduce the cost.</li> <li>Distributed training:  </li> </ul>"},{"location":"blog/2024/06/10/sagemaker-deployment-options/#deploy","title":"Deploy \ud83d\ude9f","text":"<ul> <li>Model Monitoring:</li> <li>Endpoints: Multi model endpoint, Multi container endpoint.</li> <li>Inference: Based on traffic patterns, we can have following </li> <li>Real time: For persistent, one prediction at time</li> <li>Serverless: Workloads which can tolerate idle periods between spikes and can tolerate cold starts</li> <li>Batch: To get predictions for an entire dataset, use SageMaker batch transform</li> <li>Async: Requests with large payload sizes up to 1GB, long processing times, and near real-time latency requirements, use Amazon SageMaker Asynchronous Inference</li> </ul>"},{"location":"blog/2024/06/10/sagemaker-deployment-options/#inference-types","title":"Inference Types","text":""},{"location":"blog/2024/06/10/sagemaker-deployment-options/#batch-inference","title":"Batch Inference \ud83d\udce6","text":"<p>Batch inference refers to model inference performed on data that is in batches, often large batches, and asynchronous in nature. It fits use cases that collect data infrequently, that focus on group statistics rather than individual inference, and that do not need to have inference results right away for downstream processes. </p> <p>Projects that are <code>research oriented</code>, for example, do not require model inference to be returned for a data point right away. Researchers often collect a chunk of data for testing and evaluation purposes and care about overall statistics and performance rather than individual predictions. They can conduct the inference in batches and wait for the prediction for the whole batch to complete before they move on</p> <p>Doing batch transform in S3 \ud83e\udea3</p> <p>Depending on how you organize the data, SageMaker batch transform can split a single large text file in S3 by lines into a small and manageable size (mini-batch) that would fit into the memory before making inference against the model; it can also distribute the files by S3 key into compute instances for efficient computation.</p> <p>SageMaker batch transform saves the results after assembly to the specified S3 location with <code>.out</code> appended to the input filename.</p>"},{"location":"blog/2024/06/10/sagemaker-deployment-options/#async-transformation","title":"Async Transformation \u2699\ufe0f","text":"<p>Amazon SageMaker Asynchronous Inference is a capability in SageMaker that queues incoming requests and processes them asynchronously. This option is ideal for requests with large payload sizes (up to 1GB), long processing times (up to one hour), and near real-time latency requirements. Asynchronous Inference enables you to save on costs by autoscaling the instance count to zero when there are no requests to process, so you only pay when your endpoint is processing requests.</p>"},{"location":"blog/2024/06/10/sagemaker-deployment-options/#real-time-inference","title":"Real-time Inference \u23f3","text":"<p>In today's fast-paced digital environment, receiving inference results for incoming data points in real-time is crucial for effective decision-making. </p> <p>Take interactive chatbots, for instance; they rely on live inference capabilities to operate effectively. Users expect instant responses during their conversations\u2014waiting until the discussion is over or enduring delays of even a few seconds is simply not acceptable. </p> <p>For companies striving to deliver top-notch customer experiences, ensuring that inferences are generated and communicated to customers instantly is a top priority.</p>"},{"location":"blog/2024/06/10/sagemaker-deployment-options/#host-real-time-endpoints","title":"Host real-time endpoints","text":"<p>The deployment process consists of the following steps:</p> <ol> <li> <p>Create a <code>model</code>, <code>container</code>, and associated <code>inference code</code> in SageMaker. The model refers to the training artifact, model.tar.gz. The container is the runtime environment for the code and the model.</p> </li> <li> <p>Create an HTTPS endpoint configuration. This configuration carries information about <code>compute instance type</code> and quantity, models, and traffic patterns to model variants.</p> </li> <li> <p>Create ML instances and an HTTPS endpoint. SageMaker creates a fleet of ML instances and an HTTPS endpoint that handles the traffic and authentication. The final step is to put everything together for a working HTTPS endpoint that can interact with client-side requests.</p> </li> </ol>"},{"location":"blog/2024/06/10/sagemaker-deployment-options/#serverless-inference","title":"Serverless Inference \ud83d\udda5\ufe0f","text":"<p>On-demand Serverless Inference is ideal for workloads which have idle periods between traffic spurts and can tolerate cold starts. Serverless endpoints automatically launch compute resources and scale them in and out depending on traffic, eliminating the need to choose instance types or manage scaling policies. This takes away the undifferentiated heavy lifting of selecting and managing servers.</p> <p>Serverless Inference integrates with AWS Lambda to offer you high availability, built-in fault tolerance and automatic scaling. With a pay-per-use model, Serverless Inference is a cost-effective option if you have an infrequent or unpredictable traffic pattern.</p> <p>During times when there are no requests, Serverless Inference scales your endpoint down to 0, helping you to minimize your costs. </p> Invoke Serverless Endpoint<pre><code>runtime = boto3.client(\"sagemaker-runtime\")\n\nendpoint_name = \"&lt;your-endpoint-name&gt;\"\ncontent_type = \"&lt;request-mime-type&gt;\"\npayload = &lt;your-request-body&gt;\n\nresponse = runtime.invoke_endpoint(\n    EndpointName=endpoint_name,\n    ContentType=content_type,\n    Body=payload\n)\n</code></pre> <p>Possible cold-start during updating serverless endpoint</p> <p>Note that when updating your endpoint, you can experience cold starts when making requests to the endpoint because SageMaker must re-initialize your container and model.</p> <pre><code>response = client.update_endpoint(\n  EndpointName=\"&lt;your-endpoint-name&gt;\",\n  EndpointConfigName=\"&lt;new-endpoint-config&gt;\",\n)\n</code></pre>"},{"location":"blog/2024/06/10/sagemaker-deployment-options/#use-provisioned-concurrency","title":"Use Provisioned Concurrency","text":"<p>Amazon SageMaker automatically scales in or out on-demand serverless endpoints. For serverless endpoints with Provisioned Concurrency you can use Application Auto Scaling to scale up or down the Provisioned Concurrency based on your traffic profile, thus optimizing costs. </p>"},{"location":"blog/2024/06/10/sagemaker-deployment-options/#optimizations","title":"Optimizations","text":""},{"location":"blog/2024/06/10/sagemaker-deployment-options/#multi-model-endpoints","title":"Multi Model Endpoints \u27dc","text":"<p>Amazon SageMaker multi-model endpoint (MME) enables you to cost-effectively deploy and host multiple models in a single endpoint and then horizontally scale the endpoint to achieve scale. </p> <p>When to use MME?</p> <p>Multi-model endpoints are suitable for use cases where you have models that are built in the same framework (XGBoost in this example), and where it is tolerable to have latency on less frequently used models.</p> <p>As illustrated in the following figure, this is an effective technique to implement multi-tenancy of models within your machine learning (ML) infrastructure. </p> <p> </p> <p>Use Cases</p> <p>Shared tenancy: <code>SageMaker multi-model endpoints</code> are well suited for hosting a large number of models that you can serve through a shared serving container and you don\u2019t need to access all the models at the same time. Depending on the size of the endpoint instance memory, a model may occasionally be unloaded from memory in favor of loading a new model to maximize efficient use of memory, therefore your application needs to be tolerant of occasional latency spikes on unloaded models.</p> <p>Co host models: MME is also designed for co-hosting models that use the same ML framework because they use the shared container to load multiple models. Therefore, if you have a mix of ML frameworks in your model fleet (such as PyTorch and TensorFlow), SageMaker dedicated endpoints or multi-container hosting is a better choice.</p> <p>Ability to handle cold starts: Finally, MME is suited for applications that can tolerate an occasional cold start latency penalty, because models are loaded on first invocation and infrequently used models can be offloaded from memory in favor of loading new models. Therefore, if you have a mix of frequently and infrequently accessed models, a multi-model endpoint can efficiently serve this traffic with fewer resources and higher cost savings.</p> <p>Host both CPU and GPU: Multi-model endpoints support hosting both CPU and GPU backed models. By using GPU backed models, you can lower your model deployment costs through increased usage of the endpoint and its underlying accelerated compute instances.</p>"},{"location":"blog/2024/06/10/sagemaker-deployment-options/#advanced-configs","title":"Advanced Configs","text":"<p>Increase inference parallelism per model</p> <p>MMS creates one or more Python worker processes per model based on the value of the <code>default_workers_per_model</code> configuration parameter. These Python workers handle each individual inference request by running any preprocessing, prediction, and post processing functions you provide. </p> <p>Design for spikes</p> <p>Each MMS process within an endpoint instance has a request queue that can be configured with the job_queue_size parameter (default is 100). This determines the number of requests MMS will queue when all worker processes are busy. Use this parameter to fine-tune the responsiveness of your endpoint instances after you\u2019ve decided on the optimal number of workers per model.</p> <p>Setup AutoScaling</p> <p>Before you can use auto scaling, you must have already created an <code>Amazon SageMaker model endpoint</code>. You can have multiple model versions for the same endpoit. Each model is referred to as a production (model) variant.</p> <p>Setup cooldown policy</p> <p>A cooldown period is used to protect against over-scaling when your model is scaling in (reducing capacity) or scaling out (increasing capacity). It does this by slowing down subsequent scaling activities until the period expires. Specifically, it blocks the deletion of instances for scale-in requests, and limits the creation of instances for scale-out requests. </p>"},{"location":"blog/2024/06/10/sagemaker-deployment-options/#muti-container-endpoints","title":"Muti-container Endpoints \u22fa","text":"<p>SageMaker multi-container endpoints enable customers to deploy multiple containers, that use different models or frameworks (XGBoost, Pytorch, TensorFlow), on a <code>single SageMaker endpoint</code>. </p> <p>The containers can be run in a sequence as an inference pipeline, or each container can be accessed individually by using direct invocation to improve endpoint utilization and optimize costs.</p>"},{"location":"blog/2024/06/10/sagemaker-deployment-options/#load-testing","title":"Load Testing \ud83d\ude9b","text":"<p>Load testing is a technique that allows us to understand how our ML model hosted in an endpoint with a compute resource configuration responds to online traffic. </p> <p>There are factors such as model size, ML framework, number of CPUs, amount of RAM, autoscaling policy, and traffic size that affect how your ML model performs in the cloud. Understandably, it's not easy to predict how many requests can come to an endpoint over time. </p> <p>It is prudent to understand how your model and endpoint behave in this complex situation. Load testing creates artificial traffic and requests to your endpoint and stress tests how your model and endpoint respond in terms of model latency, instance CPU utilization, memory footprint, and so on.</p>"},{"location":"blog/2024/06/10/sagemaker-deployment-options/#elastic-interface-ei","title":"Elastic Interface (EI)","text":"<p>EI attaches fractional GPUs to a SageMaker hosted endpoint. It increases the inference throughput and decreases the model latency for your deep learning models that can benefit from GPU acceleration</p>"},{"location":"blog/2024/06/10/sagemaker-deployment-options/#sagemaker-neo","title":"SageMaker Neo \ud83d\udc7e","text":"<p>Neo is a capability of Amazon SageMaker that enables machine learning models to train once and run anywhere in the cloud and at the edge. </p> <p>Why use NEO</p> <p>Problem Statement</p> <p>Generally, optimizing machine learning models for inference on multiple platforms is difficult because you need to hand-tune models for the specific hardware and software configuration of each platform. If you want to get optimal performance for a given workload, you need to know the hardware architecture, instruction set, memory access patterns, and input data shapes, among other factors. For traditional software development, tools such as compilers and profilers simplify the process. For machine learning, most tools are specific to the framework or to the hardware. This forces you into a manual trial-and-error process that is unreliable and unproductive.</p> <p>Solution</p> <p>Neo automatically optimizes Gluon, Keras, MXNet, PyTorch, TensorFlow, TensorFlow-Lite, and ONNX models for inference on Android, Linux, and Windows machines based on processors from Ambarella, ARM, Intel, Nvidia, NXP, Qualcomm, Texas Instruments, and Xilinx. Neo is tested with computer vision models available in the model zoos across the frameworks. SageMaker Neo supports compilation and deployment for two main platforms:</p> <ul> <li>Cloud instances (including Inferentia)</li> <li>Edge devices.</li> </ul>"},{"location":"blog/2024/06/10/sagemaker-deployment-options/#ec2-inf1-instances","title":"EC2 inf1 instances \ud83c\udf6a","text":"<p>Inf1 provide high performance and low cost in thecloud with AWS Inferentia chips designed and built by AWS for ML inference purposes.You can compile supported ML models using SageMaker Neo and select Inf1 instances todeploy the compiled model in a SageMaker hosted endpoint.</p>"},{"location":"blog/2024/06/10/sagemaker-deployment-options/#optimize-llms-on-sagemaker","title":"Optimize LLM's on Sagemaker","text":""},{"location":"blog/2024/06/10/sagemaker-deployment-options/#lmi-containers","title":"LMI containers \ud83d\udea2","text":"<p>LMI containers are a set of high-performance Docker Containers purpose built for large language model (LLM) inference.</p> <p>With these containers, you can leverage high performance open-source inference libraries like vLLM, TensorRT-LLM, Transformers NeuronX to deploy LLMs on AWS SageMaker Endpoints. These containers bundle together a model server with open-source inference libraries to deliver an all-in-one LLM serving solution.</p>"},{"location":"blog/2024/07/20/mlflow-in-sagemaker/","title":"MLFlow in SageMaker","text":""},{"location":"blog/2024/07/20/mlflow-in-sagemaker/#mlflow-capabilities","title":"MLFlow Capabilities","text":"<p>SageMaker features a capability called <code>Bring Your Own Container (BYOC)</code>, which allows you to run <code>custom Docker containers</code> on the inference endpoint. These containers must meet specific requirements, such as running a web server that exposes certain REST endpoints, having a designated container entrypoint, setting environment variables, etc. Writing a Dockerfile and serving script that meets these requirements can be a tedious task.</p> <p>How MLFlow integrates with S3 and ECR?</p> <p>MLflow automates the process by building a <code>Docker image</code> from the MLflow Model on your behalf. Subsequently, it pushed the image to <code>Elastic Container Registry</code> and creates a <code>SageMaker endpoint</code> using this image. It also uploads the model artifact to an <code>S3 bucket</code> and configures the endpoint to download the model from there.</p> <p>The container provides the same REST endpoints as a local inference server. For instance, the <code>/invocations</code> endpoint accepts CSV and JSON input data and returns prediction results. </p>"},{"location":"blog/2024/07/20/mlflow-in-sagemaker/#step-1-run-model-locally","title":"Step 1. Run model locally","text":"<p>It\u2019s recommended to test your model locally before deploying it to a production environment. The mlflow deployments run-local command deploys the model in a Docker container with an identical image and environment configuration, making it ideal for pre-deployment testing.</p> <pre><code>$ mlflow deployments run-local -t sagemaker -m runs:/&lt;run_id&gt;/model -p 5000\n</code></pre> <p>You can then test the model by sending a POST request to the endpoint:</p> <pre><code>$ curl -X POST -H \"Content-Type:application/json; format=pandas-split\" --data '{\"columns\":[\"a\",\"b\"],\"data\":[[1,2]]}' http://localhost:5000/invocations\n</code></pre>"},{"location":"blog/2024/07/20/mlflow-in-sagemaker/#step-2-build-a-docker-image-and-push-to-ecr","title":"Step 2. Build a Docker Image and Push to ECR","text":"<p>The mlflow sagemaker build-and-push-container command builds a Docker image compatible with SageMaker and uploads it to ECR.</p> <pre><code>$ mlflow sagemaker build-and-push-container  -m runs:/&lt;run_id&gt;/model\n</code></pre>"},{"location":"blog/2024/07/20/mlflow-in-sagemaker/#step-3-deploy-to-sagemaker-endpoint","title":"Step 3. Deploy to SageMaker Endpoint","text":"<p>The mlflow deployments create command deploys the model to an Amazon SageMaker endpoint. MLflow uploads the Python Function model to S3 and automatically initiates an Amazon SageMaker endpoint serving the model.</p> <pre><code>$ mlflow deployments create -t sagemaker -m runs:/&lt;run_id&gt;/model \\\n    -C region_name=&lt;your-region&gt; \\\n    -C instance-type=ml.m4.xlarge \\\n    -C instance-count=1 \\\n    -C env='{\"DISABLE_NGINX\": \"true\"}''\n</code></pre>"},{"location":"blog/2024/07/20/what-are-embeddings/","title":"What are embeddings","text":""},{"location":"blog/2024/07/20/what-are-embeddings/#what-are-embeddings","title":"What are embeddings?","text":"<p>Embeddings are numerical representations of real-world objects that machine learning (ML) and artificial intelligence (AI) systems use to understand complex knowledge domains like humans do.</p> <p>Example</p> <p>A <code>bird-nest</code> and a <code>lion-den</code> are analogous pairs, while <code>day-night</code> are opposite terms. Embeddings convert real-world objects into complex mathematical representations that capture inherent properties and relationships between real-world data. The entire process is automated, with AI systems self-creating embeddings during training and using them as needed to complete new tasks.</p>"},{"location":"blog/2024/07/20/what-are-embeddings/#advantages-of-using-embeddings","title":"Advantages of using embeddings","text":""},{"location":"blog/2024/07/20/what-are-embeddings/#dimentionality-reduction","title":"Dimentionality reduction:","text":"<p>DS use embeddings to represent <code>high-dimensional data</code> in a <code>low-dimensional space</code>. In data science, the term dimension typically refers to a feature or attribute of the data. Higher-dimensional data in AI refers to datasets with many features or attributes that define each data point.</p>"},{"location":"blog/2024/07/20/what-are-embeddings/#train-large-language-models","title":"Train large language Models","text":"<p>Embeddings improve data quality when training/re-training large language models (LLMs).</p>"},{"location":"blog/2024/07/20/what-are-embeddings/#types-of-embeddings","title":"Types of embeddings","text":"<ul> <li> <p>Image Embeddigns - With image embeddings, engineers can build high-precision computer vision applications for object detection, image recognition, and other visual-related tasks. </p> </li> <li> <p>Word Embeddings - With word embeddings, natural language processing software can more accurately understand the context and relationships of words.</p> </li> <li> <p>Graph Embeddings - <code>Graph embeddings</code> extract and categorize related information from interconnected nodes to support <code>network analysis</code>.</p> </li> </ul>"},{"location":"blog/2024/07/20/what-are-embeddings/#what-are-vectors","title":"What are Vectors?","text":"<p><code>ML models</code> cannot interpret information intelligibly in their raw format and require numerical data as input. They use <code>neural network embeddings</code> to convert real-word information into numerical representations called vectors.</p> <p>Vectors are numerical values that represent information in a multi-dimensional space. They help ML models to find similarities among sparsely distributed items. </p> <pre><code>The Conference (Horror, 2023, Movie)\n\nUpload (Comedy, 2023, TV Show, Season 3)\n\nCrypt Tales (Horror, 1989, TV Show, Season 7)\n\nDream Scenario (Horror-Comedy, 2023, Movie)\n</code></pre> <p>Their embeddings are shown below</p> <pre><code>The Conference (1.2, 2023, 20.0)\n\nUpload (2.3, 2023, 35.5)\n\nCrypt Tales (1.2, 1989, 36.7)\n\nDream Scenario (1.8, 2023, 20.0)\n</code></pre>"},{"location":"blog/2024/07/20/what-are-embeddings/#embedding-models","title":"Embedding Models?","text":"<p>Data scientists use <code>embedding models</code> to enable <code>ML models</code> to comprehend and reason with high-dimensional data.</p> <p>Types of embedding models are shown below</p>"},{"location":"blog/2024/07/20/what-are-embeddings/#pca","title":"PCA","text":"<p><code>Principal component analysis</code> (PCA) is a dimensionality-reduction technique that reduces complex data types into low-dimensional vectors. It finds data points with similarities and compresses them into embedding vectors that reflect the original data.</p>"},{"location":"blog/2024/07/20/what-are-embeddings/#svd","title":"SVD","text":"<p><code>Singular value decomposition</code> (SVD) is an embedding model that transforms a matrix into its singular matrices. The resulting matrices retain the original information while allowing models to better comprehend the semantic relationships of the data they represent.</p>"},{"location":"databricks/data-lakehouse/","title":"Data Lakehouse","text":""},{"location":"databricks/data-lakehouse/#data-lake","title":"Data Lake","text":"<ul> <li>Streaming support into data lake </li> <li>Structured, un-structured and semi-structured data support</li> <li>Support for AI and ML</li> </ul> <p>Issues with data lake</p> <p>They are not for transactional needs</p> <p> </p>"},{"location":"databricks/db-arch/","title":"Databricks Architecture","text":"<p>Databricks  operates out of a control plane and a compute plane.  </p> <ol> <li> <p>The <code>control plane</code> includes the backend services that Databricks manages in your Databricks account. Notebook commands and many other workspace configurations are stored in the control plane and encrypted at rest.</p> </li> <li> <p>The <code>compute plane</code> is where your data is processed.</p> <ul> <li> <p>For most Databricks computation, the compute resources are in your AWS account in what is called the <code>classic compute plane</code>. This refers to the network in your AWS account and its resources. Databricks uses the classic compute plane for your notebooks, jobs, and for pro and classic Databricks SQL warehouses.</p> </li> <li> <p>For <code>serverless SQL warehouses</code> or <code>Model Serving</code>, the serverless compute resources run in a serverless compute plane in your Databricks account.</p> </li> </ul> </li> </ol> <p> </p> <p>Important</p> <p>Previously, Databricks referred to the <code>compute plane</code> as the <code>data plane</code></p>"},{"location":"databricks/db-arch/#serverless-architecture","title":"Serverless Architecture","text":"<p><code>Azure Databricks</code> creates a <code>serverless compute plane</code> in the same Azure region as your workspace\u2019s <code>classic compute plane</code>.</p> <p> </p> <p>Arch for model serving is shown below</p> <p> </p>"},{"location":"gcp/anthos/","title":"Anthos","text":"<p>Google Cloud Anthos is a <code>hybrid</code>, <code>cloud-agnostic container environment</code>. Google Cloud Anthos is a software product that enables enterprises to use container clusters instead of cloud virtual machines (VMs) to bridge gaps between legacy software and cloud hardware. </p> <p>So what exactly is Anthos?</p> <p>Anthos is to Kubernetes what Kubernetes is to containers. And more. When you create a Deployment in Kubernetes, K8S makes sure your Pods always match the desired state. A pod crashes? Kubernetes will schedule a new one, to get from the actual state into the desired state. Kubernetes does that in a declarative way, as an end user you probably let Kubernetes know what you\u2019d like by using YAML files.</p> <p>Just the same, Anthos is a platform that will manage your Kubernetes clusters for you. It does 3 things, and it does them well:</p> <ul> <li>Infrastructure provisioning in both cloud and on-premises.</li> <li>Infrastructure management tooling, security, policies and compliance solutions.</li> <li>Streamlined application development, service discovery and telemetry, service management, and workload migration from on-premises to cloud.</li> </ul> <p>You tell Anthos how you want your Kubernetes clusters to look like, and it will make sure your demands are met. The great thing here is that those different clusters are not limited to GKE clusters. With the Anthos GKE Connect Agent installed on your Kubernetes cluster, that cluster can reside anywhere, as long as it can connect to Anthos.</p> <p> </p> <p>Tip</p> <p>To create Anthos, Google partnered with hardware providers, such as Cisco, Dell EMC, Hewlett Packard Enterprise, NetApp and Robin.io to deliver the Anthos systems prepackaged in the software. However, Google Cloud Anthos can only run on servers capable of hosting Kubernetes clusters with the Google Kubernetes Engine.</p>"},{"location":"gcp/apigee/","title":"Apigee","text":""},{"location":"gcp/app-engine/","title":"App Engine","text":""},{"location":"gcp/big-query/","title":"Big Query","text":""},{"location":"gcp/cloud-armor/","title":"Cloud Armor","text":""},{"location":"gcp/cloud-cdn/","title":"Cloud CDN","text":""},{"location":"gcp/cloud-composer/","title":"Cloud Composer","text":""},{"location":"gcp/cloud-dns/","title":"Cloud DNS","text":"<p>DNS is a hierarchical distributed database that lets you store IP addresses and other data and look them up by name. Cloud DNS lets you publish your zones and records in DNS without the burden of managing your own DNS servers and software.</p> <p>Cloud DNS offers both <code>public zones</code> and <code>private managed DNS zones</code>. A public zone is visible to the public internet, while a private zone is visible only from one or more Virtual Private Cloud (VPC) networks that you specify.</p> <p>Remember</p> <p>Cloud DNS supports Identity and Access Management (IAM) permissions at the project level and individual DNS zone level</p>"},{"location":"gcp/cloud-interconnect/","title":"Cloud Interconnect","text":""},{"location":"gcp/cloud-logging/","title":"Cloud Logging","text":""},{"location":"gcp/cloud-monitoring/","title":"Cloud Monitoring","text":""},{"location":"gcp/cloud-profiler/","title":"Cloud Profiler","text":""},{"location":"gcp/cloud-spanner/","title":"Cloud Spanner","text":""},{"location":"gcp/cloud-sql/","title":"cloud-sql","text":""},{"location":"gcp/cloud-storage/","title":"Cloud Storage","text":""},{"location":"gcp/cloud_functions/","title":"Cloud Functions","text":"<p>Cloud Functions is a lightweight, event-based, <code>asynchronous compute solution</code> that allows you to create small, single-purpose functions that respond to cloud events without the need to manage a server or a runtime environment.</p> <p>These functions can be used to construct applications from bite-sized business logic.</p> <p>Cloud Functions can also be used to connect and extend cloud services. You are billed to the nearest 100 milliseconds, but only while your code is running.</p> <p> </p>"},{"location":"gcp/cloudrun/","title":"Cloud Run","text":"<p>Cloud Run is a <code>managed compute platform</code> that lets you run stateless containers via web requests or Pub/Sub events.</p> <p>Cloud Run is serverless; That means it removes all infrastructure management tasks so you can focus on developing applications.</p> <p>It is built on Knative, an open API and runtime environment built on Kubernetes. It can be fully managed on Google Cloud, on Google Kubernetes Engine, or anywhere <code>Knative</code> runs.</p> <p>Cloud Run is fast. It can automatically scale up and down from zero almost instantaneously, and it charges only for the resources used, calculated down to the nearest 100 milliseconds, so you\u2018ll never pay for over-provisioned resources</p> <p>==You can deploy code written in any programming language on Cloud Run if you can build a container image from it.</p> <p>Do we need to build containers?</p> <p>In fact, building container images is optional. If you're using Go, Node.js, Python, Java, .NET Core, or Ruby, you can use the source-based deployment option that builds the container for you, using the best practices for the language you're using.</p> <p> </p> <p>When to use Cloud Run services?</p> <p>Cloud Run services are great for code that handles requests or events. Example use cases include:</p> <ul> <li> <p>Websites and web applications: Build your web app using your favorite stack, access your SQL database, and render dynamic HTML pages.</p> </li> <li> <p>APIs and microservices: You can build a REST API, or a GraphQL API or private microservices that communicate over HTTP or gRPC.</p> </li> <li> <p>Streaming data processing: Cloud Run services can receive messages from Pub/Sub push subscriptions and events from Eventarc. </p> </li> </ul>"},{"location":"gcp/cloudrun/#pricing-model","title":"Pricing Model","text":"<p>The pricing model on Cloud Run is unique; as you only pay for the system resources you use while a container is handling web requests, with a granularity of 100ms, and when it is starting or shutting down.</p> <p> </p> <p>You do not pay for anything if your container does not handle requests. Additionally, there is a small fee for every one million requests you serve.</p> <p>The price of container time increases with CPU and memory. A container with more vCPU and memory is more expensive. Today, Cloud run can allocate up to 4 vCPUs and 8GB of memory.</p> <p>Most of the other compute products (such as Compute Engine), charge for servers as long as they are running, even if you are not using them. That means you\u2019re often paying for idle server capacity.</p>"},{"location":"gcp/compute_engine/","title":"Compute Engine","text":"<p>Compute Engine is a computing and hosting service that lets you create and run virtual machines on Google infrastructure. Compute Engine offers scale, performance, and value that lets you easily launch large compute clusters on Google's infrastructure</p> <p> </p>"},{"location":"gcp/compute_engine/#capacity-planner","title":"Capacity Planner","text":"<p>Capacity Planner provides visibility into your project's use of virtual machine (VM) instance resources.</p>"},{"location":"gcp/file-storage/","title":"File Storage","text":""},{"location":"gcp/fire-store/","title":"Fire Store","text":""},{"location":"gcp/firebase/","title":"FireBase","text":""},{"location":"gcp/gcp_concepts/","title":"GCP Concepts","text":""},{"location":"gcp/gcp_concepts/#knative","title":"Knative","text":"<p>Knative offers features like <code>scale-to-zero</code>, autoscaling, in-cluster builds, and eventing framework for cloud-native applications on Kubernetes. Whether on-premises, in the cloud, or in a third-party data center, Knative codifies the best practices shared by successful real-world Kubernetes-based frameworks.</p> <p>Run your serverless at your own terms</p> <p>Knative provides an open API and runtime environment that enables you to run your serverless workloads anywhere you choose: - Fully managed on Google Cloud, - Anthos on Google Kubernetes Engine (GKE) - On your own Kubernetes cluster</p> <p>Knative makes it easy to start with <code>Cloud Run</code> and later move to Cloud Run for <code>Anthos</code> or start in your own <code>Kubernetes cluster</code> and migrate to Cloud Run in the future. By using Knative as the underlying platform, you can move your workloads freely across platforms, while significantly reducing the switching costs.</p>"},{"location":"gcp/iam-gcp/","title":"IAM (GCP)","text":""},{"location":"gcp/lb-gcp/","title":"Load Balancer","text":"<p>Cloud Load Balancing allows you to put your resources behind a single IP address that is externally accessible or internal to your Virtual Private Cloud (VPC) network.</p> <p>Different Types of Load Balancers</p> <p>The following diagram summarizes all the available deployment modes for Cloud Load Balancing.</p> <p> </p> <p>To determine which Cloud Load Balancing product to use, you must first determine what traffic type your load balancers must handle. </p> <p>As a general rule, you'd choose an Application Load Balancer when you need a flexible feature set for your applications with HTTP(S) traffic. You'd choose a proxy <code>Network Load Balancer</code> to implement TCP proxy load balancing to backends in one or more regions. And, you'd choose a passthrough Network Load Balancer to preserve client source IP addresses, avoid the overhead of proxies, and to support additional protocols like UDP, ESP, and ICMP.</p> <p>You can further narrow down your choices depending on your application's requirements: whether your application is <code>external (internet-facing)</code> or <code>internal</code> and whether you need backends deployed <code>globally</code> or <code>regionally</code>.</p>"},{"location":"gcp/lb-gcp/#use-cases","title":"Use Cases","text":""},{"location":"gcp/lb-gcp/#use-case-1","title":"Use Case #1","text":"<p>You want to build a publicly accessible website.  The website needs to have multiple instances running on Compute Engine.  And the VMs need to be hosted in the US, Europe, and China.  </p> <p> </p> <p>Okay, so since this website is going to be public, we need an external load balancer.  And because it is a website, that means we are going to need to be using HTTP and/or HTTPS.  We are using VMs in multiple regions, so it can\u2019t be regional.  So in this case, you would select the Global external HTTP(S) load balancer.</p>"},{"location":"gcp/lb-gcp/#use-case-2","title":"Use Case #2","text":"<p>You are building a private, internal web service.  All backend instances will be running on Compute Engine and you will connect to them via REST API.  All client connections are expected to originate from a specific Google VPC. </p> <p> </p> <p>Okay, since this is a private service, you are going to choose an internal load balancer.  The REST API means we will be using HTTP and/or HTTPS.  So your best choice here is the Internal HTTP(S) load balancer.</p>"},{"location":"gcp/lb-gcp/#use-case-3","title":"Use Case #3","text":"<p>You need to build a VoIP solution for your company using SIP.  SIP is a peer-to-peer protocol that is transported by UDP over TCP.</p> <p>Now, for your employees to be able to connect to the service, your load balancer needs to be external.  And you know you need to support the UDP protocol.  So, the right choice should be the External TCP/UDP network load balancer.</p> <p> </p> <p>Alright, so now you should have a good understanding of the available load balancing options on Google Cloud Platform.  If you can remember the four main attributes, you should have no problem identifying the correct load balancer for any scenario.</p>"},{"location":"gcp/secret-manager/","title":"Secret Manager","text":""},{"location":"gcp/security-command-center/","title":"security-command-center","text":""},{"location":"gcp/vertex-ai/","title":"Vertex AI","text":""},{"location":"gcp/vpc-gcp/","title":"VPC","text":"<p>An important thing to remember is that <code>VPCs are global</code>, but your subnets are not. </p> <p>Your VPCs can span multiple regions.  Your subnets are locked to a single region.</p> <p>Also, remember that you can use <code>Shared VPC</code> to create a common set of resources for everyone in the same company.</p> <p> </p>"},{"location":"gcp/vpc-gcp/#key-features","title":"Key features","text":""},{"location":"gcp/vpc-gcp/#vpc-network","title":"VPC network","text":"<p>VPC can automatically set up your <code>virtual topology</code>, configuring prefix ranges for your subnets and network policies, or you can configure your own. You can also expand CIDR ranges without downtime.</p>"},{"location":"gcp/vpc-gcp/#vpc-flow-logs","title":"VPC flow logs","text":"<p>Flow logs capture information about the IP traffic going to and from network interfaces on Compute Engine. </p> <p>VPC flow logs help with network monitoring, forensics, real-time security analysis, and expense optimization. </p> <p>Google Cloud flow logs are updated every five seconds, providing immediate visibility.</p>"},{"location":"gcp/vpc-gcp/#vpc-peering","title":"VPC Peering","text":"<p>Configure private communication across the same or different organizations without bandwidth bottlenecks or single points of failure.</p>"},{"location":"gcp/vpc-gcp/#shared-vpc","title":"Shared VPC","text":"<p>You can Configure a VPC network to be shared across several projects in your organization. </p> <p>Connectivity routes and firewalls associated are managed centrally.</p> <p>Your developers have their own projects with <code>separate billing and quotas</code>, while they simply connect to a shared private network where they can communicate.</p>"},{"location":"gcp/vpc-gcp/#bring-your-own-ips","title":"Bring your own IPs \ud83d\udcf2","text":"<p>Bring your own IP addresses to Google\u2019s network across all regions to minimize downtime during migration and reduce your networking infrastructure cost.</p> <p>After you bring your own IPs, Google Cloud will advertise them globally to all peers. Your prefixes can be broken into blocks as small as 16 addresses (/28), creating more flexibility with your resources.</p>"},{"location":"gcp/vpc-gcp/#private-service-connect","title":"Private Service Connect \ud83d\udd10","text":"<p>Private Service Connect is a capability of Google Cloud networking that allows consumers to access managed services privately from inside their VPC network. Similarly, it allows managed service producers to host these services in their own separate VPC networks and offer a private connection to their consumers.</p> <p>For example, when you use Private Service Connect to access Cloud SQL, you are the service consumer, and Google is the service producer.</p> <p> </p> <p>TLDR</p> <p>With Private Service Connect, consumers can use their <code>own internal IP addresses</code> to access services without leaving their VPC networks. Traffic remains entirely within Google Cloud. Private Service Connect provides service-oriented access between consumers and producers with granular control over how services are accessed.</p>"},{"location":"gcp/vpc-gcp/#private-endpoint-connection","title":"Private Endpoint Connection \ud83d\udd0c","text":"<p>Private Service Connect lets you send traffic to endpoints that forward the traffic to published services in another VPC network.</p> <p>The below diagram shows a Private Service Connect endpoint that targets a published service that is running in a separate VPC network and organization. Private Service Connect endpoints and published services let two <code>independent companies</code> communicate with each other by using internal IP addresses.</p> <p> </p>"},{"location":"gcp/vpc-gcp/#private-google-access","title":"Private Google Access \ud83d\udd75\ufe0f","text":"<p>VM instances that only have internal IP addresses (no external IP addresses) can use Private Google Access. They can reach the external IP addresses of Google APIs and services. The source IP address of the packet can be the primary internal IP address of the network interface or an address in an alias IP range that is assigned to the interface. If you disable Private Google Access, the VM instances can no longer reach Google APIs and services; they can only send traffic within the VPC network.</p> <p>Implementation of Private Google Access</p> <p> </p> <p>The VPC network has been configured to meet the DNS, routing, and firewall network requirements for Google APIs and services. Private Google Access has been enabled on <code>subnet-a</code>, but not on <code>subnet-b</code>.</p> <ul> <li> <p><code>VM A1</code> can access Google APIs and services, including Cloud Storage, because its network interface is located in <code>subnet-a</code>, which has Private Google Access enabled. Private Google Access applies to the instance because it only has an internal IP address.</p> </li> <li> <p><code>VM B1</code> cannot access Google APIs and services because it only has an internal IP address and Private Google Access is disabled for <code>subnet-b</code>.</p> </li> <li> <p><code>VM A2</code> and <code>VM B2</code> can both access Google APIs and services, including Cloud Storage, because they each have external IP addresses. Private Google Access has no effect on whether or not these instances can access Google APIs and services because both have external IP addresses.</p> </li> </ul>"},{"location":"k8s/cka-exam-commands/","title":"CKA Exam Commands","text":""},{"location":"k8s/cka-exam-commands/#basics","title":"Basics","text":"Create pod in finance namespace <pre><code>k run redis --image=redis -n finance\n</code></pre> Create a service and expose it on port 6379 <pre><code>apiVersion: v1\nkind: Service\nmetadata:\nname: redis-service\nspec:\nselector:\n    app.kubernetes.io/name: MyApp\nports:\n    - protocol: TCP\n    port: 6379\n    targetPort: 6379\n</code></pre> Create a deployment named webapp using the image kodekloud/webapp-color with 3 replicas <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: webapp\nlabels:\n    app: nginx\nspec:\nreplicas: 3\nselector:\n    matchLabels:\n    app: nginx\ntemplate:\n    metadata:\n    labels:\n        app: nginx\n    spec:\n    containers:\n    - name: nginx\n        image: kodekloud/webapp-color\n        ports:\n        - containerPort: 80\n</code></pre> Create a new pod called custom-nginx using the nginx image and expose it on container port 8080 <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\nname: custom-nginx\nlabels:\n    tier: db\nspec:\ncontainers:\n- name: nginx\n    image: nginx\n    ports:\n    - containerPort: 8080\n</code></pre> <p>Create a new namespace called dev-ns.</p> <pre><code>k create ns dev-ns\n</code></pre> Create a pod called httpd using the image httpd:alpine in the default namespace. Next, create a service of type ClusterIP by the same name (httpd). The target port for the service should be 80 <pre><code>k run httpd --image=httpd:alpine --port=80 --expose\n\nservice/httpd created\npod/httpd created\n</code></pre>"},{"location":"k8s/cka-exam-commands/#scheduling","title":"Scheduling","text":""},{"location":"k8s/cka-exam-commands/#node-affinity","title":"Node Affinity","text":"<pre><code># apply label\nk label nodes node01 color=blue\n\n# create a deployment \nk create deployment blue --replicas=3 --image=nginx\n\n# check the taints on the node\nkubectl describe node controlplane | grep -i taints \n</code></pre> Create a new deployment named red with the nginx image and 2 replicas, and ensure it gets placed on the controlplane node only. Use the label key <code>node-role.kubernetes.io/control-plane</code> which is already set on the controlplane node. <pre><code># use the exists operator as shown below\n    spec:\n    affinity:\n        nodeAffinity:\n        requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n            - key: node-role.kubernetes.io/control-plane\n                operator: Exists\n</code></pre>"},{"location":"k8s/cka-exam-commands/#manual-scheduling","title":"Manual Scheduling","text":"Manually schedule the pod on node01 <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\nname: nginx\nspec:\nnodeName: node01\ncontainers:\n-  image: nginx\n    name: nginx\n</code></pre>"},{"location":"k8s/cka-exam-commands/#labels-and-selectors","title":"Labels and Selectors","text":"<pre><code># count the number of pods with env=dev label\nk get po -l env=dev --show-labels | wc\n\n# Identify the POD which is part of the prod environment, the finance BU and of frontend tier?\nk get pod  --selector env=prod,bu=finance,tier=frontend  --show-labels\n</code></pre>"},{"location":"k8s/cka-exam-commands/#taints-and-tolerations","title":"Taints and Tolerations","text":"<p>Node affinity is a property of Pods that attracts them to a set of nodes (either as a preference or a hard requirement). Taints are the opposite -- they allow a node to repel a set of pods.</p> <p>Tolerations are applied to pods. Tolerations allow the scheduler to schedule pods with matching taints. Tolerations allow scheduling but don't guarantee scheduling: the scheduler also evaluates other parameters as part of its function.</p> <p>Taints and tolerations work together to ensure that pods are not scheduled onto inappropriate nodes. One or more taints are applied to a node; this marks that the node should not accept any pods that do not tolerate the taints.</p> <pre><code># You add a taint to a node using kubectl taint. For example,\nkubectl taint nodes node1 key1=value1:NoSchedule\n\n\n# To remove the taint added by the command above, you can run:\nkubectl taint nodes node1 key1=value1:NoSchedule-\n\n# Specify a toleration for a pod in the PodSpec.\ntolerations:\n- key: \"key1\"\n  operator: \"Equal\"\n  value: \"value1\"\n  effect: \"NoSchedule\"\n\n\ntolerations:\n- key: \"key1\"\n  operator: \"Exists\"\n  effect: \"NoSchedule\"\n\n# Here's an example of a pod that uses tolerations:\napiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\n  labels:\n    env: test\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    imagePullPolicy: IfNotPresent\n  tolerations:\n  - key: \"example-key\"\n    operator: \"Exists\"\n    effect: \"NoSchedule\"\n</code></pre> Check taints on the nodes <pre><code>k describe  nodes  node01 | grep -i taint # use describe instead of get\n</code></pre> Create a taint on node01 with key of spray, value of mortein and effect of NoSchedule <pre><code>k taint node node01 spray=mortain:NoSchedule\n</code></pre> Create another pod named bee with the nginx image, which has a toleration set to the taint mortein. <p>First do dry run using </p> <p><code>k run bee --image=nginx --dry-run=client -o yaml &gt; test_pod.yaml</code></p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\ncreationTimestamp: null\nlabels:\n    run: bee\nname: bee\nspec:\ntolerations:\n- key: \"spray\"\n    operator: \"Equal\"\n    value: \"mortein\"\n    effect: \"NoSchedule\"\ncontainers:\n- image: nginx\n    name: bee\n    resources: {}\ndnsPolicy: ClusterFirst\nrestartPolicy: Always\nstatus: {}\n</code></pre>"},{"location":"k8s/cka-exam-commands/#resource-limits","title":"Resource Limits","text":"<p>If the node where a Pod is running has enough of a resource available, it's possible (and allowed) for a container to use more resource than its request for that resource specifies. However, a container is not allowed to use more than its resource limit.</p> <pre><code># replace the pod using the replace command\nk replace --force  -f /tmp/kubectl-edit-2304618812.yaml\n\npod \"elephant\" deleted\npod/elephant replaced\n</code></pre>"},{"location":"k8s/cka-exam-commands/#deamonsets","title":"DeamonSets","text":"<p>A DaemonSet ensures that all (or some) Nodes run a copy of a Pod. As nodes are added to the cluster, Pods are added to them. As nodes are removed from the cluster, those Pods are garbage collected. Deleting a DaemonSet will clean up the Pods it created.</p> <p>Some typical uses of a DaemonSet are:</p> <pre><code>running a cluster storage daemon on every node\nrunning a logs collection daemon on every node\nrunning a node monitoring daemon on every node\n</code></pre> On how many nodes are the pods scheduled by the DaemonSet kube-proxy? <pre><code>k -n kube-system describe ds kube-proxy  # check the pod status\n</code></pre> <p>Deploy a DaemonSet for FluentD Logging with Name: elasticsearch, Namespace: kube-system and Image: registry.k8s.io/fluentd-elasticsearch:1.20</p> <p>How to create a DS ?</p> <p>An easy way to create a DaemonSet is to first generate a YAML file for a Deployment with the command <code>kubectl create deployment elasticsearch --image=registry.k8s.io/fluentd-elasticsearch:1.20 -n kube-system --dry-run=client -o yaml &gt; fluentd.yaml</code>. Next, remove the replicas, strategy and status fields from the YAML file using a text editor. Also, change the kind from Deployment to DaemonSet. Finally, create the Daemonset by running <code>kubectl create -f fluentd.yaml</code></p>"},{"location":"k8s/cka-exam-commands/#static-pods","title":"Static Pods","text":"How many static pods exist in this cluster in all namespaces? <p>Run the command <code>kubectl get pods --all-namespaces</code> and look for those with <code>-controlplane</code> appended in the name</p> What is the path of the directory holding the static pod definition files? <p><code>/etc/kubernetes/manifests/</code></p> Create a static pod named static-busybox that uses the busybox image and the command sleep 1000 <pre><code>kubectl run --restart=Never --image=busybox static-busybox --dry-run=client -o yaml --command -- sleep 1000 &gt; /etc/kubernetes/manifests/static-busybox.yaml\n</code></pre> The path need not be /etc/kubernetes/manifests. Make sure to check the path configured in the kubelet configuration file. <pre><code>root@controlplane:~# ssh node01 \nroot@node01:~# ps -ef |  grep /usr/bin/kubelet \nroot        4147       1  0 14:05 ?        00:00:00 /usr/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime-endpoint=unix:///var/run/containerd/containerd.sock --pod-infra-container-image=registry.k8s.io/pause:3.9\nroot        4773    4733  0 14:05 pts/0    00:00:00 grep /usr/bin/kubelet\n\nroot@node01:~# grep -i staticpod /var/lib/kubelet/config.yaml\nstaticPodPath: /etc/just-to-mess-with-you\n</code></pre>"},{"location":"k8s/cka-exam-commands/#logging-and-monitoring","title":"Logging and Monitoring","text":"Identify the POD that consumes the most Memory(bytes) in default namespace. <pre><code>k top pod\nNAME       CPU(cores)   MEMORY(bytes)   \nelephant   19m          32Mi            \nlion       1m           18Mi            \nrabbit     129m         252Mi    \n</code></pre>"},{"location":"k8s/cka-exam-commands/#appilcation-lifecycle-maintainance","title":"Appilcation Lifecycle Maintainance","text":"What command is run at container startup? <pre><code>FROM python:3.6-alpine\n\nRUN pip install flask\n\nCOPY . /opt/\n\nEXPOSE 8080\n\nWORKDIR /opt\n\nENTRYPOINT [\"python\", \"app.py\"]\n</code></pre> <p>Ans is <code>app.py</code></p> <pre><code># Another question\nFROM python:3.6-alpine\n\nRUN pip install flask\n\nCOPY . /opt/\n\nEXPOSE 8080\n\nWORKDIR /opt\n\nENTRYPOINT [\"python\", \"app.py\"]\n\nCMD [\"--color\", \"red\"]\n</code></pre> <p>Ans is <code>python app.py --color red</code></p> <pre><code># Question 3\n\napiVersion: v1 \nkind: Pod \nmetadata:\nname: webapp-green\nlabels:\n    name: webapp-green \nspec:\ncontainers:\n- name: simple-webapp\n    image: kodekloud/webapp-color\n    command: [\"--color\",\"green\"]\n\n---\n\nFROM python:3.6-alpine\n\nRUN pip install flask\n\nCOPY . /opt/\n\nEXPOSE 8080\n\nWORKDIR /opt\n\nENTRYPOINT [\"python\", \"app.py\"]\n\nCMD [\"--color\", \"red\"]    \n</code></pre> <p>Ans is <code>--color green</code></p> What command is run at container startup? Assume the image was created from the Dockerfile in this directory <pre><code>FROM python:3.6-alpine\n\nRUN pip install flask\n\nCOPY . /opt/\n\nEXPOSE 8080\n\nWORKDIR /opt\n\nENTRYPOINT [\"python\", \"app.py\"]\n\nCMD [\"--color\", \"red\"]\n\n---\napiVersion: v1 \nkind: Pod \nmetadata:\nname: webapp-green\nlabels:\n    name: webapp-green \nspec:\ncontainers:\n- name: simple-webapp\n    image: kodekloud/webapp-color\n    command: [\"python\", \"app.py\"]\n    args: [\"--color\", \"pink\"]\n</code></pre> <p>Ans is <code>python app.py --color pink</code></p> #Create a pod with the given specifications. By default it displays a blue background. Set the given command line arguments to change it to green. <code>Command line arguments: --color=green</code> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\ncreationTimestamp: null\nlabels:\n    run: webapp-green\nname: webapp-green\nspec:\ncontainers:\n- image: kodekloud/webapp-color\n    args: [\"--color\",\"green\"]\n    name: webapp-green\n    resources: {}\ndnsPolicy: ClusterFirst\nrestartPolicy: Always\nstatus: {}\n</code></pre>"},{"location":"k8s/cka-exam-commands/#env-variables","title":"Env variables","text":"<pre><code># create a cm with 2 params as shown below\nk create cm webapp-config-map --from-literal APP_COLOR=darkblue  --from-literal APP_OTHER=disregard\n\n# Update the environment variable on the POD to use only the APP_COLOR key from the newly created ConfigMap. \napiVersion: v1\nkind: Pod\nmetadata:\n  labels:\n    name: webapp-color\n  name: webapp-color\n  namespace: default\nspec:\n  containers:\n    - name: webapp-color\n      image: kodekloud/webapp-color\n      env:\n      - name: APP_COLOR\n        valueFrom:\n          configMapKeyRef:\n            name: webapp-config-map\n            key: APP_COLOR\n</code></pre>"},{"location":"k8s/cka-exam-commands/#secrets","title":"Secrets","text":"<pre><code># The reason the application is failed is because we have not created the secrets yet. Create a new secret named db-secret with the data given below.\nk create secret generic db-secret --from-literal DB_Host=sql01 --from-literal DB_User=root --from-literal DB_Password=password123\n</code></pre> Configure webapp-pod to load environment variables from the newly created secret. <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\ncreationTimestamp: \"2023-11-19T06:03:03Z\"\nlabels:\n    name: webapp-pod\nname: webapp-pod\nnamespace: default\nspec:\ncontainers:\n- image: kodekloud/simple-webapp-mysql\n    imagePullPolicy: Always\n    name: webapp\n    envFrom:\n    - secretRef:\n        name: db-secret\n</code></pre>"},{"location":"k8s/cka-exam-commands/#multi-container-pods","title":"Multi container pods","text":"Create a multi-container pod with 2 containers <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\ncreationTimestamp: null\nlabels:\n    run: yellow\nname: yellow\nspec:\ncontainers:\n- image: busybox\n    name: lemon\n    resources: {}\n- image: redis\n    name: gold\ndnsPolicy: ClusterFirst\nrestartPolicy: Always\nstatus: {}\n</code></pre>"},{"location":"k8s/cka-exam-commands/#init-containers","title":"Init Containers","text":"<pre><code># Sample\napiVersion: v1\nkind: Pod\nmetadata:\n  name: myapp-pod\n  labels:\n    app.kubernetes.io/name: MyApp\nspec:\n  containers:\n  - name: myapp-container\n    image: busybox:1.28\n    command: ['sh', '-c', 'echo The app is running! &amp;&amp; sleep 3600']\n  initContainers:\n  - name: init-myservice\n    image: busybox:1.28\n    command: ['sh', '-c', \"until nslookup myservice.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for myservice; sleep 2; done\"]\n  - name: init-mydb\n    image: busybox:1.28\n    command: ['sh', '-c', \"until nslookup mydb.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for mydb; sleep 2; done\"]\n</code></pre>"},{"location":"k8s/cka-exam-commands/#cluster-maintainance","title":"Cluster Maintainance","text":"We need to take node01 out for maintenance. Empty the node of all applications and mark it unschedulable. <pre><code>k drain node01 --ignore-daemonsets \n\nnode/node01 cordoned\nWarning: ignoring DaemonSet-managed Pods: kube-flannel/kube-flannel-ds-f5ttj, kube-system/kube-proxy-vg6wp\nevicting pod default/blue-6b478c8dbf-vrh86\nevicting pod default/blue-6b478c8dbf-j7g8d\npod/blue-6b478c8dbf-j7g8d evicted\npod/blue-6b478c8dbf-vrh86 evicted\nnode/node01 drained\n</code></pre> drain error due to no controller <pre><code>k drain node01 --ignore-daemonsets \nnode/node01 cordoned\nerror: unable to drain node \"node01\" due to error:cannot delete Pods declare no controller (use --force to override): default/hr-app, continuing command...\nThere are pending nodes to be drained:\nnode01\ncannot delete Pods declare no controller (use --force to override): default/hr-app\n</code></pre> What is the current version of the cluster? <pre><code>kubectl get nodes and look at the VERSION\n</code></pre> What is the latest stable version of Kubernetes as of today? <pre><code>kubeadm upgrade plan\n[upgrade/config] Making sure the configuration is correct:\n[upgrade/config] Reading configuration from the cluster...\n[upgrade/config] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'\n[preflight] Running pre-flight checks.\n[upgrade] Running cluster health checks\n[upgrade] Fetching available versions to upgrade to\n[upgrade/versions] Cluster version: v1.26.0\n[upgrade/versions] kubeadm version: v1.26.0\nI1119 12:04:28.812883   18925 version.go:256] remote version is much newer: v1.28.4; falling back to: stable-1.26\n[upgrade/versions] Target version: v1.26.11\n[upgrade/versions] Latest version in the v1.26 series: v1.26.11\n\nComponents that must be upgraded manually after you have upgraded the control plane with 'kubeadm upgrade apply':\nCOMPONENT   CURRENT       TARGET\nkubelet     2 x v1.26.0   v1.26.11\n\nUpgrade to the latest version in the v1.26 series:\n\nCOMPONENT                 CURRENT   TARGET\nkube-apiserver            v1.26.0   v1.26.11\nkube-controller-manager   v1.26.0   v1.26.11\nkube-scheduler            v1.26.0   v1.26.11\nkube-proxy                v1.26.0   v1.26.11\nCoreDNS                   v1.9.3    v1.9.3\netcd                      3.5.6-0   3.5.6-0\n</code></pre> <p>its <code>v1.28.4</code> as shown above</p> Upgrade the controlplane components to exact version v1.27.0 <p>Upgrade the kubeadm tool (if not already), then the controlplane components, and finally the kubelet. Practice referring to the Kubernetes documentation page.</p> <p>Note: While upgrading kubelet, if you hit dependency issues while running the apt-get upgrade kubelet command, use the <code>apt install kubelet=1.27</code>.0-00 command instead.</p> <pre><code># On the node01 node, run the following commands:\n# If you are on the controlplane node\nssh node01 # to log in to the node01.\n# This will update the package lists from the software repository.\n\napt-get update\n\n#  This will install the kubeadm version 1.27.0.\n\napt-get install kubeadm=1.27.0-00\n\n#  This will upgrade the node01 configuration.\n\nkubeadm upgrade node\n\n#  This will update the kubelet with the version 1.27.0.\n\napt-get install kubelet=1.27.0-00 \n\n#  You may need to reload the daemon and restart the kubelet service after it has been upgraded.\n\nsystemctl daemon-reload\nsystemctl restart kubelet\n</code></pre> At what address can you reach the ETCD cluster from the controlplane node?  <pre><code>k -n kube-system describe po etcd-controlplane \nName:                 etcd-controlplane\nNamespace:            kube-system\nPriority:             2000001000\nPriority Class Name:  system-node-critical\nNode:                 controlplane/192.25.158.9\nStart Time:           Sun, 19 Nov 2023 12:39:56 -0500\nLabels:               component=etcd\n                    tier=control-plane\nAnnotations:          kubeadm.kubernetes.io/etcd.advertise-client-urls: https://192.25.158.9:2379\n                    kubernetes.io/config.hash: 88719a0e6555d94fd96af8b6011a2af6\n                    kubernetes.io/config.mirror: 88719a0e6555d94fd96af8b6011a2af6\n                    kubernetes.io/config.seen: 2023-11-19T12:39:38.200107235-05:00\n                    kubernetes.io/config.source: file\nStatus:               Running\nSeccompProfile:       RuntimeDefault\nIP:                   192.25.158.9\nIPs:\nIP:           192.25.158.9\nControlled By:  Node/controlplane\nContainers:\netcd:\n    Container ID:  containerd://f21102066ab677d48612ffc74802a43ae023daa92feeab805b0a80da2e53f495\n    Image:         registry.k8s.io/etcd:3.5.7-0\n    Image ID:      registry.k8s.io/etcd@sha256:51eae8381dcb1078289fa7b4f3df2630cdc18d09fb56f8e56b41c40e191d6c83\n    Port:          &lt;none&gt;\n    Host Port:     &lt;none&gt;\n    Command:\n    etcd\n    --advertise-client-urls=https://192.25.158.9:2379\n    --cert-file=/etc/kubernetes/pki/etcd/server.crt\n    --client-cert-auth=true\n    --data-dir=/var/lib/etcd\n    --experimental-initial-corrupt-check=true\n    --experimental-watch-progress-notify-interval=5s\n    --initial-advertise-peer-urls=https://192.25.158.9:2380\n    --initial-cluster=controlplane=https://192.25.158.9:2380\n    --key-file=/etc/kubernetes/pki/etcd/server.key\n    --listen-client-urls=https://127.0.0.1:2379,https://192.25.158.9:2379\n    --listen-metrics-urls=http://127.0.0.1:2381\n    --listen-peer-urls=https://192.25.158.9:2380\n    --name=controlplane\n    --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt\n    --peer-client-cert-auth=true\n    --peer-key-file=/etc/kubernetes/pki/etcd/peer.key\n    --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt\n    --snapshot-count=10000\n    --trusted-ca-file=/et\n</code></pre> <p>check the <code>listen-client-urls</code> as shown above</p> Backup the Etcd <p>The master node in our cluster is planned for a regular maintenance reboot tonight. While we do not anticipate anything to go wrong, we are required to take the necessary backups. Take a snapshot of the ETCD database using the built-in snapshot functionality.</p> <p>Store the backup file at location /opt/snapshot-pre-boot.db</p> <pre><code>ETCDCTL_API=3 etcdctl --endpoints=https://[127.0.0.1]:2379 \\\n--cacert=/etc/kubernetes/pki/etcd/ca.crt \\\n--cert=/etc/kubernetes/pki/etcd/server.crt \\\n--key=/etc/kubernetes/pki/etcd/server.key \\\nsnapshot save /opt/snapshot-pre-boot.db\n</code></pre> How many clusters are defined in the kubeconfig on the student-node? <pre><code>k config get-clusters \n\nNAME\ncluster2\ncluster1\n</code></pre> How to switch from one cluster to another? <pre><code>k config use-context cluster1\n</code></pre> <pre><code>#If you check out the pods running in the kube-system namespace in cluster1, you will notice that etcd is running as a pod:\n\n$  kubectl config use-context cluster1\nSwitched to context \"cluster1\".\n\n$  kubectl get pods -n kube-system | grep etcd\netcd-cluster1-controlplane                      1/1     Running   0              9m26s\n\n\n# This means that ETCD is set up as a Stacked ETCD Topology where the distributed data storage cluster provided by etcd is stacked on top of the cluster formed by the nodes managed by kubeadm that run control plane components.\n</code></pre> <pre><code># Using the external etcd\n\nIf you check out the pods running in the kube-system namespace in cluster2, you will notice that there are NO etcd pods running in this cluster!\n\nstudent-node ~ \u279c  kubectl config use-context cluster2\nSwitched to context \"cluster2\".\n\nstudent-node ~ \u279c  kubectl get pods -n kube-system  | grep etcd\n\nstudent-node ~ \u2716 \n\nAlso, there is NO static pod configuration for etcd under the static pod path:\n\nstudent-node ~ \u2716 ssh cluster2-controlplane\nWelcome to Ubuntu 18.04.6 LTS (GNU/Linux 5.4.0-1086-gcp x86_64)\n\n* Documentation:  https://help.ubuntu.com\n* Management:     https://landscape.canonical.com\n* Support:        https://ubuntu.com/advantage\nThis system has been minimized by removing packages and content that are\nnot required on a system that users do not log into.\n\nTo restore this content, you can run the 'unminimize' command.\nLast login: Wed Aug 31 05:05:04 2022 from 10.1.127.14\n\ncluster2-controlplane ~ \u279c  ls /etc/kubernetes/manifests/ | grep -i etcd\n\ncluster2-controlplane ~ \u2716 \n\nHowever, if you inspect the process on the controlplane for cluster2, you will see that that the process for the kube-apiserver is referencing an external etcd datastore:\n\ncluster2-controlplane ~ \u2716 ps -ef | grep etcd\nroot        1705    1320  0 05:03 ?        00:00:31 kube-apiserver --advertise-address=10.1.127.3 --allow-privileged=true --authorization-mode=Node,RBAC --client-ca-file=/etc/kubernetes/pki/ca.crt --enable-admission-plugins=NodeRestriction --enable-bootstrap-token-auth=true --etcd-cafile=/etc/kubernetes/pki/etcd/ca.pem --etcd-certfile=/etc/kubernetes/pki/etcd/etcd.pem --etcd-keyfile=/etc/kubernetes/pki/etcd/etcd-key.pem --etcd-servers=https://10.1.127.10:2379 --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key --requestheader-allowed-names=front-proxy-client --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt --requestheader-extra-headers-prefix=X-Remote-Extra- --requestheader-group-headers=X-Remote-Group --requestheader-username-headers=X-Remote-User --secure-port=6443 --service-account-issuer=https://kubernetes.default.svc.cluster.local --service-account-key-file=/etc/kubernetes/pki/sa.pub --service-account-signing-key-file=/etc/kubernetes/pki/sa.key --service-cluster-ip-range=10.96.0.0/12 --tls-cert-file=/etc/kubernetes/pki/apiserver.crt --tls-private-key-file=/etc/kubernetes/pki/apiserver.key\nroot        5754    5601  0 05:15 pts/0    00:00:00 grep etcd\n\ncluster2-controlplane ~ \u279c  \n\n# You can see the same information by inspecting the kube-apiserver pod (which runs as a static pod in the kube-system namespace):\n</code></pre> What is the IP address of the External ETCD datastore used in cluster2? <pre><code>ps -ef | grep etcd # after doing ssh to controlPlane\nroot        1747    1383  0 20:07 ?        00:05:56 kube-apiserver --advertise-address=192.28.229.12 --allow-privileged=true --authorization-mode=Node,RBAC --client-ca-file=/etc/kubernetes/pki/ca.crt --enable-admission-plugins=NodeRestriction --enable-bootstrap-token-auth=true --etcd-cafile=/etc/kubernetes/pki/etcd/ca.pem --etcd-certfile=/etc/kubernetes/pki/etcd/etcd.pem --etcd-keyfile=/etc/kubernetes/pki/etcd/etcd-key.pem --etcd-servers=https://192.28.229.24:2379 --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key --requestheader-allowed-names=front-proxy-client --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt --requestheader-extra-headers-prefix=X-Remote-Extra- --requestheader-group-headers=X-Remote-Group --requestheader-username-headers=X-Remote-User --secure-port=6443 --service-account-issuer=https://kubernetes.default.svc.cluster.local --service-account-key-file=/etc/kubernetes/pki/sa.pub --service-account-signing-key-file=/etc/kubernetes/pki/sa.key --service-cluster-ip-range=10.96.0.0/12 --tls-cert-file=/etc/kubernetes/pki/apiserver.crt --tls-private-key-file=/etc/kubernetes/pki/apiserver.key\n</code></pre> <p>IP is <code>192.28.229.24</code></p> What is the default data directory used the for ETCD datastore used in cluster1? <pre><code>ps -ef | grep -i etcd\nroot        1867    1383  0 20:08 ?        00:02:25 etcd --advertise-client-urls=https://192.28.229.9:2379 --cert-file=/etc/kubernetes/pki/etcd/server.crt --client-cert-auth=true --data-dir=/var/lib/etcd --experimental-initial-corrupt-check=true --initial-advertise-peer-urls=https://192.28.229.9:2380 --initial-cluster=cluster1-controlplane=https://192.28.229.9:2380 --key-file=/etc/kubernetes/pki/etcd/server.key --listen-client-urls=https://127.0.0.1:2379,https://192.28.229.9:2379 --listen-metrics-urls=http://127.0.0.1:2381 --listen-peer-urls=https://192.28.229.9:2380 --name=cluster1-controlplane --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt --peer-client-cert-auth=true --peer-key-file=/etc/kubernetes/pki/etcd/peer.key --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt --snapshot-count=10000 --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt\n</code></pre> <p>Ans is <code>var/lib/etcd</code></p>  ---  <pre><code># First set the context to cluster1:\n\n$  kubectl config use-context cluster1\nSwitched to context \"cluster1\".\n\n\n# Next, inspect the endpoints and certificates used by the etcd pod. We will make use of these to take the backup.\n\n$ kubectl describe  pods -n kube-system etcd-cluster1-controlplane  | grep advertise-client-urls\n\n--advertise-client-urls=https://10.1.218.16:2379\n\n$  kubectl describe  pods -n kube-system etcd-cluster1-controlplane  | grep pki\n    --cert-file=/etc/kubernetes/pki/etcd/server.crt\n    --key-file=/etc/kubernetes/pki/etcd/server.key\n    --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt\n    --peer-key-file=/etc/kubernetes/pki/etcd/peer.key\n    --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt\n    --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt\n    /etc/kubernetes/pki/etcd from etcd-certs (rw)\n\n    Path:          /etc/kubernetes/pki/etcd\n\n# SSH to the controlplane node of cluster1 and then take the backup using the endpoints and certificates we identified above:\n\ncontrolplane$ \nETCDCTL_API=3 etcdctl \\\n--endpoints=https://10.1.220.8:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt \\\n--cert=/etc/kubernetes/pki/etcd/server.crt \\\n--key=/etc/kubernetes/pki/etcd/server.key \\\nsnapshot save /opt/cluster1.db\n\nSnapshot saved at /opt/cluster1.db\n\n\n# Finally, copy the backup to the student-node. To do this, go back to the student-node and use scp as shown below:\n\n$  scp cluster1-controlplane:/opt/cluster1.db /opt \n</code></pre> An ETCD backup for cluster2 is stored at <code>/opt/cluster2.db</code>. Use this snapshot file to carryout a restore on cluster2 to a new path <code>/var/lib/etcd-data-new</code> <pre><code># Step 1. Copy the snapshot file from the student-node to the etcd-server. In the example below, we are copying it to the /root directory:\n\nstudent-node ~  scp /opt/cluster2.db etcd-server:/root\ncluster2.db                                                                                                        100% 1108KB 178.5MB/s   00:00    \n\nstudent-node ~ \u279c  \n\n# Step 2: Restore the snapshot on the cluster2. Since we are restoring directly on the etcd-server, we can use the endpoint https:/127.0.0.1. Use the same certificates that were identified earlier. Make sure to use the data-dir as /var/lib/etcd-data-new:\n\netcd-server ~ \u279c  ETCDCTL_API=3 etcdctl --endpoints=https://127.0.0.1:2379 --cacert=/etc/etcd/pki/ca.pem --cert=/etc/etcd/pki/etcd.pem --key=/etc/etcd/pki/etcd-key.pem snapshot restore /root/cluster2.db --data-dir /var/lib/etcd-data-new\n\n{\"level\":\"info\",\"ts\":1662004927.2399247,\"caller\":\"snapshot/v3_snapshot.go:296\",\"msg\":\"restoring snapshot\",\"path\":\"/root/cluster2.db\",\"wal-dir\":\"/var/lib/etcd-data-new/member/wal\",\"data-dir\":\"/var/lib/etcd-data-new\",\"snap-dir\":\"/var/lib/etcd-data-new/member/snap\"}\n{\"level\":\"info\",\"ts\":1662004927.2584803,\"caller\":\"membership/cluster.go:392\",\"msg\":\"added member\",\"cluster-id\":\"cdf818194e3a8c32\",\"local-member-id\":\"0\",\"added-peer-id\":\"8e9e05c52164694d\",\"added-peer-peer-urls\":[\"http://localhost:2380\"]}\n{\"level\":\"info\",\"ts\":1662004927.264258,\"caller\":\"snapshot/v3_snapshot.go:309\",\"msg\":\"restored snapshot\",\"path\":\"/root/cluster2.db\",\"wal-dir\":\"/var/lib/etcd-data-new/member/wal\",\"data-dir\":\"/var/lib/etcd-data-new\",\"snap-dir\":\"/var/lib/etcd-data-new/member/snap\"}\n\netcd-server ~ \u279c  \n\n# Step 3: Update the systemd service unit file for etcdby running vi /etc/systemd/system/etcd.service and add the new value for data-dir:\n\n[Unit]\nDescription=etcd key-value store\nDocumentation=https://github.com/etcd-io/etcd\nAfter=network.target\n\n[Service]\nUser=etcd\nType=notify\nExecStart=/usr/local/bin/etcd \\\n--name etcd-server \\\n--data-dir=/var/lib/etcd-data-new \\\n---End of Snippet---\n\n\n# Step 4: make sure the permissions on the new directory is correct (should be owned by etcd user):\n\netcd-server /var/lib \u279c  chown -R etcd:etcd /var/lib/etcd-data-new\n\netcd-server /var/lib \u279c \n\n\netcd-server /var/lib \u279c  ls -ld /var/lib/etcd-data-new/\ndrwx------ 3 etcd etcd 4096 Sep  1 02:41 /var/lib/etcd-data-new/\netcd-server /var/lib \u279c \n\n#  Step 5: Finally, reload and restart the etcd service.\n\netcd-server ~/default.etcd \u279c  systemctl daemon-reload \netcd-server ~ \u279c  systemctl restart etcd\n\n\n#  Step 6 (optional): It is recommended to restart controlplane components (e.g. kube-scheduler, kube-controller-manager, kubelet) to ensure that they don't rely on some stale data. \n</code></pre>"},{"location":"k8s/cka-exam-commands/#security","title":"Security","text":""},{"location":"k8s/cka-exam-commands/#view-cert-details","title":"View Cert Details","text":"Identify the certificate file used for the kube-api server and Identify the Certificate file used to authenticate kube-apiserver as a client to ETCD Server <pre><code>controlplane /etc/kubernetes/pki \u279c  ls /etc/kubernetes/pki/ | grep .crt\napiserver.crt\napiserver-etcd-client.crt\napiserver-kubelet-client.crt\nca.crt\nfront-proxy-ca.crt\nfront-proxy-client.crt\n</code></pre> <p>Ans is <code>apiserver.crt</code> for 1<sup>st</sup> question and <code>apiserver-etcd-client.crt</code> for 2<sup>nd</sup></p> <pre><code>controlplane /etc/kubernetes/pki \u279c  ls /etc/kubernetes/pki/ | grep .key\napiserver-etcd-client.key\napiserver.key\napiserver-kubelet-client.key # key used to authenticate kubeapi-server to the kubelet server.\nca.key\nfront-proxy-ca.key\nfront-proxy-client.key\nsa.key  \n</code></pre> Identify the ETCD Server Certificate used to host ETCD server <pre><code># TIP: Look for cert-file option in the file /etc/kubernetes/manifests/etcd.yaml.\ncontrolplane /etc/kubernetes/manifests \u279c  cat etcd.yaml | grep -i .crt\n    - --cert-file=/etc/kubernetes/pki/etcd/server.crt # answer\n    - --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt\n    - --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt\n    - --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt\n</code></pre> Identify the ETCD Server CA Root Certificate used to serve ETCD Server <p>ETCD can have its own CA. So this may be a different CA certificate than the one used by kube-api server.</p> <pre><code># TIP: Look for CA Certificate (trusted-ca-file) in file /etc/kubernetes/manifests/etcd.yaml.\n\ncontrolplane /etc/kubernetes/manifests \u279c  cat etcd.yaml | grep -i .crt\n    - --cert-file=/etc/kubernetes/pki/etcd/server.crt\n    - --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt\n    - --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt\n    - --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt # ans\n</code></pre> What is the Common Name (CN) configured on the Kube API Server Certificate? <p>OpenSSL Syntax: openssl x509 -in file-path.crt -text -noout</p> <pre><code># TIP: Run the command openssl x509 -in /etc/kubernetes/pki/apiserver.crt -text and look for Subject CN.\n\ncontrolplane /etc/kubernetes/pki \u2716 openssl x509 -in apiserver.crt -text | grep -i cn\n        Issuer: CN = kubernetes             # Name of CA who issued the cert\n        Subject: CN = kube-apiserver        # What is the Common Name (CN) configured on the Kube API Server\nMIIDjDCCAnSgAwIBAgIIVPn/5jFVfAMwDQYJKoZIhvcNAQELBQAwFTETMBEGA1UE\nnUyXcccNRLfQfrhu9NoD+4Nq7gM99y5QRpD8QimBnv1DBzXk+XWoC2Ka3EpmRzZZ\n</code></pre> Which of the below alternate names is not configured on the Kube API Server Certificate? <pre><code>#TIP: Run the command openssl x509 -in /etc/kubernetes/pki/apiserver.crt -text and look at Alternative Names as shown below\n\nX509v3 Subject Alternative Name: \nDNS:controlplane, DNS:kubernetes, DNS:kubernetes.default, DNS:kubernetes.default.svc, DNS:kubernetes.default.svc.cluster.local, IP Address:10.96.0.1, IP Address:192.2.128.9\n</code></pre> How long, from the issued date, is the Kube-API Server Certificate valid for? <pre><code>#TIP:  Run the command openssl x509 -in /etc/kubernetes/pki/apiserver.crt -text and check on the Expiry date.\n        Issuer: CN = etcd-ca\n        Validity\n            Not Before: Nov 20 00:50:29 2023 GMT\n            Not After : Nov 19 00:50:29 2024 GMT\n</code></pre> How long, from the issued date, is the Root CA Certificate valid for? <pre><code>#TIP: Run the command openssl x509 -in /etc/kubernetes/pki/ca.crt -text and look for the validity.\n    Data:\n        Version: 3 (0x2)\n        Serial Number: 0 (0x0)\n        Signature Algorithm: sha256WithRSAEncryption\n        Issuer: CN = kubernetes\n        Validity\n            Not Before: Nov 20 00:50:28 2023 GMT\n            Not After : Nov 17 00:50:28 2033 GMT\n        Subject: CN = kubernetes\n</code></pre> The kube-api server stopped again! Check it out. Inspect the kube-api server logs and identify the root cause and fix the issue <p>Run crictl ps -a command to identify the kube-api server container. Run crictl logs container-id command to view the logs.</p> <pre><code>crictl ps -a\nCONTAINER           IMAGE               CREATED              STATE               NAME                      ATTEMPT             POD ID              POD\n3bac921cb4a5a       6f707f569b572       16 seconds ago       Running             kube-apiserver            0                   fec26f7c6715a       kube-apiserver-controlplane\n33527b14bc1bf       f73f1b39c3fe8       22 seconds ago       Running             kube-scheduler            2                   fb3dc26664afc       kube-scheduler-controlplane\n627a986414afe       95fe52ed44570       25 seconds ago       Running             kube-controller-manager   2                   c674e558cf141       kube-controller-manager-controlplane\n5d7b588da6e90       86b6af7dd652c       About a minute ago   Running             etcd                      0                   efd0a8d9c600e       etcd-controlplane\n762ccbdfde7a8       f73f1b39c3fe8       4 minutes ago        Exited              kube-scheduler            1                   fb3dc26664afc       kube-scheduler-controlplane\nb3e4272eefb7b       95fe52ed44570       4 minutes ago        Exited              kube-controller-manager   1                   c674e558cf141       kube-controller-manager-controlplane\n7fa9a78979c3e       ead0a4a53df89       35 minutes ago       Running             coredns                   0                   c23b177006c7a       coredns-5d78c9869d-p8tjq\nbf84f7ebb5d43       ead0a4a53df89       35 minutes ago       Running             coredns                   0                   274222b10e501       coredns-5d78c9869d-84jrn\n247e828f3e5e3       8b675dda11bb1       35 minutes ago       Running             kube-flannel              0                   579d556b90be1       kube-flannel-ds-6xdvn\nb412cc3976452       8b675dda11bb1       35 minutes ago       Exited              install-cni               0                   579d556b90be1       kube-flannel-ds-6xdvn\n63734700d5255       fcecffc7ad4af       35 minutes ago       Exited              install-cni-plugin        0                   579d556b90be1       kube-flannel-ds-6xdvn\n7c4e662a2827d       5f82fc39fa816       35 minutes ago       Running             kube-proxy                0                   60aabd71d5180       kube-proxy-hs4c4\n</code></pre> <pre><code>crictl logs da40e86464c04\nI1120 01:29:05.406801       1 server.go:551] external host was not specified, using 192.2.128.9\nI1120 01:29:05.407768       1 server.go:165] Version: v1.27.0\nI1120 01:29:05.407793       1 server.go:167] \"Golang settings\" GOGC=\"\" GOMAXPROCS=\"\" GOTRACEBACK=\"\"\nI1120 01:29:05.685211       1 shared_informer.go:311] Waiting for caches to sync for node_authorizer\nI1120 01:29:05.694542       1 plugins.go:158] Loaded 12 mutating admission controller(s) successfully in the following order: NamespaceLifecycle,LimitRanger,ServiceAccount,NodeRestriction,TaintNodesByCondition,Priority,DefaultTolerationSeconds,DefaultStorageClass,StorageObjectInUseProtection,RuntimeClass,DefaultIngressClass,MutatingAdmissionWebhook.\nI1120 01:29:05.694560       1 plugins.go:161] Loaded 13 validating admission controller(s) successfully in the following order: LimitRanger,ServiceAccount,PodSecurity,Priority,PersistentVolumeClaimResize,RuntimeClass,CertificateApproval,CertificateSigning,ClusterTrustBundleAttest,CertificateSubjectRestriction,ValidatingAdmissionPolicy,ValidatingAdmissionWebhook,ResourceQuota.\nW1120 01:29:05.700731       1 logging.go:59] [core] [Channel #1 SubChannel #2] grpc: addrConn.createTransport failed to connect to {\n\"Addr\": \"127.0.0.1:2379\",\n\"ServerName\": \"127.0.0.1\",\n\"Attributes\": null,\n\"BalancerAttributes\": null,\n\"Type\": 0,\n\"Metadata\": null\n}. Err: connection error: desc = \"transport: authentication handshake failed: tls: failed to verify certificate: x509: certificate signed by unknown authority\"\nW1120 01:29:06.688471       1 logging.go:59] [core] [Channel #4 SubChannel #6] grpc: addrConn.createTransport failed to connect to {\n\"Addr\": \"127.0.0.1:2379\",\n\"ServerName\": \"127.0.0.1\",\n\"Attributes\": null,\n\"BalancerAttributes\": null,\n\"Type\": 0,\n\"Metadata\": null\n</code></pre> <p>Here specify the right <code>ca cert</code> file  </p> A new member akshay joined our team. He requires access to our cluster <p>The Certificate Signing Request is at the /root location.</p> <pre><code>Use this command to generate the base64 encoded format as following: -\n\ncat akshay.csr | base64 -w 0\n\nFinally, save the below YAML in a file and create a CSR name akshay as follows: -\n\n---\napiVersion: certificates.k8s.io/v1\nkind: CertificateSigningRequest\nmetadata:\nname: akshay\nspec:\ngroups:\n- system:authenticated\nrequest: &lt;Paste the base64 encoded value of the CSR file&gt;\nsignerName: kubernetes.io/kube-apiserver-client\nusages:\n- client auth\n\n\nkubectl apply -f akshay-csr.yaml\n</code></pre> <p>Sample shown below</p> <pre><code>controlplane ~ \u279c  cat csr-mani.yaml \napiVersion: certificates.k8s.io/v1\nkind: CertificateSigningRequest\nmetadata:\nname: akshay\nspec:\nrequest: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURSBSRVFVRVNULS0tLS0KTUlJQ1ZqQ0NBVDRDQVFBd0VURVBNQTBHQTFVRUF3d0dZV3R6YUdGNU1JSUJJakFOQmdrcWhraUc5dzBCQVFFRgpBQU9DQVE4QU1JSUJDZ0tDQVFFQTc2WjA5ZlgzLzhaanB1Rlc2aE9xeG1tYW1qb3VRVDZvSDJ0WHltd0xVOGx5Cmg4dTQwV1RtTVRxbzk4Kzk0a3lnOTdKUFRWbDdsWkNRbkZKdmlpTlAzVlRRa0tOU3FOakQzcGRESUxsUXErcHQKeDV2bXhhcUxmTlZocEt5QzdkZlk1L1VEZHNPT05CYit4dWNkNmx4YU5kdTJqMml4alF2aisyOXdRdExvaUYxNQpQNDZ5NkQ0c1dnb04zcWc4Y1RhNTRNcnRPc1FBem1CZHdQcnVXNXFlODBNaGMrQk9HWmx2YlZPcmIzREVINmFOCmNTMzA2SGlwUzl5TkpOMzArdThwd1FtcS9QM0JneHJuOS9DNkhPY1JiaHQ0WTE2Q2hjZUk3anFjcVRHbithcE4KemgxeDN2ZGg3dVNCam1Pb3JsNVpTYW1FcHhOdnBpVkdqNUZMaVBYYW93SURBUUFCb0FBd0RRWUpLb1pJaHZjTgpBUUVMQlFBRGdnRUJBRFpINmFtcXdEMjZDM1dwVjlKNzM1N3hibGhIQVkrK3FOSzR6Qk1jZE01dnUwV1VYK3dGCkFRd2czREFORW52UThMdWJnV3RLaEkwUGxPbjRWK1JSZzIxK01qUFhsUzNDWkZodEN6VE9oY0hwUGVBQnZZQnEKWkthTHBTTVlTdEVqYnNsWGg1dVhiZmxMRHBRSllZNEdTc2tXRStsZnVzTUNyNFhGNzNSVUNFWHdHZGFFNFdpcAp6WGFsb0x1ZFdneGFmVlRSR1JWK2RKMXNuV2pMaWRySVU3NDZxUVZiUW1Gc0pWU2VaTjZNSGRiU0xIZFZNZjJMCmR3dThNcUVpRUQzeUhJT2dCdlowaWJQT1VtMmFrZVFUT2F5cEhCQVJJanRSYVA5cnhuYUM4ckNZK0czb0MvdEQKY1hpajk3ak5pRzU0UWJMMEN5NUdqcEQ2YndTek5iM0dpN3c9Ci0tLS0tRU5EIENFUlRJRklDQVRFIFJFUVVFU1QtLS0tLQo=\nsignerName: kubernetes.io/kube-apiserver-client\nexpirationSeconds: 86400  # one day\nusages:\n- client auth\n</code></pre> <p>csr is created as shown below <pre><code>controlplane ~ \u279c  k apply -f csr-mani.yaml \ncertificatesigningrequest.certificates.k8s.io/akshay created\n</code></pre></p> <p>check the CSR's <pre><code>controlplane ~ \u279c  k get csr\nNAME        AGE   SIGNERNAME                                    REQUESTOR                  REQUESTEDDURATION   CONDITION\nakshay      73s   kubernetes.io/kube-apiserver-client           kubernetes-admin           24h                 Pending\ncsr-8wnnl   17m   kubernetes.io/kube-apiserver-client-kubelet   system:node:controlplane   &lt;none&gt;              Approved,Issued\n</code></pre></p> Please approve the CSR <pre><code>controlplane ~ \u2716 k certificate approve akshay\ncertificatesigningrequest.certificates.k8s.io/akshay approved\n</code></pre> Describe the new CSR you got <pre><code># TIP: use the get -o yaml instead of describe\ncontrolplane ~ \u279c  k get  csr agent-smith -o yaml \n\napiVersion: certificates.k8s.io/v1\nkind: CertificateSigningRequest\nmetadata:\ncreationTimestamp: \"2023-11-20T02:04:44Z\"\nname: agent-smith\nresourceVersion: \"2170\"\nuid: 8eeaf351-96ac-488a-8d83-cab1edc14605\nspec:\ngroups:\n- system:masters\n- system:authenticated\nrequest: XXXX_OMITTED\nsignerName: kubernetes.io/kube-apiserver-client\nusages:\n- digital signature\n- key encipherment\n- server auth\nusername: agent-x\nstatus: {}\n</code></pre>"},{"location":"k8s/cka-exam-commands/#kubeconfig","title":"Kubeconfig","text":"<p>Where is the default kubeconfig file located in the current environment?</p> <pre><code>/root/.kube/config\n</code></pre> How many clusters are defined in the default kubeconfig file? <pre><code># Run the kubectl config view command and count the number of clusters.\ncontrolplane ~/.kube \u279c  k config view\napiVersion: v1\nclusters:\n- cluster:\n    certificate-authority-data: DATA+OMITTED\n    server: https://controlplane:6443\nname: kubernetes\ncontexts:\n- context:  \n    cluster: kubernetes\n    user: kubernetes-admin\nname: kubernetes-admin@kubernetes\ncurrent-context: kubernetes-admin@kubernetes\nkind: Config\npreferences: {}\nusers:  # numnber of users\n- name: kubernetes-admin\nuser:\n    client-certificate-data: DATA+OMITTED\n    client-key-data: DATA+OMITTED\n</code></pre> <p>Lets check the kube config file</p> <pre><code>~/.kube \u279c  cat /root/my-kube-config \napiVersion: v1\nkind: Config\n\nclusters: # 4 clusters are configured\n- name: production\ncluster:\n    certificate-authority: /etc/kubernetes/pki/ca.crt\n    server: https://controlplane:6443\n\n- name: development\ncluster:\n    certificate-authority: /etc/kubernetes/pki/ca.crt\n    server: https://controlplane:6443\n\n- name: kubernetes-on-aws\ncluster:\n    certificate-authority: /etc/kubernetes/pki/ca.crt\n    server: https://controlplane:6443\n\n- name: test-cluster-1\ncluster:\n    certificate-authority: /etc/kubernetes/pki/ca.crt\n    server: https://controlplane:6443\n\ncontexts:\n- name: test-user@development\ncontext:\n    cluster: development\n    user: test-user\n\n- name: aws-user@kubernetes-on-aws\ncontext:\n    cluster: kubernetes-on-aws\n    user: aws-user\n\n- name: test-user@production\ncontext:\n    cluster: production\n    user: test-user\n\n- name: research\ncontext:\n    cluster: test-cluster-1\n    user: dev-user  # user for research context\n\nusers:\n- name: test-user\nuser:\n    client-certificate: /etc/kubernetes/pki/users/test-user/test-user.crt\n    client-key: /etc/kubernetes/pki/users/test-user/test-user.key\n- name: dev-user\nuser:\n    client-certificate: /etc/kubernetes/pki/users/dev-user/developer-user.crt\n    client-key: /etc/kubernetes/pki/users/dev-user/dev-user.key\n- name: aws-user\nuser:\n    client-certificate: /etc/kubernetes/pki/users/aws-user/aws-user.crt\n    client-key: /etc/kubernetes/pki/users/aws-user/aws-user.key\n\ncurrent-context: test-user@development # current context\npreferences: {}\n</code></pre> I would like to use the dev-user to access test-cluster-1. Set the current context to the right one so I can do that <pre><code># TIP: use the right context file as well\ncontrolplane ~ \u279c  k config --kubeconfig /root/my-kube-config use-context research\nSwitched to context \"research\".\n\n# Test it\ncontrolplane ~ \u279c  k config --kubeconfig /root/my-kube-config current-context \nresearch\n</code></pre>"},{"location":"k8s/cka-exam-commands/#rbac","title":"RBAC","text":"<p>Role-based access control (RBAC) is a method of regulating access to computer or network resources based on the roles of individual users within your organization.</p> <p>The RBAC API declares four kinds of Kubernetes object: Role, ClusterRole, RoleBinding and ClusterRoleBinding. </p> <p>An RBAC Role or ClusterRole contains rules that represent a set of permissions. Permissions are purely additive (there are no \"deny\" rules). A Role always sets permissions within a particular namespace; when you create a Role, you have to specify the namespace it belongs in. ClusterRole, by contrast, is a non-namespaced resource. The resources have different names (Role and ClusterRole) because a Kubernetes object always has to be either namespaced or not namespaced; it can't be both.</p> <pre><code># Read role\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  namespace: default\n  name: pod-reader\nrules:\n- apiGroups: [\"\"] # \"\" indicates the core API group\n  resources: [\"pods\"]\n  verbs: [\"get\", \"watch\", \"list\"]\n</code></pre> <p>ClusterRole example shown below</p> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  # \"namespace\" omitted since ClusterRoles are not namespaced\n  name: secret-reader\nrules:\n- apiGroups: [\"\"]\n  #\n  # at the HTTP level, the name of the resource for accessing Secret\n  # objects is \"secrets\"\n  resources: [\"secrets\"]\n  verbs: [\"get\", \"watch\", \"list\"]\n</code></pre> <p>A role binding grants the permissions defined in a role to a user or set of users. It holds a list of subjects (users, groups, or service accounts), and a reference to the role being granted. A RoleBinding grants permissions within a specific namespace whereas a ClusterRoleBinding grants that access cluster-wide.</p> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\n# This role binding allows \"jane\" to read pods in the \"default\" namespace.\n# You need to already have a Role named \"pod-reader\" in that namespace.\nkind: RoleBinding\nmetadata:\n  name: read-pods\n  namespace: default\nsubjects:\n# You can specify more than one \"subject\"\n- kind: User\n  name: jane # \"name\" is case sensitive\n  apiGroup: rbac.authorization.k8s.io\nroleRef:\n  # \"roleRef\" specifies the binding to a Role / ClusterRole\n  kind: Role #this must be Role or ClusterRole\n  name: pod-reader # this must match the name of the Role or ClusterRole you wish to bind to\n  apiGroup: rbac.authorization.k8s.io\n</code></pre> <p>Using ClusterRole in RoleBinding</p> <p>A RoleBinding can also reference a ClusterRole to grant the permissions defined in that ClusterRole to resources inside the RoleBinding's namespace. This kind of reference lets you define a set of common roles across your cluster, then reuse them within multiple namespaces.</p> <p>To grant permissions across a whole cluster, you can use a ClusterRoleBinding. The following ClusterRoleBinding allows any user in the group \"manager\" to read secrets in any namespace.</p> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\n# This cluster role binding allows anyone in the \"manager\" group to read secrets in any namespace.\nkind: ClusterRoleBinding\nmetadata:\n  name: read-secrets-global\nsubjects:\n- kind: Group\n  name: manager # Name is case sensitive\n  apiGroup: rbac.authorization.k8s.io\nroleRef:\n  kind: ClusterRole\n  name: secret-reader\n  apiGroup: rbac.authorization.k8s.io\n</code></pre> <p>Question</p> <p>Inspect the environment and identify the authorization modes configured on the cluster.</p> <pre><code># Use the command kubectl describe pod kube-apiserver-controlplane -n kube-system and look for --authorization-mode.\n\ncontrolplane ~ \u279c  k -n kube-system describe po kube-apiserver-controlplane | grep -i auth\n      --authorization-mode=Node,RBAC\n      --enable-bootstrap-token-auth=true\n</code></pre> What are the resources the kube-proxy role in the kube-system namespace is given access to? <pre><code>controlplane ~ \u279c  k describe role kube-proxy -n kube-system \nName:         kube-proxy\nLabels:       &lt;none&gt;\nAnnotations:  &lt;none&gt;\nPolicyRule:\nResources   Non-Resource URLs  Resource Names  Verbs\n---------   -----------------  --------------  -----\nconfigmaps  []                 [kube-proxy]    [get]\n</code></pre> Which account is the kube-proxy role assigned to? <pre><code>controlplane ~ \u279c  kubectl describe rolebinding kube-proxy -n kube-system\nName:         kube-proxy\nLabels:       &lt;none&gt;\nAnnotations:  &lt;none&gt;\nRole:\nKind:  Role\nName:  kube-proxy\nSubjects:\nKind   Name                                             Namespace\n----   ----                                             ---------\nGroup  system:bootstrappers:kubeadm:default-node-token \n</code></pre> A user dev-user is created. User's details have been added to the kubeconfig file. Inspect the permissions granted to the user. Check if the user can list pods in the default namespace. <pre><code>k get pods --as dev-user\n</code></pre> Create the necessary roles and role bindings required for the dev-user to create, list and delete pods in the default namespace <pre><code>controlplane ~ \u2716 kubectl create role developer  --verb=create --verb=list --verb=delete --resource=pods\n\nrole.rbac.authorization.k8s.io/developer created\n</code></pre> <p>Create a binding for it as shown below <pre><code>controlplane ~ \u2716 kubectl create rolebinding dev-user-binding --role=developer --user=dev-user --namespace=default\n\nrolebinding.rbac.authorization.k8s.io/dev-user-binding created\n\n\ncontrolplane ~ \u279c  k get pods --as dev-user\nNAME                   READY   STATUS    RESTARTS   AGE\nred-697496b845-2srbh   1/1     Running   0          18m\nred-697496b845-n4zsd   1/1     Running   0          18m\n</code></pre></p> What user/groups are the cluster-admin role bound to? <p>The ClusterRoleBinding for the role is with the same name.</p> <pre><code>controlplane ~ \u279c  k get  clusterrolebindings.rbac.authorization.k8s.io cluster-admin -o yaml\n\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\nannotations:\n    rbac.authorization.kubernetes.io/autoupdate: \"true\"\ncreationTimestamp: \"2023-11-21T01:48:21Z\"\nlabels:\n    kubernetes.io/bootstrapping: rbac-defaults\nname: cluster-admin\nresourceVersion: \"134\"\nuid: 4330de0f-ef56-42ef-8ea9-c2bb3e26f4a2\nroleRef:\napiGroup: rbac.authorization.k8s.io\nkind: ClusterRole\nname: cluster-admin\nsubjects:\n- apiGroup: rbac.authorization.k8s.io\nkind: Group\nname: system:masters # answer\n</code></pre> What level of permission does the cluster-admin role grant? <pre><code>controlplane ~ \u279c  k get  clusterrole  cluster-admin -o yaml\n\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\nannotations:\n    rbac.authorization.kubernetes.io/autoupdate: \"true\"\ncreationTimestamp: \"2023-11-21T01:48:20Z\"\nlabels:\n    kubernetes.io/bootstrapping: rbac-defaults\nname: cluster-admin\nresourceVersion: \"72\"\nuid: c5bab975-ad5f-48bb-837c-65aae7200b9e\nrules:\n- apiGroups:\n- '*'\nresources:\n- '*'\nverbs:\n- '*'\n- nonResourceURLs:\n- '*'\nverbs:\n- '*'\n</code></pre> <p>Answer: Perform any role on any resource in the cluster</p> A new user michelle joined the team. She will be focusing on the nodes in the cluster. Create the required ClusterRoles and ClusterRoleBindings so she gets access to the nodes <pre><code>controlplane ~ \u279c  kubectl create clusterrole node-reader --verb=get,list,watch --resource=nodes\n\nclusterrole.rbac.authorization.k8s.io/node-reader created\n\n\ncontrolplane ~ \u2716 kubectl create clusterrolebinding node-reader-cluster-binding --clusterrole=node-reader --user=michelle\n\nclusterrolebinding.rbac.authorization.k8s.io/node-reader-cluster-binding created\n</code></pre> Michelle's responsibilities are growing and now she will be responsible for storage as well. Create the required ClusterRoles and ClusterRoleBindings to allow her access to Storage <p>Get the API groups and resource names from command kubectl api-resources. Use the given spec:</p> <pre><code>ClusterRole: storage-admin\nResource: persistentvolumes\nResource: storageclasses\nClusterRoleBinding: michelle-storage-admin\nClusterRoleBinding Subject: michelle\nClusterRoleBinding Role: storage-admin\n</code></pre> <p>Answer is shown below</p> <pre><code>controlplane ~ \u279c  kubectl create clusterrole storage-admin  --verb=get,list,watch --resource=persistentvolumes --verb=get,list --resource=storageclasses\nclusterrole.rbac.authorization.k8s.io/storage-admin created\n\ncontrolplane ~ \u279c  kubectl create clusterrolebinding michelle-storage-admin  --clusterrole=storage-admin --user=michelle\nclusterrolebinding.rbac.authorization.k8s.io/michelle-storage-admin created\n</code></pre>"},{"location":"k8s/cka-exam-commands/#service-accounts","title":"Service Accounts","text":"<ul> <li> <p>A service account is a type of non-human account that, in Kubernetes, provides a distinct identity in a Kubernetes cluster.</p> </li> <li> <p>Application Pods, system components, and entities inside and outside the cluster can use a specific ServiceAccount's credentials to identify as that ServiceAccount.</p> </li> <li> <p>Each service account is bound to a Kubernetes namespace. Every namespace gets a default ServiceAccount upon creation.</p> </li> </ul> <p>Which sercive account is used by deployment?</p> <pre><code>controlplane ~ \u279c  k get po web-dashboard-97c9c59f6-x2zdd -o yaml | grep -i service\n\n- mountPath: /var/run/secrets/kubernetes.io/serviceaccount\nenableServiceLinks: true\nserviceAccount: default\nserviceAccountName: default\n    - serviceAccountToken:\n</code></pre> <p>The application needs a ServiceAccount with the Right permissions to be created to authenticate to Kubernetes. The default ServiceAccount has limited access. Create a new ServiceAccount named dashboard-sa.</p> <pre><code>controlplane ~ \u279c  k create sa dashboard-sa\n\nserviceaccount/dashboard-sa created\n</code></pre> <p>create a new token for a SA <pre><code>controlplane ~ \u279c  k create token dashboard-sa \n</code></pre></p>"},{"location":"k8s/cka-exam-commands/#image-security","title":"Image Security","text":"<p>What secret type must we choose for docker registry? <pre><code>root@controlplane ~ \u279c  k create secret --help \nCreate a secret using specified subcommand.\n\nAvailable Commands:\n  docker-registry   Create a secret for use with a Docker registry # answer\n  generic           Create a secret from a local file, directory, or literal value\n  tls               Create a TLS secret\n\nUsage:\n  kubectl create secret [flags] [options]\n</code></pre></p> <p>We decided to use a modified version of the application from an internal private registry. Update the image of the deployment to use a new image from myprivateregistry.com:5000</p> <pre><code># update the image as shown below\n    spec:\n      containers:\n      - image: myprivateregistry.com:5000/nginx:alpine\n        imagePullPolicy: IfNotPresent\n</code></pre> <p>Create a secret object with the credentials required to access the registry.</p> <pre><code>Name: private-reg-cred\nUsername: dock_user\nPassword: dock_password\nServer: myprivateregistry.com:5000\nEmail: dock_user@myprivateregistry.com\n</code></pre> <p>Create using below <pre><code>root@controlplane ~ \u279c  kubectl create secret docker-registry private-reg-cred   --docker-email=dock_user@myprivateregistry.com    --docker-username=dock_user   --docker-password=dock_password   --docker-server=myprivateregistry.com:5000\n\nsecret/private-reg-cred created\n</code></pre></p> <p>Configure the deployment to use credentials from the new secret to pull images from the private registry</p> <pre><code># Add the imagepull secret as shown below\n    spec:\n      containers:\n      - image: myprivateregistry.com:5000/nginx:alpine\n        imagePullPolicy: IfNotPresent\n        name: nginx\n        resources: {}\n        terminationMessagePath: /dev/termination-log\n        terminationMessagePolicy: File\n      dnsPolicy: ClusterFirst\n      imagePullSecrets:\n      - name: private-reg-cred\n</code></pre>"},{"location":"k8s/cka-exam-commands/#security-contexts","title":"Security Contexts","text":"<p>A security context defines privilege and access control settings for a Pod or Container. Security context settings include, but are not limited to:</p> <pre><code>Discretionary Access Control: Permission to access an object, like a file, is based on user ID (UID) and group ID (GID).\n\nSecurity Enhanced Linux (SELinux): Objects are assigned security labels.\n\nRunning as privileged or unprivileged.\n\nLinux Capabilities: Give a process some privileges, but not all the privileges of the root user.\n</code></pre> <pre><code># Sample Security Context \napiVersion: v1\nkind: Pod\nmetadata:\n  name: security-context-demo\nspec:\n  securityContext:  # For container\n    runAsUser: 1000\n    runAsGroup: 3000\n    fsGroup: 2000\n  volumes:\n  - name: sec-ctx-vol\n    emptyDir: {}\n  containers:\n  - name: sec-ctx-demo\n    image: busybox:1.28\n    command: [ \"sh\", \"-c\", \"sleep 1h\" ]\n    volumeMounts:\n    - name: sec-ctx-vol\n      mountPath: /data/demo\n    securityContext:   # for Pod\n      allowPrivilegeEscalation: false\n</code></pre> What is the user used to execute the sleep process within the ubuntu-sleeper pod? <pre><code># Check the user by checking the security context or doing the exec -it\n\ncontrolplane ~ \u279c  k get  po ubuntu-sleeper -o yaml | grep -i securi\nsecurityContext: {}\n\n\n\ncontrolplane ~ \u279c  k exec -it ubuntu-sleeper -- /bin/bash\nroot@ubuntu-sleeper:/# whoami\nroot\n</code></pre> Edit the pod ubuntu-sleeper to run the sleep process with user ID 1010 <pre><code>spec:\ncontainers:\n- command:\n    - sleep\n    - \"4800\"\n    image: ubuntu\n    imagePullPolicy: Always\n    name: ubuntu\n    resources: {}\n    terminationMessagePath: /dev/termination-log\n    terminationMessagePolicy: File\n    volumeMounts:\n    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount\n    name: kube-api-access-grl6f\n    readOnly: true\ndnsPolicy: ClusterFirst\nenableServiceLinks: true\nnodeName: controlplane\npreemptionPolicy: PreemptLowerPriority\npriority: 0\nrestartPolicy: Always\nschedulerName: default-scheduler\nsecurityContext:\n    runAsUser: 1010  ## add this\n</code></pre> A Pod definition file named multi-pod.yaml is given. With what user are the processes in the web container started? <pre><code>controlplane ~ \u279c  cat multi-pod.yaml \napiVersion: v1\nkind: Pod\nmetadata:\nname: multi-pod\nspec:\nsecurityContext:\n    runAsUser: 1001\ncontainers:\n-  image: ubuntu\n    name: web\n    command: [\"sleep\", \"5000\"]\n    securityContext:\n    runAsUser: 1002   # ans is this as local will override the global user\n\n-  image: ubuntu\n    name: sidecar\n    command: [\"sleep\", \"5000\"]\n</code></pre> Update pod ubuntu-sleeper to run as Root user and with the SYS_TIME capability <pre><code>controlplane ~ \u279c  cat multi-pod.yaml \n\napiVersion: v1\nkind: Pod\nmetadata:\nname: ubuntu-sleeper\nspec:\ncontainers:\n-  image: ubuntu\n    name: web\n    command: [\"sleep\", \"5000\"]\n    securityContext:  # added to the container, not the pod\n    capabilities:\n        add:  [\"SYS_TIME\"] \n</code></pre>"},{"location":"k8s/cka-exam-commands/#network-policies","title":"Network Policies","text":"<p>If you want to control traffic flow at the IP address or port level for TCP, UDP, and SCTP protocols, then you might consider using Kubernetes NetworkPolicies for particular applications in your cluster.</p> <p>NetworkPolicies are an application-centric construct which allow you to specify how a pod is allowed to communicate with various network \"entities\" (we use the word \"entity\" here to avoid overloading the more common terms such as \"endpoints\" and \"services\", which have specific Kubernetes connotations) over the network</p> <ul> <li>By default, a pod is non-isolated for egress; all outbound connections are allowed.</li> <li>By default, a pod is non-isolated for ingress; all inbound connections are allowed. </li> </ul> <p>Network Plugin</p> <p>`Network policies`` are implemented by the network plugin. To use network policies, you must be using a networking solution which supports NetworkPolicy. Creating a NetworkPolicy resource without a controller that implements it will have no effect.</p> <pre><code># Network Policy Example\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: test-network-policy\n  namespace: default\nspec:\n  podSelector:\n    matchLabels:\n      role: db\n  policyTypes:\n    - Ingress\n    - Egress\n  ingress:\n    - from:\n        - ipBlock:\n            cidr: 172.17.0.0/16\n            except:\n              - 172.17.1.0/24\n        - namespaceSelector:\n            matchLabels:\n              project: myproject\n        - podSelector:\n            matchLabels:\n              role: frontend\n      ports:\n        - protocol: TCP\n          port: 6379\n  egress:\n    - to:\n        - ipBlock:\n            cidr: 10.0.0.0/24\n      ports:\n        - protocol: TCP\n          port: 5978\n</code></pre> Default deny egress <pre><code>---\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\nname: default-deny-egress\nspec:\npodSelector: {}\npolicyTypes:\n- Egress\n</code></pre> Default allow egress <pre><code>apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\nname: allow-all-egress\nspec:\npodSelector: {}\negress:\n- {}   # egress is defined here\npolicyTypes:\n- Egress\n</code></pre> Default deny all ingress and all egress traffic <pre><code>---\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\nname: default-deny-all\nspec:\npodSelector: {}\npolicyTypes:\n- Ingress\n- Egress\n</code></pre> What is meaning of this policy? <pre><code>controlplane ~ \u279c  k describe networkpolicies.networking.k8s.io payroll-policy\nName:         payroll-policy\nNamespace:    default\nCreated on:   2023-11-20 22:47:02 -0500 EST\nLabels:       &lt;none&gt;\nAnnotations:  &lt;none&gt;\nSpec:\nPodSelector:     name=payroll\nAllowing ingress traffic:\n    To Port: 8080/TCP\n    From:\n    PodSelector: name=internal  # traffic from internal pod to payroll pod is allowed\nNot affecting egress traffic\nPolicy Types: Ingress\n</code></pre> Use the spec given below. You might want to enable ingress traffic to the pod to test your rules in the UI <p>Policy Name: internal-policy</p> <p>Policy Type: Egress</p> <p>Egress Allow: payroll</p> <p>Payroll Port: 8080</p> <p>Egress Allow: mysql</p> <p>MySQL Port: 3306</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\nname: internal-policy\nnamespace: default\nspec:\npolicyTypes:\n    - Ingress\n    - Egress\ningress:\n    -  {}\negress:\n    - to:\n        - podSelector:\n            matchLabels:\n            name: payroll\n    ports:\n        - protocol: TCP\n        port: 8080\n    - to:\n        - podSelector:\n            matchLabels:\n            name: mysql\n    ports:\n        - protocol: TCP\n        port: 3306\n</code></pre>"},{"location":"k8s/cka-exam-commands/#storage","title":"Storage","text":""},{"location":"k8s/cka-exam-commands/#pv-and-pvc","title":"PV and PVC","text":"<p>A PersistentVolume (PV) is a piece of storage in the cluster that has been provisioned by an administrator or dynamically provisioned using Storage Classes. It is a resource in the cluster just like a node is a cluster resource. PVs are volume plugins like Volumes, but have a lifecycle independent of any individual Pod that uses the PV.</p> <p>A PersistentVolumeClaim (PVC) is a request for storage by a user. It is similar to a Pod. Pods consume node resources and PVCs consume PV resources. Pods can request specific levels of resources (CPU and Memory). Claims can request specific size and access modes (e.g., they can be mounted ReadWriteOnce, ReadOnlyMany or ReadWriteMany</p> <p>Reclaim Policy</p> <p>When a user is done with their volume, they can delete the PVC objects from the API that allows reclamation of the resource. The reclaim policy for a PersistentVolume tells the cluster what to do with the volume after it has been released of its claim. Currently, volumes can either be Retained, Recycled, or Deleted.</p> <ul> <li>Retain reclaim policy allows for manual reclamation of the resource. When the PersistentVolumeClaim is deleted, the PersistentVolume still exists and the volume is considered \"released\". But it is not yet available for another claim because the previous claimant's data remains on the volume. </li> <li>For volume plugins that support the Delete reclaim policy, deletion removes both the PersistentVolume object from Kubernetes, as well as the associated storage asset in the external infrastructure, such as an AWS EBS or GCE PD volume.</li> <li>If supported by the underlying volume plugin, the Recycle reclaim policy performs a basic scrub (rm -rf /thevolume/*) on the volume and makes it available again for a new claim.</li> </ul> Configure a volume to store these logs at /var/log/webapp on the host using <p>Name: webapp Image Name: kodekloud/event-simulator Volume HostPath: /var/log/webapp Volume Mount: /log</p> <pre><code>controlplane ~ \u2716 cat po.yaml\n\napiVersion: v1\nkind: Pod\nmetadata:\nname: webapp\nnamespace: default\nspec:\ncontainers:\n- image: kodekloud/event-simulator\n    imagePullPolicy: Always\n    name: event-simulator\n    resources: {}\n    volumeMounts:\n    - mountPath: /log\n    name: log-vol\n    readOnly: true\nvolumes:\n- name: log-vol\n    hostPath:\n    # directory location on host\n    path: /var/log/webapp\n    # this field is optional\n    type: Directory\n</code></pre> Create a Persistent Volume with the given specification <pre><code>Volume Name: pv-log\nStorage: 100Mi\nAccess Modes: ReadWriteMany\nHost Path: /pv/log\nReclaim Policy: Retain\n</code></pre> <pre><code>controlplane ~ \u279c  cat pv1.yaml \n\napiVersion: v1\nkind: PersistentVolume\nmetadata:\nname: pv-log\nspec:\ncapacity:\n    storage: 100Mi\nhostPath:\n    path: /pv/log\naccessModes:\n    - ReadWriteMany\npersistentVolumeReclaimPolicy: Retain\n</code></pre> Let us claim some of that storage for our application. Create a Persistent Volume Claim with the given specification <p>Volume Name: claim-log-1 Storage Request: 50Mi Access Modes: ReadWriteOnce</p> <pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\nname: claim-log-1\nspec:\naccessModes:\n    - ReadWriteOnce\nresources:\n    requests:\n    storage: 50Mi\n</code></pre> Check if the claim is bound or not? <pre><code>controlplane ~ \u2716 k get pv,pvc\nNAME                      CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                 STORAGECLASS   REASON   AGE\npersistentvolume/pv-log   100Mi      RWX            Retain           Bound    default/claim-log-1                           5m56s\n\nNAME                                STATUS   VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE\npersistentvolumeclaim/claim-log-1   Bound    pv-log   100Mi      RWX                           13s\n</code></pre> Update the webapp pod to use the persistent volume claim as its storage <p>Replace hostPath configured earlier with the newly created PersistentVolumeClaim.</p> <pre><code>Name: webapp\n\nImage Name: kodekloud/event-simulator\n\nVolume: PersistentVolumeClaim=claim-log-1\n\nVolume Mount: /log\n</code></pre> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\nname: webapp\nnamespace: default\nspec:\ncontainers:\n- image: kodekloud/event-simulator\n    imagePullPolicy: Always\n    name: event-simulator\n    resources: {}\n    volumeMounts:\n    - mountPath: /log\n    name: pv-claim\nvolumes:\n- name: pv-claim\n    persistentVolumeClaim:\n    claimName: claim-log-1\n</code></pre>"},{"location":"k8s/cka-exam-commands/#storage-class","title":"Storage Class","text":"<p>A StorageClass provides a way for administrators to describe the \"classes\" of storage they offer. Different classes might map to quality-of-service levels, or to backup policies, or to arbitrary policies determined by the cluster administrators.</p> <p>When a PVC does not specify a storageClassName, the default StorageClass is used. The cluster can only have one default StorageClass</p> <pre><code># SC example\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: standard\nprovisioner: kubernetes.io/aws-ebs\nparameters:\n  type: gp2\nreclaimPolicy: Retain\nallowVolumeExpansion: true\nmountOptions:\n  - debug\nvolumeBindingMode: Immediate\n</code></pre> Create a new PersistentVolumeClaim by the name of local-pvc that should bind to the volume local-pv <p>Inspect the pv local-pv for the specs.</p> <pre><code>PVC: local-pvc\n\nCorrect Access Mode?\n\nCorrect StorageClass Used?\n\nPVC requests volume size = 500Mi?\n</code></pre> <pre><code>controlplane ~ \u279c  cat pvc.yaml \n\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\nname: local-pvc\nspec:\naccessModes:\n    - ReadWriteOnce\nvolumeMode: Filesystem\nresources:\n    requests:\n    storage: 500Mi\nstorageClassName: local-storage\n</code></pre> Why is the PVC in a pending state despite making a valid request to claim the volume called local-pv? <pre><code># The StorageClass used by the PVC uses WaitForFirstConsumer volume binding mode. This means that the persistent volume will not bind to the claim until a pod makes use of the PVC to request storage.\ncontrolplane ~ \u2716  k describe pvc local-pvc | grep -A4 Events\nEvents:\nType    Reason                Age                   From                         Message\n----    ------                ----                  ----                         -------\nNormal  WaitForFirstConsumer  11s (x16 over 3m47s)  persistentvolume-controller  waiting for first consumer to be created before binding\n</code></pre> Create a new pod called nginx with the image nginx:alpine. The Pod should make use of the PVC local-pvc and mount the volume at the path /var/www/html <p>The PV local-pv should be in a bound state.</p> <pre><code>Pod created with the correct Image?\n\nPod uses PVC called local-pvc?\n\nlocal-pv bound?\n\nnginx pod running?\n\nVolume mounted at the correct path?\n</code></pre> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\ncreationTimestamp: null\nlabels:\n    run: nginx\nname: nginx\nspec:\ncontainers:\n- image: nginx:alpine\n    name: nginx\n    volumeMounts:\n    - mountPath: \"/var/www/html\"\n    name: mypd\nvolumes:\n    - name: mypd\n    persistentVolumeClaim:\n        claimName: local-pvc\ndnsPolicy: ClusterFirst\nrestartPolicy: Always\nstatus: {}\n</code></pre> Create a new Storage Class called delayed-volume-sc that makes use of the below specs: <p>provisioner: kubernetes.io/no-provisioner</p> <p>volumeBindingMode: WaitForFirstConsumer</p> <pre><code>controlplane ~ \u279c  cat sc.yaml \n\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\nname: delayed-volume-sc\nprovisioner: kubernetes.io/no-provisioner\nvolumeBindingMode: WaitForFirstConsumer\n</code></pre>"},{"location":"k8s/cka-exam-commands/#jq","title":"JQ","text":"<pre><code>k get nodes -o json | jq -c 'paths'  \n</code></pre>"},{"location":"k8s/cka-exam-commands/#networking","title":"Networking","text":"What is the network interface configured for cluster connectivity on the controlplane node? <pre><code>controlplane ~ \u2716 k get no -o wide\nNAME           STATUS   ROLES           AGE     VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION   CONTAINER-RUNTIME\ncontrolplane   Ready    control-plane   9m2s    v1.27.0   192.9.180.3   &lt;none&gt;        Ubuntu 20.04.5 LTS   5.4.0-1106-gcp   containerd://1.6.6  # get the IP for controlPlane\nnode01         Ready    &lt;none&gt;          8m37s   v1.27.0   192.9.180.6   &lt;none&gt;        Ubuntu 20.04.5 LTS   5.4.0-1106-gcp   containerd://1.6.6\n\n\ncontrolplane ~ \u279c  ip a | grep -B3 192.9.180.3 # get the interface from the IP\n366: eth0@if367: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UP group default \n    link/ether 02:42:c0:09:b4:03 brd ff:ff:ff:ff:ff:ff link-netnsid 0\n    inet 192.9.180.3/24 brd 192.9.180.255 scope global eth0\n</code></pre> Get the IP of default gateway <pre><code>controlplane ~ \u279c  ip route show default\n\ndefault via 172.25.0.1 dev eth1 \n</code></pre> What is the port the kube-scheduler is listening on in the controlplane node? <pre><code>controlplane ~ \u279c  netstat -lntp | grep scheduler\n\ntcp        0      0 127.0.0.1:10259         0.0.0.0:*               LISTEN      3586/kube-scheduler \n</code></pre> Notice that ETCD is listening on two ports. Which of these have more client connections established? <pre><code>controlplane ~ \u279c  netstat -lntp | grep etcd\ntcp        0      0 192.9.180.3:2379        0.0.0.0:*               LISTEN      3600/etcd           \ntcp        0      0 127.0.0.1:2379          0.0.0.0:*               LISTEN      3600/etcd           \ntcp        0      0 192.9.180.3:2380        0.0.0.0:*               LISTEN      3600/etcd           \ntcp        0      0 127.0.0.1:2381          0.0.0.0:*               LISTEN      3600/etcd  \n</code></pre>"},{"location":"k8s/cka-exam-commands/#cni","title":"CNI","text":"<p>Networking is a central part of Kubernetes, but it can be challenging to understand exactly how it is expected to work. There are 4 distinct networking problems to address:</p> <pre><code>Highly-coupled container-to-container communications: this is solved by Pods and localhost communications.\nPod-to-Pod communications: this is the primary focus of this document.\nPod-to-Service communications: this is covered by Services.\nExternal-to-Service communications: this is also covered by Services.\n</code></pre> <p>CNI (Container Network Interface), a Cloud Native Computing Foundation project, consists of a specification and libraries for writing plugins to configure network interfaces in Linux containers, along with a number of supported plugins. CNI concerns itself only with network connectivity of containers and removing allocated resources when the container is deleted. Because of this focus, CNI has a wide range of support and the specification is simple to implement.</p> <p>Inspect the kubelet service and identify the container runtime endpoint value is set for Kubernetes.</p> <pre><code>controlplane ~ \u279c  ps aux | grep kubelet | grep endpoint\n\nroot        4567  0.0  0.0 3848468 99736 ?       Ssl  01:06   0:06 /usr/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime-endpoint=unix:///var/run/containerd/containerd.sock --pod-infra-container-image=registry.k8s.io/pause:3.9\n</code></pre> Identify which of the below plugins is not available in the list of available CNI plugins on this host? <pre><code># check the below path\n\ncontrolplane ~ \u279c  ls /opt/cni/bin/\nbandwidth  dhcp   firewall  host-device  ipvlan    macvlan  ptp  static  vlan\nbridge     dummy  flannel   host-local   loopback  portmap  sbr  tuning  vrf\n</code></pre> What is the CNI plugin configured to be used on this kubernetes cluster? <pre><code>Run the command: `ls /etc/cni/net.d/`` and identify the name of the plugin.\n</code></pre>"},{"location":"k8s/cka-exam-commands/#kubeadm","title":"Kubeadm","text":"<pre><code>kubeadm upgrade plan [version]\n</code></pre>"},{"location":"k8s/cka-exam-commands/#lightning-lab","title":"Lightning Lab","text":"Upgrade the current version of kubernetes from `1.26.0` to `1.27.0` exactly using the kubeadm utility.  There is currently an issue with this lab which requires an extra step. This may be addressed in the near future.   On controlplane node  1. Drain node      <pre><code>kubectl drain controlplane --ignore-daemonsets\n</code></pre>  2. Upgrade kubeadm      <pre><code>apt-get update\napt-mark unhold kubeadm\napt-get install -y kubeadm=1.27.0-00\n</code></pre>  3. Plan and apply upgrade      <pre><code>kubeadm upgrade plan\nkubeadm upgrade apply v1.27.0\n</code></pre>  4. Remove taint on controlplane node. This is the issue described above. As part of the upgrade specifically to 1.26, some taints are added to all controlplane nodes. This will prevent the `gold-nginx` pod from being rescheduled to the controlplane node later on.      <pre><code>kubectl describe node controlplane | grep -A 3 taint\n</code></pre>      Output:      <pre><code>Taints:   node-role.kubernetes.io/control-plane:NoSchedule\n            node.kubernetes.io/unschedulable:NoSchedule\n</code></pre>      Let's remove them      <pre><code>kubectl taint node controlplane node-role.kubernetes.io/control-plane:NoSchedule-\nkubectl taint node controlplane node.kubernetes.io/unschedulable:NoSchedule-\n</code></pre>  5. Upgrade the kubelet      <pre><code>apt-mark unhold kubelet\napt-get install -y kubelet=1.27.0-00\nsystemctl daemon-reload\nsystemctl restart kubelet\n</code></pre>  6. Reinstate controlplane node      <pre><code>kubectl uncordon controlplane\n</code></pre>  7. Upgrade kubectl      <pre><code>apt-mark unhold kubectl\napt-get install -y kubectl=1.27.0-00\n</code></pre>  8. Re-hold packages      <pre><code>apt-mark hold kubeadm kubelet kubectl\n</code></pre>  9. Drain the worker node      <pre><code>kubectl drain node01 --ignore-daemonsets\n</code></pre>  10. Go to worker node      <pre><code>ssh node01\n</code></pre>  11. Upgrade kubeadm      <pre><code>apt-get update\napt-mark unhold kubeadm\napt-get install -y kubeadm=1.27.0-00\n</code></pre>  12. Upgrade node      <pre><code>kubeadm upgrade node\n</code></pre>  13. Upgrade the kubelet      <pre><code>apt-mark unhold kubelet\napt-get install kubelet=1.27.0-00\nsystemctl daemon-reload\nsystemctl restart kubelet\n</code></pre>  14. Re-hold packages      <pre><code>apt-mark hold kubeadm kubelet\n</code></pre>  15. Return to controlplane      <pre><code>exit\n</code></pre>  16. Reinstate worker node      <pre><code>kubectl uncordon node01\n</code></pre>  17. Verify `gold-nginx` is scheduled on controlplane node      <pre><code>kubectl get pods -o wide | grep gold-nginx\n</code></pre> Print the names of all deployments in the admin2406 namespace in the following format <p>This is a job for <code>custom-columns</code> output of kubectl</p> <pre><code>kubectl -n admin2406 get deployment -o custom-columns=DEPLOYMENT:.metadata.name,CONTAINER_IMAGE:.spec.template.spec.containers[].image,READY_REPLICAS:.status.readyReplicas,NAMESPACE:.metadata.namespace --sort-by=.metadata.name &gt; /opt/admin2406_data\n</code></pre> A kubeconfig file called admin.kubeconfig has been created in /root/CKA. There is something wrong with the configuration. Troubleshoot and fix it <p>First, let's test this kubeconfig</p> <pre><code>kubectl get pods --kubeconfig /root/CKA/admin.kubeconfig\n</code></pre> <p>Notice the error message.</p> <p>Now look at the default kubeconfig for the correct setting.</p> <pre><code>cat ~/.kube/config\n</code></pre> <p>Make the correction</p> <pre><code>vi /root/CKA/admin.kubeconfig\n</code></pre> <p>Test</p> <pre><code>kubectl get pods --kubeconfig /root/CKA/admin.kubeconfig\n</code></pre> Create a new deployment called nginx-deploy, with image nginx:1.16 and 1 replica. Next upgrade the deployment to version 1.17 using rolling update. <pre><code>kubectl create deployment nginx-deploy --image=nginx:1.16\nkubectl set image deployment/nginx-deploy nginx=nginx:1.17 --record\n</code></pre> <p>You may ignore the deprecation warning.</p> A new deployment called <code>alpha-mysql</code> has been deployed in the alpha namespace. However, the pods are not running. Troubleshoot and fix the issue <p>The deployment should make use of the persistent volume alpha-pv to be mounted at /var/lib/mysql and should use the environment variable MYSQL_ALLOW_EMPTY_PASSWORD=1 to make use of an empty root password.</p> <p>Important: Do not alter the persistent volume.</p> <p>Inspect the deployment to check the environment variable is set. Here I'm using <code>yq</code> which is like <code>jq</code> but for YAML to not have to view the entire deployment YAML, just the section beneath <code>containers</code> in the deployment spec.</p> <pre><code>kubectl get deployment -n alpha alpha-mysql  -o yaml | yq e .spec.template.spec.containers -\n</code></pre> <p>Find out why the deployment does not have minimum availability. We'll have to find out the name of the deployment's pod first, then describe the pod to see the error.</p> <pre><code>kubectl get pods -n alpha\nkubectl describe pod -n alpha alpha-mysql-xxxxxxxx-xxxxx\n</code></pre> <p>We find that the requested PVC isn't present, so create it. First, examine the Persistent Volume to find the values for access modes, capacity (storage), and storage class name</p> <pre><code>kubectl get pv alpha-pv\n</code></pre> <p>Now use <code>vi</code> to create a PVC manifest</p> <pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: mysql-alpha-pvc\n  namespace: alpha\nspec:\n  accessModes:\n  - ReadWriteOnce\n  resources:\n    requests:\n      storage: 1Gi\n  storageClassName: slow\n</code></pre> Take the backup of ETCD at the location <code>/opt/etcd-backup.db</code> on the controlplane node <p>This question is a bit poorly worded. It requires us to make a backup of etcd and store the backup at the given location. Know that the certificates we need for authentication of <code>etcdctl</code> are located in <code>/etc/kubernetes/pki/etcd</code></p> <p>Get the certificates as shown below</p> <pre><code>controlplane ~ \u279c  k -n kube-system get pod etcd-controlplane -o yaml | grep -i crt\n- --cert-file=/etc/kubernetes/pki/etcd/server.crt\n- --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt\n- --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt\n- --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt\n</code></pre> <p>Get the command to take backup from docs</p> <pre><code>ETCDCTL_API='3' etcdctl snapshot save /opt/etcd-backup.db \\\n  --cacert=/etc/kubernetes/pki/etcd/ca.crt \\\n  --cert=/etc/kubernetes/pki/etcd/server.crt \\\n  --key=/etc/kubernetes/pki/etcd/server.key \n</code></pre> <p>check if backup is taken using <pre><code>controlplane ~ \u279c  ETCDCTL_API=3 etcdctl --write-out=table snapshot status /opt/etcd-backup.db \n+----------+----------+------------+------------+\n|   HASH   | REVISION | TOTAL KEYS | TOTAL SIZE |\n+----------+----------+------------+------------+\n| bc0cb4cf |     5914 |        973 |     2.2 MB |\n+----------+----------+------------+------------+\n</code></pre></p> <p>Whilst we could also use the argument <code>--endpoints=127.0.0.1:2379</code>, it is not necessary here as we are on the controlplane node, same as <code>etcd</code> itself. The default endpoint is the local host.</p> Create a pod called secret-1401 in the <code>admin1401 namespace</code> using the busybox image <p>The container within the pod should be called <code>secret-admin</code> and should sleep for 4800 seconds. The container should mount a read-only secret volume called secret-volume at the path <code>/etc/secret-volume</code>. The secret being mounted has already been created for you and is called <code>dotfile-secret</code>.</p> <ol> <li> <p>Use imperative command to get a starter manifest</p> <pre><code>kubectl run secret-1401 -n admin1401 --image busybox --dry-run=client -o yaml --command -- sleep 4800 &gt; admin.yaml\n</code></pre> </li> <li> <p>Edit this manifest to add in the details for mounting the secret</p> <pre><code>vi admin.yaml\n</code></pre> <p>Add in the volume and volume mount sections seen below</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  creationTimestamp: null\n  labels:\n    run: secret-1401\n  name: secret-1401\n  namespace: admin1401\nspec:\n  volumes:\n  - name: secret-volume\n    secret:\n      secretName: dotfile-secret\n  containers:\n  - command:\n    - sleep\n    - \"4800\"\n    image: busybox\n    name: secret-admin\n    volumeMounts:\n    - name: secret-volume\n      readOnly: true\n      mountPath: /etc/secret-volume\n</code></pre> </li> <li> <p>And create the pod</p> <pre><code>kubectl create -f admin.yaml\n</code></pre> </li> </ol>"},{"location":"k8s/cka-exam-commands/#mock-exam","title":"Mock Exam","text":"Deploy a pod named nginx-pod using the nginx:alpine image. <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\ncreationTimestamp: null\nlabels:\n    run: nginx-pod\nname: nginx-pod\nspec:\ncontainers:\n- image: nginx:alpine\n    name: nginx-pod\n    resources: {}\ndnsPolicy: ClusterFirst\nrestartPolicy: Always\nstatus: {}\n</code></pre> Deploy a messaging pod using the <code>redis:alpine</code> image with the labels set to <code>tier=msg</code> <p>Run below command which create a pod with labels:</p> <pre><code>kubectl run messaging --image=redis:alpine --labels=tier=msg\n</code></pre> Create a namespace named <code>apx-x9984574</code> <p>Run below command to create a namespace:</p> <pre><code>    kubectl create namespace apx-x9984574\n</code></pre> Get the list of nodes in JSON format and store it in a file at <code>/opt/outputs/nodes-z3444kd9.json</code> <p>Use the below command which will redirect the o/p:</p> <pre><code>kubectl get nodes -o json &gt; /opt/outputs/nodes-z3444kd9.json\n</code></pre> Create a service messaging-service to expose the messaging application within the cluster on port 6379. <p>Execute below command which will expose the pod on port 6379:</p> <pre><code>kubectl expose pod messaging --port=6379 --name messaging-service\n</code></pre> Create a deployment named hr-web-app using the image <code>kodekloud/webapp-color</code> with 2 replicas. <p>In v1.19, we can add <code>--replicas</code> flag with <code>kubectl create deployment</code> command:</p> <p><pre><code>kubectl create deployment hr-web-app --image=kodekloud/webapp-color --replicas=2\n</code></pre> </p> Create a static pod named static-busybox on the controlplane node that uses the busybox image and the command <code>sleep 1000</code> <p>To Create a static pod, copy it to the static pods directory. In this case, it is <code>/etc/kubernetes/manifests</code>. Apply below manifests:</p> <pre><code> k run static-busybox --image=busybox --command sleep 1000 --dry-run=client -o yaml &gt; static-busybox.yaml\n</code></pre> <p>This will create the below manifest</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  creationTimestamp: null\n  labels:\n    run: static-busybox\n  name: static-busybox\nspec:\n  containers:\n  - command:\n    - sleep\n    - \"1000\"\n    image: busybox\n    name: static-busybox\n    resources: {}\n  dnsPolicy: ClusterFirst\n  restartPolicy: Always\nstatus: {}\n</code></pre> Create a POD in the finance namespace named temp-bus with the image redis:alpine. <p>Run below command to create a pod in namespace <code>finance</code>:</p> <pre><code>kubectl run temp-bus --image=redis:alpine -n finance\n</code></pre> A new application orange is deployed. There is something wrong with it. Identify and fix the issue. <p>Run below command and troubleshoot step by step:</p> <pre><code>kubectl describe pod orange\n</code></pre> <p>Export the running pod using below command and correct the spelling of the command <code>sleeeep</code> to <code>sleep</code> </p> <pre><code>kubectl edit pod orange # make changes and save\nk replace --force -f temp_file.yaml \n</code></pre> Expose the <code>hr-web-app</code> as service <code>hr-web-app-service</code> application on port 30082 on the nodes on the cluster. <p>Apply below manifests:</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\nname: hr-web-app-service\nspec:\ntype: NodePort\nselector:\n    app: hr-web-app\nports:\n    - port: 8080\n    targetPort: 8080\n    nodePort: 30082\n</code></pre> Use JSON PATH query to retrieve the osImages of all the nodes and store it in a file <code>/opt/outputs/nodes_os_x43kj56.txt</code> <p>Run the below command to redirect the o/p:</p> <pre><code> kubectl get nodes -o=jsonpath='{.items[0].status.nodeInfo.osImage}'  &gt; /opt/outputs/nodes_os_x43kj56.txt\n</code></pre> Create a Persistent Volume with the given specification <p>Volume name: pv-analytics</p> <p>Storage: 100Mi</p> <p>Access mode: ReadWriteMany</p> <p>Host path: /pv/data-analytics</p> <pre><code>apiVersion: v1\nkind: PersistentVolume\nmetadata:\n    name: pv-analytics\nspec:\n    capacity:\n    storage: 100Mi\n    volumeMode: Filesystem\n    accessModes:\n    - ReadWriteMany\n    hostPath:\n        path: /pv/data-analytics\n</code></pre> Create a Pod called redis-storage with image: <code>redis:alpine</code> with a Volume of type emptyDir that lasts for the life of the Pod. <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\ncreationTimestamp: null\nlabels:\n    run: redis-storage\nname: redis-storage\nspec:\ncontainers:\n- image: redis:alpine\n    name: redis-storage\n    volumeMounts:\n    - mountPath: /data/redis\n    name: cache-volume\nvolumes:\n- name: cache-volume\n    emptyDir:\n    sizeLimit: 500Mi\ndnsPolicy: ClusterFirst\nrestartPolicy: Always\nstatus: {}\n</code></pre> Create a new pod called super-user-pod with image busybox:1.28. Allow the pod to be able to set <code>system_time</code> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\ncreationTimestamp: null\nlabels:\n    run: super-user-pod\nname: super-user-pod\nspec:\ncontainers:\n- image: busybox:1.28\n    name: super-user-pod\n    resources: {}\n    command:\n    - sleep\n    - \"4800\"\n    securityContext:\n    capabilities:\n        add: [\"SYS_TIME\"]\ndnsPolicy: ClusterFirst\nrestartPolicy: Always\nstatus: {}\n</code></pre> Create a new user called john. Grant him access to the cluster. John should have permission to create, list, get, update and delete pods in the development namespace . The private key exists in the location: <code>/root/CKA/john.key</code> and csr at <code>/root/CKA/john.csr</code> <p>Important Note: As of kubernetes 1.19, the CertificateSigningRequest object expects a signerName.</p> <p>Please refer the documentation to see an example. The documentation tab is available at the top right of terminal.</p> <pre><code>CSR: john-developer Status:Approved\nRole Name: developer, namespace: development, Resource: Pods\nAccess: User 'john' has appropriate permissions\n</code></pre> <p>Form the CSR request</p> <p><pre><code>apiVersion: certificates.k8s.io/v1\nkind: CertificateSigningRequest\nmetadata:\nname: myuser\nspec:\nrequest: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURSBSRVFVRVNULS0tLS0KTUlJQ1ZEQ0NBVHdDQVFBd0R6RU5NQXNHQTFVRUF3d0VhbTlvYmpDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRApnZ0VQQURDQ0FRb0NnZ0VCQUxqR2J3T1NDUFZycHB5QnhDL0ZDSURoTE90TXJyN21nWUlDbFYvcDlkaHZjSXdqCno0ZmpJdTlITDdrZlpTT2kxT21NSDNtTUlMaDRLa3J2bnFaeENXUGFrVEVDN005T1lsNHoyWXlWWDZ5R3p0WEYKYXZqSUcrVUJ5Zmo0V2M5c0l6cEdJS0dqN3JaQmVZamV3STlpUU5yQzc2RFJpcStKZU1oRFhIT2ZtSm9oU0J3YgpsQm9rSEp5aVNITzM1OGx6WEs1UElZaTVqKy9waUFhSHRKbjg3Vzl1K2tpNzJsc3IxN0JoV0FMTzQrOHFDOUgvCjMzZ2VQNUxhMXJTanVjYVk1eE9IL2s2dVdabGVVUUVyeVBqUDg0TW1sUnhrZEVHdTJ6dmY5c2pmZUFWNE1QTkoKYXYxcTMrc0ZNbHB2VndGb2RIbFgzL2ZzK25abHFhYWp2eW5yc1hFQ0F3RUFBYUFBTUEwR0NTcUdTSWIzRFFFQgpDd1VBQTRJQkFRQWhrMVVrTklqSzhDZmx2QTB0bEpOWi83TlgvZUlMQzF6d2h1T0NpQm14R2dUaGZReDdqYWtICnNyMmdUSXlpU0RsdVdVKzVZeW1CeElhL0xHVmRackhpSlBLRzgyVlNmck9DUHgrME1Bbk5PNTZpWWNUZ2RXZ3IKanByaUJYbDdrVkV0UUZjVTVwSGt0aW92Nk5mb0htRzZqT2w5dzVNYzRNMDJGbUN1Yi9sSngrNThIQnI1ekZLQQp4bGRNaXZ5V05CTlY3S3p0a1FkWElsLzR0emllME11ekdxRkxZNWh6R3pDSnVwekd5bmZXc0hmd2JaeWVKTVlrCnlmWldTV0FRSHhEZk5HRWxvNXhja1FTOVBWU29NK0YyNFoveXA3ZEI1Mlc0bE1yYVRsa2VNTy9pU25hRU5tdGwKazhPTDNielhXYS82K0hkdnNremtGK2hpVHFoRW9XTEIKLS0tLS1FTkQgQ0VSVElGSUNBVEUgUkVRVUVTVC0tLS0tCg== \nsignerName: kubernetes.io/kube-apiserver-client\nexpirationSeconds: 86400  # one day\nusages:\n- client auth\n</code></pre> Create it as shown below</p> <pre><code># create the CSR\ncontrolplane ~/CKA \u279c  k apply -f csr-john.yaml \n\ncertificatesigningrequest.certificates.k8s.io/john created\n</code></pre> <p>View and approve it</p> <pre><code>controlplane ~/CKA \u279c  k get csr\nNAME        AGE     SIGNERNAME                                    REQUESTOR                  REQUESTEDDURATION   CONDITION\ncsr-q6l5t   45m     kubernetes.io/kube-apiserver-client-kubelet   system:bootstrap:rsxu7y    &lt;none&gt;              Approved,Issued\ncsr-tnnwr   45m     kubernetes.io/kube-apiserver-client-kubelet   system:node:controlplane   &lt;none&gt;              Approved,Issued\njohn        3m47s   kubernetes.io/kube-apiserver-client           kubernetes-admin           24h                 Pending\n\ncontrolplane ~/CKA \u279c  k certificate approve john\ncertificatesigningrequest.certificates.k8s.io/john approved\n</code></pre> <p>Create the role</p> <pre><code>controlplane ~/CKA \u279c  kubectl create role developer  --verb=get,list,create,update,delete  --resource=pods -n development\nrole.rbac.authorization.k8s.io/developer created\n</code></pre> <p>Create a role binding <pre><code>controlplane ~/CKA \u279c  kubectl create rolebinding developer-rb --role=developer --user=john -n development \nrolebinding.rbac.authorization.k8s.io/developer-rb created\n</code></pre></p> <p>Check if it worked <pre><code>controlplane ~/CKA \u2716 k auth can-i get pods -n development --as john\n\nyes\n</code></pre></p> Create a nginx pod called <code>nginx-resolver</code> using image nginx, expose it internally with a service called <code>nginx-resolver-service</code>. Test that you are able to look up the service and pod names from within the cluster. Use the <code>image: busybox:1.28</code> for dns lookup. Record results in  <code>/root/CKA/nginx.svc</code> and <code>/root/CKA/nginx.pod</code> <p>Expose the pod after creating it <pre><code>/CKA \u2716 kubectl expose pod nginx-resolver  --name=nginx-resolver-service --port=8080\nservice/nginx-resolver-service exposed\n</code></pre></p> <p>Create a test pod with sleep time <pre><code>controlplane ~/CKA \u279c  k run test --image=busybox:1.28 -- sleep 5000\npod/test created\n</code></pre></p> <p>run it to show the nslookup</p> <pre><code>controlplane ~/CKA \u279c  k exec test -- nslookup nginx-resolver-service\nServer:    10.96.0.10\nAddress 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local\n\nName:      nginx-resolver-service\nAddress 1: 10.104.187.189 nginx-resolver-service.default.svc.cluster.local\n</code></pre> <p>Get the pod records</p> <pre><code>controlplane ~/CKA \u279c  k exec test -- nslookup 10-244-192-4.default.pod.cluster.local\nServer:    10.96.0.10\nAddress 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local\n\nName:      10-244-192-4.default.pod.cluster.local\nAddress 1: 10.244.192.4 10-244-192-4.nginx-resolver-service.default.svc.cluster.local\n\ncontrolplane ~/CKA \u279c  k exec test -- nslookup 10-244-192-4.default.pod.cluster.local &gt; /root/CKA/nginx.pod\n</code></pre> Create a static pod on node01 called nginx-critical with image nginx and make sure that it is recreated/restarted automatically in case of a failure. <pre><code>controlplane ~ \u279c  cat static-pod.yaml \napiVersion: v1\nkind: Pod\nmetadata:\ncreationTimestamp: null\nlabels:\n    run: nginx-critical\nname: nginx-critical\nspec:\ncontainers:\n- image: nginx\n    name: nginx-critical\n    resources: {}\ndnsPolicy: ClusterFirst\nrestartPolicy: Always\nstatus: {}\n</code></pre> Create a new service account with the name pvviewer. Grant this Service account access to list all PersistentVolumes in the cluster by creating an appropriate cluster role called <code>pvviewer-role</code> and ClusterRoleBinding called <code>pvviewer-role-binding</code>. Next, create a pod called pvviewer with the image: redis and serviceAccount: <code>pvviewer</code> in the default namespace <pre><code>controlplane ~ \u279c  k create sa pvviewer\n</code></pre> <pre><code>controlplane ~ \u279c    kubectl create clusterrole pvviewer-role  --verb=list --resource=persistentvolumes\nclusterrole.rbac.authorization.k8s.io/pvviewer-role created\n</code></pre> <p>Create role binding <pre><code>controlplane ~ \u279c    kubectl create clusterrolebinding pvviewer-role-binding  --clusterrole=pvviewer-role  --serviceaccount=default:pvviewerclusterrolebinding.rbac.authorization.k8s.io/\n\npvviewer-role-binding created\n</code></pre></p> <pre><code>controlplane ~ \u2716 cat pod-view.yaml \napiVersion: v1\nkind: Pod\nmetadata:\ncreationTimestamp: null\nlabels:\n    run: pvviewer\nname: pvviewer\nspec:\nserviceAccountName: pvviewer\ncontainers:\n- image: redis\n    name: pvviewer\n    resources: {}\ndnsPolicy: ClusterFirst\nrestartPolicy: Always\nstatus: {}\n</code></pre> List the <code>InternalIP</code> of all nodes of the cluster. Save the result to a file <code>/root/CKA/node_ips</code> <p>Answer should be in the format: InternalIP of controlplaneInternalIP of node01 (in a single line) <pre><code>kubectl get nodes -o=jsonpath='{.items[*].status.addresses[0].address}' &gt; /root/CKA/node_ips\n</code></pre> Create a pod called multi-pod with two containers. Container 1, name: alpha, image: nginx Container 2: name: beta, image: busybox, command: sleep 4800 <p>Environment Variables: container 1: name: alpha</p> <p>Container 2: name: beta</p> <pre><code>controlplane ~/CKA \u279c  cat multi-pod.yaml \napiVersion: v1\nkind: Pod\nmetadata:\nname: multi-pod\nspec:\ncontainers:\n- name: alpha\n    image: nginx\n    env:\n    - name: name\n    value: \"alpha\"\n- name: beta\n    image: busybox\n    env:\n    - name: name\n    value: \"beta\"\n    command:\n    - sleep\n    - \"4800\"\n</code></pre> Create a Pod called non-root-pod , image: redis:alpine, runAsUser: 1000 and fsGroup: 2000 <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\nname: non-root-pod\nspec:\nsecurityContext:\n    runAsUser: 1000\n    fsGroup: 2000\ncontainers:\n- name: sec-ctx-demo\n    image: redis:alpine\n</code></pre> We have deployed a new pod called np-test-1 and a service called np-test-service. Incoming connections to this service are not working. Troubleshoot and fix it. Create NetworkPolicy, by the name ingress-to-nptest that allows incoming connections to the service over port 80 <pre><code>controlplane ~/CKA \u279c  k apply -f policy-pod.yaml \nnetworkpolicy.networking.k8s.io/ingress-to-nptest created\n\ncontrolplane ~/CKA \u279c  cat policy-pod.yaml \napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\nname: ingress-to-nptest\nspec:\npodSelector:\n    matchLabels:\n    run: np-test-1\ningress:\n    - from:\n    - podSelector:\n        matchLabels:\n            run: np-test-1\n    ports:\n        - protocol: TCP\n        port: 80\npolicyTypes:\n- Ingress\n</code></pre> Taint the worker node node01 to be Unschedulable. Once done, create a pod called dev-redis, image redis:alpine, to ensure workloads are not scheduled to this worker node. Finally, create a new pod called prod-redis and image: redis:alpine with toleration to be scheduled on node01. <p>key: env_type, value: production, operator: Equal and effect: NoSchedule</p> <pre><code>controlplane ~/CKA \u279c    kubectl taint nodes node01  env_type=production:NoSchedule\nnode/node01 tainted\n</code></pre> <p>Create the pod</p> <pre><code>controlplane ~/CKA \u279c  cat tolerated-pod.yaml \n\napiVersion: v1\nkind: Pod\nmetadata:\ncreationTimestamp: null\nlabels:\n    run: pod-redis\nname: prod-redis\nspec:\ncontainers:\n- image: redis:alpine\n    name: pod-redis\ntolerations:\n- key: \"env_type\"\n    operator: \"Equal\"\n    value: \"production\"\n    effect: \"NoSchedule\"\ndnsPolicy: ClusterFirst\nrestartPolicy: Always\nstatus: {}\n</code></pre> create pod and add 2 labels as shown below <pre><code>controlplane ~/CKA \u279c  k -n hr label pod hr-pod envionment=production\npod/hr-pod labeled\n\n\ncontrolplane ~/CKA \u279c  k -n hr get pod --show-labels \nNAME     READY   STATUS    RESTARTS   AGE   LABELS\nhr-pod   1/1     Running   0          69s   envionment=production,run=hr-pod,tier=frontend\n</code></pre>"},{"location":"k8s/commands/","title":"CKAD Commands","text":""},{"location":"k8s/commands/#non-admin-commands","title":"Non-Admin Commands","text":"<pre><code>.\n\u251c\u2500 docs/\n\u2502  \u2514\u2500 blog/\n\u2502     \u251c\u2500 posts/\n\u2502     \u2514\u2500 index.md\n\u2514\u2500 mkdocs.yml\n</code></pre>"},{"location":"k8s/commands/#how-k-apply-command-works","title":"How <code>k apply</code> command works","text":"<p>It compares the </p> <ul> <li><code>local file</code> with <code>last applied config</code></li> <li><code>local file</code> with <code>kubernetes live object</code></li> </ul> <p> </p> <p>Where is the last applied config stored?</p> <p>It is stored in the with <code>kubernetes live object</code> in an annotation as shown below  </p>"},{"location":"k8s/commands/#commands-for-ckackad-exam","title":"Commands for CKA/CKAD exam","text":"<p><code>-o yaml</code>: This will output the resource definition in YAML format on screen.</p> <p><code>--dry-run</code>: By default as soon as the command is run, the resource will be created. If you simply want to test your command , use the <code>--dry-run=client</code> option. This will not create the resource, instead, tell you whether the resource can be created and if your command is right.</p> <pre><code># replace the file with temp created yaml file for forbidden updates\nk replace --force -f &lt;file-name.yaml&gt;\n\n# create an nginx pod\nk run nginx --image =nginx\n\n# Generate a pod manifest file using the dry-run which does not create a resource. Also save the file to local\nk run nginx --image =nginx --dry-run=client -o yaml &gt; pod.yaml\n\n# Generate a deployment manifest file using the dry-run which does not create a resource\nk create deployment --image =nginx dep_name --dry-run=client -o yaml &gt; dep.yaml\n\n# Generate a deployment manifest file with 4 replicas using the dry-run which does not create a resource\nk create deployment --image =nginx dep_name --replicas=4 --dry-run=client -o yaml &gt; dep\n\n# Scale deployment\nkubectl scale deployment nginx --replicas=4\n\n# Create a Service named redis-service of type ClusterIP to expose pod redis on port 6379\nkubectl expose pod redis --port=6379 --name redis-service --dry-run=client -o yaml\n\n#Create a Service named nginx of type NodePort to expose pod nginx's port 80 on port 30080 on the nodes\n# This will automatically use the pod's labels as selectors, but you cannot specify the node port.\nkubectl expose pod nginx --type=NodePort --port=80 --name=nginx-service --dry-run=client -o yaml\n\n# create a pod with labels\nk run redis  --image=redis:alpine --labels=tier=db\n\n# select the resources with node name label\nk get po/deploy/ep -l kubernetes.io/hostname=&lt;node-name&gt;\n</code></pre>"},{"location":"k8s/commands/#namespace","title":"Namespace","text":"<pre><code># create a namescpace \nkubectl create -f custom-namespace.yaml\n\n# using declarative command\nkubectl create namespace custom-namespace\n\n# create a resource in namespace\nkubectl create -f kubia-manual.yaml -n custom-namespace\n\n# delete namespace\nkubectl delete ns custom-namespace\n</code></pre>"},{"location":"k8s/commands/#get-the-detailed-request-logs","title":"Get the detailed request logs","text":"<pre><code>k get po -v=6\n</code></pre>"},{"location":"k8s/commands/#pod","title":"Pod","text":"POD CreationPOD LogsSSHGet/DeleteLogs <pre><code># create a pod using dry-run\nkubectl run busybox --image=busybox --restart=Never --dry-run -o yaml &gt; testPod.yaml\n\n# create a pod with come commands using dry-run\nkubectl run busyboxWithCommands --image=busybox --restart=Never --dry-run -o yaml -- bin/sh -c \"sleep 3600; ls\" &gt; testPod.yaml\n\n# settting image\nkubectl set image pod podname nginx=nginx:1.15-alpine\n\n# it's better to edit pod sometime and make quick changes\nkubectl edit pod podName\n\n# Create the pod named amardev with version 1.17.4 and expose it on port 80\nkubectl run amardev --image=nginx:1.17.4 --restart=Never --port=80\n\n# add the command in the pod by editing it\ncommand: ['/bin/bash', '-c', 'sleep 5000']\n\n# adding args in the pod\nargs: ['--color', 'green']\n</code></pre> <pre><code># get logs for nginx pod\nkubectl logs nginx\n\n# get previous logs of the pod\nkubectl logs nginx -p\n</code></pre> <pre><code># just open the terminal for the pod with one container\nkubectl exec -it nginx -- /bin/sh\n\n# echo hello world in the container\nkubectl exec -it nginx -c containerName -- /bin/sh -c 'echo hello world'\n\n# ssh to multi-container pod\nkubectl exec -it  multi-cont-pod -c main-container -- sh cat /var/log/main.txt\n</code></pre> <pre><code># get all pods\nkubectl get pods\n\n# get info for particular pod\nkubectl get po kubia-liveness\n\n# show labels while showing pods\nkubectl get pods --show-labels\n\n# select pods with multiple labels\nkubectl get all --selector env=prod,bu=finance,tier=frontend\n\n# Warn: delete all pods\nkubectl delete po --all \n\n# Warning: delete pods in all namespaces\nkubectl delete all --all\n\n# delete pod\nkubectl delete po podName\n\n# Important: delete pods using labels\nkubectl delete po -l creation_method=manual\n\n# get the metircs about the nodes\nkubectl top node/pod\n</code></pre> <pre><code># See the pod logs\nkubectl logs podName | less\n\n# Tail the pod logs using `-f`\nkubectl logs -f podName\n</code></pre>"},{"location":"k8s/commands/#labels-and-annotations","title":"Labels and annotations","text":"<pre><code># show labels\nkubectl get pods --show-labels\n\n# apply label\nkubectl run nginx-dev1 --image=nginx --restart=Never --labels=env=dev\n\n#Get the pods with label env=dev\nkebectl get pods -l env=dev --show-labels\n\n# show labels which env in dev and prod\nkubectl get pods -l 'env in (dev,prod)'\n\n# update the label with overwrite\nkubectl label po podone area=monteal --overwrite\n\n# remove label named env\nkubectl label pods podone env-\n\n# show the labels for the nodes\nkubectl get nodes --show-labels\n\n# Annotate the pods with name=webapp\nkubectl annnotate po podone name=webapp\n</code></pre>"},{"location":"k8s/commands/#configmap","title":"Configmap","text":"<pre><code># create a configmap\nkubectl create configmap &lt;cm-name&gt; --from-literal=special.how=very --from-literal=special.type=charm\n\n# Example\nk create cm webapp-config-map --from-literal APP_COLOR=darkblue --from-literal APP_OTHER=disregard\n</code></pre>"},{"location":"k8s/commands/#env-variables","title":"Env variables","text":"<pre><code># set an env variable while creating a pod\nkubectl run nginx --image=nginx --restart=Never --env=var1=val1\n\n# get all of the env variables for a pod\nkubectl exec -it nginx -- env\n</code></pre>"},{"location":"k8s/commands/#logs","title":"Logs","text":"<pre><code># check logs for a container 1 and 2 in the pod busybox\nkubectl logs busybox -c busybox1\nkubectl logs busybox -c busybox2\n\n# Check the previous logs of the second container busybox2 if any\nkubectl logs busybox -c busybox2 --previous\n\n# Run command ls in the third container busybox3 of the above pod\nkubectl exec busybox -c busybox3 -- ls\n\n# Show metrics of the above pod containers and puts them into the file.log and verify\nkubectl top pod busybox --containers &gt; file.log\n</code></pre>"},{"location":"k8s/commands/#deployment","title":"Deployment","text":"<pre><code># create a deployment with a name and replicas of 3\nkubectl create deployment webapp --image=nginx --replicas=3\n\n# scale deployment to have 20 replicas\nkubectl scale deploy webapp --replicas=20\n\n# get the rollout status\nkubectl rollout status deploy webapp\n\n# get the rollout history\nkubectl rollout history deploy webapp\n\n# delete the deployment and watch it getting deleted\nkubectl delete deployment webapp --watch\n\n# update the image in the deployment. Note that here nginx in the container name\n# set image will cause the rollut to happen, so you can check the status using the `kubectl rollout status` command\nkubectl set image deployment depName nginx=nginx:1.17.4\n\n# undo the deployment to revision 1\nkubectl rollout undo deployment webapp --to-revision=1\n\n#  Check the history of the specific revision of that deployment\nkubectl rollout history deployment webapp --revision=3\n\n# pause the rollout\nkubectl rollout pause deployment webapp\n\n# resume the rollout\nkubectl rollout resume deployment webapp\n\n# scale a deployment\nkubectl autoscale deployment webapp --min=5 --max=10 --cpu-percent=80\n</code></pre>"},{"location":"k8s/commands/#jobs","title":"Jobs","text":"<pre><code># jobs will create a pod and then finish when work is done\n# then we can see the output of that job using the logs command for the pod created\nkubectl create job jobName --image=nginx -- node -v\n\n# create a job which will save the TIME information\nkubectl create job jonname --image=nginx -- /bin/sh -c \"date; echo 'time container'\"\n\n# delete all the jobs\nkubectl delete jobs --all\n</code></pre>"},{"location":"k8s/commands/#cron-jobs","title":"Cron Jobs","text":"<pre><code># get all the cronjobs\nkubectl get cj\n\n# create a cronjob which will show the time everyminute\nkubectl create cronjob timecj  --image=nginx --schedule=\"*/1 * * * *\" -- /bin/sh -c \"date\"\n</code></pre>"},{"location":"k8s/commands/#secret","title":"Secret","text":"<pre><code># create a secret and add some values\nkubectl create secret generic db-secret --from-literal=DB_Host=sql01\n</code></pre>"},{"location":"k8s/commands/#base64-encodedecode","title":"Base64 encode/decode","text":"EncodingDe-coding <pre><code>echo -n 'Lets learn K8S' | base64\n</code></pre> <pre><code>echo -n 'TGV0cyBsZWFybiBLOFM=' | base64 -- decode\n</code></pre>"},{"location":"k8s/commands/#data-secret","title":"Data secret","text":"<p>In this secret, the values are base64 encoded as shown below </p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: mysecret\ntype: Opaque\ndata:\n  username: YWRtaW4=\n  password: MWYyZDFlMmU2N2Rm\n</code></pre>"},{"location":"k8s/commands/#data-stringdata-secret","title":"Data + stringData secret","text":"<pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: mysecret\ntype: Opaque\ndata:\n  username: YWRtaW4=\nstringData:\n  name: Amarjit\n</code></pre>"},{"location":"k8s/commands/#see-secret-contents","title":"See Secret contents","text":"Encodedde-coded <pre><code>kubectl get secret sampleSecret -o jsonpath='{.data}'\n</code></pre> <pre><code>kubectl get secret sampleSecret -o jsonpath='{.data}' | base64 --decode\n</code></pre> <p>We can see the contents of secret that was created using</p>"},{"location":"k8s/commands/#hpa","title":"HPA","text":"<pre><code># horizontal pod autoscaling\nkubectl get hpa\n</code></pre>"},{"location":"k8s/commands/#taint-and-toleration","title":"Taint and Toleration","text":"<pre><code># creating taints\nkubectl taint node nodename key=value:NoSchedule\n\n###Example\nk taint node node01 spray=mortein:NoSchedule\n\n# see the taint using describe\nkubectl describe deployment depname | grep -i taint\n\n# remove the Taints in the node using, add - at end to remove the taint\nkubectl taint node nodename node-role.kubernetes.io/master:NoSchedule-\n\n# Add toleration to pod\n</code></pre>"},{"location":"k8s/commands/#role","title":"Role","text":"Create a role<pre><code>kubectl create role developer --namespace=default --verb=list,create,delete --resource=pods\n</code></pre> Create a role binding<pre><code>kubectl create rolebinding dev-user-binding --namespace=default --role=developer --user=dev-user\n</code></pre>"},{"location":"k8s/commands/#security-context","title":"Security Context","text":"<pre><code># to add the user id we can defice security context in the container and pod level using\nsecurityContext:\n  runAsUser: 1000\n  runAsOwner: 1010=\n\n# capabilities are added at the container level not at the pod level\ncontainers:\n    securityContext:\n      capabilities:\n        add: ['SYS_TIME']\n      runAsUser: 1000\n      runAsOwner: 1010\n</code></pre>"},{"location":"k8s/commands/#session-affinity","title":"Session Affinity","text":"<pre><code># use the below affinity in the pod spec\naffinity:\n    nodeAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        nodeSelectorTerms:\n        - matchExpressions:\n          - key: labelKey\n            operator: In\n            values:\n            - labelValue\n</code></pre>"},{"location":"k8s/commands/#notes","title":"Notes","text":"<ol> <li><code>NodeSelector</code> is used in the pod if we want it to get allocated to a particular node</li> <li>A Job creates one or more Pods and ensures that a specified number of them successfully terminate. As pods successfully complete, the Job tracks the successful completions. When a specified number of successful completions is reached, the task (ie, Job) is complete. Deleting a Job will clean up the Pods it created.</li> <li>A <code>hostPath</code> volume mounts a file or directory from the host node\u2019s filesystem into your Pod.</li> <li>pod talks to API-server using the service account</li> <li>Use <code>nodeaffinity</code> to place the pods in the right nodes. Use the label to select the node. First apply a label on the node and then use that label on the node affinity</li> <li>Startup probe, the application will have a maximum of 5 minutes <code>(30 * 10 = 300s)</code> to finish its startup. Once the startup probe has succeeded once, the liveness probe takes over to provide a fast response to container deadlocks. If the startup probe never succeeds, the container is killed after 300s and subject to the pod's <code>restartPolicy</code></li> </ol> <p>See special chars in VIM</p> <p>use <code>:set list</code> to show the special chars and <code>:set nolist</code> to go back to normal</p>"},{"location":"k8s/commands/#admin-commands","title":"Admin Commands","text":""},{"location":"k8s/commands/#get-current-context","title":"Get current context","text":"<pre><code># show all conexts, this is detailed\nk config get-contexts\n\n# just the context name\nk config current-context\n</code></pre>"},{"location":"k8s/commands/#see-the-access-level","title":"See the access level","text":"<p>Check access level for you <pre><code># Check if you can create pod\nk auth can-i create pod \n\n# Can I delete node?\nk auth can-i delete node\n\n# check all you can access\n k auth can-i --list=true\n</code></pre></p> <p>Check access for someone else</p> Only Admin can check this<pre><code># Can dev user delete the node\nk auth can-i delete node --as dev-user\n</code></pre>"},{"location":"k8s/commands/#see-list-of-resources","title":"See list of resources","text":"Namespaced Resorces<pre><code>k api-resources --namespaced=true\n</code></pre> Non-Namespaced Resorces<pre><code>k api-resources --namespaced=false\n</code></pre>"},{"location":"k8s/commands/#see-resources-without-headers","title":"See resources without headers","text":"<pre><code># get the pods\nk get po --no-headers\n\n# count pods\nk get pods --no-headers | wc -l\n</code></pre>"},{"location":"k8s/commands/#network-related-commands","title":"Network related commands","text":"<p><pre><code>ps aux  # see process names\n\ncat /etc/hosts # local DNS\n\nip link\n\n# example to see the MAC address, here the state is shown as up\ncontrolplane ~ \u2716 ip link show cni0\n3: cni0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1400 qdisc noqueue state UP mode DEFAULT group default qlen 1000\n    link/ether d6:8c:12:d8:fe:88 brd ff:ff:ff:ff:ff:ff\nip addr\n\nip addr add 192.168.1.2/24 dev eth0\n\nip route\n\nip route add 192.168.1.2/24 via 192.168.2.1\n\narp\n\n\nroute\n</code></pre> See the default gateway and all gateways <pre><code>controlplane ~ \u279c  ip route\ndefault via 172.25.0.1 dev eth1 \n10.244.0.0/24 dev cni0 proto kernel scope link src 10.244.0.1 \n10.244.1.0/24 via 10.244.1.0 dev flannel.1 onlink \n172.25.0.0/24 dev eth1 proto kernel scope link src 172.25.0.70 \n192.25.128.0/24 dev eth0 proto kernel scope link src 192.25.128.12 \n\n## See Default route\nip route show default\n</code></pre></p> <p>Check all the ports the services are listening on using <code>netstat -nltp</code></p> Example<pre><code>controlplane ~ \u279c  netstat -nplt\nActive Internet connections (only servers)\nProto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name    \ntcp        0      0 127.0.0.1:10257         0.0.0.0:*               LISTEN      3563/kube-controlle \ntcp        0      0 127.0.0.1:10259         0.0.0.0:*               LISTEN      3496/kube-scheduler \ntcp        0      0 127.0.0.1:35253         0.0.0.0:*               LISTEN      1074/containerd     \ntcp        0      0 127.0.0.53:53           0.0.0.0:*               LISTEN      640/systemd-resolve \ntcp        0      0 127.0.0.11:34293        0.0.0.0:*               LISTEN      -                   \ntcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      1079/sshd: /usr/sbi \ntcp        0      0 127.0.0.1:10248         0.0.0.0:*               LISTEN      4526/kubelet        \ntcp        0      0 127.0.0.1:10249         0.0.0.0:*               LISTEN      4988/kube-proxy     \ntcp        0      0 192.25.128.12:2379      0.0.0.0:*               LISTEN      3550/etcd           \ntcp        0      0 127.0.0.1:2379          0.0.0.0:*               LISTEN      3550/etcd           \ntcp        0      0 192.25.128.12:2380      0.0.0.0:*               LISTEN      3550/etcd           \ntcp        0      0 127.0.0.1:2381          0.0.0.0:*               LISTEN      3550/etcd           \ntcp        0      0 0.0.0.0:8080            0.0.0.0:*               LISTEN      1066/ttyd           \ntcp6       0      0 :::22                   :::*                    LISTEN      1079/sshd: /usr/sbi \ntcp6       0      0 :::8888                 :::*                    LISTEN      4558/kubectl        \ntcp6       0      0 :::10250                :::*                    LISTEN      4526/kubelet        \ntcp6       0      0 :::6443                 :::*                    LISTEN      3551/kube-apiserver \ntcp6       0      0 :::10256                :::*                    LISTEN      4988/kube-proxy     \n</code></pre>"},{"location":"k8s/commands/#check-service-status","title":"Check service status","text":"<pre><code># ssh to worker name\nssh workerNodeName\n\n# get the services\nsystemctl list-units --type=service --state=active\n\n# check logs of kubelet service\njournalctl -u kubelet\n\n# static files link \n /etc/kubernetes/*\n</code></pre>"},{"location":"k8s/deployment_python/","title":"Deployment","text":"<ul> <li>A Deployment provides declarative updates for Pods and ReplicaSets.</li> <li>You describe a desired state in a Deployment, and the Deployment Controller changes the actual state to the desired state at a controlled rate</li> <li>A deployment can create a RS</li> <li>Do not manage ReplicaSets owned by a Deployment.</li> <li></li> </ul>"},{"location":"k8s/deployment_python/#rollout-and-rollbacks","title":"Rollout and rollbacks","text":"<p>A rollout is created in deployment a status of which can be shown as below <pre><code># check rollout status using below\nkubectl rollout status deployment &lt;nginx-deployment&gt;\n\n#To check the revisions and history of rollout, you can use following command\nk rollout history deployment &lt;myapp-deployment&gt;\n</code></pre></p>"},{"location":"k8s/deployment_python/#update-a-deployment-using-rolling-update","title":"Update a deployment using rolling update","text":"<p>2 strategies are used mainly: 1. Re-create 2. Rolling update.</p> <p>Remember</p> <p>If you don\u2019t specify a strategy while creating the deployment it will assume it to be rolling update. In other words rolling update is the default deployment strategy.</p> <pre><code># here nginx indicates the Container the update will take place and nginx:1.16.1 indicates the new image and its tag.\nk set image deployment &lt;nginx-deployment&gt; podName=nginx:1.16.1\n\n# Example\nk set image deployment frontend simple-webapp=kodekloud/webapp-color:v2\n</code></pre>"},{"location":"k8s/deployment_python/#rollback","title":"Rollback","text":"<p>Say for instance, once you upgrade your application you realize something is inferior right. Something is wrong with the new version of the build when you used to upgrade.</p> <p>So you would like to roll back your update. <code>Kubernetes deployments</code> allow you to roll back to a previous revision. To undo a change, run the following command.</p> <pre><code>$ kubectl rollout undo deployment/myapp-deployment\n</code></pre>"},{"location":"k8s/deployment_python/#create-a-deployment","title":"Create a deployment","text":"<p>Use the below template <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment   # This name will become the basis for the ReplicaSets and Pods which are created later.\n  labels:\n    app: nginx\nspec:\n  replicas: 3   # The Deployment creates a ReplicaSet that creates three replicated Pods, indicated by the .spec.replicas field.\n  selector:\n    matchLabels:\n      app: nginx   # The .spec.selector field defines how the created ReplicaSet finds which Pods to manage.\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.14.2\n        ports:\n        - containerPort: 80\n</code></pre></p>"},{"location":"k8s/deployment_python/#create-k8s-deployment-via-python","title":"Create K8S deployment via Python","text":"<p>Install the K8S client using</p> <pre><code>$ pip install kubernetes\n</code></pre> <p>Create 2 files</p> <ul> <li><code>create-deployment.py</code></li> <li><code>k8s-deployment.yaml</code></li> </ul>"},{"location":"k8s/deployment_python/#python-file-code","title":"Python file code","text":"create-deployment.py<pre><code>from os import path\nimport yaml\nfrom kubernetes import client, config\n\ndef main():\n    # Configs can be set in Configuration class directly or using helper\n    # utility. If no argument provided, the config will be loaded from\n    # default location.\n    config.load_kube_config()\n\n    with open(path.join(path.dirname(__file__), \"k8s-deployment.yaml\")) as f:\n        dep = yaml.safe_load(f)\n        k8s_apps_v1 = client.AppsV1Api()\n        resp = k8s_apps_v1.create_namespaced_deployment(\n            body=dep, namespace=\"default\")\n        print(\"Deployment created. status='%s'\" % resp.metadata.name)\n\nif __name__ == '__main__':\n    main()\n</code></pre>"},{"location":"k8s/deployment_python/#deployment-file","title":"Deployment file","text":"k8s-deployment.yaml<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: k8s-deployment\n  labels:\n    app: nginx\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.15.4\n        ports:\n        - containerPort: 80\n</code></pre>"},{"location":"k8s/deployment_python/#creating-deployment","title":"Creating deployment","text":"<pre><code>python3 create-deployment.py\n</code></pre>"},{"location":"k8s/ds/","title":"DeamonSet","text":"<ul> <li>A <code>DaemonSet</code> ensures that all (or some) Nodes run a copy of a Pod. </li> <li>As nodes are added to the cluster, Pods are added to them. As nodes are removed from the cluster, those Pods are garbage collected.</li> <li>Deleting a <code>DaemonSet</code> will clean up the Pods it created.</li> </ul> <p>Typical uses of a DaemonSet</p> <ul> <li>Running a cluster storage daemon on every node </li> <li>Log collection: Running a logs collection daemon on every node</li> <li>Monitoring: Running a node monitoring daemon on every node</li> </ul>"},{"location":"k8s/ds/#commands","title":"Commands","text":"<pre><code>k get ds\n</code></pre> <p>We can not create ds using the kubectl, so we can </p> <p>The fastest way to create is to create a deployment</p> <pre><code>kubectl create deploy nginx --image=nginx --dry-run -o yaml &gt; nginx-ds.yaml\n</code></pre> <p>Now replace the line kind: Deployment with kind: DaemonSet in <code>nginx-ds.yaml</code> and remove the line <code>replicas: 1</code> , <code>strategy {}</code> and <code>status {}</code> as well. Otherwise, it shows error for some required fields like this</p> <pre><code>error: error validating \"nginx-ds.yaml\": error validating data: [ValidationError(DaemonSet.spec): unknown field \"strategy\" in io.k8s.api.apps.v1.DaemonSetSpec, ValidationError(DaemonSet.status): missing required field \"currentNumberScheduled\" in io.k8s.api.apps.v1.DaemonSetStatus,ValidationError(DaemonSet.status): missing required field \"numberMisscheduled\" in io.k8s.api.apps.v1.DaemonSetStatus, ValidationError(DaemonSet.status): missing required field \"desiredNumberScheduled\" in io.k8s.api.apps.v1.DaemonSetStatus, ValidationError(DaemonSet.status): missing required field \"numberReady\" in io.k8s.api.apps.v1.DaemonSetStatus]; if you choose to ignore these errors, turn validation off with --validate=false\n</code></pre>"},{"location":"k8s/ingress/","title":"Ingress","text":""},{"location":"k8s/ingress/#ingress-fanout","title":"Ingress Fanout","text":"Fanout Ingress Example<pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: simple-fanout-example\nspec:\n  rules:\n  - host: ammarjitdhillon.com\n    http:\n      paths:\n      - path: /sa\n        pathType: Prefix\n        backend:\n          service:\n            name: serviceA\n            port:\n              number: 4200\n      - path: /sb\n        pathType: Prefix\n        backend:\n          service:\n            name: serviceB\n            port:\n              number: 8080\n</code></pre>"},{"location":"k8s/karpenter/","title":"Karpenter","text":"<p>Why is Karpenter required?</p> <p>It helps improve your application availability and cluster efficiency by rapidly launching right-sized compute resources in response to changing application load. Karpenter also provides <code>just-in-time</code> compute resources to meet your application\u2019s needs and will soon automatically optimize a cluster\u2019s compute resource footprint to reduce costs and improve performance.</p> <p>Before Karpenter, Kubernetes users needed to dynamically adjust the compute capacity of their clusters to support applications using <code>Amazon EC2 Auto Scaling groups</code> and the <code>Kubernetes Cluster Autoscaler</code>. Nearly half of Kubernetes customers on AWS report that configuring cluster auto scaling using the Kubernetes Cluster Autoscaler is challenging and restrictive.</p> <p> Karpenter process flow <p></p>"},{"location":"k8s/keda/","title":"KEDA","text":"<p>KEDA (<code>Kubernetes-based Event Driven Autoscaler</code>) lets you drive the autoscaling of Kubernetes workloads based on the number of events, such as a custom metric scraped breaching a specified threshold, or when there\u2019s a message in a Amazon Managed Streaming for Apache Kafka queue.</p>"},{"location":"k8s/node-affinity/","title":"Node affinity","text":"<p>How to place a pod to a node?</p> <p>They are 3 ways to place a POD on a specific node:</p> <pre><code>- Node Selectors: With the Node Selectors we cannot provide advanced expressions like OR or NOT with node selectors. nodeSelector is the simplest recommended form of node selection constraint. You can add the nodeSelector field to your Pod specification and specify the node labels you want the target node to have. Kubernetes only schedules the Pod onto nodes that have each of the labels you specify.\n- Taints and Tolerations: \n- Node Affinity\n</code></pre> <ul> <li>Node affinity allows you to tell Kubernetes to schedule pods only to specific subsets of nodes.</li> </ul> <p>Some benefits of affinity and anti-affinity over the <code>nodeSelectors</code> are</p> <ul> <li>The <code>affinity/anti-affinity</code> language is more expressive. <code>nodeSelector</code> only selects nodes with all the specified labels. <code>Affinity/anti-affinity</code> gives you more control over the selection logic. </li> <li>You can indicate that a rule is <code>soft</code> or <code>preferred</code>, so that the scheduler still schedules the Pod even if it can't find a matching node. </li> <li>You can constrain a Pod using labels on other Pods running on the node (or other topological domain), instead of just node labels, which allows you to define rules for which Pods can be co-located on a node.</li> </ul> pod with node affinity and anti-affinity<pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: with-node-affinity\nspec:\n  affinity:\n    nodeAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        nodeSelectorTerms:\n        - matchExpressions:\n          - key: topology.kubernetes.io/zone\n            operator: In\n            values:\n            - antarctica-east1\n            - antarctica-west1\n      preferredDuringSchedulingIgnoredDuringExecution:\n      - weight: 1\n        preference:\n          matchExpressions:\n          - key: another-node-label-key\n            operator: In\n            values:\n            - another-node-label-value\n  containers:\n  - name: with-node-affinity\n    image: registry.k8s.io/pause:2.0\n</code></pre>"},{"location":"k8s/ns/","title":"Namespace","text":""},{"location":"k8s/ns/#basics","title":"Basics","text":"<ul> <li>In Kubernetes, namespaces provides a mechanism for isolating groups of resources within a single cluster.</li> <li>Names of resources need to be unique within a namespace, but not across namespaces.</li> <li>Namespace-based scoping is applicable only for namespaced objects (e.g. Deployments, Services, etc) and not for cluster-wide objects (e.g. StorageClass, Nodes, PersistentVolumes, etc).</li> </ul> <p>Remember</p> <p>Namespaces cannot be nested inside one another and each Kubernetes resource can only be in one namespace.</p> <p>An example of namespace is shown below:  </p>"},{"location":"k8s/ns/#initial-namespaces","title":"Initial namespaces","text":"<p>Kubernetes starts with 4 initial namespaces:</p>"},{"location":"k8s/ns/#default","title":"default","text":"<p>Kubernetes includes this namespace so that you can start using your new cluster without first creating a namespace.</p>"},{"location":"k8s/ns/#kube-node-lease","title":"kube-node-lease","text":"<p>This namespace holds Lease objects associated with each node. Node leases allow the kubelet to send <code>heartbeats</code> so that the control plane can detect node failure.</p>"},{"location":"k8s/ns/#kube-public","title":"kube-public","text":"<p>This namespace is readable by all clients (including those not authenticated). This namespace is mostly reserved for cluster usage, in case that some resources should be visible and readable publicly throughout the whole cluster. The public aspect of this namespace is only a convention, not a requirement.</p>"},{"location":"k8s/ns/#kube-system","title":"kube-system","text":"<p>The namespace for objects created by the Kubernetes system.</p>"},{"location":"k8s/ns/#namespaces-and-dns","title":"Namespaces and DNS","text":"<p>When you create a Service, it creates a corresponding DNS entry. This entry is of the form <code>&lt;service-name&gt;.&lt;namespace-name&gt;.svc.cluster.local</code>, which means that if a container only uses <code>&lt;service-name&gt;</code>, it will resolve to the service which is local to a namespace. This is useful for using the same configuration across multiple namespaces such as Development, Staging and Production. If you want to reach across namespaces, you need to use the <code>fully qualified domain name (FQDN)</code>.</p>"},{"location":"k8s/ns/#commands","title":"Commands","text":""},{"location":"k8s/ns/#set-the-default-namespace","title":"Set the default namespace","text":"<pre><code>kubectl config set-context --current --namespace=&lt;insert-namespace-name-here&gt;\n\n# Validate it\nkubectl config view --minify | grep namespace:\n</code></pre>"},{"location":"k8s/ns/#see-namespaced-and-non-namespaced-resources","title":"See namespaced and non-namespaced resources","text":"<p>Most Kubernetes resources (e.g. pods, services, replication controllers, and others) are in some namespaces. However, namespace resources are not themselves in a namespace. And low-level resources, such as <code>nodes</code> and <code>persistentVolumes</code>, are not in any namespace.</p> <pre><code># In a namespace\nkubectl api-resources --namespaced=true\n\n# Not in a namespace\nkubectl api-resources --namespaced=false\n</code></pre>"},{"location":"k8s/poc_minikube/","title":"Demo Python app using Minikube","text":""},{"location":"k8s/poc_minikube/#simple-echo-container","title":"Simple echo container \ud83d\uddf3","text":"<p>Create a Dockerfile</p> Dockerfile<pre><code>FROM alpine \nCMD [\"echo\", \"Welcome to my blog\"]\n</code></pre> <p>Build image in minikube and run the pod \ud83d\ude80</p> Build image and run it<pre><code>minikube image build -t test . # build image to minikube directly\n\nkubectl run test-pod --image=test --image-pull-policy=Never --restart=Never\n\nk logs test-pod #check pod logs\n\n# below is the output\nWelcome to my blog\n</code></pre>"},{"location":"k8s/poc_minikube/#docker-setup","title":"Docker setup \ud83d\udea2","text":""},{"location":"k8s/poc_minikube/#build-an-image","title":"Build an image \ud83d\udc77\ud83c\udffb\u200d\u2642\ufe0f","text":"<pre><code>docker build -f ../pathToDockerFile/Dockerfile -t demo:latest .\n</code></pre>"},{"location":"k8s/poc_minikube/#run-image","title":"Run image \ud83c\udfc3\u200d\u2642\ufe0f","text":"<pre><code># run image\ndocker run -p 5000:5000 demo # (1)\n\n# get pid on a port num\nlsof -i:port_num\n\nkill -9 pid\n\n# single liner to kill process on a certain port number\nkill -9 $(lsof -t -i:5000)\n\n# Remove all containers to unbind the port (Not recommended)\ndocker rm -fv $(docker ps -aq)  \n</code></pre> <ol> <li>Here we are mapping the 5000 port of container to the 5000 port of host</li> </ol>"},{"location":"k8s/poc_minikube/#remove-image-from-registry","title":"Remove image from registry \u274c","text":"<p>Before removing the image from docker registry, we need to stop and delete the container</p> <pre><code>docker ps -all            # see list of containers\ndocker rm CONTAINER_ID    # remove container that is running image we want to remove.\n\ndocker image ls           # list of all images\ndocker rmi -f IMAGE_ID    # remove the image\n</code></pre>"},{"location":"k8s/poc_minikube/#generic-docker-commands","title":"Generic Docker commands \ud83d\udda5","text":"Some important Docker commands \ud83d\udef3<pre><code># Build an image from a Dockerfile\ndocker image build  \n\n# Show the history of an image\ndocker image history\n\n# Import the contents from a tarball to create a filesystem image\ndocker image import \n\n# Display detailed information on one or more images\ndocker image inspect\n\n# Load an image from a tar archive or STDIN\ndocker image load   \n\n# List images\ndocker image ls\n\n# Remove unused images\ndocker image prune  \n\n# Pull an image or a repository from a registry\ndocker image pull   \n\n# Push an image or a repository to a registry\ndocker image push   \n\n# Remove one or more images\ndocker image rm\n\n# Remove one or more images\ndocker image save   #Save one or more images to a tar archive (streamed to STDOUT by default)\n\n# Create a tag TARGET_IMAGE that refers to SOURCE_IMAGE\ndocker image tag    \n</code></pre>"},{"location":"k8s/poc_minikube/#minikube-setup","title":"** Minikube setup ** \u2693\ufe0f","text":"<p>What is Hyperkit?</p> <p>HyperKit is an open-source hypervisor for macOS hypervisor, optimized for lightweight virtual machines and container deployment. This is the default hypervisor for Minikube in Mac.</p>"},{"location":"k8s/poc_minikube/#start-minikube","title":"Start minikube","text":"<pre><code># using docker as hypervisor \nminikube start --driver=docker\n\n# enable kubelet service, if needed\nsystemctl enable kubelet.service\n</code></pre>"},{"location":"k8s/poc_minikube/#check-status","title":"Check status \ud83d\udcc8","text":"<p><code>minikube status</code> will result in something similar as shown below</p> Output of minikube status is shown below<pre><code>minikube\ntype: Control Plane\nhost: Running\nkubelet: Running\napiserver: Running\nkubeconfig: Configured\n</code></pre> <p>Check the nodes of minikube using</p> <pre><code>kubectl get nodes # (1) \n</code></pre> <ol> <li>This will show <code>control-plane</code> in case of minikube as whole setup is on local</li> </ol>"},{"location":"k8s/poc_minikube/#lauch-dashboard","title":"Lauch dashboard \ud83d\udda5","text":"<p>This is helpful to see various resources in one place.</p> <p><pre><code>minikube dashboard\n</code></pre> </p>"},{"location":"k8s/poc_minikube/#registry-options","title":"Registry options:","text":"<p>Get current registry</p> <pre><code>docker info | grep Regis\n</code></pre> <p>There are 3 options</p> <ol> <li>Use existing docker resigtry</li> <li>Setup a private registry</li> <li>Directly build images on Minikube</li> </ol>"},{"location":"k8s/poc_minikube/#1-docker-container-registry","title":"1. Docker container registry","text":"<pre><code>minikube config set driver docker # set the driver\n\nminikube config get driver        # confirm the driver status (1)\n</code></pre> <ol> <li>Should get <code>docker</code> as output</li> </ol>"},{"location":"k8s/poc_minikube/#setup-registry-addon","title":"Setup registry addon \ud83d\udce2","text":"Steps to setup registry addon<pre><code>$ minikube addons configure registry-creds # (1)\n\nDo you want to enable AWS Elastic Container Registry? [y/n]: n\n\nDo you want to enable Google Container Registry? [y/n]: n\n\nDo you want to enable Docker Registry? [y/n]: y # (2)\n-- Enter docker registry server url: https://index.docker.io/v1/\n-- Enter docker registry username: amar\n-- Enter docker registry password: \n\nDo you want to enable Azure Container Registry? [y/n]: n\n\u2705  registry-creds was successfully configured\n</code></pre> <ol> <li> <p><code>GCR/ECR/ACR/Docker</code>: minikube has an addon, registry-creds which maps credentials into minikube to support pulling from Google Container Registry (GCR), Amazon\u2019s EC2 Container Registry (ECR), Azure Container Registry (ACR), and Private Docker registries. You will need to run minikube addons configure registry-creds and minikube addons enable registry-creds to get up and running.</p> </li> <li> <p>Setup your own username and password</p> </li> </ol>"},{"location":"k8s/poc_minikube/#cofigure-to-use-docker-registry","title":"Cofigure to use Docker registry","text":"<p>If you want to create the <code>registry</code> on minikube's Docker then run </p> <pre><code>eval $(minikube docker-env)\n</code></pre> <p>What does this command do?</p> <p>The command <code>minikube docker-env</code> returns a set of bash environment variable exports to configure your local environment to re-use the <code>Docker daemon</code> inside the Minikube instance.</p> <p>Passing this output through eval causes bash to evaluate these exports and put them into effect.</p> <p>You can review the specific commands which will be executed in your shell by omitting the evaluation step and running minikube docker-env directly. However, this will not perform the configuration \u2013 the output needs to be evaluated for that.</p>"},{"location":"k8s/poc_minikube/#enable-registry-addon","title":"Enable registry addon \ud83d\udccc","text":"<pre><code>$ minikube addons enable registry-creds\n\u2757  registry-creds is a 3rd party addon and not maintained or verified by minikube maintainers, enable at your own risk.\n    \u25aa Using image upmcenterprises/registry-creds:1.10\n\ud83c\udf1f  The 'registry-creds' addon is enabled\n</code></pre> <p>check more info on https://minikube.sigs.k8s.io/docs/handbook/registry/</p>"},{"location":"k8s/poc_minikube/#2-setup-private-registry","title":"2. Setup private registry \ud83d\uddf3","text":"<p>To make docker available on the host machine's terminal</p> <p>Otherwise enter in the virtual machine via <code>minikube ssh</code>, and then proceed with the following steps</p> <p>Depending on your operating system, minikube will automatically mount your homepath onto the VM.</p> <p>you'll need to add the local registry as insecure in order to use http (may not apply when using localhost but does apply if using the local hostname) Don't use http in production, make the effort for securing things up.</p> <p>Use a local registry:</p> Create a local resigtry<pre><code>docker run -d -p 5000:5000 \\\n--restart=always \\\n--name local-registry registry:2\n</code></pre> <p>Now tag your image properly:</p> Tag an image<pre><code>docker tag ubuntu localhost:5000/ubuntu\n</code></pre> <p>Note</p> <p>The localhost should be changed to dns name of the machine running registry container.</p> <p>Now push your image to local registry:</p> <p>Push Image<pre><code>docker push localhost:5000/ubuntu\n</code></pre> You should be able to pull it back:</p> Pull Image<pre><code>docker pull localhost:5000/ubuntu\n</code></pre> <p>Now, change your <code>yaml file</code> to use the local registry.</p>"},{"location":"k8s/poc_minikube/#3building-on-minikube","title":"3.Building on Minikube","text":"<pre><code>minikube image build -t test_app .\n</code></pre> <p>Confirm it using</p> <pre><code>minikube image ls\n</code></pre>"},{"location":"k8s/poc_minikube/#create-k8s-manifests-generic","title":"Create K8S manifests: Generic","text":""},{"location":"k8s/poc_minikube/#crate-a-pod","title":"Crate a pod","text":"<p>You can create a pod if needed as shown below</p> create a pod manifest using dry run<pre><code>kubectl run mypod --image=docker.io/library/demo:latest \\\n--labels app=demo-dp \\\n--dry-run=client -o yaml &gt; test_pod.yaml\n</code></pre> <p>In this example we are directly creating a deployment, so creating a pod is not required.</p>"},{"location":"k8s/poc_minikube/#create-a-deployment","title":"Create a deployment","text":"Create depyloyment using declarative commands<pre><code>kubectl create deployment demo-dp \\\n--image=docker.io/library/demo:latest \\\n--replicas=1 -o yaml &gt; dep_demo.yaml\n</code></pre>"},{"location":"k8s/poc_minikube/#create-a-service","title":"Create a service \ud83c\udf7b","text":"<p>The <code>service yaml</code> config can be</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: backend\n  name: backend\nspec:\n  ports:\n  - name: api\n    protocol: TCP\n    port: 10000\n  selector:\n    app: backend\n</code></pre>"},{"location":"k8s/poc_minikube/#confirm-the-resources","title":"Confirm the resources","text":"<p>Check if service and deployment is created. You can check the endpoint using</p> <pre><code>k get ep # (1)\n</code></pre> <ol> <li>Check if endpoint is created</li> </ol> <p><code>ssh</code> into pod using</p> <pre><code> k exec -it POD_ID -- /bin/sh\n ```\n\nNow check the servcie status using\n\n```bash\n$ k exec -it POD_ID -- /bin/sh\n\nprintenv | grep SERVICE # run this command after doing `exec -it`\n</code></pre> Sample output of printenv \ud83d\udcdd<pre><code>KUBERNETES_SERVICE_PORT=443\nHELLO_PYTHON_SERVICE_PORT_6000_TCP_ADDR=10.109.182.67\nHELLO_PYTHON_SERVICE_PORT_6000_TCP_PORT=6000\nHELLO_PYTHON_SERVICE_PORT_6000_TCP_PROTO=tcp\nDEMO_SERVICE_PORT_6000_TCP_ADDR=10.103.151.245\nDEMO_SERVICE_PORT_6000_TCP_PORT=6000\nDEMO_SERVICE_PORT_6000_TCP_PROTO=tcp\nHELLO_PYTHON_SERVICE_SERVICE_HOST=10.109.182.67\nHELLO_PYTHON_SERVICE_PORT_6000_TCP=tcp://10.109.182.67:6000\nDEMO_SERVICE_SERVICE_HOST=10.103.151.245\nHELLO_PYTHON_SERVICE_PORT=tcp://10.109.182.67:6000\nHELLO_PYTHON_SERVICE_SERVICE_PORT=6000\nDEMO_SERVICE_PORT_6000_TCP=tcp://10.103.151.245:6000\nKUBERNETES_SERVICE_PORT_HTTPS=443\nDEMO_SERVICE_PORT=tcp://10.103.151.245:6000\nDEMO_SERVICE_SERVICE_PORT=6000\nKUBERNETES_SERVICE_HOST=10.96.0.1\n</code></pre>"},{"location":"k8s/poc_minikube/#poc-example-using-fast-api","title":"POC example using Fast API","text":"<p>Below are the steps to create a sample POC</p>"},{"location":"k8s/poc_minikube/#install-fastapi-on-local","title":"Install FastAPI on local","text":"<p>Install fastapi using</p> <pre><code>$ python -m pip install fastapi uvicorn[standard]\n</code></pre> <p>Run server on local using</p> <pre><code>$ uvicorn demo_main:app --reload\n</code></pre> <p>Get access to swagger docs using</p> <pre><code>http://127.0.0.1:8000/docs\n</code></pre> <p>Create the FastAPI Code</p> <ul> <li>Create an app directory and enter it.</li> <li>Create an empty file init.py.</li> <li>Create a main.py file with below code</li> </ul> FastAPI sample code \ud83d\udc68\ud83c\udffb\u200d\ud83d\udcbb<pre><code>from typing import Union\n\nfrom fastapi import FastAPI\n\napp = FastAPI()\n\n\n@app.get(\"/\")\ndef read_root():\n    return {\"Hello\": \"World\"}\n\n\n@app.get(\"/items/{item_id}\")\ndef read_item(item_id: int, q: Union[str, None] = None):\n    return {\"item_id\": item_id, \"q\": q}\n</code></pre> <p>Why we need init.py?</p> <p>Files named <code>__init__.py</code> are used to mark directories on disk as Python package directories. If you have the files</p> <p><pre><code>mydir/spam/__init__.py\nmydir/spam/module.py\n</code></pre> and <code>mydir</code> is on your path, you can import the code in <code>module.py</code> as</p> <pre><code>import spam.module\n</code></pre> <p>or</p> <pre><code>from spam import module\n</code></pre> <p>If you remove the <code>__init__.py</code> file, Python will no longer look for submodules inside that directory, so attempts to import the module will fail.</p> <p>The  <code>__init__.py</code> file is usually empty, but can be used to export selected portions of the package under more convenient name, hold convenience functions, etc. Given the example above, the contents of the init module can be accessed as</p> <p><code>import spam</code></p>"},{"location":"k8s/poc_minikube/#requirements-file","title":"Requirements file","text":"<p>An <code>requirements.txt</code> is shown below </p> <pre><code>fastapi\npydantic\nuvicorn\n</code></pre>"},{"location":"k8s/poc_minikube/#create-a-dockerfile","title":"Create a DockerFile","text":"<p>Now in the same project directory create a file Dockerfile with:</p> Dockerfile for FastAPI \ud83d\ude80<pre><code>FROM python:3.9\n\nWORKDIR /code\n\nCOPY ./requirements.txt /code/requirements.txt\n\nRUN pip install --no-cache-dir --upgrade -r /code/requirements.txt\n\nCOPY ./app /code/app\n\nCMD [\"uvicorn\", \"app.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"80\"]\n</code></pre> <p>You should now have a directory structure like:</p> <pre><code>.\n\u251c\u2500\u2500 app\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 __init__.py\n\u2502   \u2514\u2500\u2500 main.py\n\u251c\u2500\u2500 Dockerfile\n\u2514\u2500\u2500 requirements.txt\n</code></pre>"},{"location":"k8s/poc_minikube/#create-image-on-minikube","title":"Create image on Minikube","text":"<p>Create a image using the sample code </p> <pre><code>minikube image build -t test_app .\n</code></pre>"},{"location":"k8s/poc_minikube/#create-k8s-manifests-for-poc","title":"Create k8s manifests for POC","text":"<ol> <li>Using Kubectl</li> <li>Using Python</li> </ol>"},{"location":"k8s/poc_minikube/#using-kubectl","title":"Using Kubectl","text":"<p>Given the fact that we have 2 files</p> <ol> <li>create_deployment.yaml</li> <li>create_servcie.yaml</li> </ol> <p>we can create a deployment using the following</p> <pre><code>kubectl apply -f create_service\nkubectl apply -f create_deployment\n</code></pre>"},{"location":"k8s/poc_minikube/#using-python","title":"Using Python","text":"<p>Install the required package</p> <pre><code>pip install kubernetes\n</code></pre> <p>Confirm the installation using</p> <pre><code>$ pip list | grep kuber\nkubernetes                 24.2.0\n</code></pre> <p>Imagine that we have below service and deployment code</p> create_service.yaml<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: demo-service-api\nspec:\n  selector:\n    app: demo-api\n  ports:\n  - protocol: \"TCP\"\n    port: 80\n    targetPort: 80\n  type: LoadBalancer\n</code></pre> <p>A sample deployment manifest</p> create_deployment.yaml<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: demo-deploy-api\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: demo-api\n  template:\n    metadata:\n      labels:\n        app: demo-api\n    spec:\n      containers:\n      - name: demo-api\n        image: fast\n        imagePullPolicy: Never\n        ports:\n        - containerPort: 80\n</code></pre> <p>code to create manifests using Python K8S API</p> createApplication.py<pre><code>from os import path\nimport yaml\nfrom kubernetes import client, config\nfrom kubernetes.client import V1DeleteOptions\n\ndef main():\n    # Configs can be set in Configuration class directly or using helper\n    # utility. If no argument provided, the config will be loaded from\n    # default location.\n    config.load_kube_config()\n    k8s_apps_v1 = client.AppsV1Api()\n    core_v1_api = client.CoreV1Api()\n\n    create_service(core_v1_api)\n    create_deployment(k8s_apps_v1)\n\n    del_opts = V1DeleteOptions()\n    api_client = get_custom_objects_api_client()\n\ndef create_deployment(k8s_apps_v1):\n    with open(path.join(path.dirname(__file__), \"demo_deployment.yaml\")) as f:\n        dep = yaml.safe_load(f)\n        resp = k8s_apps_v1.create_namespaced_deployment(body=dep, namespace=\"default\")\n        print(\"Deployment created. status='%s'\" % resp.metadata.name)\n\n\ndef create_service(core_v1_api):\n    with open(path.join(path.dirname(__file__), \"demo_service.yaml\")) as f:        \n        service = yaml.safe_load(f)\n        resp = core_v1_api.create_namespaced_service(body=service, namespace=\"default\")\n        resp = core_v1_api.delete\n        print(\"Service status='%s'\" % resp.metadata.name)\n\nif __name__ == '__main__':\n    main()\n</code></pre> <p>Create the resources using</p> <pre><code>python createApplication.py\n</code></pre>"},{"location":"k8s/pod/","title":"Pod","text":""},{"location":"k8s/pod/#static-pods","title":"Static Pods","text":"<ul> <li>It is possible to create Pods by writing a file to a certain directory watched by Kubelet.</li> <li>These are called static pods. Unlike DaemonSet, static Pods cannot be managed with kubectl or other Kubernetes API clients.</li> <li>Static Pods do not depend on the apiserver, making them useful in cluster bootstrapping cases.</li> <li>The kubelet agent is responsible to watch each static Pod and restart it if it crashes.</li> <li>Also, static Pods may be deprecated in the future.</li> </ul> <p>Note</p> <p>The static Pods running on a node are visible on the API server but cannot be controlled by the API Server.</p>"},{"location":"k8s/pod/#edit-an-existing-pod","title":"Edit an existing POD","text":"<p>Warning</p> <p>Remember, you CANNOT edit specifications of an existing POD other than the below.</p> <p><pre><code>spec.containers[*].image\n\nspec.initContainers[*].image\n\nspec.activeDeadlineSeconds\n\nspec.tolerations\n</code></pre> For example, you cannot edit the environment variables, service accounts, resource limits (all of which we will discuss later) of a running pod. But if you really want to, you have 2 options:</p>"},{"location":"k8s/pod/#option-a","title":"Option A","text":"<ul> <li>For the 1<sup>st</sup> option, run the <code>kubectl edit pod &lt;pod name&gt;</code> command.  This will open the pod specification in an editor (<code>vi editor</code>). Then edit the required properties. When you try to save it, you will be denied. This is because you are attempting to edit a field on the pod that is not editable.</li> </ul> <p>A copy of the file with your changes is saved in a temporary location as shown above.</p> <p>You can then delete the existing pod by running the command:</p> <pre><code>kubectl delete pod webapp\n</code></pre> <p>Then create a new pod with your changes using the temporary file</p> <pre><code>kubectl create -f /tmp/kubectl-edit-ccvrq.yaml\n</code></pre>"},{"location":"k8s/pod/#option-b","title":"Option B","text":"<ul> <li>The 2<sup>nd</sup> option is to extract the pod definition in YAML format to a file using the command</li> </ul> <p><pre><code>kubectl get pod webapp -o yaml &gt; my-new-pod.yaml\n</code></pre> Then make the changes to the exported file using an editor (vi editor). Save the changes</p> <p><pre><code>vi my-new-pod.yaml\n</code></pre> Then delete the existing pod</p> <p><pre><code>kubectl delete pod webapp\n</code></pre> Then create a new pod with the edited file</p> <pre><code>kubectl create -f my-new-pod.yaml\n</code></pre>"},{"location":"k8s/pod/#multi-container-pods","title":"Multi-container pods","text":"<p>In a <code>multi-container pod</code>, each container is expected to run a process that stays alive as long as the POD's lifecycle. For example in the multi-container pod that has a web application and logging agent, both the containers are expected to stay alive at all times. The process running in the <code>log agent container</code> is expected to stay alive as long as the <code>web application</code> is running. If any of them fails, the POD restarts.</p> <p>But at times you may want to run a process that runs to completion in a container. For example a process that pulls a code or binary from a repository that will be used by the main web application. That is a task that will be run only  one time when the pod is first created. Or a process that waits  for an external service or database to be up before the actual application starts. That's where initContainers comes in.</p>"},{"location":"k8s/pod/#init-container","title":"Init Container","text":"<p>An initContainer is configured in a pod like all other containers, except that it is specified inside a initContainers section,  like this:</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: amar-app-pod\n  labels:\n    app: amar-app\nspec:\n  containers:\n  - name: myapp-container\n    image: busybox:1.28\n    command: ['sh', '-c', 'echo The app is running! &amp;&amp; sleep 3600']\n  initContainers:\n  - name: init-myservice\n    image: busybox\n    command: ['sh', '-c', 'git clone &lt;some-repository-that-will-be-used-by-application&gt; ; done;']\n</code></pre> <p>Init container must run to completion</p> <p>When a POD is first created the <code>initContainer</code> is run, and the process in the initContainer must run to a completion before the real container hosting the application starts.  </p>"},{"location":"k8s/pod/#multiple-init-containers","title":"Multiple init containers","text":"<p>You can configure multiple such initContainers as well, like how we did for multi-containers pod. In that case each init container is run one at a time in sequential order.</p> <p>If any of the initContainers fail to complete, Kubernetes restarts the Pod repeatedly until the Init Container succeeds.</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: amar-app-pod\n  labels:\n    app: amar-app\nspec:\n  containers:\n  - name: myapp-container\n    image: busybox:1.28\n    command: ['sh', '-c', 'echo The app is running! &amp;&amp; sleep 3600']\n  initContainers:\n  - name: init-myservice\n    image: busybox:1.28\n    command: ['sh', '-c', 'until nslookup myservice; do echo waiting for myservice; sleep 2; done;']\n  - name: init-mydb\n    image: busybox:1.28\n    command: ['sh', '-c', 'until nslookup mydb; do echo waiting for mydb; sleep 2; done;']\n</code></pre>"},{"location":"k8s/rc/","title":"Replication Controller","text":"<p>How RC works?</p> <p>If there are too many pods, the ReplicationController terminates the extra pods. If there are too few, the ReplicationController starts more pods. Unlike manually created pods, the pods maintained by a ReplicationController are automatically replaced if they fail, are deleted, or are terminated.</p> <ul> <li>A <code>ReplicationController</code> ensures that a specified number of pod replicas are running at any one time. In other words, a ReplicationController makes sure that a pod or a homogeneous set of pods is always up and available.</li> <li>Replication controller can span multiple nodes.</li> <li>Replication controller (RC) is the old tech while Replica Set (RS) is the new one.</li> </ul> Exmaple RC<pre><code>apiVersion: v1\nkind: ReplicationController\nmetadata:           # this metadata is for RC\n  name: nginx\n  labels:\n    env: dev        # label for RC\nspec:               # spec for RC\n  replicas: 3       # total replicas for RC. Defaults to 1 if not specified\n  selector:\n    app: nginx      # A ReplicationController manages all the pods with labels that match this selector\n  template:         # This template is for pod, if new one has to be created. This is the required component\n    metadata:       # this metadata is for POD\n      name: nginx\n      labels:\n        app: nginx  # If specified, the .spec.template.metadata.labels must be equal to the .spec.selector, or it will be rejected by the API\n    spec:           # spec for pods\n      containers:\n      - name: nginx-by-amar\n        image: nginx\n        ports:\n        - containerPort: 80\n</code></pre>"},{"location":"k8s/rc/#deleting-only-a-replicationcontroller","title":"Deleting only a ReplicationController","text":"<p>You can delete a ReplicationController without affecting any of its pods. Using kubectl, specify the <code>--cascade=orphan</code> option to kubectl delete.</p>"},{"location":"k8s/rs/","title":"Replica Set","text":"<ul> <li>Replication controller (RC) is the old tech while Replica Set (RS) is the new one.</li> <li>A ReplicaSet's purpose is to maintain a stable set of replica Pods running at any given time. As such, it is often used to guarantee the availability of a specified number of identical Pods.</li> <li>A ReplicaSet then fulfills its purpose by creating and deleting Pods as needed to reach the desired number. When a ReplicaSet needs to create new Pods, it uses its Pod template.</li> <li></li> </ul> <p>Using Deployment and RS together</p> <p><code>ReplicaSet</code> ensures that a specified number of pod replicas are running at any given time. However, a Deployment is a higher-level concept that manages ReplicaSets and provides declarative updates to Pods along with a lot of other useful features.</p>"},{"location":"k8s/rs/#imperative-create","title":"Imperative create","text":"<pre><code>k create -f RS-FILE.YAML\n\n# or use this one\nk apply -f RS-FILE.YAML\n</code></pre> <p>A sample file is given below <pre><code>apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: frontend\n  labels:\n    app: guestbook\n    tier: frontend\nspec:\n  # modify replicas according to your case\n  replicas: 3\n  selector:\n    matchLabels:\n      tier: frontend\n  template:\n    metadata:\n      labels:\n        tier: frontend\n    spec:\n      containers:\n      - name: php-redis\n        image: gcr.io/google_samples/gb-frontend:v3\n</code></pre></p>"},{"location":"k8s/rs/#scaling-an-rs","title":"Scaling an RS","text":"<pre><code># Get the values from the file\nk replace -f RS-FILE.YAML\n\n# the file will be used while replicas will be replaced by the number you specify\nk scale --replicas=6 -f RS-FILE.YAML\n\n# this one is recommended if already existing\nk scale --replicas=6 rs RS-NAME\n</code></pre>"},{"location":"k8s/svc/","title":"Networking","text":""},{"location":"k8s/svc/#service","title":"Service","text":"<ul> <li>A <code>Kubernetes Service</code> is a resource you create to make a single, constant point of entry to a group of pods providing the same service.</li> <li>Each service has an IP address and port that never change while the service exists. Clients can open connections to that IP and port, and those connections are then routed to one of the pods backing that service. This way, clients of service don\u2019t need to know the location of individual pods providing the service, allowing those pods to be moved around the cluster at any time. </li> <li>If we do an <code>exec</code> to service IP, then we will be able to <code>exec</code> to one of the pods backed by it.</li> </ul> <p>Info</p> <p>List the pods with the <code>kubectl get pods</code> command and choose one as your target for the <code>exec</code> command (in the following example, I've chosen the <code>kubia-7nog1 pod</code> as the target). You'll also need to obtain the cluster IP of your service (using <code>kubectl get svc</code>). When running the following commands yourself, be sure to replace the pod name and the service IP with your own:</p> <pre><code>kubectl exec kubia-7nog1 -- curl -s http://10.111.249.153\n</code></pre> <p>Various service types are:</p> <ol> <li>NodePort </li> <li>ClusterIP</li> <li>Load Balancer</li> <li>ExternalName</li> </ol> <p> </p>"},{"location":"k8s/svc/#nodeport","title":"NodePort","text":"<p>By creating a NodePort service, you make Kubernetes <code>reserve a port</code> on all its nodes (the same port number is used across all of them) and forward incoming connections to the pods that are part of the service.</p> <p> </p> <ul> <li>If you set the type field to NodePort, the Kubernetes control plane allocates a port from a range specified by --service-node-port-range flag (default: 30000-32767). Each node proxies that port (the same port number on every Node) into your Service.</li> <li>Every node in the cluster configures itself to listen on that assigned port and to forward traffic to one of the ready endpoints associated with that Service. You'll be able to contact the type: NodePort Service, from outside the cluster, by connecting to any node using the appropriate protocol (for example: TCP), and the appropriate port (as assigned to that Service).</li> </ul> <p> </p> <p>As shown above, how the same port  is allocated to all the nodes in order to make the service accessible. </p> <p> </p> <p>Remember</p> <ul> <li>NodePort service can be accessed through the <code>service's internal cluster IP</code> and through any node's IP and the reserved <code>node port</code></li> <li>Specifying the port isn\u2019t mandatory; Kubernetes will choose a random port if you omit it.</li> </ul> example of NodePort<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-service\nspec:\n  type: NodePort\n  selector:\n    app.kubernetes.io/name: MyApp\n  ports:\n      # By default and for convenience, the `targetPort` is set to the same value as the `port` field.\n    - port: 80\n      targetPort: 80\n      # Optional field\n      # By default and for convenience, the Kubernetes control plane will allocate a port from a range (default: 30000-32767)\n      nodePort: 30007\n</code></pre>"},{"location":"k8s/svc/#clusterip","title":"ClusterIP","text":"<p>Tldr</p> <p>Creates <code>internal IP addresses</code> for communication within the AKS cluster. This is ideal for internal communication between components. This is the default service type</p> <p> </p> <p>To make the pod accessible from the outside, we need to expose it through a <code>Service</code> object. You\u2019ll create a special service of type <code>LoadBalancer</code>, because if you create a regular service (a <code>ClusterIP service</code>), like the pod, it would also only be accessible from inside the cluster. By creating a LoadBalancer-type service, an external load balancer will be created and you can connect to the pod through the load balancer\u2019s public IP.</p> <p>The ClusterIP provides a load-balanced IP address. One or more pods that match a label selector can forward traffic to the IP address. The ClusterIP service must define one or more ports to listen on with target ports to forward TCP/UDP traffic to containers. The IP address that is used for the ClusterIP is not routable outside the cluster, like the pod IP address is</p>"},{"location":"k8s/svc/#loadbalancer","title":"LoadBalancer","text":"<p>Kubernetes clusters running on cloud providers usually support the automatic provision of a load balancer from the cloud infrastructure. All you need to do is set the service\u2019s type to LoadBalancer instead of NodePort. The load balancer will have its own unique, publicly accessible IP address and will redirect all connections to your service. You can thus access your service through the load balancer\u2019s IP address</p> <p>A more detailed LB example is shown below. This works out of box on various cloud platfroms such as GCP and Azure etc.</p> <p> </p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-service\nspec:\n  selector:\n    app.kubernetes.io/name: MyApp\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 9376\n  clusterIP: 10.0.171.239\n  type: LoadBalancer\nstatus:\n  loadBalancer:\n    ingress:\n    - ip: 192.0.2.127\n</code></pre> <p>Warning</p> <p>Some cloud providers allow you to specify the loadBalancerIP. In those cases, the load-balancer is created with the user-specified loadBalancerIP. If the loadBalancerIP field is not specified, the loadBalancer is set up with an ephemeral IP address. If you specify a loadBalancerIP but your cloud provider does not support the feature, the load balancer IP field that you set is ignored.</p>"},{"location":"k8s/svc/#externalname","title":"ExternalName","text":"<p>Creates a specific DNS entry for easier application access.</p>"},{"location":"k8s/svc/#network-plugins","title":"Network Plugins","text":""},{"location":"k8s/svc/#kubenet","title":"Kubenet","text":"<p>The kubenet networking option is the <code>default configuration</code> for AKS cluster creation. With kubenet:</p> <ol> <li>Nodes receive an IP address from the Azure VNet subnet. </li> <li>Pods receive an IP address from a logically different address space than the nodes' Azure virtual network subnet. </li> <li>NAT is then configured so that the pods can reach resources on the Azure virtual network. </li> <li>The source IP address of the traffic is translated to the node's primary IP address.</li> </ol> <p>Why to use Kubenet</p> <p>Only the nodes receive a routable IP address. The pods use NAT to communicate with other resources outside the AKS cluster. This approach reduces the number of IP addresses you need to reserve in your network space for pods to use.</p>"},{"location":"k8s/svc/#cni","title":"CNI","text":"<p>With Azure CNI, every pod gets an IP address from the subnet and can be accessed directly. These IP addresses must be planned in advance and unique across your network space.</p> <p>IP exhaustation</p> <p>Each node has a configuration parameter for the maximum number of pods it supports. The equivalent number of IP addresses per node are then reserved up front. This approach can lead to IP address exhaustion or the need to rebuild clusters in a larger subnet as your application demands grow, so it's important to plan properly.</p>"},{"location":"k8s/admin/monitoring/","title":"Monitoring","text":"<p>There are a number of open-source solutions available today, such as </p> <ul> <li><code>Metrics server</code></li> <li><code>Prometheus</code></li> <li><code>Elastic stack</code></li> </ul> <p>Proprietary solutions are:</p> <ul> <li><code>DataDog</code></li> <li><code>Dynatrace</code> </li> </ul>"},{"location":"k8s/admin/monitoring/#heapster-vs-metrics-server","title":"Heapster vs Metrics Server","text":"<p><code>Heapster</code> is the one of the original projects that enabled monitoring and analysis feature for Kubernetes.</p> <p>Remember</p> <p>Heapster is now deprecated and a slimmed down version was formed known as the Metric Server. </p> <p>You can have one metrics server per Kubernetes cluster. The metric server retrieves metrics from each of the Kubernetes nodes and PODs, aggregates them and stores them in memory.</p> <p>You will see a lot of reference online when you look for reference architecture on monitoring Kubernetes.</p> <p>Can I use metrics server for historical data?</p> <p>Note that the <code>metric server</code> is only an in-memory monitoring solution and doesn\u2019t store the metrics on the disk and as a result you cannot see historical performance data. </p>"},{"location":"k8s/admin/monitoring/#logging","title":"Logging","text":"<pre><code># see logs for a pod\nk logs &lt;podname&gt;\n\n# see logs for a container\nkubectl logs &lt;podname&gt; -c &lt;container_name&gt;\n\n# tail the logs\nk logs -f &lt;podname&gt;\n</code></pre> <p>Check out How log rotation is handled?</p>"},{"location":"k8s/admin/networking/","title":"Pre-requisite DNS","text":"<p>In this section, we will take a look at DNS in the Linux</p>"},{"location":"k8s/admin/networking/#name-resolution","title":"Name Resolution","text":"<p>With help of the <code>ping</code> command. Checking the reachability of the IP Addr on the Network.</p> <pre><code>$ ping 172.17.0.64\nPING 172.17.0.64 (172.17.0.64) 56(84) bytes of data.\n64 bytes from 172.17.0.64: icmp_seq=1 ttl=64 time=0.384 ms\n64 bytes from 172.17.0.64: icmp_seq=2 ttl=64 time=0.415 ms\n</code></pre> <p>Checking with their hostname</p> <pre><code>$ ping web\nping: unknown host web\n</code></pre> <p>Adding entry in the <code>/etc/hosts</code> file to resolve by their hostname.</p> <pre><code>$ cat &gt;&gt; /etc/hosts\n172.17.0.64  web\n\n\n# Ctrl + c to exit\n</code></pre> <p>It will look into the <code>/etc/hosts</code> file.</p> <pre><code>$ ping web\nPING web (172.17.0.64) 56(84) bytes of data.\n64 bytes from web (172.17.0.64): icmp_seq=1 ttl=64 time=0.491 ms\n64 bytes from web (172.17.0.64): icmp_seq=2 ttl=64 time=0.636 ms\n\n$ ssh web\n\n$ curl http://web\n</code></pre>"},{"location":"k8s/admin/networking/#dns","title":"DNS","text":"<p>Every host has a DNS resolution configuration file at <code>/etc/resolv.conf</code>.</p> <pre><code>$ cat /etc/resolv.conf\nnameserver 127.0.0.53\noptions edns0\n</code></pre> <p>To change the order of dns resolution, we need to do changes into the <code>/etc/nsswitch.conf</code> file.</p> <pre><code>$ cat /etc/nsswitch.conf\n\nhosts:          files dns\nnetworks:       files\n</code></pre> <ul> <li>If it fails in some conditions.</li> </ul> <pre><code>$ ping wwww.github.com\nping: www.github.com: Temporary failure in name resolution\n</code></pre> <ul> <li>Adding well known public nameserver in the <code>/etc/resolv.conf</code> file.</li> </ul> <p><pre><code>$ cat /etc/resolv.conf\nnameserver   127.0.0.53\nnameserver   8.8.8.8\noptions edns0\n</code></pre> <pre><code>$ ping www.github.com\nPING github.com (140.82.121.3) 56(84) bytes of data.\n64 bytes from 140.82.121.3 (140.82.121.3): icmp_seq=1 ttl=57 time=7.07 ms\n64 bytes from 140.82.121.3 (140.82.121.3): icmp_seq=2 ttl=57 time=5.42 ms\n</code></pre></p>"},{"location":"k8s/admin/networking/#networking-tools","title":"Networking Tools","text":"<p>Useful networking tools to test dns name resolution.</p>"},{"location":"k8s/admin/networking/#nslookup","title":"nslookup","text":"<pre><code>$ nslookup www.google.com\nServer:         127.0.0.53\nAddress:        127.0.0.53#53\n\nNon-authoritative answer:\nName:   www.google.com\nAddress: 172.217.18.4\nName:   www.google.com\n</code></pre>"},{"location":"k8s/admin/networking/#dig","title":"dig","text":"<pre><code>$ dig www.google.com\n\n; &lt;&lt;&gt;&gt; DiG 9.11.3-1 ...\n;; Got answer:\n;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 8738\n;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1\n\n;; OPT PSEUDOSECTION:\n; EDNS: version: 0, flags:; udp: 65494\n;; QUESTION SECTION:\n;www.google.com.                        IN      A\n\n;; ANSWER SECTION:\nwww.google.com.         63      IN      A       216.58.206.4\n\n;; Query time: 6 msec\n;; SERVER: 127.0.0.53#53(127.0.0.53)\n</code></pre>"},{"location":"k8s/admin/scheduler/","title":"Scheduler","text":"<ul> <li>If no scheduler is defined, then we can manually specify the node mame as shown below  </li> </ul>"},{"location":"k8s/admin/security/","title":"K8S Security","text":""},{"location":"k8s/admin/security/#autht","title":"AuthT","text":"<ul> <li>we define who can access.</li> <li>For machines, we can create <code>Service Accounts</code></li> <li>K8s does not create/manage user accounts but it does for service accounts</li> </ul>"},{"location":"k8s/admin/security/#authn","title":"AuthN","text":"<ul> <li>Using RBAC</li> </ul> <p>Various Authorization methods are</p>"},{"location":"k8s/admin/security/#alwaysallow","title":"AlwaysAllow","text":"<p>Allows all calls</p>"},{"location":"k8s/admin/security/#alwaysdeny","title":"AlwaysDeny","text":"<p>Deny all calls</p>"},{"location":"k8s/admin/security/#abac","title":"ABAC","text":"<p>Attribute-based access control (ABAC) defines an access control paradigm whereby access rights are granted to users through the use of policies which combine attributes together.</p>"},{"location":"k8s/admin/security/#rbac","title":"RBAC","text":"<p>Role-based access control (RBAC) is a method of regulating access to computer or network resources based on the roles of individual users within your organization. The RBAC API declares four kinds of Kubernetes object:</p> <ul> <li>Role</li> <li>ClusterRole</li> <li>RoleBinding</li> <li>ClusterRoleBinding</li> </ul> <p>Role is Namespaced but not ClusterRole</p> <p>A Role always sets permissions within a particular namespace; when you create a Role, you have to specify the namespace it belongs in.</p> <p>ClusterRole, by contrast, is a non-namespaced resource. The resources have different names (Role and ClusterRole) because a Kubernetes object always has to be either namespaced or not namespaced; it can't be both.</p>"},{"location":"k8s/admin/security/#role","title":"Role","text":"Role in the default namespace that can be used to grant read access to pods<pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  namespace: default\n  name: pod-reader\nrules:\n- apiGroups: [\"\"] # \"\" indicates the core API group\n  resources: [\"pods\"]\n  verbs: [\"get\", \"watch\", \"list\"]\n</code></pre>"},{"location":"k8s/admin/security/#rolebinding","title":"Rolebinding","text":"allows 'amar' to read pods in the default namespace.<pre><code>apiVersion: rbac.authorization.k8s.io/v1\n# This role binding allows \"amar\" to read pods in the \"default\" namespace.\n# You need to already have a Role named \"pod-reader\" in that namespace.\nkind: RoleBinding\nmetadata:\n  name: read-pods\n  namespace: default\nsubjects:\n# You can specify more than one \"subject\"\n- kind: User\n  name: amar # \"name\" is case-sensitive\n  apiGroup: rbac.authorization.k8s.io\nroleRef:\n  # \"roleRef\" specifies the binding to a Role / ClusterRole\n  kind: Role #this must be Role or ClusterRole\n  name: pod-reader # this must match the name of the Role or ClusterRole you wish to bind to\n  apiGroup: rbac.authorization.k8s.io\n</code></pre>"},{"location":"k8s/admin/security/#clusterrole","title":"ClusterRole","text":""},{"location":"k8s/admin/security/#clusterrolebinding","title":"ClusterRolebinding","text":""},{"location":"k8s/admin/security/#node-authorizer","title":"Node Authorizer","text":"<p>The requests from kubelet are handled by Node Authorizer. Node authorization is a special-purpose authorization mode that specifically authorizes API requests made by kubelets.</p> <p>Note</p> <p>They are used for access within the cluster</p>"},{"location":"k8s/admin/security/#webhook","title":"Webhook","text":"<p>Used to manage it externally using <code>Open Policy Agent</code> for example</p>"},{"location":"k8s/admin/security/#tls-certificates","title":"TLS certificates","text":"<p>An <code>SSL/TLS</code> certificate is a digital object that allows systems to verify the identity &amp; subsequently establish an encrypted network connection to another system using the <code>Secure Sockets Layer/Transport Layer</code> Security (SSL/TLS) protocol. Certificates are used within a cryptographic system known as a <code>Public Key Infrastructure (PKI)</code>. </p> <p>PKI provides a way for one party to establish the identity of another party using certificates if they both trust a third-party - known as a certificate authority.</p> <p>The format for public and private keys are shown below:</p> <p> </p> <p>Here are the various kinds of certs involved - client certificate - Server certificate - Root certificate (CA)</p> <p> </p> <p>Various keys used by server in K8s are  </p> <p>Put it all together  </p>"},{"location":"k8s/admin/security/#viewapprove-csrs","title":"View/Approve CSR's","text":"<pre><code># View CSR's using\nk get csr\n\n# See more details for CSR\nk get csr &lt;amar-dhillon&gt; -o yaml\n\n# Approve the CSR using\nk certificate approve &lt;certificate-name&gt;\n\n# Deny CSR using\nkubectl certificate deny &lt;certificate-name&gt;\n\n# Delete CSR using\nkubectl  delete csr &lt;certificate-name&gt;\n</code></pre> Delete CSR<pre><code>controlplane ~ \u279c  k delete csr agent-smith \ncertificatesigningrequest.certificates.k8s.io \"agent-smith\" deleted\n</code></pre>"},{"location":"k8s/admin/security/#view-certificates-in-k8s","title":"View Certificates in K8s","text":"<p>run the below command</p> <pre><code>cat /etc/kubernetes/manifests/kube-apiserver.yaml\n# look for the below `--tls-cert-file` line\n</code></pre> <p>Below is the example to see cert file used by Kube API Server</p> <pre><code>controlplane ~ \u279c  cat /etc/kubernetes/manifests/kube-apiserver.yaml | grep -i tls\n    - --tls-cert-file=/etc/kubernetes/pki/apiserver.crt\n    - --tls-private-key-file=/etc/kubernetes/pki/apiserver.key\n</code></pre> Identify the Certificate file used to authenticate kube-apiserver as a client to ETCD Server.<pre><code>controlplane ~ \u279c  cat /etc/kubernetes/manifests/kube-apiserver.yaml | grep -i etcd\n    - --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt\n    - --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt\n    - --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key\n    - --etcd-servers=https://127.0.0.1:2379\n</code></pre> Identify the key used to authenticate kubeapi-server to the kubelet server.<pre><code># Look for the certs in the api-server file\ncontrolplane ~ \u279c  cat /etc/kubernetes/manifests/kube-apiserver.yaml | grep -i kubelet\n    - --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt\n    - --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key\n    - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname\n</code></pre> Identify the ETCD Server Certificate used to host ETCD server.<pre><code># Look for the certs in the etcd `cert-file`\ncontrolplane ~ \u279c  cat /etc/kubernetes/manifests/etcd.yaml | grep cert\n    - --cert-file=/etc/kubernetes/pki/etcd/server.crt\n    - --client-cert-auth=true\n    - --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt\n    - --peer-client-cert-auth=true\n      name: etcd-certs\n    name: etcd-certs\n</code></pre> <p>Warning</p> <p>ETCD can have its own CA. So this may be a different CA certificate than the one used by kube-api server.</p> Identify the ETCD Server CA Root Certificate used to serve ETCD Server.<pre><code># Cert file is shown below as `peer-trusted-ca-file` property\ncontrolplane ~ \u279c  cat /etc/kubernetes/manifests/etcd.yaml | grep ca\n    - --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt\n    - --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt\n  priorityClassName: system-node-critical\n</code></pre> check for issuer of certificate<pre><code># check for issuer in this command\n openssl x509 -in /etc/kubernetes/pki/apiserver.crt -text\n</code></pre> Which of the below alternate names is not configured on the Kube API Server Certificate?<pre><code># Look at Alternative Names.\nopenssl x509 -in /etc/kubernetes/pki/apiserver.crt -text\n</code></pre> What is the Common Name (CN) configured on the ETCD Server certificate?<pre><code># check for Subject CN here\nopenssl x509 -in /etc/kubernetes/pki/etcd/server.crt --text\n</code></pre> How long, from the issued date, is the Kube-API Server Certificate valid for?<pre><code># Check expiry date in this file \nopenssl x509 -in /etc/kubernetes/pki/apiserver.crt  --text\n</code></pre>"},{"location":"k8s/admin/security/#check-kube-api-server-logs","title":"Check Kube API Server logs","text":"<p>Run <code>crictl ps -a</code> command to identify the kube-api server container. Run <code>crictl logs container-id</code> command to view the logs.</p>"},{"location":"k8s/admin/security/#kubeconfig","title":"Kubeconfig","text":"<p>A Kubeconfig is a YAML file with all the Kubernetes cluster details, certificates, and secret token to authenticate the cluster. You might get this config file directly from the cluster administrator or from a cloud platform if you are using managed Kubernetes cluster.</p> <p> </p> Default location for kubeconfig is<pre><code>~/.kube/config\n</code></pre> See current context<pre><code>kubectl config --kubeconfig=/root/my-kube-config current-context\n</code></pre>"},{"location":"k8s/admin/security/#update-context","title":"update context","text":"<pre><code>kubectl config --kubeconfig=/root/my-kube-config use-context research\n\n# if using the default context file, then\nkubectl config use-context &lt;amar-dev-cluster&gt;\n</code></pre>"},{"location":"k8s/admin/security/#using-the-kubeconfig-file-with-kubectl","title":"Using the Kubeconfig File With Kubectl","text":"<p>You can pass the Kubeconfig file with the Kubectl command to override the current context and KUBECONFIG env variable.</p> <pre><code>kubectl get nodes --kubeconfig=$HOME/.kube/dev_cluster_config\n</code></pre>"},{"location":"k8s/admin/security/#network-policy","title":"Network Policy","text":"<ul> <li>By default all pods can talk to one another.</li> <li>NetworkPolicies are an application-centric construct which allow you to specify how a pod is allowed to communicate with various network \"entities\"</li> <li>The entities that a Pod can communicate with are identified through a combination of the following 3 identifiers:</li> <li><code>Other pods</code> that are allowed (exception: a pod cannot block access to itself)</li> <li><code>Namespaces</code> that are allowed </li> <li><code>IP blocks</code> (exception: traffic to and from the node where a Pod is running is always allowed, regardless of the IP address of the Pod or the node)</li> </ul> <p>How to grant access?</p> <p>When defining a pod- or namespace- based NetworkPolicy, you use a selector to specify what traffic is allowed to and from the Pod(s) that match the selector. Meanwhile, when IP based NetworkPolicies are created, we define policies based on IP blocks (CIDR ranges).</p> <p>A sample Network policy is shown below</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: internal-policy\n  namespace: default\nspec:\n  podSelector:\n    matchLabels:\n      name: internal\n  policyTypes:\n    - Egress\n  egress:\n    - to:\n        - podSelector:\n            matchLabels:\n              name: payroll\n      ports:\n        - protocol: TCP\n          port: 8080\n    - to:\n        - podSelector:\n            matchLabels:\n              name: mysql\n      ports:\n        - protocol: TCP\n          port: 3306\n</code></pre>"},{"location":"k8s/admin/storage/","title":"Storage","text":""},{"location":"k8s/admin/storage/#storage","title":"Storage","text":""},{"location":"k8s/arch/apiserver/","title":"API Server","text":"<p>Main uses are</p> <ul> <li>Authenticate a user</li> <li>Validate a request</li> <li>Retrive data</li> <li>Update ETCD (only component to do that): The list of ETCD servers is updated in the API server</li> <li>Communicate with Scheduler</li> <li>Commnunicate with Kubelet</li> </ul>"},{"location":"k8s/arch/apiserver/#view-api-sever-options","title":"View API sever options","text":"<pre><code>cat /etc/systemd/system/kube-apisever.service\n</code></pre> <p>See the process</p> <pre><code>ps aux | grep kube-apiservice \n</code></pre>"},{"location":"k8s/arch/arch/","title":"K8S Architecture","text":"<p>K8S on high level</p> <ul> <li>A Kubernetes cluster consists of a master, which manages the cluster, and nodes, which run the services.</li> <li>Developers and the deployment pipeline interact with Kubernetes through the API server, which along with other cluster-management software runs on the master.</li> <li>Application containers run on nodes.</li> <li>Each node runs a <ul> <li>Kubelet, which manages the application container</li> <li>kube-proxy, which routes application requests to the pods, either directly as a proxy or indirectly by configuring iptables routing rules built into the Linux kernel.</li> </ul> </li> </ul> <p> </p>"},{"location":"k8s/arch/arch/#components","title":"Components","text":""},{"location":"k8s/arch/arch/#master-node","title":"Master Node","text":"<p>The master node consists of four components:</p> <ul> <li>The API server -  When you run commands on <code>kubectl</code>, this is what it communicated with to perform operations. The <code>API server</code> exposes an API for both external users and other components within the cluster.</li> <li>The scheduler \u2014 This is responsible for selecting an appropriate node where a <code>pod</code> will run, given priority, resource needs, and other constraints.</li> <li>The controller manager \u2014 This is responsible for executing <code>control loops</code>: the continual observe-diff-act operation that underpins the operation of Kubernetes.</li> <li>A distributed key-value data store, etcd \u2014 This stores the underlying state of the cluster and thereby makes sure it persists when nodes fail or restarts are required.</li> </ul>"},{"location":"k8s/arch/arch/#worker-node","title":"Worker node","text":"<p>Each worker node uses the following components to run and monitor applications:</p> <ul> <li>A container runtime \u2014 This can be Docker.</li> <li>The kubelet \u2014 This interacts with the Kubernetes master to start, stop, and monitor containers on the node.</li> <li>The kube-proxy \u2014 It is managing the virtual networking on each node. The proxy handles network routes and IP addressing for services and pods.</li> </ul>"},{"location":"k8s/arch/arch/#concepts","title":"Concepts","text":"<ul> <li><code>Control Groups</code>: it is a feature of the Linux kernel that limits, accounts for, and isolates the resource usage (CPU, Memory, Disk I/O, Network, etc.) of a collection of processes.</li> <li><code>CRI-O</code>: It is a lightweight container runtime and was specifically designed for k8s by Red Hat.</li> </ul>"},{"location":"k8s/arch/controllermgr/","title":"Controller Manager","text":"<ul> <li>It manages various controllers</li> <li>They do<ul> <li>watch the status</li> <li>remediate the situation</li> </ul> </li> </ul>"},{"location":"k8s/arch/controllermgr/#various-controllers","title":"Various controllers","text":"<p>Info</p> <p>Various controllers are</p> <ol> <li>Node controller</li> <li>Replication Controller</li> <li>Namespace controller</li> <li>PV controller</li> <li>RS controller</li> <li>Deployment controller</li> <li>Endpoint controller</li> </ol>"},{"location":"k8s/arch/docker/","title":"Docker file","text":"<p>A <code>Dockerfile</code> is a plain text file containing all the commands needed to build an image. Dockerfiles are written in a minimal scripting language designed for building and configuring images. They document the operations required to build an image, starting with a base image.</p> Sample Dockerfile<pre><code>FROM mcr.microsoft.com/dotnet/core/sdk:2.2\nWORKDIR /app\nCOPY myapp_code .\nRUN dotnet build -c Release -o /rel\nEXPOSE 80\nWORKDIR /rel\nENTRYPOINT [\"dotnet\", \"myapp.dll\"]\n</code></pre> <p>By convention, applications meant to be packaged as Docker images typically have a Dockerfile located in the root of their source code, and it's almost always named\u00a0<code>Dockerfile</code>.</p> <p>The\u00a0<code>docker build</code>\u00a0command creates a new image by running a Dockerfile. The syntax or this command has several parameters:</p> <ul> <li>The\u00a0f\u00a0flag indicates the name of the Dockerfile to use.</li> <li>The\u00a0t\u00a0flag specifies the name of the image to be created, in this example,\u00a0myapp:v1.</li> <li>The final parameter,\u00a0., provides the\u00a0build context\u00a0for the source files for the\u00a0COPY\u00a0command: the set of files on the host computer needed during the build process.</li> </ul> <pre><code>docker build -t myapp:v1 .\n</code></pre> <p>Behind the scenes, the\u00a0<code>docker build</code>\u00a0command creates a container, runs commands in it, then commits the changes to a new image</p>"},{"location":"k8s/arch/docker/#steps","title":"Steps","text":"<ul> <li>Create a <code>Dockerfile</code> for a new container image based on a starter image from Docker Hub</li> <li>Add files to an image using Dockerfile commands</li> <li>Configure an image's startup command with Dockerfile commands</li> <li>Build and run a web application packaged in a Docker image</li> <li>Deploy a Docker image using the Azure Container Instance service</li> </ul>"},{"location":"k8s/arch/etcd/","title":"ETCD","text":"<ul> <li>It is a distributed key-value store</li> <li>Uses RAFT consensus protocol</li> <li>Only once change is written to ETCD, the change is considered as complete</li> <li>ETCD runs in an HA environment and in this case the service should be configured so that various ETCD servers knows about one another</li> </ul> <p>The API version used by etcdctl to speak to etcd may be set to version 2 or 3 via the ETCDCTL_API environment variable. By default, etcdctl on master (3.4) uses the v3 API and earlier versions (3.3 and earlier) default to the v2 API.</p> set version<pre><code>export ETCDCTL_API=3\n</code></pre> <p>Warning</p> <p>When API version is not set, it is assumed to be set to version 2. And version 3 commands listed above don't work. When API version is set to version 3, version 2 commands listed above don't work.</p>"},{"location":"k8s/arch/etcd/#write-key","title":"Write key","text":"<p>Applications store keys into the etcd cluster by writing to keys. Every stored key is replicated to all etcd cluster members through the Raft protocol to achieve consistency and reliability.</p> set key<pre><code>$ ./etcdctl.exe put name amar\nOK\n</code></pre>"},{"location":"k8s/arch/etcd/#get-the-key","title":"Get the key","text":"get key<pre><code>$ ./etcdctl.exe get name\nname\namar\n</code></pre>"},{"location":"k8s/arch/etcd/#get-keys-in-k8s","title":"get keys in k8s","text":"<pre><code>k exec etcd-master -n kube-system etcdctl get / --prefix --keys-only \n</code></pre>"},{"location":"k8s/arch/kubelet/","title":"Kubelet","text":"<ul> <li>They are captain of the ship (worker node)</li> <li>It requrests the container runtime engine to do the job</li> <li>We need to install Kubelet on the worker nodes </li> </ul> <p>Rememmer</p> <p>The Kubelet monitors the state of worker node and reports back to API server.</p>"},{"location":"k8s/arch/kubelet/#check-the-process","title":"Check the process","text":"<pre><code>ps aux | grep kubelet\n</code></pre>"},{"location":"k8s/arch/kubeproxy/","title":"Kube Proxy","text":"<ul> <li>The Kubernetes network proxy runs on each node.</li> <li>It is used to setup the pod network, which is an internal network</li> <li>Kube proxy create an <code>iptable</code> route for each service, so that the traffic to the service is directed to the right pod.</li> </ul>"},{"location":"k8s/arch/scheduler/","title":"Scheduler","text":"<ul> <li>It decides on which node to assign the pod</li> <li>The 2 step process for node selection is <ul> <li>Filter nodes: Filter nodes which does not match the critera</li> <li>Rank nodes: </li> </ul> </li> </ul>"},{"location":"rpa/ai-rpa/","title":"AI + RPA","text":""},{"location":"rpa/ai-rpa/#specialized-ai","title":"Specialized AI","text":"<p>Specialized AI is designed to excel at specific tasks or domains, delivering highly accurate results. However, it doesn't carry the broad cognitive abilities of humans and can't easily adapt to tasks outside its training without further adjustments.</p> <p>UiPath leverages Specialized AI for enterprise automation, enhancing the automation of documents, screens, tasks, and processes. </p> <p>A good example is invoice extraction, where UiPath has created ML models specialized in identifying and extracting data from invoices from multiple countries, having been meticulously refined for this purpose over the years.</p>"},{"location":"rpa/ai-rpa/#generative-ai","title":"Generative AI","text":"<p>Generative AI is a type of artificial intelligence that can create new content, such as text, images, or music, by learning patterns from existing data. </p> <p>It has various business applications, including content creation (e.g., marketing campaign ideas, interview questions, email responses), content understanding (e.g., deciphering communication intent), and code comprehension, among others.</p> <p>UiPath leverages Large Language Models (LLM) a type of Generative AI that has been trained on huge amounts of text, like books, articles, and websites, to understand and generate human-like text. LLMs can help us with tasks like answering questions, writing essays, translating languages, and even chatting with us. A popular example of LLM is Chat GPT.</p>"},{"location":"rpa/ai-rpa/#use-cases","title":"Use cases","text":"<ul> <li>Extracting structured data from unstructured data: In this scenario, there is an email from a customer requesting a bank transfer. Even though the data in the email is unstructured, AI interprets the message, extracts the necessary information for the request and maps it to the SAP fields.</li> <li>Extracting structured data to enter them into an ERP system: In this example, artificial intelligence is leveraged to map the spreadsheet data to the form fields. As a user, you have control over the mapping, you can correct the data and modify it. Remember how training and fine-tuning are part of the process of creating an AI? Any corrections you make will be included in the training so next time, your preferences are remembered. </li> <li>Filling out a complex form with data from different documents: </li> </ul>"},{"location":"rpa/ai-rpa/#clipboardai","title":"ClipboardAI","text":"<p>Clipboard AI is a tool designed to help you with your data entry tasks, acting as a clever copy-paste buddy.</p>"},{"location":"rpa/rpa-basics/","title":"RPA Basics","text":""},{"location":"rpa/rpa-basics/#unattended-robot","title":"Unattended Robot","text":"<ul> <li>This type of robot operates with minimal human intervention, executing fully automated processes that involve high-volume transactions. It's typically used in task-intensive back-end environments.</li> <li>Unattended robots are triggered:<ul> <li>Via an automation management tool (UiPath Orchestrator).</li> <li>Based on a set schedule.</li> <li>By any activity or event.</li> <li>By another robot.</li> </ul> </li> </ul>"},{"location":"rpa/rpa-basics/#attended-robot","title":"Attended Robot","text":"<p>This type of robot operates on your computer, becoming your trusted ally, enhancing both quality and productivity. It takes charge of repetitive tasks, enabling you to accelerate processes promptly.</p> <p>Attended robots can be triggered:     - By the user.     - By certain events on the user's machine.</p>"},{"location":"rpa/rpa-basics/#automation-models","title":"Automation Models","text":"<p>Remember</p> <p>In an Attended in Tandem model, automation works alongside users, allowing them to work on higher-value tasks while the bot handles repetitive tasks in the background. In a Human in the Loop model, automation encompasses most of the process, while human input is required for specific decision-making or validation steps.</p> <p>In a Fully Unattended model, the entire process is automated without any human intervention. Robots are scheduled to execute tasks, and the optimal number of robots is allocated to maximize efficiency.</p>"},{"location":"rpa/rpa-basics/#fully-unattended","title":"Fully unattended","text":"<p>Fully Unattended robots are the perfect solution. They handle data processing without human intervention, ensuring efficiency and accuracy. </p>"},{"location":"rpa/rpa-basics/#partially-unattended","title":"Partially unattended","text":"<p>Tired of tedious and repetitive business processes? This automation model offers a solution! Start with human involvement and then hand off the rest to a Partially Unattended robot. This frees you to focus on high-value activities while ensuring the process is completed successfully with thorough documentation and notifications. </p>"},{"location":"rpa/rpa-basics/#human-in-the-loop","title":"Human in the loop","text":"<p>Certain process types requiring intermittent human intervention and complex decision-making can be challenging to automate with traditional tools. However, with this automation model, business rules are embedded into the process, allowing the robot to perform tasks and prompt for input only when necessary, simplifying your role.</p>"},{"location":"rpa/rpa-basics/#attended","title":"Attended","text":"<p>In this automation model, the attended robot can launch on-demand, run processes from the robot tray, and mimic your actions precisely. The only drawback is that sometimes your machine is occupied during the robot's task. However, you can use the free time for offline tasks.</p>"},{"location":"rpa/rpa-basics/#attended-in-tandem","title":"Attended in Tandem","text":"<p>In certain situations, it may not be practical or efficient for a robot to take control of your machine. Instead, consider the Attended in Tandem automation model. This approach allows the robot to work alongside you, providing valuable information while you focus on your primary tasks </p> <p>Note</p> <p><code>Attended in Tandem model</code> supports users in real-time while they perform their tasks. In the given scenario, the customer service agent requires support in accessing and presenting data during a customer call. With an attended robot operating in tandem, the robot seamlessly works in the background, gathering and presenting the necessary information to the agent while the agent focuses on addressing the customer needs.</p>"},{"location":"rpa/rpa-basics/#hybrid","title":"Hybrid","text":"<p>Hybrid automation merges attended and unattended robots within a unified platform. Attended robots assist customer-facing tasks, while unattended robots handle heavy back-end processing. It automates end-to-end business processes, providing flexibility and scalability. </p>"},{"location":"rpa/rpa-basics/#studio","title":"Studio","text":""},{"location":"rpa/rpa-basics/#workflow","title":"Workflow","text":"<p>The workflow is divided in 2 parts:     - Configuring a template.     - Publishing an automation.</p>"},{"location":"rpa/rpa-basics/#package","title":"Package","text":"<p>An automation project published as a NuGet package from the Studio family to Orchestrator or locally. </p>"},{"location":"rpa/rpa-basics/#process","title":"Process","text":"<p>A package in Orchestrator linked to an Orchestrator folder where human users or robots have access. If stored locally, packages can be used as processes when they are run by attended robots. </p>"},{"location":"rpa/rpa-basics/#job","title":"Job","text":"<p>A single execution of an automation process by an attended or unattended robot. If the robots are connected to Orchestrator, both attended and unattended jobs appear there. </p>"},{"location":"rpa/uipath-capabilities/","title":"uipath capabilities","text":"<p>Steps - Discover   - Process mining   -  - Automate   - Orchestration   - Use NLP and AI   - Intelligent document processing - Operate   - Analytics   - Testing   - Deploy on cloud-native</p>"},{"location":"rpa/uipath-capabilities/#core-components","title":"Core components","text":"<p>Info</p> <p>Studio, Orchestrator and Robots are what we call the core RPA components of the UiPath Platform.</p> <p> </p> <p>Unattended bot is shown below</p> <p> </p>"},{"location":"rpa/uipath-capabilities/#studio","title":"Studio","text":"<p>UiPath Studio is an integrated development environment for Automation Developers to design, develop, and debug automation projects. </p> <p>Studio can be connected to Orchestrator, making it easy to publish automation projects as NuGet packages, via the dedicated feeds. From there, they're distributed to robots to be executed.</p> <p>When we say Studio, we can think of two profiles, namely Studio and StudioX. </p> <p>These are meant to match different coding skills that developers may have:</p> <ul> <li><code>StudioX</code> is meant for business users looking to automate tasks for themselves and their immediate teams.</li> <li><code>Studio</code> is meant for Automation developers looking to build complex unattended or attended process automations.</li> </ul>"},{"location":"rpa/uipath-capabilities/#orchestrator","title":"Orchestrator","text":"<p>Tldr</p> <p>Its a web application</p> <p>Orchestrator, the heart of automation management, is a web application that allows managing, controlling, and monitoring the robots and the automations. </p> <p>With Orchestrator we can deploy, trigger, measure, provision, track, and ensure the security of every robot in the organization.</p> <p>Orchestrator also functions as a repository for libraries, reusable components, assets, and processes used by robots or by developers.</p> <p>The main capabilities of Orchestrator are:</p> <ul> <li> <p><code>Provisioning</code>: creates and maintains the connection with robots.</p> </li> <li> <p><code>Control and license distribution</code>: enables the creation, assignment and maintenance of licenses, roles, permissions, groups, and folder hierarchies.</p> </li> <li> <p><code>Automation storage and distribution</code>: allows the controlled storage and distribution of automation projects, assets, and credentials, as well as large files used in automations.</p> </li> <li> <p><code>Running automation jobs in unattended mode</code>: enables the creation and distribution of automation jobs in various ways, including through queues and triggers.</p> </li> <li> <p><code>Monitoring</code>: allows monitoring of jobs and robots and stores logs for auditing and analytics.</p> </li> </ul> <p> </p>"},{"location":"rpa/uipath-capabilities/#assistantrobots","title":"Assistant/Robots","text":"<p>Bots send heartbeat to the orchestrator</p> <p> </p> <p>Kinds of robots:</p> <ol> <li>Attended: Human users can trigger robots, through the UiPath Assistant tool to execute processes on their machines. We call these robots attended robots.</li> </ol> <p>They're digital helpers for human users. They work on the same machines as us humans, during the same hours. They're triggered directly by humans (usually through UiPath Assistant) or by an event related to what the human user does. For example, opening an application or receiving an email.\u202f</p> <p>UiPath Assistant\u202fis the component that provides a friendly interface to interact with attended robots. It is the tool that we use to easily access, manage, and run automations.\u202f</p> <ol> <li>Un-attended: The robots deployed on separate machines, working without direct human intervention, are called unattended robots. </li> </ol> <p>These are meant to work non-stop, with as little input from human users as possible. They're deployed on separate machines and their jobs are triggered exclusively from Orchestrator.</p> <p>Their interactions with human users are typically handled with as little disruption as possible, by creating and sending requests for human input or validation as tasks.</p> <p>While these await to be processed, unattended robots can continue their work by picking up other jobs. When human input is finally provided, unattended robots can resume their work on the process.</p>"},{"location":"rpa/uipath-capabilities/#ai-center","title":"AI Center","text":"<p><code>UiPath AI Center\u2122</code> is a service that allows you to deploy, manage, and continuously improve Machine Learning models and consume them within RPA workflows in Studio. This chapter treats the subject of model deployment and management which is done through the AI Center web application available through your Automation Cloud\u2122 Portal.</p>"},{"location":"rpa/uipath-capabilities/#re-framework","title":"RE Framework","text":"<p>The <code>Robotic Enterprise Framework (ReFramework)</code> is a UiPath Studio template built using <code>state machines</code>. It acts as the starting point for production-ready RPA projects, especially the ones that require scalable processing. It is created to fit all of the best practices regarding logging, exception handling, application initialization, being ready to tackle any business scenario. </p> <p>The Dispatcher and Performer (Producer and Consumer) Model</p> <p>The Dispatcher and Performer (Producer and Consumer) model is a pre-designed solution to separate the two main stages of a process by placing a queue in between. </p> <p>This way, the production of transaction items is entirely independent from their consumption. This asynchronism breaks the dependency between the producer and the consumer. </p> <p>So, the two main stages of a process involving queues are separated as follows: </p> <ul> <li> <p><code>Producer</code>: The stage in which data is read, organized and fed into a queue in Orchestrator is called Dispatcher or Producer. </p> </li> <li> <p><code>Consumer</code>: The stage in which the queue data is processed is called Performer or Consumer. </p> </li> </ul> <p>The performer process pulls transaction items from an Orchestrator queue and processes them as needed by the company. Queue items are processed one at a time.</p> <p>It uses error handling and retry mechanisms for each processed item.</p> <p>A major advantage of the performer is its scalability (multiple performers can be used with a single queue).</p> <p>Dispatcher and Performer model</p> <p>Let's take an example: we have a list of people and their email addresses in a spreadsheet. An email needs to be sent to each of them with a personalized message based on a template. </p> <p>We can use the Dispatcher and Performer model in the following manner:</p> <p>The Dispatcher reads rows from the input spreadsheet and adds the data (i.e., name and email) to a queue; each queue item will have both name and email from one spreadsheet row.</p> <p>The Performer retrieves an item from the same queue and does the necessary actions using that data, like replacing template values and sending an email.</p>"},{"location":"rpa/uipath-capabilities/#config-file","title":"Config File","text":"<p>To make it easier to maintain a project and quickly change configuration values, it is a good practice to keep them separated from the workflows themselves. </p> <p>As we've seen in the previous video, in the <code>InitAllSettings.xaml</code> workflow the ReFramework offers a configuration file. It is named Config.xlsx and can be used to define project configuration values. These values are then read into the Config dictionary. </p> <p>Warning</p> <p>As a final note about <code>Config.xlsx</code>, since the configuration file is not encrypted, it should not be used to directly store credentials. Instead, it is safer to use <code>Orchestrator assets</code> or <code>Windows Credential Manager</code> to save sensitive data.</p>"},{"location":"rpa/uipath-capabilities/#end-process-state","title":"End Process State","text":"<p>The <code>Process Transaction state</code> includes the key transaction processing steps. There are three possible outcomes:</p> <ul> <li> <p>The Success transition is triggered when no exception has been encountered during processing and the automation attempts to get the next transaction item.</p> </li> <li> <p>The Business Rule Exception transition is triggered when a predefined business rule is broken and the data needs to be checked by a human. The Transaction item is marked as such and the automation attempts to get the next transaction item.</p> </li> <li> <p>The System Error transition is triggered when a system exception is encountered during processing. The automation sets the correct status for the item, launches the retry mechanism, closes all applications, and loops back to the Initialization state.</p> </li> </ul>"},{"location":"sys_design/concepts/consistentHashing/","title":"Consistent Hashing","text":"<p>Info</p> <p><code>Consistent Hashing</code> maps data to physical nodes and ensures that only a small set of keys move when servers are added or removed. It stores the data managed by a distributed system in a ring. Each node in the ring is assigned a range of data. </p> <p> Server Token Range Start Range End Server 1 1 1 25 Server 2 26 26 50 Server 3 51 51 75 Server 4 76 76 100 <p></p> <p> </p> <p>The Consistent Hashing scheme described above works great when a node is added or removed from the ring, as in these cases, since only the next node is affected. For example, when a node is removed, the next node becomes responsible for all of the keys stored on the outgoing node. However, this scheme can result in non-uniform data and load distribution. This problem can be solved with the help of Virtual nodes.</p>"},{"location":"sys_design/concepts/dataPartitioning/","title":"Data partitioning","text":"<p><code>Data partitioning</code>: It is the process of distributing data across a set of servers. It improves the scalability and performance of the system.</p> <p><code>Data replication</code>: It is the process of making multiple copies of data and storing them on different servers. It improves the availability and durability of the data across the system</p> <p>Tip</p> <p>A carefully designed scheme for partitioning and replicating the data enhances the performance, availability, and reliability of the system and also defines how efficiently the system will be scaled and managed.</p> <p>A naive approach will use a suitable hash function to map the data key to a number. Then, find the server by applying modulo on this number and the total number of servers.</p> <p>Warning</p> <p>The scheme described in the above diagram solves the problem of finding a server for storing/retrieving the data. But when we add or remove a server, all our existing mappings will be broken. This is because the total number of servers will be changed, which was used to find the actual server storing the data. So to get things working again, we have to remap all the keys and move our data based on the new server count \ud83d\ude15</p>"},{"location":"sys_design/concepts/graphql/","title":"GraphQL","text":"<p>GraphQL is a popular <code>data query language</code> developed by Facebook. </p> <p>Issues with REST</p> <p>It was designed as an alternative to REST (Representational State Transfer) because of REST\u2019s perceived weaknesses such as:</p> <pre><code>1. Multiple round-trips \n2. Over-fetching\n3. Problems with versioning\n</code></pre> <p>GraphQL attempts to solve these problems by providing a hierarchical, declarative way of performing queries from a single end point.</p>"},{"location":"sys_design/concepts/graphql/#why-graphql-is-powerful","title":"why GraphQL is powerful?","text":"<p>GraphQL gives power to the client. Instead of specifying the structure of the response on the server, it\u2019s defined on the client. The client can specify what properties and relationships to return. </p> <p>GraphQL aggregates data from multiple sources and returns it to the client in a single round trip, which makes it an efficient system for retrieving data.</p> <p> </p>"},{"location":"sys_design/concepts/jwt/","title":"Json Web Tokens","text":""},{"location":"sys_design/concepts/jwt/#structure","title":"Structure","text":"<p>JWTs consist of three parts separated by dots (.), which are:</p> <ul> <li>Header</li> <li>Payload</li> <li>Signature</li> </ul> <p>Therefore, a JWT typically looks like the following.</p> <p>JWT structure</p> <p><code>encoded_Header.encoded_Payload.encoded_Signature</code></p> <p>Structure is shown in the below example from jwt.io</p> <p> </p>"},{"location":"sys_design/concepts/jwt/#header","title":"Header","text":"<p>The header typically consists of two parts: the type of the token, which is JWT, and the hashing algorithm such as <code>HMAC SHA256</code> or <code>RSA</code>.</p> JWT Header example<pre><code>{\n  'alg': 'HS256',\n  'typ': 'JWT'\n}\n</code></pre>"},{"location":"sys_design/concepts/jwt/#payload","title":"Payload","text":"<p>The second part of the token is the payload, which contains the claims. Claims are statements about an entity (typically, the user) and additional metadata. </p> JWT Payload example (claims + metadata)<pre><code>{\n  'sub': '1234567890',\n  'name': 'Amarjit Singh',\n  'admin': true\n}\n</code></pre> <p>There are three types of claims: </p> <ul> <li>Reserved claim</li> <li>Public claim</li> <li>Private claim</li> </ul>"},{"location":"sys_design/concepts/jwt/#reserved-claims","title":"Reserved claims","text":"<p>These are a set of predefined claims, which are not mandatory but recommended, thought to provide a set of useful, interoperable claims. Some of them are: <code>iss (issuer)</code>, <code>exp (expiration time)</code>, <code>sub (subject)</code>, <code>aud (audience)</code></p> <p>Notice that the claim names are only three characters long as JWT is meant to be compact.</p>"},{"location":"sys_design/concepts/jwt/#public-claims","title":"Public claims","text":"<p>These can be defined at will by those using JWTs. But to avoid collisions they should be defined in the <code>IANA JSON Web Token Registry</code> or be defined as a URI that contains a collision resistant namespace.</p>"},{"location":"sys_design/concepts/jwt/#private-claims","title":"Private claims","text":"<p>These are the custom claims created to share information between parties that agree on using them.</p>"},{"location":"sys_design/concepts/jwt/#signature","title":"Signature","text":"<p>To create the signature part you have to take the encoded header (base64), the encoded payload (base64), a secret, the algorithm specified in the header, and sign that.</p> <p>For example if you want to use the <code>HMAC SHA256 algorithm</code>, the signature will be created in the following way.</p> JWT Signature example<pre><code>HMACSHA256(\n  base64UrlEncode(header) + '.' +\n  base64UrlEncode(payload),\n  secret)\n</code></pre> <p>The signature is used to verify that the sender of the JWT is who it says it is and to ensure that the message was\u2019t changed in the way.</p>"},{"location":"sys_design/concepts/jwt/#jwt-vs-saml","title":"JWT vs SAML","text":"<p><code>Size</code>: As JSON is less verbose than XML, when it is encoded its size is also smaller; making JWT more compact than SAML. This makes JWT a good choice to be passed in HTML and HTTP environments.</p> <p><code>Security</code>: While JWT and SAML tokens can also use a <code>public/private key pair</code> in the form of a <code>X.509 certificate</code> to sign them. However, signing XML with XML Digital Signature without introducing obscure security holes is very difficult compared to the simplicity of signing JSON.</p> <p><code>Parsing</code>: JSON parsers are common in most programming languages, because they map directly to objects, conversely XML doesn\u2019t have a natural document-to-object mapping. This makes it easier to work with JWT than SAML assertions.</p>"},{"location":"sys_design/concepts/jwt/#bearer-schemaformat","title":"Bearer Schema/Format","text":"<p>The <code>Authorization: &lt;type&gt; &lt;credentials&gt;</code> pattern was introduced by the W3C in HTTP 1.0, and has been reused in many places since. Many web servers support multiple methods of authorization. In those cases sending just the token isn't sufficient.</p> <p>Sites that use the <code>Authorization : Bearer xxxxxx</code> format are most likely implementing <code>OAuth 2.0 bearer tokens</code>.</p> <p>Sending GET Request with Bearer Token Authorization Header</p> <p>To send a <code>GET request</code> with a <code>Bearer Token authorization header</code>, you need to make an HTTP GET request and provide your Bearer Token with the <code>Authorization: Bearer {token}</code> HTTP header. Bearer Authentication (also called token authentication) is an HTTP authentication scheme created as part of OAuth 2.0 but is now used on its own. For security reasons, bearer tokens are only sent over HTTPS (SSL). </p> <pre><code>GET /echo/get/json HTTP/1.1\nHost: amarjitdhillon.com\nAccept: application/json\nAuthorization: Bearer {token}\n</code></pre>"},{"location":"sys_design/concepts/long_polling/","title":"Long Polling","text":"<p> Long Polling example in AWS <p></p>"},{"location":"sys_design/concepts/microservices/","title":"Microservices","text":"<p>What are microservices?</p> <p> Generic Microservice application example <p></p> <p>Checkout below talk</p> <p></p> <ul> <li>Microservices includes serverless applications, where your application\u2019s software is constructed of multiple and loosely coupled services</li> <li>Microservices architectures reduce the complexity of the individual component but introduce a different type of complexity at scale related to the number of these independent components.</li> </ul>"},{"location":"sys_design/concepts/microservices/#key-considerations","title":"Key considerations","text":"<p>Here are the key factors that you should consider while building the serverless applications:</p> <ul> <li>Service integrations: Check for compatability</li> <li>Timeouts: Timeout of individual services</li> <li>Retry behaviors</li> <li>Throughput</li> <li>Payload size: Payload size of SQS can be 256KB and Gateway can be 2GB</li> </ul>"},{"location":"sys_design/concepts/protobuf/","title":"Protobuf","text":"<p><code>Protobuf (Protocol Buffer)</code> is a data serializing protocol like a JSON or XML. But unlike them, the protobuf is not for humans, serialized data is compiled bytes and hard for the human reading.</p> <p>Why is it not so popular yet?</p> <p>Because protobuf is not so as simple as Google says. Protobuf must preprocess from <code>.proto</code> files to the sources of your programming language. Unfortunately, for some platforms, the <code>protoc generator</code> produces a very impractical code and it\u2019s too hard to debug it. Also protobuf is hard to develop when services are so many and we have more than one team in development, because it\u2019s not a human reading standard.</p>"},{"location":"sys_design/concepts/webhook/","title":"Webhook","text":"<p>Real world analogy</p> <p>There are two ways your apps can communicate with each other to share information: <code>polling</code> and <code>webhooks</code>. </p> <p>Polling is like knocking on your friend's door and asking if they have any sugar (aka information), but you have to go and ask for it every time you want it.</p> <p><code>Webhooks</code> are like someone tossing a bag of sugar at your house whenever they buy some. You don't have to ask\u2014they just automatically punt it over every time it's available.</p> <p> Long Polling Example <p> </p> <ul> <li> <p>Webhooks are a way for one application to provide other applications with <code>real-time information</code>. They allow one application to send a notification to another application when a certain event occurs rather than constantly polling for new data. This can help save on server resources and costs.</p> </li> <li> <p>Instead of one application making a request to another to receive a response, a webhook is a service that allows one program to send data to another as soon as a particular <code>event</code> takes place.</p> </li> <li> <p>Webhooks are sometimes referred to as <code>push APIs</code> or <code>reverse APIs</code> because instead of pulling data from one system to another, they push the data to update it in real-time. In either case, the webhook's meaning is the same; it enables you to share data. Webhooks work by making an HTTP request from one application to another.</p> </li> </ul> <p>Slack Example</p> <p>Say for instance you want to receive Slack notifications when tweets that mention a certain account and contain a specific hashtag are published. Instead of Slack continuously asking Twitter for new posts meeting these criteria, it makes much more sense for Twitter to send a notification to Slack only when this event takes place. This is the purpose of a webhook\u2013\u2013instead of having to repeatedly request the data, the receiving application can sit back and get what it needs without having to send repeated requests to another system. </p>"},{"location":"sys_design/concepts/webhook/#webhook-vs-apis","title":"Webhook VS API's","text":"<ul> <li>Webhooks are different from APIs, which allow for communication between different applications but work in a different way. An API is a set of protocols and routines for building and interacting with software applications, whereas a webhook is a way for one application to notify another application when a specific event occurs.</li> </ul> <p>In other words, an API allows you to retrieve data, while a webhook allows data to be pushed to you. This means that instead of having to poll for new data, you can receive data in real time through webhooks.</p>"},{"location":"sys_design/concepts/webhook/#setting-up-webhooks","title":"Setting up Webhooks","text":"<ul> <li> <p>Determine the events to be notified: Identify the events that you want to receive notifications for. For example, you may want to be notified whenever a new customer is added to a CRM system or a new order is placed in an e-commerce store.</p> </li> <li> <p>Create the webhook endpoint: Set up a webhook endpoint in your application to receive the notifications. This endpoint should be a public URL that can receive HTTP POST requests. You may also need to add security measures, such as authentication and encryption, to protect against unauthorized access.</p> </li> <li> <p>Register the webhook: Register the webhook with the application or service that will be sending the notifications. This may involve configuring settings in the application\u2019s user interface or making API calls to register the webhook endpoint.</p> </li> <li> <p>Handle the webhook notifications: When a webhook notification is received at the endpoint, parse the payload and handle the data or event accordingly. This may involve triggering a custom action, storing data in a database, or sending notifications to users</p> </li> <li> <p>Handle errors and retries: Webhooks can fail for various reasons, such as network errors or server downtime. Make sure to handle these errors gracefully and implement retry logic to ensure that missed notifications are resent.</p> </li> <li> <p>Monitor and maintain: Monitor the webhook endpoint and the application\u2019s logs to ensure that notifications are being received and processed correctly. Make updates and improvements as needed to maintain the webhook architecture.</p> </li> </ul>"},{"location":"sys_design/concepts/webhook/#why-webhook-is-limited","title":"Why webhook is limited?","text":"<p>Also, unlike APIs, webhooks do not allow the sending system to add, update and delete data on the receiving end, which is why webhooks alone are too limited to offer full integration between two applications.</p>"},{"location":"sys_design/concepts/webhook/#limitations-of-webhooks","title":"Limitations of Webhooks","text":"<p>In the case of webhooks vs. APIs, you might wonder why anyone would still use an API since they're less efficient and productive. A few limitations of webhooks include the following:</p>"},{"location":"sys_design/concepts/webhook/#not-always-supported","title":"Not always supported","text":"<p>Unfortunately, not all applications support webhooks. However, several types of third-party app providers can help you send webhooks by connecting apps that don't have integrations and allowing them to pass data.</p>"},{"location":"sys_design/concepts/webhook/#less-functionality-than-apis","title":"Less functionality than APIs","text":"<p>Webhooks only allow for data to be received from one application for another. Therefore, they can't be used for complicated integrations that require bi-directional communication.</p>"},{"location":"sys_design/concepts/webhook/#potential-for-lost-data","title":"Potential for lost data","text":"<ul> <li>With webhooks, you won't be alerted if an application or server is down for tails to send data. </li> <li>Since you'll only receive data when events occur, you won't receive any information if the other system is down. However, with APIs, you'll receive an error response alerting you that the system isn't functional. </li> <li>Webhooks will attempt to resend data, but they will only try so many times before stopping. Therefore, you'll need another system to know when an application is down to prevent you from losing information</li> </ul>"},{"location":"sys_design/concepts/webhook/#other-issues","title":"Other issues \u26a0\ufe0f","text":"<ul> <li> <p>Security risks: Webhooks require a publicly accessible endpoint, which can be a potential security risk if not properly secured. Malicious actors could potentially send fake or malicious payloads to your endpoint.</p> </li> <li> <p>Debugging can be challenging: Since webhooks are triggered by external events, debugging issues can be more challenging. It may not always be clear why a webhook was not received or processed correctly.</p> </li> <li> <p>Can be overwhelming: If you receive a large volume of webhooks, it can be challenging to process and handle all of them efficiently. This can cause performance issues or delays in processing.</p> </li> <li> <p>Limited control: Since webhooks are triggered by external events, you have limited control over when they are sent or how frequently they are sent. This can make it challenging to predict when events will occur and how frequently you will need to handle them.</p> </li> </ul>"},{"location":"sys_design/concepts/websockets/","title":"WebSockets","text":""},{"location":"sys_design/concepts/websockets/#sse","title":"SSE","text":"<p>According to the definition, it is a server push technology enabling a client to receive automatic updates from a server. Using SSE, the clients make a persistent long-term connection with the server. Then, the server uses this connection to send the data to the client.</p> <p>Client can't send request twice</p> <p>It is unidirectional, meaning once the client sends the request it can only receive the responses without the ability to send new requests over the same connection.</p> <p> </p> <ol> <li>The client makes a request to the server.</li> <li>The connection between client and server is established, and it remains open.</li> <li>The server sends responses or events to the client when new data is available.</li> </ol>"},{"location":"sys_design/concepts/websockets/#websockets_1","title":"WebSockets","text":"<p>WebSocket provides <code>full-duplex communication</code> channels over a single TCP connection. It is a persistent connection between a client and a server that both parties can use to start sending data at any time.</p> <p>How this connection works?</p> <p>The client establishes a WebSocket connection through a process known as the <code>WebSocket handshake</code>. If the process succeeds, then the server and client can exchange data in both directions at any time. The WebSocket protocol enables the communication between a client and a server with lower overheads, facilitating <code>real-time data transfer</code> from-and-to the server.</p> <p> </p> <ol> <li>The client initiates a WebSocket handshake process by sending a request.</li> <li>The request also contains an HTTP Upgrade header that allows the request to switch to the WebSocket protocol <code>ws://</code></li> <li>The server sends a response to the client, acknowledging the WebSocket handshake request.</li> <li>A WebSocket connection will be opened once the client receives a successful handshake response.</li> <li>Now the client and server can start sending data in both directions allowing real-time communication.</li> <li>The connection is closed once the server or the client decides to close the connection.</li> </ol> <p>Example</p> <p> Step function example <p> </p>"},{"location":"sys_design/designPatterns/bff/","title":"Backend for Frontend (BFF)","text":"<p>BFF is essentially a variant of the API Gateway pattern. It also provides an additional layer between microservices and clients.</p> <p> </p> <p>But rather than being a single point of entry, it introduces multiple gateways for each client as shown below.</p> <p> </p> <p>With BFF, you can add an API tailored to the needs of each client, removing a lot of the bloat caused by keeping it all in one place.</p> <p>Abstract</p> <p>BFF is a variant of the API Gateway pattern, but it also provides an additional layer between microservices and each client type separately. Instead of a single point of entry, it introduces multiple gateways. Because of that, you can have a tailored API that targets the needs of each client (mobile, web, desktop, voice assistant, etc.), and remove a lot of the bloat caused by keeping it all in one place.</p>"},{"location":"sys_design/designPatterns/bff/#benefits","title":"Benefits","text":""},{"location":"sys_design/designPatterns/bff/#decoupling","title":"Decoupling","text":"<p>Decoupling of Backend and Frontend\u200b for sure gives us faster time to market as frontend teams can have dedicated backend teams serving their unique needs. The release of new features of one frontend does not affect the other.</p>"},{"location":"sys_design/designPatterns/bff/#faster-development","title":"Faster development","text":"<p>We can much easier maintain and modify APIs\u200b and even provide API versioning dedicated for specific frontend, which is a big plus from a mobile app perspective as many users do not update the app immediately.</p>"},{"location":"sys_design/designPatterns/bff/#multiple-device-types-can-call-the-backend-in-parallel","title":"Multiple device types can call the backend in parallel","text":"<p>While the browser is making a request to the browser BFF, the mobile devices can do the same. It will help obtain responses from the services faster.</p>"},{"location":"sys_design/designPatterns/bff/#better-security","title":"Better security","text":"<p>Certain sensitive information can be hidden, and unnecessary data to the frontend can be omitted when sending back a response to the frontend. The abstraction will make it harder for attackers to target the application.</p>"},{"location":"sys_design/designPatterns/cdn/","title":"CDN","text":"<p>A <code>content delivery network (CDN)</code> is a distributed network of servers that can efficiently deliver web content to users. CDNs store cached content on <code>edge servers</code> that are close to the end users to minimize latency.</p> <p>Caching and CDN</p> <p>Caching is the process of storing data locally so that future requests for that data can be accessed more quickly. In the most common type of caching, <code>web browser caching</code>, a web browser stores copies of <code>static data</code> locally on a local hard drive. By using caching, the web browser can avoid making <code>multiple round-trips</code> to the server and instead access the same data locally, thus saving time and resources. Caching is well-suited for locally managing small, static data such as static images, CSS files, and JavaScript files.</p> <p>Similarly, caching is used by a content delivery network on edge servers close to the user to avoid requests traveling back to the origin and reducing end-user latency. Unlike a web browser cache, which is used only for a single user, the CDN has a shared cache. In a CDN shared cache, a file that is requested by one user can be accessed later by other users, which greatly decreases the number of requests to the origin server.</p> <ul> <li>CDNs are typically used to deliver static content such as images, style sheets, documents, client-side scripts, and HTML pages.</li> <li>The major advantages of using a CDN are lower latency of content to users, regardless of their geographical location in relation to the datacenter where the application is hosted.</li> <li>CDNs can also help to reduce load on a web application, because the application does not have to service requests for the content that is hosted in the CDN.</li> </ul> <p>Tip</p> <p>Static content is easy to serve becasue it does not need to be modified for each request.</p>"},{"location":"sys_design/designPatterns/cdn/#caching-at-various-levels","title":"Caching at various levels","text":"<p>Caching can occur at multiple levels between the origin server and the end user:</p> <ul> <li><code>Web server (Origin)</code>: Uses a shared cache (for multiple users).</li> <li><code>Content delivery network</code>: Uses a shared cache (for multiple users).</li> <li><code>Internet service provider (ISP)</code>: Uses a shared cache (for multiple users).</li> <li><code>Web browser (Client)</code>: Uses a private cache (for one user).</li> </ul>"},{"location":"sys_design/designPatterns/cdn/#cdn-internal-architecture","title":"CDN Internal Architecture","text":"<ol> <li>A user types in <code>www.amarjitdhillon.com</code> in the browser. The browser looks up the domain name in the local DNS cache.</li> <li>If the domain name does not exist in the local DNS cache, the browser goes to the DNS resolver to resolve the name. The DNS resolver usually sits in the <code>Internet Service Provider (ISP)</code>.</li> <li>The DNS resolver recursively resolves the domain name. Finally, it asks the authoritative name server to resolve the domain name. </li> <li>If we don\u2019t use CDN, the authoritative name server returns the IP address for <code>www.amarjitdhillon.com</code>. But with CDN, the authoritative name server has an alias pointing to <code>www.amarjitdhillon.cdn.com</code> (the domain name of the CDN server). </li> <li>The DNS resolver asks the authoritative name server to resolve <code>www.amarjitdhillon.cdn.com</code>. </li> <li>The authoritative name server returns the domain name for the load balancer of CDN <code>www.amarjitdhillon.lb.com</code>. </li> <li>The DNS resolver asks the CDN load balancer to resolve <code>www.amarjitdhillon.lb.com</code>. The load balancer chooses an optimal CDN edge server based on the user\u2019s IP address, user\u2019s ISP, the content requested, and the server load. </li> <li>The CDN load balancer returns the CDN edge server\u2019s IP address for <code>www.amarjitdhillon.lb.com</code>.</li> <li>Now we finally get the actual IP address to visit. The DNS resolver returns the IP address to the browser. </li> <li>The browser visits the CDN edge server to load the content. There are two types of contents cached on the CDN servers: static contents and dynamic contents. The former contains static pages, pictures, and videos; the latter one includes results of edge computing. </li> <li>If the <code>edge CDN server</code> cache doesn't contain the content, it goes upward to the <code>regional CDN server</code>. If the content is still not found, it will go upward to the <code>central CDN server</code></li> </ol>"},{"location":"sys_design/designPatterns/cdn/#challenges-of-using-cdn","title":"Challenges of using CDN","text":"<p>There are several challenges to take into account when planning to use a CDN.</p> <ul> <li> <p><code>Deployment</code>: Decide the origin from which the CDN fetches the content, and whether you need to deploy the content in more than one storage system. Take into account the process for deploying static content and resources. For example, you may need to implement a separate step to load content into Azure blob storage.</p> </li> <li> <p><code>Versioning and cache-control</code>: Consider how you will update static content and deploy new versions. Understand how the CDN performs caching and time-to-live (TTL). </p> </li> <li> <p><code>Testing</code>. It can be difficult to perform local testing of your CDN settings when developing and testing an application locally or in a staging environment.</p> </li> <li> <p><code>Search engine optimization (SEO)</code>: Content such as images and documents are served from a different domain when you use the CDN. This can have an effect on SEO for this content.</p> </li> <li> <p><code>Content security</code>. Not all CDNs offer any form of access control for the content. Some CDN services, including Azure CDN, support <code>token-based authentication</code> to protect CDN content. </p> </li> <li> <p><code>Client security</code>. Clients might connect from an environment that does not allow access to resources on the CDN. This could be a security-constrained environment that limits access to only a set of known sources, or one that prevents loading of resources from anything other than the page origin. A fallback implementation is required to handle these cases.</p> </li> </ul>"},{"location":"sys_design/designPatterns/choreography/","title":"Choreography","text":"<p>Tldr</p> <p>The goal of this pattern is to have each component of the system participate in the decision-making process about the workflow of a business transaction, instead of relying on a <code>central point of control (Orchestration)</code>.</p>"},{"location":"sys_design/designPatterns/choreography/#problem","title":"Problem","text":"<p>In Microservices, a common pattern for communication is to use a <code>centralized service</code> that acts as the orchestrator. It acknowledges all incoming requests and delegates operations to the respective services. In doing so, it also manages the workflow of the entire business transaction. Each service just completes an operation and is not aware of the overall workflow.</p>"},{"location":"sys_design/designPatterns/choreography/#solution","title":"Solution","text":"<p>A client request publishes messages to a <code>message queue</code>. As messages arrive, they are pushed to subscribers, or services, interested in that message. Each subscribed service does their operation as indicated by the message and responds to the message queue with success or failure of the operation. </p> <p>In case of success, the service can push a message back to the same queue or a different message queue so that another service can continue the workflow if needed. If an operation fails, the message bus can retry that operation.</p>"},{"location":"sys_design/designPatterns/cqrs/","title":"CQRS","text":"<p>This pattern is used to solve an issue of how to implement a query that retrieves data from multiple services in a microservice architecture?</p>"},{"location":"sys_design/designPatterns/cqrs/#solution","title":"Solution","text":"<p>The command query responsibility segregation (CQRS) pattern separates the data mutation, or the command part of a system, from the query part. You can use the CQRS pattern to separate updates and queries if they have different requirements for throughput, latency, or consistency.</p> <p>The CQRS pattern splits the application into two parts\u2014the command side and the query side\u2014as shown in the following diagram. The command side handles <code>create</code>, <code>update</code>, and <code>delete</code> requests. The query side runs the query part by using the <code>read replicas</code>.</p> <p> CQRS Example <p> </p> <p>The diagram shows the following process:</p> <ol> <li>The business interacts with the application by sending commands through an API. Commands are actions such as creating, updating or deleting data.</li> <li>The application processes the incoming command on the command side. This involves validating, authorizing, and running the operation.</li> <li>The application persists the command\u2019s data in the write (command) database.</li> <li>After the command is stored in the write database, events are triggered to update the data in the read (query) database.</li> <li>The read (query) database processes and persists the data. Read databases are designed to be optimized for specific query requirements.  </li> <li>The business interacts with read APIs to send queries to the query side of the application.</li> <li>The application processes the incoming query on the query side and retrieves the data from the read database.</li> </ol> <p>Example of CQRS in AWS</p> <p>In the following illustration, a NoSQL data store, such as <code>DynamoDB</code>, is used to optimize the write throughput and provide flexible query capabilities. This achieves high write scalability on workloads that have well-defined access patterns when you add data. A relational database, such as Amazon Aurora, provides complex query functionality. A DynamoDB stream sends data to a <code>Lambda function</code> that updates the Aurora table.</p> <p> CQRS Example </p>"},{"location":"sys_design/designPatterns/durableFunctions/","title":"Durable Functions","text":"<p><code>Durable Functions</code> is an extension of <code>Azure Functions</code> that lets you write stateful functions in a serverless compute environment. The extension lets you define stateful workflows by writing orchestrator functions and stateful entities by writing entity functions using the <code>Azure Functions</code> programming model.</p>"},{"location":"sys_design/designPatterns/durableFunctions/#patterns","title":"Patterns","text":""},{"location":"sys_design/designPatterns/durableFunctions/#function-chaining","title":"Function chaining","text":"<ul> <li>In the function chaining pattern, a sequence of functions executes in a specific order. </li> <li>In this pattern, the output of one function is applied to the input of another function.</li> <li>The use of queues between each function ensures that the system stays durable and scalable, even though there is a flow of control from one function to the next.</li> </ul> <p>In this example, the values F1, F2, F3, and F4 are the names of other functions in the same function app. You can implement control flow by using normal imperative coding constructs. Code executes from the top down.</p>"},{"location":"sys_design/designPatterns/durableFunctions/#fan-outfan-in","title":"Fan-out/fan-in","text":"<p>In the fan out/fan in pattern, you execute multiple functions in parallel and then wait for all functions to finish. Often, some <code>aggregation work</code> is done on the results that are returned from the functions.</p> <p>How to do fan in?</p> <p>Fanning back in is much more challenging. To fan in, in a normal function, you write code to track when the queue-triggered functions end, and then store function outputs</p>"},{"location":"sys_design/designPatterns/durableFunctions/#async-http-apis","title":"Async HTTP APIs","text":"<p>The async HTTP API pattern addresses the problem of coordinating the state of long-running operations with external clients. A common way to implement this pattern is by having an HTTP endpoint trigger the long-running action. Then, redirect the client to a status endpoint that the client polls to learn when the operation is finished.</p>"},{"location":"sys_design/designPatterns/durableFunctions/#http-reference","title":"HTTP reference","text":"<p>reference docs</p>"},{"location":"sys_design/designPatterns/materializedView/","title":"Materialized View","text":"<p>Tldr</p> <p>Generate <code>prepopulated views</code> over the data in one or more data stores when the data is formatted in a way that does not favor the required query operations. This pattern can help to support efficient querying and data extraction, and improve application performance.</p>"},{"location":"sys_design/designPatterns/materializedView/#problem-context","title":"Problem Context","text":"<p>When storing data, the priority for developers and data administrators is often focused on how the data is stored, as opposed to how it is read. The chosen storage format is usually closely related to the format of the data, requirements for managing data size and data integrity, and the kind of store in use. </p> <p><code>For example</code>, when using NoSQL Document store such as MongoDB, the data is often represented as a series of aggregates, each of which contains all of the information for that entity.</p> <p>However, this may have a negative effect on queries. When a query requires only a subset of the data from some entities, such as a summary of orders for several customers without all of the order details, it must extract all of the data for the relevant entities in order to obtain the required in for</p>"},{"location":"sys_design/designPatterns/materializedView/#solution","title":"Solution","text":"<p>To support efficient querying, a common solution is to generate, in advance, a view that materializes the data in a format most suited to the required results set. The Materialized View pattern describes generating prepopulated views of data in environments where the source data is not in a format that is suitable for querying, where generating a suitable query is difficult, or where query performance is poor due to the nature of the data or the data store.</p> <p>These <code>materialized views</code>, which contain only data required by a query, allow applications to quickly obtain the information they need. In additionto joining tables or combining data entities, materialized views may include the current values of calculated columns or data items, the results of combining values or executing transformations on the data items, and values specified as part of the query. A materialized view may even be optimized for just a single query.</p> <p>A key point is that a materialized view and the data it contains is completely disposable because it can be entirely rebuilt from the source data stores. A materialized view is never updated directly by an application, and so it is effectively a specialized cache.</p> <p>When the source data for the view changes, the view must be updated to include the new information. This may occur automatically on an appropriate schedule, or when the system detects a change to the original data. In other cases it may be necessary to regenerate the view manually.</p>"},{"location":"sys_design/designPatterns/priority_queue/","title":"Priority Queue Pattern","text":"<p>The goal is to prioritize requests sent to services so that requests with a <code>higher priority</code> are received and processed more quickly than those with a <code>lower priority</code>. This pattern is useful in applications that offer different service level guarantees to individual clients.</p>"},{"location":"sys_design/designPatterns/saga/","title":"Saga","text":"<p>Tldr</p> <p>The Saga design pattern is a way to manage data consistency across microservices in distributed transaction scenarios. A saga is a sequence of transactions that updates each service and publishes a message or event to trigger the next transaction step. If a step fails, the saga executes compensating transactions that counteract the preceding transactions.</p>"},{"location":"sys_design/designPatterns/saga/#how-saga-functions","title":"How Saga functions?","text":"<p>The Saga pattern provides transaction management using a sequence of <code>local transactions</code>. A local transaction is the atomic work effort performed by a saga participant. Each local transaction updates the database and publishes a <code>message or event</code> to trigger the next local transaction in the saga. If a local transaction fails, the saga executes a series of compensating transactions that undo the changes that were made by the preceding local transactions.</p> <p>Example of Saga</p> <p>In this example, the 2 related steps of <code>book hotel</code>, <code>book flight</code>, and <code>book rental</code> car must all be successful to book a trip.</p> <p>The first step in the overall transaction is to book the hotel. If that task is successful, then the BookFlight task is executed. If that task is successful, then the BookRental task is executed. </p> <p>But what if one of the tasks fails? For example, if the <code>BookFlight</code> task is not successful, then the <code>CancelFlight</code> task executes, followed by the <code>CancelHotel</code> task to reverse the transaction. Likewise, any failures with the <code>BookRental</code> task would lead to the <code>CancelRental</code> task, which then initiates the <code>CancelFlight</code> task, which then executes the <code>CancelHotel</code> task.</p> <p> Flight booking Saga Example </p>"},{"location":"sys_design/designPatterns/saga/#when-to-use-saga","title":"When to use Saga?","text":"<p>Use the Saga pattern when you need to:</p> <ul> <li>Ensure data consistency in a distributed system without tight coupling.</li> <li>Roll back or compensate if one of the operations in the sequence fails.</li> </ul> <p>The Saga pattern is less suitable for:</p> <ul> <li>Tightly coupled transactions.</li> <li>Compensating transactions that occur in earlier participants.</li> <li>Cyclic dependencies.</li> </ul>"},{"location":"sys_design/designPatterns/saga/#orchestration-based-saga","title":"Orchestration based Saga","text":"<ol> <li>The saga orchestrator sends a <code>Verify Consumer command</code> to <code>Consumer Service</code>.</li> <li><code>Consumer Service</code> replies with a Consumer Verified message.</li> <li>The saga orchestrator sends a <code>Create Ticket command</code> to <code>Kitchen Service</code>.</li> <li>Kitchen Service replies with a Ticket Created message.</li> <li>The saga orchestrator sends an Authorize Card message to <code>Accounting Service</code>.</li> <li>Accounting Service replies with a Card Authorized message.\u2260</li> <li>The saga orchestrator sends an <code>Approve Ticket command</code> to Kitchen Service.</li> <li>The saga orchestrator sends an <code>Approve Order command</code> to Order Service.</li> </ol>"},{"location":"sys_design/designPatterns/saga/#choreo-based-saga","title":"Choreo based Saga","text":"<ol> <li><code>Order Service</code> creates an Order in the <code>APPROVAL_PENDING</code> state and publishes an <code>OrderCreated event</code>.</li> <li><code>Consumer  Service</code>  consumes  the  <code>OrderCreated  event</code>,  verifies  that  the  con- sumer can place the order, and publishes a <code>ConsumerVerified event</code>.</li> <li><code>Kitchen  Service</code>  consumes the <code>OrderCreated  event</code>, validates the Order, cre- ates a Ticket in a <code>CREATE_PENDING state</code>, and publishes the <code>TicketCreated event</code>.</li> <li><code>Accounting Service</code> consumes the <code>OrderCreated event</code> and creates a Credit- CardAuthorization in a PENDING state.</li> <li><code>Accounting  Service</code>  consumes  the  <code>TicketCreated</code>  and  <code>ConsumerVerified events</code>,  charges  the  consumer\u2019s  credit  card,  and  publishes  the  <code>CreditCard-Authorized event</code>.</li> <li><code>Kitchen  Service</code>  consumes the <code>CreditCardAuthorized  event</code> and changes the state of the Ticket to AWAITING_ACCEPTANCE.</li> <li><code>Order Service</code> receives the <code>CreditCardAuthorized events</code>, changes the state of the Order to APPROVED, and publishes an <code>OrderApproved</code> event.</li> </ol>"},{"location":"sys_design/designPatterns/serverless-migration/","title":"Serverless Migration","text":"<p>At a high level, there are 3 migration patterns that you might follow to migrate your <code>legacy applications</code> to a <code>serverless model</code>.</p>"},{"location":"sys_design/designPatterns/serverless-migration/#leapfrog","title":"Leapfrog \ud83d\udc38","text":"<p>As the name suggests, you bypass interim steps and go straight from an on-premises legacy architecture to a serverless cloud architecture</p>"},{"location":"sys_design/designPatterns/serverless-migration/#organic","title":"Organic \ud83c\udf40","text":"<p>You move on-premises applications to the cloud in more of a <code>lift and shift</code> model.</p> <p>In this model, existing applications are kept intact, either running on Amazon Elastic Compute Cloud (Amazon EC2) instances or with some limited rewrites to container services like Amazon Elastic Kubernetes Service (Amazon EKS)/Amazon Elastic Container Service (Amazon ECS) or AWS Fargate.</p> <p>At some point in the adoption curve, you take a more strategic look at how serverless and microservices might address business goals like market agility, developer innovation, and total cost of ownership.</p> <p>You get buy-in for a more long-term commitment to invest in modernizing your applications and select a production workload as a pilot. With initial success and lessons learned, adoption accelerates, and more applications are migrated to microservices and serverless.</p>"},{"location":"sys_design/designPatterns/serverless-migration/#strangler","title":"Strangler \ud83d\udc54","text":"<p>With the <code>strangler pattern</code>, an organization incrementally and systematically decomposes monolithic applications by creating APIs and building <code>event-driven components</code> that gradually replace components of the legacy application.</p> <p>Distinct API endpoints can point to <code>old vs new components</code>, and safe deployment options (like canary deployments) let you point back to the legacy version with very little risk.</p> <p>Take care of decommisioning services</p> <p><code>New feature branches</code> can be \u201cserverless first,\u201d and legacy components can be decommissioned as they are replaced. This pattern represents a more systematic approach to adopting serverless, allowing you to move to critical improvements where you see benefit quickly but with less risk and upheaval than the leapfrog pattern.</p>"},{"location":"sys_design/designPatterns/serviceRegistry/","title":"Service Registry Pattern","text":""},{"location":"sys_design/designPatterns/serviceRegistry/#services-discovery-in-monolithicsoamicroservices","title":"Services discovery in Monolithic/SOA/Microservices","text":"<p>Services typically need to call one another.</p> <ul> <li>In a <code>monolithic application</code>, services invoke one another through language-level method or procedure calls.</li> <li>In a traditional <code>distributed system</code> SOA deployment, services run at fixed, well known locations (hosts and ports) and so can easily call one another using HTTP/REST or some RPC mechanism. </li> <li>However, a modern <code>microservice-based application</code> typically runs in a virtualized or containerized environments where the number of instances of a service and their locations changes dynamically.</li> </ul>"},{"location":"sys_design/designPatterns/serviceRegistry/#why-we-need-it","title":"Why we need it?","text":"<p>Each microservice has a <code>unique name/URL</code> that's used to resolve its location. Your microservice needs to be addressable wherever it's running.  In the same way that <code>DNS</code> resolves a URL to a particular computer IP, your microservice needs to have a <code>unique name</code> so that its current location is discoverable.</p> <p>Handling service failure</p> <p>If a computer fails, the registry service must be able to indicate where the service is running now.</p> <p> Caching/Storing the results</p> <p>The registry is a database containing the network locations of service instances. A service registry needs to be highly available and up-to-date. Clients could cache network locations obtained from the service registry. </p>"},{"location":"sys_design/designPatterns/serviceRegistry/#types-of-service-discovery","title":"Types of service discovery","text":""},{"location":"sys_design/designPatterns/serviceRegistry/#client-side","title":"Client-side","text":"<p>When using <code>client\u2011side discovery</code>, the client is responsible for determining the network locations of available service instances and load balancing requests across them. The client queries a <code>service registry</code>, which is a database of available service instances. The client then uses a <code>load\u2011balancing algorithm</code> to select one of the available service instances and makes a request.</p> <p>Note</p> <p>The network location of a service instance is registered with the service registry when it starts up. It is removed from the service registry when the instance terminates. The service instance\u2019s registration is typically refreshed periodically using a heartbeat mechanism.</p> <p> </p>"},{"location":"sys_design/designPatterns/serviceRegistry/#sever-side","title":"Sever-side","text":"<p>When making a request to a service, the client makes a request via a router (a.k.a load balancer) that runs at a well known location. The router queries a service registry, which might be built into the router, and forwards the request to an available service instance.</p> <p> </p>"},{"location":"sys_design/designPatterns/serviceRegistry/#registry-for-aks","title":"Registry for AKS","text":"<p>Using service registy in AKS</p> <p>In some microservice deployment environments (called clusters, to be covered in a later section), service discovery is built in. For example, an <code>Azure Kubernetes Service (AKS)</code> environment can handle service <code>instance registration</code> and <code>deregistration</code>. It also runs a proxy on each cluster host that plays the role of server-side discovery router.</p>"},{"location":"sys_design/designPatterns/serviceRegistry/#servcie-discovery-example","title":"servcie discovery example","text":"<p>ELB as Service registry and server-side discovery router</p> <p>An AWS <code>Elastic Load Balancer (ELB)</code> is an example of a <code>server-side discovery router</code>. A client makes HTTP requests (or opens TCP connections) to the ELB, which load balances the traffic amongst a set of EC2 instances. An ELB can <code>load balance</code> either external traffic from the Internet or, when deployed in a VPC, load balance internal traffic. </p> <p>An ELB also functions as a <code>Service Registry</code>. EC2 instances are registered with the ELB either explicitly via an API call or automatically as part of an auto-scaling group.</p>"},{"location":"sys_design/designPatterns/serviceRegistry/#service-registry","title":"Service Registry","text":"<p>The service registry is a key part of service discovery. It is a database containing the network locations of service instances. </p> <p>A service registry needs to be highly available and up to date. Clients can cache network locations obtained from the service registry. However, that information eventually becomes out of date and clients become unable to discover service instances.</p> <p>How to stay updated?</p> <p>A service registry consists of a cluster of servers that use a replication protocol to maintain consistency.</p>"},{"location":"sys_design/designPatterns/serviceRegistry/#examples","title":"Examples","text":"<ul> <li> <p>Netflix Eureka: it is a RESTful (Representational State Transfer) service that is primarily used in the AWS cloud for the purpose of discovery, load balancing and failover of middle-tier servers. It plays a critical role in Netflix mid-tier infra.</p> </li> <li> <p>Etcd \u2013 A highly available, distributed, consistent, key\u2011value store that is used for shared configuration and service discovery. Two notable projects that use etcd are Kubernetes and Cloud Foundry.</p> </li> <li> <p>Consul \u2013 A tool for discovering and configuring services. It provides an API that allows clients to register and discover services. Consul can perform health checks to determine service availability.</p> </li> <li> <p>Apache Zookeeper \u2013 A widely used, high\u2011performance coordination service for distributed applications. Apache Zookeeper was originally a subproject of Hadoop but is now a top\u2011level project.</p> </li> </ul>"},{"location":"sys_design/designPatterns/sidecar/","title":"Sidecar","text":"<ul> <li>This pattern is named Sidecar because it resembles a sidecar attached to a motorcycle. </li> <li>In the pattern, the sidecar is attached to a parent application and provides supporting features for the application. The sidecar also shares the same lifecycle as the parent application, being created and retired alongside the parent. The sidecar pattern is sometimes referred to as the sidekick pattern and is a decomposition pattern.</li> </ul> <p>Why this pattern is needed?</p> <p>Applications and services often require cross-cutting-concerns functionality, such as <code>monitoring</code>, <code>logging</code>, <code>configuration</code>, and <code>networking services</code>. These peripheral tasks can be implemented as separate components or services.</p>"},{"location":"technical_stuff/SAP/","title":"SAP","text":""},{"location":"technical_stuff/SAP/#notes","title":"Notes","text":"<ul> <li>Bring-your-own software and license is the primary model for most SAP applications and database products on AWS</li> <li>Some SAP solutions are available on AWS with on-demand, free trial, and free developer licenses</li> </ul> <p>SAP licensing for AWS works in two ways:</p> <pre><code>Bring-your-own software and bring-your-own license models\nSelect licenses available as on-demand, free trial, or free developer\n</code></pre> <p>--</p> <p>S/4HANA transformation is ideal for customers who want to completely modernize their SAP landscape, implement advanced analytics in ERP, and enable real-time business processes.</p> <p>Key adoption scenarios are lift and shift, refactoring on HANA, S/4HANA transformation, and Beyond Infrastructure.</p> <p>SAP offers traditional solutions, platform solutions, and SaaS solutions, as well as products that can be deployed either on-premises or in the cloud. </p> <p>AWS helps customers migrate their workloads to AWS and also creates more value by enabling them to integrate their SAP applications with AWS services and build extensions.</p> <p>. The SAP Lens is a collection of customer-proven design principles and best practices for ensuring SAP workloads on AWS are well-architected. It is based on insights that AWS has gathered from customers, AWS Partners, and the SAP specialist community. The lens has been designed to help you adopt a cloud-first approach to running SAP. It highlights some of the most common areas for improvement, aligned to the six pillars of the AWS Well-Architected Framework: operational excellence, security, reliability, performance efficiency, cost optimization, and sustainability. </p> <p>AWS supports SAP solutions like SAP HANA, SAP BusinessObjects, SAP NetWeaver, and SAP AnyDB.</p> <p>You can operate existing SAP AnyDB workloads on AWS. Supported database platforms for SAP AnyDB on AWS include the following: </p> <ul> <li>Microsoft SQL Server</li> <li>IBM DB2</li> <li>Oracle Database</li> <li>Amazon Relational Database Service (Amazon RDS) (Only supported for SAP BusinessObjects BI and SAP Commerce, previously known as SAP Hybris Commerce)</li> <li>Amazon Aurora (Only supported for SAP Commerce)</li> </ul>"},{"location":"technical_stuff/SAP/#agenda","title":"Agenda","text":"<ul> <li>Describe networking components for SAP on AWS using Amazon Virtual Private Cloud (Amazon VPC)</li> <li>Identify how to connect an SAP environment to Amazon Web Services (AWS)</li> </ul>"},{"location":"technical_stuff/SAP/#aws-data-provider-for-sap","title":"AWS Data Provider for SAP","text":"<p>To run SAP workloads, you should set up the AWS Data Provider for SAP. This is a daemon or service that automatically starts with your operating system and collects, aggregates, and exposes metrics to the SAP platform. The AWS Data Provider for SAP is a mandatory component for receiving integrated support from SAP and AWS. It gathers the following data: </p> <ul> <li>Specific AWS information about instance type and instance ID</li> <li>Key system configuration, such as number of processors, main memory, and disks</li> <li>Key parameters of CPU, memory, disks, and network resource consumption data</li> </ul>"},{"location":"technical_stuff/powerbi/","title":"Power BI","text":""},{"location":"technical_stuff/powerbi/#concepts","title":"Concepts","text":"<p>The five major building blocks of Power BI are: </p> <ul> <li>Dashboards</li> <li>Reports</li> <li>Workbooks</li> <li>Datasets</li> <li>Dataflows</li> </ul> <p>They're all organized into <code>workspaces</code>, and they're created on <code>capacities</code>. It's important to understand capacities and workspaces before we dig into the five building blocks, so let's start there.</p>"},{"location":"technical_stuff/powerbi/#capacity","title":"Capacity","text":"<p>Capacities are a core Power BI concept representing a set of resources (storage, processor, and memory) used to host and deliver your Power BI content. Capacities are either <code>shared</code> or <code>reserved</code>.</p> <p>A shared capacity is shared with other Microsoft customers, while a reserved capacity is reserved for a single customer.</p>"},{"location":"technical_stuff/powerbi/#workspaces","title":"Workspaces","text":"<p>How to share powerBI content?</p> <p>A workspace is like a shared folder between a team of users. This can be a place to share some of the Power BI content</p> <p>There are two types of licenses for Power BI; capacity-based and user-based licensing. The capacity-based licensing is usually better for organizations with more than hundreds of users, and user-based licensing is good for small to medium size businesses</p> <p> </p> <p>There are two types of workspaces: </p> <ul> <li>My workspace (user based)</li> <li>Workspaces (Capacity based)</li> </ul>"},{"location":"technical_stuff/powerbi/#my-workspace","title":"My workspace","text":"<p>It is the personal workspace for any Power BI customer to work with your own content. Only you have access to your My workspace. You can share dashboards and reports from your My Workspace. </p>"},{"location":"technical_stuff/powerbi/#workspaces_1","title":"Workspaces","text":"<p>Workspaces are used to collaborate and share content with colleagues. You can add colleagues to your workspaces and collaborate on dashboards, reports, workbooks, and datasets. </p> <p>Warning</p> <p>Each workspace member needs a Power BI Pro or Premium Per User (PPU) license. </p>"},{"location":"technical_stuff/powerbi/#dashboard","title":"Dashboard","text":"<p>Tldr</p> <p>A Power BI dashboard is a single page, often called a canvas, that uses visualizations to tell a story. Because it is limited to one page, a well-designed dashboard contains only the most-important elements of that story.</p> <ul> <li> <p>It is a single screen with tiles of interactive visuals, text, and graphics. A dashboard collects your most important metrics, on one screen, to tell a story or answer a question.</p> </li> <li> <p>if a business user is given permissions to the report, they can build their own dashboards too.</p> </li> <li> <p>As the capacity must share resources, limitations are imposed to ensure \"fair play\", such as the maximum model size (1 GB) and maximum daily refresh frequency (8 times per day).</p> </li> </ul> <p>The visualizations on a dashboard come from reports and each report is based on one dataset. In fact, one way to think of a dashboard is as an entryway into the underlying reports and datasets. Selecting a visualization takes you to the report that was used to create it.  </p>"},{"location":"technical_stuff/powerbi/#visualization","title":"Visualization:","text":"<p>It is a type of chart built by Power BI designers. The visuals display the data from reports and datasets.</p> <p>How many datasets are feasible?</p> <p>All of the visualizations in a report come from a single dataset.  Power BI Desktop can combine more than one data source into a single dataset in a report, and that report can be imported into Power BI.</p>"},{"location":"technical_stuff/powerbi/#report","title":"Report","text":"<p>Report is one or more pages of interactive visuals, text, and graphics that together make up a single report. Power BI bases a report on a single dataset.</p>"},{"location":"technical_stuff/powerbi/#tile","title":"Tile","text":"<p>A tile is a snapshot of your data, pinned to a dashboard by a designer. Designers can create tiles from a report, dataset, dashboard, the Q&amp;A question box, Excel, SQL Server Reporting Services (SSRS), and more.</p>"},{"location":"technical_stuff/powerbi/#dax","title":"DAX","text":"<p>Data Analysis Expressions\u202f(DAX) is a programming language that is used throughout Microsoft Power BI for creating calculated columns, measures, and custom tables. It is a collection of functions, operators, and constants that can be used in a formula, or expression, to calculate and return one or more values.</p> <p>using DAX to calculate columns</p> <p>DAX allows you to augment the data that you bring in from different data sources by creating a calculated column that didn't originally exist in the data source. This feature should be used sparingly</p>"},{"location":"technical_stuff/powerbi/#dax-and-mscript","title":"DAX and MScript","text":"<p>Power BI has 2 mighty hands, DAX (powered by <code>SQL Server Analysis Service</code>) and <code>M Script</code> (powered by <code>Power Query</code>), to perform the ETL jobs and visualization related calculations. Both of these engines are efficient with their way of performing calculations. </p> <p>where not to use DAX and MScript?</p> <p>When it comes to performing calculations in an optimized way, both under-perform compared to database engines, since they are <code>in-memory calculation engines</code> best suited to play with data. They lack indexing of data, which database handles while storing of data. Also, database engine re-uses query results of last few queries, by keeping track of changing data, which is something complex for both DAX &amp; Power Query. As the data size grows, performance difference is quite noticeable.</p>"},{"location":"technical_stuff/powerbi/#calculated-column-and-measure","title":"Calculated column and measure","text":"<p>The fundamental difference between a <code>calculated column</code> and a <code>measure</code> is that a calculated column creates a value for each row in a table. For example, if the table has 1,000 rows, it will have 1,000 values in the calculated column. Calculated column values are stored in the Power BI .pbix file. Each calculated column will increase the space that is used in that file and potentially increase the refresh time.</p> <p>Measures are calculated on demand. Power BI calculates the correct value when the user requests it. Measures do not add to the overall disk space of the Power BI <code>.pbix file</code>.</p>"},{"location":"technical_stuff/powerbi/#dataflow","title":"Dataflow","text":"<p>Tldr</p> <p>Dataflow is a collection of Power Query queries that are scheduled and executed together.</p> <p>It's up to you how you organize the staged data in dataflows. For example, if you need to stage some tables from Dynamics 365, you can create one dataflow that has a query for each table you want to stage. So, dataflows allow you to logically group related Power Query queries</p>"},{"location":"technical_stuff/powerbi/#query-folding","title":"Query folding","text":"<p><code>Query folding</code> is the ability for a <code>Power Query query</code> to generate a single query statement that retrieves and transforms source data.</p> <p>Query folding may occur for an entire Power Query query, or for a subset of its steps. When query folding cannot be achieved\u2014either partially or fully\u2014the Power Query mashup engine must compensate by processing data transformations itself. This process can involve retrieving source query results, which for large datasets is very resource intensive and slow.</p>"},{"location":"technical_stuff/powerbi/#powerbi-service","title":"PowerBI Service","text":"<p>There are two hosting options for Power BI reports</p> <ol> <li> <p>Cloud-based hosting (called <code>Power BI Service</code> or website): In the <code>Power BI Service</code>, organizations are separated using <code>Tenants</code>. Tenants can be managed by <code>Azure Active Directory</code> or <code>Office 365</code>. Under tenants, there will be users. These users are Azure Active Directory users.</p> </li> <li> <p>On-premises hosting (called <code>Power BI Report Server</code>)</p> </li> </ol>"},{"location":"technical_stuff/powerbi/#storage-mode","title":"Storage mode","text":"<p>Today PowerBI offers 4 different storage modes for tables:</p>"},{"location":"technical_stuff/powerbi/#import-mode","title":"Import mode","text":"<p>In this mode, Power BI connects with underlying data source &amp; downloads entire data from the datasource. This data is stored in Power BI model (in an in-memory cache). Fresh copy of this data can be downloaded by pressing Refresh button. <code>PBIX file</code> internally stores model data in compressed format. This published datset model on Power BI Service, internally is stored on Common Data Model, which is sort of <code>Azure Managed SQL Server</code> instance in the backend.</p> <p> </p> <p>Which is the fastest method</p> <p>The fastest method is <code>Import mode</code>. Essentially, this mode allows you to load the data once into Power BI, where it is then stored. Power BI uses the <code>in-memory VertiPaq/xVelocity engine</code>, which is exceptionally fast and delivers results almost immediately. It offers <code>full DAX and PowerQuery support</code>. A quick rule of thumb is that you can typically expect about 10x compression when importing data into Power BI</p> <p>The drawback is that it only permits you to refresh data 8 times per day.</p>"},{"location":"technical_stuff/powerbi/#directquery-mode","title":"DirectQuery mode","text":"<p>With <code>DirectQuery datasets</code>, no data is imported into Power BI. Instead, your Power BI dataset is simply metadata (e.g., tables, relationships) of how your model is structured to query the data source. Data is only brought into Power BI reports and dashboards at query-time (e.g., a user runs a report, or changes filters).</p> <p> </p> <p>Pros: - Dataset size limits do not apply as all data still resides in the underlying database. - Datasets do not require a scheduled refresh as data is only retrieved at query-time.</p> <p>Cons: - Typically, query-time performance will suffer, even when querying a cloud data warehouse like Snowflake. - Concurrency (e.g., multiple users running reports) against DirectQuery datasets could cause performance issues against the underlying database.</p>"},{"location":"technical_stuff/powerbi/#dual-mode","title":"Dual mode","text":"<p>Dudata is imported into cache memory, but can also be served directly from the data source at the query time</p>"},{"location":"technical_stuff/powerbi/#hybrid-tables-composite-mode","title":"Hybrid tables/ Composite mode","text":"<p>Tldr</p> <ul> <li>Cold data in <code>Import mode</code>, Hot data in <code>DirectQuery</code>. </li> <li>Hybrid tables can only be applied on a table that incremental refresh is set on it.</li> <li>Hybrid tables are partitioned, so their most recent partition is a DirectQuery from the data source, and their historical data is imported into other partitions.</li> <li>If your data source doesn\u2019t support query folding, For example, it is a CSV file. Then Power BI, when connected to it, reads the entire data anyway.</li> <li>Incremental Refresh doesn\u2019t need a Premium or PPU license. You can even set it up using a Power BI Pro license. However, Hybrid tables require a Power BI Premium capacity or PPU.</li> </ul> <p> </p> <p><code>Composite models aka hybid tables</code> attempt to combine the best aspects from <code>Import and DirectQuery modes</code> into a single dataset. With Composite models, data modelers can configure the storage mode for each table in the model</p> <p> </p>"},{"location":"technical_stuff/powerbi/#powerbi-architecture","title":"PowerBI Architecture","text":""},{"location":"technical_stuff/powerbi/#front-end-cluster","title":"Front-end cluster","text":"<p>The overall technical architecture consists of two clusters: </p> <ul> <li>a Web Front End (WFE) cluster </li> <li>a Back End cluster</li> </ul> <p> </p>"},{"location":"technical_stuff/powerbi/#aad","title":"AAD","text":"<p>The WFE cluster manages connectivity and authentication. Power BI relies on Azure Active Directory (AAD) to manage account authentication and management. </p>"},{"location":"technical_stuff/powerbi/#atm","title":"ATM","text":"<p>Power BI uses the Azure Traffic Manager (ATM) to direct user traffic to the nearest data center. Which data center is used is determined by the DNS record of the client attempting to connect. The DNS Service can communicate with the Azure Traffic Manager to find the nearest data center with a Power BI deployment</p>"},{"location":"technical_stuff/powerbi/#cdn","title":"CDN","text":"<p>Power BI uses the Azure Content Delivery Network (CDN) to deliver the necessary static content and files to end users based on their geographical locale. The WFE cluster nearest to the user manages the user login and authentication and provides an access token to the user once authentication is successful. The ASP.NET component within the WFE cluster parses the request to determine which organization the user belongs to, and then consults the Power BI Global Service</p>"},{"location":"technical_stuff/powerbi/#global-service","title":"Global Service","text":"<p>The Global Service is implemented as a single Azure Table that is shared among all worldwide WFE and Back End clusters. This service maps users and customer organizations to the datacenter that hosts their Power BI tenant. The WFE specifies to the browser which backend cluster houses the organization's tenant. </p> <p>Once a user is authenticated, subsequent client interactions occur with the backend cluster directly and the WFE cluster is not used.</p>"},{"location":"technical_stuff/powerbi/#back-end-cluster","title":"Back-end cluster","text":"<p>The backend cluster manages all actions the user does in Power BI Service, including visualizations, dashboards,datasets, reports, data storage, data connections, data refresh, and others. </p>"},{"location":"technical_stuff/powerbi/#gateway-and-apim","title":"Gateway and APIM","text":"<p>The Gateway Role acts as a gateway between user requests and the Power BI service. As you can see in the diagram, only the Gateway Role and Azure API Management (APIM) services are accessible from the public Internet.</p> <p>When an authenticated user connects to the Power BI Service, the connection and any request by the client is accepted and managed by the Gateway Role, which then interacts on the user's behalf with the rest of the Power BI Service. For example, when a client attempts to view a dashboard, the Gateway Role accepts that request, and then sends a request to the Presentation Role to retrieve the data needed by the browser to render the dashboard.</p>"},{"location":"technical_stuff/powerbi/#data-storage","title":"Data Storage","text":"<p>As far as data storage in the cloud goes, Power BI uses two primary repositories for storing and managing data. Data that is uploaded from users or generated by dataflows is stored in Azure BLOB storage, but all the metadata definitions (dashboards, reports, recent data sources, workspaces, organizational information, tenant information) are stored in Azure SQL Database.</p>"},{"location":"technical_stuff/powerbi/#performance-optimization-of-powerbi","title":"Performance Optimization of PowerBI","text":"<p>check this ZerbaBI report</p>"},{"location":"technical_stuff/powerbi/#analyze-the-results","title":"Analyze the results","text":"<ul> <li> <p><code>Power BI Performance Analyzer</code>: The natural starting point is Power BI Performance Analyzer, a built-in feature of Power BI Desktop. Select the View ribbon, and then select Performance Analyzer to display the Performance Analyzer pane.</p> </li> <li> <p><code>DAX Studio</code>: You can analyze performance using in another tool called <code>DAX Studio</code>. This is a a great tool for performance analysis.</p> </li> </ul>"},{"location":"technical_stuff/powerbi/#using-direct-query","title":"Using direct query?","text":"<p>There's another situation that can slow down your report. You might be creating tables that use <code>DirectQuery as the connection mode</code>. This means that the data is not loaded into Power BI. Instead, it is loaded using a query that runs on a database server. This means additional time to fetch the data.</p> <p>This shows up as another item on the report, since Power BI first needs to fetch the data, process it using a <code>PowerQuery</code> and pass it on to a <code>DAX command</code>.</p>"},{"location":"technical_stuff/powerbi/#avoid-joins","title":"Avoid Joins","text":"<p>People essentially just load all the tables into <code>Power BI</code> and start creating relationships. Then, they need to write <code>complex DAX formulas</code> to tie everything together. To make everything run faster, however, you need to combine tables and merge them before you even load them into Power BI.</p>"},{"location":"technical_stuff/powerbi/#de-normalize-data-model","title":"De-normalize data model","text":"<p>Combine or append the tables that are similar in structure and used for the same purpose. For example, fact tables like sales, actuals, plan and different forecasts can be combined or appended into a single table.</p>"},{"location":"technical_stuff/powerbi/#incremental-refresh","title":"Incremental Refresh","text":"<p>When you load data from the source into the destination (Power BI), there are two methods: Full Load or Incremental Refresh. Full Load means fetching the entire dataset each time and wiping out the previous data. </p> <p>Incremental refresh is an important feature to consider when working with large tables that you would like to import into memory. With Incremental Refresh, partitions are automatically created on your Power BI table based on the amount of history to retain, as well as the partition size you would like to set. Since data modelers can define the size of each partition, refreshes will be faster, more reliable, and able to build history over time.</p> <p>When to use incremental refresh/</p> <p>Consider you have a large dataset including 20 years of data. From that dataset, probably the data from 20 years ago won\u2019t change anymore, or even the data from 5 years ago, sometimes even a year ago. So why re-processing it again? Why re-loading data that doesn\u2019t update? Incremental Refresh is the process of loading only part of the data that might change and adding it to the previous dataset, which is no longer changing.</p> <p>When setting up Incremental Refresh, keep in mind the following:</p> <ul> <li>You must create RangeStart and RangeEnd date/time parameters.  These parameters must be set to a date/time data type and must be named RangeStart and RangeEnd.</li> <li>The initial refresh in the Power BI service will take the longest due to the creation of partitions and loading of historical data.  Subsequent refreshes will only process the latest partition(s) based on how the feature was configured.</li> </ul>"},{"location":"technical_stuff/powerbi/#partitioning","title":"Partitioning","text":"<p>Incremental Load will split the table into partitions. The quantity of the partitions will be based on the settings applied at the time of Incremental refresh. For example, if you want to have the last year\u2019s data refreshed only, a yearly partition will likely be created for every year, and the one for the current year will be refreshed on a scheduled basis.</p>"},{"location":"technical_stuff/powerbi/#hybrid-tables","title":"Hybrid tables","text":"<p>Are you using hybrid tables? </p>"},{"location":"technical_stuff/powerbi/#query-folding_1","title":"Query folding","text":"<p>Are you using query folding and is ti finishing completely?</p>"},{"location":"technical_stuff/powerbi/#avoid-row-level-security","title":"Avoid row-level security","text":"<p>Last but not least, avoid row-level security, which can be a performance killer.</p>"},{"location":"technical_stuff/powerbi/#dimentional-modelling","title":"Dimentional modelling","text":"<p>It is a set of guidelines to design database table structure for easier and faster data retrieval.</p> <p>Remember</p> <p>Data is stored in de-normalized form here.</p> <ul> <li> <p>Dimentions: Descriptive entity.</p> </li> <li> <p>Facts: Quantitative entity value. They are mostly values.</p> </li> </ul> <p>Dimentional model vs Normal form models</p> Syntax Normal form models Dimentional Purpose Transactional systems For reporting Structure Complex Less comples (Denormalized data) Operations Insert, upate and delete Mostly Select (read only) Bitmap Index Not used Heavily used <p>Various kinds of dimentional modelling are =:</p>"},{"location":"technical_stuff/powerbi/#star-schema","title":"Star Schema","text":"<p>In this data model, you use a fact table, which is the table like sales that contains facts, meaning your measures. The fact table is related to your dimensions, which are things like your salespeople, products, business units, customers and so on. The fact table then has one-to-many relationships to your dimension tables.</p>"},{"location":"technical_stuff/powerbi/#snowflake-schema","title":"Snowflake schema","text":"<p>In a snowflake schema, each property of entries in a dimension table is assigned to a new table, creating what seems like a snowflake.</p>"},{"location":"technical_stuff/powerbi/#snowflake-and-powerbi","title":"Snowflake and PowerBI","text":"<p>We need to used datawarehouse in front of PowerBI</p> <p>Data storage mode</p> <p>For larger datasets, <code>DirectQuery</code> and <code>Hybrid tables</code> is a good option. Snowflake will handle the heavy lifting. </p> <p> </p>"},{"location":"technical_stuff/powerbi/#best-practices-when-using-dw-with-powerbi","title":"Best Practices when using DW with PowerBI","text":"<ul> <li>Make sure the data visualizations are use case driven, this includes the timing of the data, number of visualizations and other components.</li> <li> <p>Data Model Design: How you design your data model will have a major impact on query performance in Power BI.</p> </li> <li> <p>Ideally, use a <code>Star Schema Design</code> for your data in Power BI with <code>Facts and Dimensions</code>. It is also important to have a more relational, normalized Data Vault type of an ODS model in Snowflake to get the best performance in Power BI.</p> </li> <li> <p>Use <code>Import mode</code> for dimension tables and <code>Direct query</code> mode for Fact tables.</p> </li> <li> <p>Use <code>aggregations</code> in Power BI for pre-aggregated data to get better query performance</p> </li> <li> <p>Keep the dashboard simple by limiting the number of data points, visuals and queries in a page to a minimum. This will also help query performance as there will be lesser number of queries to run while refreshing the visualization.</p> </li> <li> <p>Use <code>Query reduction</code> to limit the number of queries generated. This is especially helpful when using slicers in your visualization, where you only want the filters applied when the \u201cApply\u201d button is used.</p> </li> <li> <p>To make modeling easier, use the <code>Assume referential integrity</code> property on relationships. While the default property on a relationship in Power BI is to generate a <code>left outer join</code>, by using the \u201cAssume referential integrity\u201d property, you can force an inner join. This can make the queries faster. This property is available only in a Direct Query mode.</p> </li> <li> <p>Use bi-directional filter on relationships with discretion. More bi-directional filters mean you will generate more SQL queries. These can increase the complexity of the model and increase compute costs in Snowflake.</p> </li> <li> <p>Pruning micro partitions on large data sets in Snowflake, creating additional tables with larger data with alternate sets of keys driven by the end users ad-hoc query needs can result in better performance</p> </li> </ul>"},{"location":"technical_stuff/powerbi/#auth-flow","title":"Auth flow","text":"<ol> <li>The user logs into Power BI service using Microsoft Azure Active Directory (Azure AD).</li> <li>(Optional) If the identity provider is not Azure AD, then Azure AD verifies the user through SAML authentication before logging the user into the Power BI service.</li> <li>When the user connects to Snowflake, the Power BI service asks Azure AD to give it a token for Snowflake</li> <li>The <code>Power BI</code> service uses the embedded Snowflake driver to send the Azure AD token to Snowflake as part of the connection string.</li> <li><code>Snowflake</code> validates the token, extracts the username from the token, maps it to the Snowflake user, and creates a Snowflake session for the Power BI service using the user\u2019s default role.</li> </ol>"},{"location":"technical_stuff/snowflake/","title":"Snowflake","text":"<ul> <li> <p>Snowflake is true Software-as-a-Service (SaaS) for data</p> </li> <li> <p>Its purely cloud based. All components of Snowflake\u2019s service (other than optional command line clients, drivers, and connectors), run in public cloud infrastructures.</p> </li> <li> <p>Snowflake enables data storage, processing, and analytic solutions that are faster, easier to use, and far more flexible than traditional offerings.</p> </li> </ul> <p>Warning</p> <p>Snowflake cannot be run on private cloud infrastructures (on-premises or hosted).</p>"},{"location":"technical_stuff/snowflake/#architecture","title":"Architecture","text":"<p>Snowflake\u2019s architecture is a hybrid of <code>shared-disk</code> + <code>shared-nothing architecture</code></p> <ol> <li> <p><code>Shared-disk architecture</code>: Snowflake uses a central data repository for persisted data that is accessible from all compute nodes in the platform.</p> </li> <li> <p><code>Shared-nothing architecture</code>: Snowflake processes queries using MPP (massively parallel processing) compute clusters where each node in the cluster stores a portion of the entire data set locally. </p> </li> </ol> <p> </p> <p>Snowflake\u2019s unique architecture consists of three key layers:</p> <ol> <li>Storage Layer</li> <li>Query Processing Layer</li> <li>Cloud Services Layer</li> </ol>"},{"location":"technical_stuff/snowflake/#storage-layer","title":"Storage Layer","text":"<ul> <li> <p>Snowflake\u2019s centralized database storage layer holds all data, including structured and semi-structured data. </p> </li> <li> <p>As data is loaded into Snowflake, it is optimally reorganized into a compressed, columnar format and stored and maintained in Snowflake databases.</p> </li> </ul> <p>Can I view Snowflake data?</p> <p>The data objects stored by Snowflake are not directly visible nor accessible by customers; they are only accessible through SQL query operations run using Snowflake</p> <ul> <li>Data stored in Snowflake databases is always compressed and encrypted. Snowflake takes care of managing every aspect of how the data is stored.</li> </ul> <p>How data is stored in snowflake?</p> <p>Snowflake automatically organizes stored data into <code>micro-partitions</code>: which are an optimized, immutable, compressed columnar format which is encrypted using AES-256 encryption</p>"},{"location":"technical_stuff/snowflake/#query-processing-layer","title":"Query Processing Layer","text":"<ul> <li>Query execution is performed in the processing layer.</li> <li>Snowflake processes queries using \u201cvirtual warehouses\u201d. Each virtual warehouse is an MPP compute cluster composed of multiple compute nodes allocated by Snowflake from a cloud provider.</li> <li>The Snowflake compute resources are created and deployed on demand to the Snowflake user, to whom the process is transparent.</li> </ul> <p>Tip</p> <p>Snowflake\u2019s unique architecture allows for separation of storage and compute, which means any virtual warehouse can access the same data as another, without any contention or impact on performance of the other warehouses. This is because each Snowflake virtual warehouse operates independently and does not share compute resources with other virtual warehouses</p>"},{"location":"technical_stuff/snowflake/#cloud-services-layer","title":"Cloud services Layer","text":"<p>All interactions with data in a Snowflake instance begin in the cloud services layer, also called the global services layer. The Snowflake cloud services layer is a collection of services that coordinate activities such as authentication, access control, and encryption.</p> <p>Each time a user requests to log in, the request is handled by the cloud services layer. When a user submits a Snowflake query, the SQL query will be sent to the cloud services layer optimizer before being sent to the compute layer for processing.</p>"},{"location":"technical_stuff/snowflake/#managing-cache-across-azs","title":"Managing cache across AZ's","text":"<p>The Snowflake cloud services layer runs across multiple <code>availability zones</code> in each cloud provider region and holds the result cache, a cached copy of the executed query results. The metadata required for query optimization or data filtering are also stored in the cloud services layer.</p>"},{"location":"technical_stuff/snowflake/#auto-suspend-and-auto-resume","title":"Auto Suspend and Auto resume","text":"<p><code>Auto Suspend</code> is the number of seconds that the virtual warehouse will wait if no queries need to be executed before going offline.</p> <p><code>Auto Resume</code> will restart the virtual warehouse once there is an operation that requires compute resources.</p>"},{"location":"technical_stuff/snowflake/#features","title":"Features","text":"<ul> <li>A multicluster virtual warehouse allows Snowflake to scale in and out automatically.</li> <li>We can use different kinds of Warehouses in the snowflake to hanlde different kinds of requirements</li> </ul>"},{"location":"technical_stuff/snowflake/#compression","title":"Compression","text":"<p>Another fundamental engineering work from Snowflake was the proprietary and sizeable compression. Faster compression and less data transfer led to less Input Output (IO) costs and an overall faster architecture. It is not uncommon to see <code>5x compression</code> when migrating data to Snowflake.</p>"},{"location":"technical_stuff/snowflake/#common-table-expressions","title":"Common Table Expressions:","text":"<p>A Common Table Expression, also called as CTE in short form, is a temporary named result set that you can reference within a SELECT, INSERT, UPDATE, or DELETE statement. The CTE can also be used in a View.</p> <p>The WITH clause, an optional clause that precedes the SELECT statement, is used to define common table expressions (CTEs) which are referenced in the FROM clause</p> <pre><code>WITH expression_name [ ( column_name [,...n] ) ] \nAS \n( CTE_query_definition )\n</code></pre> <p>To view the CTE result we use a Select query with the CTE expression name.</p> <pre><code>Select [Column1,Column2,Column3 \u2026..] from expression_name\n</code></pre>"},{"location":"technical_stuff/snowflake/#zero-copy-cloning","title":"Zero-Copy Cloning","text":"<p>Zero-copy cloning offers the user a way to snapshot a Snowflake database, schema, or table along with its associated data. There is no additional storage charge until changes are made to the cloned object, because zero-copy data cloning is a metadataonly operation</p>"},{"location":"technical_stuff/snowflake/#time-travel","title":"Time Travel","text":"<p>Time Travel allows you to restore a previous version of a database, table, or schema. This is an incredibly helpful feature that gives you an opportunity to fix previous edits that were done incorrectly or restore items deleted in error.</p>"},{"location":"technical_stuff/snowflake/#caching","title":"Caching","text":"<p>When you submit a query, Snowflake checks to see whether that query has been previously run and, if so, whether the results are still cached. Snowflake will use the cached result set if it is still available rather than executing the query you just submitted</p> <p>There are three Snowflake caching types: </p> <ol> <li> <p><code>Query result cache</code>: The fastest way to retrieve data from Snowflake is by using the query result cache. The results of a Snowflake query are cached, or persisted, for 24 hours and then purged.</p> <p>The result cache is fully managed by the Snowflake global cloud services (GCS) layer, as shown in Figure 2-18, and is available across all virtual warehouses since virtual warehouses have access to all data.</p> </li> <li> <p><code>Virtual warehouse cache</code>: Running virtual warehouses use SSD storage to store the micro-partitions that are pulled from the centralized database storage layer when a query is processed.</p> <p>Danger</p> <p>This cache is dropped once the virtual warehouse is suspended, so you\u2019ll want to consider the trade-off between the credits that will be consumed by keeping a virtual warehouse running and the value from maintaining the cache of data from previous queries to improve performance.</p> </li> <li> <p><code>Metadata cache</code>: The Snowflake metadata repository includes table definitions and references to the micro-partition files for that table. </p> </li> </ol>"},{"location":"technical_stuff/snowflake/#commands","title":"Commands","text":"Create a warehouse<pre><code>USE ROLE SYSADMIN;\nCREATE WAREHOUSE AmarBlog_WH_S WITH WAREHOUSE_SIZE = MEDIUM\nAUTO_SUSPEND = 300 AUTO_RESUME = true INITIALLY_SUSPENDED = true;\n</code></pre> Alter warehouse size<pre><code>USE ROLE SYSADMIN;\nALTER WAREHOUSE AmarBlog_WH_S\nSET WAREHOUSE_SIZE = LARGE;\n</code></pre> Select a warehouse<pre><code>USE WAREHOUSE AmarBlog_WH_M\n</code></pre>"},{"location":"technical_stuff/sql_and_nosql/","title":"DB, DW, Data Lakes, Data Lakehouse","text":""},{"location":"technical_stuff/sql_and_nosql/#databases","title":"Databases","text":"<p>Databases are for OLTP (operational needs). In OLTP, the emphasis is on fast processing, because OLTP databases are read, written, and updated frequently. </p> <p>Characterstics of OLTP</p> <ul> <li>We need <code>Normalized databases</code> for efficiency </li> <li>Some OLTP examples are credit card activity, order entry, and ATM transactions.</li> </ul>"},{"location":"technical_stuff/sql_and_nosql/#data-warehouses","title":"Data Warehouses","text":"<ul> <li>Data Warehouses are for OLAP (Informational needs)</li> <li>OLAP applies complex queries to large amounts of historical data, aggregated from OLTP databases and other sources, for data mining, analytics, and business intelligence projects. </li> <li>In OLAP, the emphasis is on response time to these complex queries. Each query involves one or more columns of data aggregated from many rows.</li> </ul> <p>Data warehousing helps you answer those tough analytical questions that your board may be asking that aren\u2019t possible to address with your standard data analytics tool.</p> <p>Characterstics of OLAP</p> <ul> <li>OLAP is a read heavy system</li> <li>Data is non-volatile as it wont be changes often.</li> <li>OLAP is used on data warehouse or some other centralized data store.</li> <li>Data periodically refreshed with scheduled, long-running batch jobs</li> <li>For backup: Lost data can be reloaded from OLTP database as needed in lieu of regular backups.</li> <li>Denormalized databases for analysis (most data warehouses favor denormalized data models, where each table contains as many related attributes as possible. In this way, all the information can be processed by a single pass through the data.)</li> <li>An OLAP database uses a multidimensional data model, which includes features of relational, navigational, and hierarchical databases. It also consists of an OLAP cube which consists of multiple types of data.</li> </ul> <p>Unlike a database, the information isn\u2019t updated in real-time and is better for data analysis of broader trends.</p> <p> </p>"},{"location":"technical_stuff/sql_and_nosql/#data-lake","title":"Data Lake","text":"<p>Data lake is for storing any and all raw data that may or may not yet have an intended use case. A data warehouse, on the other hand, holds data that has already been processed and filtered, so it\u2019s ready to be used and analyzed.</p>"},{"location":"technical_stuff/sql_and_nosql/#data-lakehouse","title":"Data LakeHouse","text":"<p>A data lakehouse is an open data management architecture that combines the flexibility and cost-efficiency of data lakes with the data management and structure features of data warehouses, all on one data platform.</p> <p>Simply put: The data lakehouse is the only data architecture that stores all data \u2014 unstructured, semi-structured, AND structured \u2014 in your data lake while still providing the data quality and data governance standards of a data warehouse.</p>"},{"location":"technical_stuff/sql_and_nosql/#how-to-create-olap-view-from-oltp","title":"How to create OLAP view from OLTP?","text":"<ul> <li>In the first step, new transactions are copied from the operational source systems and loaded into a temporary staging area.</li> <li>Data in the staging area is then transformed to create fact and dimension tables that are used to build OLAP cube structures.</li> <li>These cubes contain precalculated aggregate structures that contain summary information which must be updated as new facts are added to the fact tables.</li> <li>The information in the OLAP cubes is then accessed from a graphical front-end tool through the security and data services layers. </li> <li>The precise meaning of data in any part of the system is stored in a separate metadata registry database that ensures data is used and interpreted consistently despite the many layers of transformation.</li> </ul> <p>Issues in moving data</p> <p>The ETL tools to move data between operational and analytical systems still usually run on single processors, perform costly join operations, and limit the amount of data that can be moved each night between the operational and analytical systems. These challenges and costs are even greater when organizations lack strong data governance policies or have inconsistent category definitions </p>"},{"location":"technical_stuff/sql_and_nosql/#olap-concepts","title":"OLAP Concepts","text":""},{"location":"technical_stuff/sql_and_nosql/#fact-table","title":"Fact table","text":"<p>A central table of events that contains foreign keys to other tables and integer and decimal values called measures.</p>"},{"location":"technical_stuff/sql_and_nosql/#dimension-table","title":"Dimension table","text":"<p>A table used to categorize every fact. Examples of dimensions include time, geography, product, or promotion.</p>"},{"location":"technical_stuff/sql_and_nosql/#star-schema","title":"Star schema","text":"<p>An arrangement of tables with one fact table surrounded by dimension tables. Each transaction isrepresented by a single row in the <code>central fact table</code>.</p>"},{"location":"technical_stuff/sql_and_nosql/#categories","title":"Categories","text":"<p>A way to divide all the facts into two or more classes. For example, products may have a Seasonal category indicating they\u2019re only stocked part of the year.</p>"},{"location":"technical_stuff/sql_and_nosql/#measures","title":"Measures","text":"<p>A number used in a column of a fact table that you can sum or average. Measures are usually things like sales counts or prices.</p>"},{"location":"technical_stuff/sql_and_nosql/#aggregates","title":"Aggregates","text":"<p>Precomputed sums used by OLAP systems to quickly display results to users.</p>"},{"location":"technical_stuff/sql_and_nosql/#data-warehouse-performance-reasons","title":"Data warehouse performance reasons \ud83e\udd14","text":"<p>We will take Amazon Redshift as an example.</p> <p>Info</p> <p>Amazon Redshift is a column-oriented, fully managed, petabyte-scale data warehouse that makes it simple and cost-effective to analyze all your data using your existing business intelligence tools. </p>"},{"location":"technical_stuff/sql_and_nosql/#mpp","title":"MPP","text":"<p>Massively parallel processing (MPP) enables fast run of the most complex queries operating on large amounts of data. Multiple compute nodes handle all query processing leading up to final result aggregation, with each core of each node running the same compiled query segments on portions of the entire data.</p>"},{"location":"technical_stuff/sql_and_nosql/#columnar-data-storage","title":"Columnar data storage","text":"<p>Storing database table information in a columnar fashion reduces the number of disk I/O requests and reduces the amount of data you need to load from disk.</p>"},{"location":"technical_stuff/sql_and_nosql/#data-compression","title":"Data compression","text":"<p>Because columnar storage stores similar data sequentially, Amazon Redshift is able to apply adaptive compression encodings specifically tied to columnar data types</p>"},{"location":"technical_stuff/sql_and_nosql/#query-optimizer","title":"Query optimizer","text":"<p>The Amazon Redshift query run engine incorporates a query optimizer that is MPP-aware and also takes advantage of the columnar-oriented data storage.</p>"},{"location":"technical_stuff/sql_and_nosql/#results-caching","title":"Results Caching","text":"<p>To reduce query runtime and improve system performance, Amazon Redshift caches the results of certain types of queries in memory on the leader node. When a user submits a query, Amazon Redshift checks the results cache for a valid, cached copy of the query results. If a match is found in the result cache, Amazon Redshift uses the cached results and doesn't run the query. Result caching is transparent to the user.</p>"},{"location":"technical_stuff/sql_and_nosql/#no-sql-data-patterns","title":"No SQL data patterns","text":""},{"location":"technical_stuff/sql_and_nosql/#key-value","title":"Key-value","text":"<p>At its core, S3 is a simple key-value store with some enhanced features such as metadata and access control</p> <ul> <li>A key-value store is a simple database that when presented with a simple string (the key) returns an arbitrary large BLOB of data (the value).</li> <li>Key-value stores have no query language; they provide a way to add and remove key-value pairs (a combination of key and value where the key is bound to the value until a new value is assigned) into/from a database.</li> <li>The dictionary is a simple key-value store where word entries represent keys and definitions represent values.</li> </ul> <p> </p> <p>Various operations on Key value are</p> <p> </p> <p>One of the benefits of not specifying a data type for the value of a key-value store is that you can store any data type that you want in the value. The system will store the information as a BLOB and return the same BLOB when a GET (retrieval) request is made. It\u2019s up to the application to determine what type of data is being used, such as a string, XML file, or binary image.</p>"},{"location":"technical_stuff/sql_and_nosql/#document-store","title":"Document store","text":""},{"location":"technical_stuff/sql_and_nosql/#column-family","title":"Column Family","text":"<p>Examples:</p> <ul> <li>Cassandra</li> <li>PostgreSQL</li> <li>HBase</li> <li>Google BigTable</li> <li>Amazon Redshift: Amazon Redshift is based on PostgreSQL.</li> </ul> <p>While a relational database is optimized for storing rows of data, typically for transactional applications, a columnar database is optimized for fast retrieval of columns of data, typically in analytical applications.</p> <p>Remember</p> <p>Column-oriented storage for database tables is an important factor in <code>analytic query performance</code> because it drastically reduces the <code>overall disk I/O requirements</code> and reduces the amount of data you need to load from disk.</p> <p>This is how data is stored for RDBMS in disk (sectors and blocks)  </p> <p>Data storage pattern for columnar family in disk  </p> <p>An added advantage is that, since each block holds the same type of data, block data can use a compression scheme selected specifically for the column data type, further reducing disk space and I/O.</p>"},{"location":"technical_stuff/sql_and_nosql/#graph-db","title":"Graph DB","text":"<p>A graph store is a system that contains a sequence of nodes and relationships that, when combined, create a graph. You know that in a key-value store there two data fields: the key and the value. In contrast, a graph store has three data fields: nodes, relationships, and properties.</p>"},{"location":"technical_stuff/blogs/cpanel/gitAndCpanel/","title":"How to clone a private Github repo into Cpanel","text":"<p>The goal of this blog post is to help you in setting up your Github private repo on Cpanel, so that when you make a push to your blog post, then those changes are synced automatically. </p> <p></p> <p>So let's get started \ud83d\ude00</p>"},{"location":"technical_stuff/blogs/cpanel/gitAndCpanel/#generate-a-key-in-cpanel","title":"Generate a key in Cpanel","text":"<p>The goal is the create a public and private key pair. Then copy the public key and save it in Github. First we will create a pair in Cpanel. For this go to <code>Cpanel</code> \u2192 <code>Security</code> \u2192 <code>SSH Access</code> \u2192 <code>Manage Keys</code> \u2192 <code>Generate New Key</code> (enter a strong password) </p>"},{"location":"technical_stuff/blogs/cpanel/gitAndCpanel/#copy-public-key-from-cpanel","title":"Copy public key from Cpanel","text":"<p>After generating a public-private key pair, now is the time to copy the public key. For doing this, go to SSH Access \u2192 <code>Manage SSH Keys</code> \u2192 <code>Public keys</code> \u2192  <code>Authorize</code> (make authorize) \u2192 Go back \u2192 Press <code>view/download</code> \u2192 <code>Download key</code> \u2192 open it using a text editor and copy it.</p>"},{"location":"technical_stuff/blogs/cpanel/gitAndCpanel/#copy-public-key-to-github","title":"Copy public key to Github","text":"<p>The goal is to save the public key in your private git repo, so that Cpanel can fetch data from it. Go to your <code>Github</code> \u2192 Your private repo \u2192 <code>Settings</code> \u2192 <code>Deploy Keys</code> \u2192 <code>add deploy key</code> \u2192  Provide any title and paste the public key.</p>"},{"location":"technical_stuff/blogs/cpanel/gitAndCpanel/#add-an-cpanelyml-file","title":"Add an <code>.cpanel.yml</code> file.","text":"<p>Before we go ahead, we need to make sure the files we add from the Github are deployed to cpanel. You will need to know 2 important things here</p> <ol> <li><code>yourUserName/repositories</code> : this is the folder where all the repo's will get synced.</li> <li><code>yourUserName/public_html</code> : This is the folder from where the actual site content is served. So, this should contain an <code>index.html</code> at minimum. </li> </ol> <p>The <code>.cpanel.yml</code> file is a basic configuration file. It can be used to copy the contents from your <code>yourUserName/repositories/subdir</code> directory to <code>yourUserName/public_html</code> for example.</p> <p>Make sure you've added a <code>.cpanel.yml</code> file at the root of your project that you are uploading. The contents of this file can be something like</p> <pre><code>---\ndeployment:\n  tasks:\n    - export DEPLOYPATH=/home/ReplaceItByYourUserName/public_html/\n    - /bin/rm -Rf $DEPLOYPATH\n    - /bin/mkdir $DEPLOYPATH\n    - /bin/cp -R FileWhereSiteContentIsLocated/* $DEPLOYPATH\n</code></pre>"},{"location":"technical_stuff/blogs/cpanel/gitAndCpanel/#clone-a-repo-to-cpanel","title":"Clone a repo to CPanel","text":"<p>After you upload your project, this file will be hidden (as this is a . file) but can be seen by going to your directory from cPanel home \u2192 Advanced \u2192 Shell</p> <pre><code>cd yourHome/Repositories\nls -a\n</code></pre> <p>Go to Cpanel \u2192 Git\u2122 Version Control \u2192 create clone url and enter the similar <code>Clone URL</code></p> <pre><code> git@github.com:&lt;user_name&gt;/&lt;repository_name&gt;.git\n</code></pre> <p>The <code>Repository Path</code> and <code>Repo Name</code> will be autofilled. Then hit create.</p>"},{"location":"technical_stuff/blogs/cpanel/gitAndCpanel/#pull-from-the-sources","title":"Pull from the sources.","text":"<p>Manage repository from list \u2192 <code>Manage</code> \u2192 pull or deploy from Github \u2192 Click on <code>Update from Remote</code>: works perfectly (any files edit or delete you fetch/pull from GitHub now)</p> <p>Cheers \ud83c\udf7b</p>"},{"location":"technical_stuff/databricks/ManipulatingTableswithDeltaLakeLab/","title":"Manipulating Tables with Delta Lake","text":"<p>This notebook provides a hands-on review of some basic functionality of Delta Lake.</p>"},{"location":"technical_stuff/databricks/ManipulatingTableswithDeltaLakeLab/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this lab, you should be able to: - Execute standard operations to create and manipulate Delta Lake tables, including:</p> <ul> <li><code>CREATE TABLE</code></li> <li><code>INSERT INTO</code></li> <li><code>SELECT FROM</code></li> <li><code>UPDATE</code></li> <li><code>DELETE</code></li> <li><code>MERGE</code></li> <li><code>DROP TABLE</code></li> </ul>"},{"location":"technical_stuff/databricks/ManipulatingTableswithDeltaLakeLab/#setup","title":"Setup","text":"<p>Run the following script to setup necessary variables and clear out past runs of this notebook. Note that re-executing this cell will allow you to start the lab over.</p> <pre><code>%run ../Includes/Classroom-Setup-2.2L\n</code></pre>"},{"location":"technical_stuff/databricks/ManipulatingTableswithDeltaLakeLab/#create-a-table","title":"Create a Table","text":"<p>In this notebook, we'll be creating a table to track our bean collection. Use the cell below to create a managed Delta Lake table named <code>beans</code>. Provide the following schema:</p> Field Name Field type name STRING color STRING grams FLOAT delicious BOOLEAN <pre><code>create table beans \n(name string, color string, grams float, delicious boolean)\n</code></pre> <p>Note</p> <p>We'll use Python to run checks occasionally throughout the lab. The following cell will return as error with a message on what needs to change if you have not followed instructions. No output from cell execution means that you have completed this step.</p> <pre><code>assert spark.table(\"beans\"), \"Table named `beans` does not exist\"\nassert spark.table(\"beans\").columns == [\"name\", \"color\", \"grams\", \"delicious\"], \"Please name the columns in the order provided above\"\nassert spark.table(\"beans\").dtypes == [(\"name\", \"string\"), (\"color\", \"string\"), (\"grams\", \"float\"), (\"delicious\", \"boolean\")], \"Please make sure the column types are identical to those provided above\"\n</code></pre>"},{"location":"technical_stuff/databricks/ManipulatingTableswithDeltaLakeLab/#insert-data","title":"Insert Data","text":"<p>Run the following cell to insert three rows into the table.</p> <pre><code>INSERT INTO beans VALUES\n(\"black\", \"black\", 500, true),\n(\"lentils\", \"brown\", 1000, true),\n(\"jelly\", \"rainbow\", 42.5, false)\n</code></pre> <p>Manually review the table contents to ensure data was written as expected.</p> <pre><code>select * from beans\n</code></pre> <p>Insert the additional records provided below. Make sure you execute this as a single transaction.</p> <pre><code>insert into beans values\n('pinto', 'brown', 1.5, true),\n('green', 'green', 178.3, true),\n('beanbag chair', 'white', 40000, false)\n</code></pre> <p>Run the cell below to confirm the data is in the proper state.</p> <pre><code>assert spark.conf.get(\"spark.databricks.delta.lastCommitVersionInSession\") == \"2\", \"Only 3 commits should have been made to the table\"\nassert spark.table(\"beans\").count() == 6, \"The table should have 6 records\"\nassert set(row[\"name\"] for row in spark.table(\"beans\").select(\"name\").collect()) == {'beanbag chair', 'black', 'green', 'jelly', 'lentils', 'pinto'}, \"Make sure you have not modified the data provided\"\n</code></pre>"},{"location":"technical_stuff/databricks/ManipulatingTableswithDeltaLakeLab/#update-records","title":"Update Records","text":"<p>A friend is reviewing your inventory of beans. After much debate, you agree that jelly beans are delicious.</p> <p>Run the following cell to update this record.</p> <pre><code>UPDATE beans\nSET delicious = true\nWHERE name = \"jelly\"\n</code></pre> <p>You realize that you've accidentally entered the weight of your pinto beans incorrectly.</p> <p>Update the <code>grams</code> column for this record to the correct weight of 1500.</p> <pre><code>update beans \nset grams = 1500\nwhere name = 'pinto'\n</code></pre> <p>Run the cell below to confirm this has completed properly.</p> <pre><code>assert spark.table(\"beans\").filter(\"name='pinto'\").count() == 1, \"There should only be 1 entry for pinto beans\"\nrow = spark.table(\"beans\").filter(\"name='pinto'\").first()\nassert row[\"color\"] == \"brown\", \"The pinto bean should be labeled as the color brown\"\nassert row[\"grams\"] == 1500, \"Make sure you correctly specified the `grams` as 1500\"\nassert row[\"delicious\"] == True, \"The pinto bean is a delicious bean\"\n</code></pre>"},{"location":"technical_stuff/databricks/ManipulatingTableswithDeltaLakeLab/#delete-records","title":"Delete Records","text":"<p>You've decided that you only want to keep track of delicious beans.</p> <p>Execute a query to drop all beans that are not delicious.</p> <pre><code>delete from beans\nwhere delicious = false\n</code></pre> <p>Run the following cell to confirm this operation was successful.</p> <pre><code>assert spark.table(\"beans\").filter(\"delicious=true\").count() == 5, \"There should be 5 delicious beans in your table\"\nassert spark.table(\"beans\").filter(\"delicious=false\").count() == 0, \"There should be 0 delicious beans in your table\"\nassert spark.table(\"beans\").filter(\"name='beanbag chair'\").count() == 0, \"Make sure your logic deletes non-delicious beans\"\n</code></pre>"},{"location":"technical_stuff/databricks/ManipulatingTableswithDeltaLakeLab/#using-merge-to-upsert-records","title":"Using Merge to Upsert Records","text":"<p>Your friend gives you some new beans. The cell below registers these as a temporary view.</p> <pre><code>CREATE OR REPLACE TEMP VIEW new_beans(name, color, grams, delicious) AS VALUES\n('black', 'black', 60.5, true),\n('lentils', 'green', 500, true),\n('kidney', 'red', 387.2, true),\n('castor', 'brown', 25, false);\n\n\nSELECT * FROM new_beans\n</code></pre> <p>In the cell below, use the above view to write a merge statement to update and insert new records to your <code>beans</code> table as one transaction.</p> <p>Make sure your logic: - Match beans by name and color - Updates existing beans by adding the new weight to the existing weight - Inserts new beans only if they are delicious</p> <pre><code>merge into beans a\nusing new_beans b\non a.name= b.name and a.color = b.color\nwhen matched then \nupdate set grams = a.grams + b.grams\nwhen not matched and b.delicious = true then\ninsert *\n</code></pre> <p>Run the cell below to check your work.</p> <pre><code>version = spark.sql(\"DESCRIBE HISTORY beans\").selectExpr(\"max(version)\").first()[0]\nlast_tx = spark.sql(\"DESCRIBE HISTORY beans\").filter(f\"version={version}\")\nassert last_tx.select(\"operation\").first()[0] == \"MERGE\", \"Transaction should be completed as a merge\"\nmetrics = last_tx.select(\"operationMetrics\").first()[0]\nassert metrics[\"numOutputRows\"] == \"3\", \"Make sure you only insert delicious beans\"\nassert metrics[\"numTargetRowsUpdated\"] == \"1\", \"Make sure you match on name and color\"\nassert metrics[\"numTargetRowsInserted\"] == \"2\", \"Make sure you insert newly collected beans\"\nassert metrics[\"numTargetRowsDeleted\"] == \"0\", \"No rows should be deleted by this operation\"\n</code></pre>"},{"location":"technical_stuff/databricks/ManipulatingTableswithDeltaLakeLab/#dropping-tables","title":"Dropping Tables","text":"<p>When working with managed Delta Lake tables, dropping a table results in permanently deleting access to the table and all underlying data files.</p> <p>NOTE: Later in the course, we'll learn about external tables, which approach Delta Lake tables as a collection of files and have different persistence guarantees.</p> <p>In the cell below, write a query to drop the <code>beans</code> table.</p> <pre><code>drop table beans\n</code></pre> <p>Run the cell below to assert that your table no longer exists. <pre><code>assert spark.sql(\"SHOW TABLES LIKE 'beans'\").collect() == [], \"Confirm that you have dropped the `beans` table from your current database\"\n</code></pre></p>"},{"location":"technical_stuff/databricks/hive/","title":"Hive","text":"<p><code>Apache Hive</code> is open-source data warehouse software designed to read, write, and manage large datasets extracted from the Apache Hadoop Distributed File System (HDFS) , one aspect of a larger Hadoop Ecosystem.</p> <p>Hive processing support</p> <p>Apache Hive supports the analysis of large datasets stored in Hadoop's HDFS and compatible file systems such as Amazon S3, Azure Blob Storage, Azure Data Lake Storage, Google Cloud Storage etc.</p> <p>It provides a SQL-like query language called <code>HiveQL</code> with schema-on-read and transparently converts queries to Apache Spark jobs, MapReduce job, and Apache Tez jobs.</p>"},{"location":"technical_stuff/databricks/hive/#the-hive-metastore","title":"The Hive Metastore","text":"<p>The central repository of the Apache Hive infrastructure, the metastore is where all of the Hive\u2019s metadata is stored. In the metastore, metadata can also be formatted into Hive tables and partitions to compare data across relational databases.</p>"},{"location":"technical_stuff/installing_flink/flink/","title":"Apache Flink","text":""},{"location":"technical_stuff/installing_flink/flink/#installing-apache-flink","title":"Installing Apache Flink","text":"<p>Building Apache Flink is very easy yet it took approximately 30 minutes.</p>"},{"location":"technical_stuff/installing_flink/flink/#steps-for-installing-apache-flink-on-macubuntu","title":"Steps for installing apache Flink on Mac/ubuntu","text":"<p>Steps are:</p> <ol> <li> <p>Unix-like environment such as Linux, Mac OS X, Cygwin.</p> </li> <li> <p>Git</p> </li> <li> <p>Make sure you have java installed, check it in terminal using</p> <pre><code>java -version\n</code></pre> </li> <li> <p>Maven is used as build tool, if you do not have maven install it using</p> <pre><code>brew install maven\n</code></pre> </li> </ol> <p>Unix-like environment (We use Linux, Mac OS X, Cygwin) is required</p> <ol> <li> <p>Go to this link and download the source version. You can also clone the source form <code>git</code> by entering following command in your terminal.     <pre><code>git clone https://github.com/apache/flink\n</code></pre></p> </li> <li> <p><code>cd</code> to the downloaded file and then unpack it by using</p> <pre><code>tar xzf *.tgz\n</code></pre> </li> </ol> <p>where <code>*</code> is filename. Alternatively in Mac you can also double click the tar file and it will be un-tared and unzipped.</p> <ol> <li> <p>Then <code>cd</code> to <code>un-tarred</code> file and enter following command in terminal</p> <pre><code>mvn clean install -DskipTests\n</code></pre> </li> </ol> <p>Let build will start and will take  almost 30 minutes and finally if everything is done successfully, then you will see following message.</p> <p></p> <ol> <li>In my system the Flink is installed at the following location.</li> </ol> <pre><code>/Users/YOUR_USER_NAME/.m2/repository/org/apache/flink\n</code></pre> <p>Success</p> <p>Congrats, we have successfully build Apache-Flink on our system.</p>"},{"location":"technical_stuff/linux/android_device/","title":"Attach android device to android studio in Ubuntu","text":"<p>Make sure you have enabled <code>developer options</code>, if not then follow these steps</p> <ol> <li>Go to <code>Settings</code> \u2192 <code>About phone</code> \u2192 <code>Build number</code>. On a Samsung Galaxy device, go to <code>Settings</code> \u2192 <code>About device</code> \u2192 <code>Build number</code>.</li> <li>Tap <code>build number</code> 7 times.</li> <li>Go back to Settings, where you\u2019ll find a developer options entry in the menu.</li> <li>In terminal type      <code>adb devices</code></li> </ol> <p>Error</p> <p>If you see something like <code>?????????? no permissions</code>, then create a new file using nano as <pre><code>sudo nano /etc/udev/rules.d/51-android.rules\n</code></pre></p> <p>Then paste the following code into that file</p> <pre><code>SUBSYSTEM==\"usb\", ATTRS{idVendor}==\"0bb4\", MODE=\"0666\"\nSUBSYSTEM==\"usb\", ATTRS{idVendor}==\"0e79\", MODE=\"0666\"\nSUBSYSTEM==\"usb\", ATTRS{idVendor}==\"0502\", MODE=\"0666\"\nSUBSYSTEM==\"usb\", ATTRS{idVendor}==\"0b05\", MODE=\"0666\"\nSUBSYSTEM==\"usb\", ATTRS{idVendor}==\"413c\", MODE=\"0666\"\nSUBSYSTEM==\"usb\", ATTRS{idVendor}==\"0489\", MODE=\"0666\"\nSUBSYSTEM==\"usb\", ATTRS{idVendor}==\"091e\", MODE=\"0666\"\nSUBSYSTEM==\"usb\", ATTRS{idVendor}==\"18d1\", MODE=\"0666\"\nSUBSYSTEM==\"usb\", ATTRS{idVendor}==\"0bb4\", MODE=\"0666\"\nSUBSYSTEM==\"usb\", ATTRS{idVendor}==\"12d1\", MODE=\"0666\"\nSUBSYSTEM==\"usb\", ATTRS{idVendor}==\"24e3\", MODE=\"0666\"\nSUBSYSTEM==\"usb\", ATTRS{idVendor}==\"2116\", MODE=\"0666\"\nSUBSYSTEM==\"usb\", ATTRS{idVendor}==\"0482\", MODE=\"0666\"\nSUBSYSTEM==\"usb\", ATTRS{idVendor}==\"17ef\", MODE=\"0666\"\nSUBSYSTEM==\"usb\", ATTRS{idVendor}==\"1004\", MODE=\"0666\"\nSUBSYSTEM==\"usb\", ATTRS{idVendor}==\"22b8\", MODE=\"0666\"\nSUBSYSTEM==\"usb\", ATTRS{idVendor}==\"0409\", MODE=\"0666\"\nSUBSYSTEM==\"usb\", ATTRS{idVendor}==\"2080\", MODE=\"0666\"\nSUBSYSTEM==\"usb\", ATTRS{idVendor}==\"0955\", MODE=\"0666\"\nSUBSYSTEM==\"usb\", ATTRS{idVendor}==\"2257\", MODE=\"0666\"\nSUBSYSTEM==\"usb\", ATTRS{idVendor}==\"10a9\", MODE=\"0666\"\nSUBSYSTEM==\"usb\", ATTRS{idVendor}==\"1d4d\", MODE=\"0666\"\nSUBSYSTEM==\"usb\", ATTRS{idVendor}==\"0471\", MODE=\"0666\"\nSUBSYSTEM==\"usb\", ATTRS{idVendor}==\"04da\", MODE=\"0666\"\nSUBSYSTEM==\"usb\", ATTRS{idVendor}==\"05c6\", MODE=\"0666\"\nSUBSYSTEM==\"usb\", ATTRS{idVendor}==\"1f53\", MODE=\"0666\"\nSUBSYSTEM==\"usb\", ATTRS{idVendor}==\"04e8\", MODE=\"0666\"\nSUBSYSTEM==\"usb\", ATTRS{idVendor}==\"04dd\", MODE=\"0666\"\nSUBSYSTEM==\"usb\", ATTRS{idVendor}==\"0fce\", MODE=\"0666\"\nSUBSYSTEM==\"usb\", ATTRS{idVendor}==\"0930\", MODE=\"0666\"\nSUBSYSTEM==\"usb\", ATTRS{idVendor}==\"19d2\", MODE=\"0666\"\nSUBSYSTEM==\"usb\", ATTRS{idVendor}==\"1bbb\", MODE=\"0666\"\n</code></pre> <p>Then enter following commands</p> <pre><code>sudo chmod 644 /etc/udev/rules.d/51-android.rules\nsudo chown root. /etc/udev/rules.d/51-android.rules\nsudo service udev restart\nsudo killall adb\n</code></pre> <p>Remove he USB cable and connect it again. Then enter</p> <pre><code>adb devices\n</code></pre>"},{"location":"technical_stuff/linux/linuxCommands/","title":"Linux commands","text":"<p>Linux includes a large number of commands, but I have chosen 37 of the most important ones to present here. Learn these commands, and you\u2019ll be much more at home at the Linux command prompt.</p> <p>The below list is presented in alphabetical order. A command\u2019s position in the list is not representative of its usefulness or simplicity. </p> <p>Info</p> <p>For the final word on a command\u2019s usage, refer to its man pages. The man command is in our list, of course\u2014it\u2019s short for \u201cmanual.\u201d</p>"},{"location":"technical_stuff/linux/linuxCommands/#alias","title":"alias","text":"<p>The alias command lets you give your own name to a command or sequence of commands. You can then type your short name, and the shell will execute the command or sequence of commands for you.</p> <p><pre><code>alias cls=clear\n</code></pre> This sets up an alias called <code>cls</code> . It will be another name for clear . When you type cls, it will clear the screen just as though you had typed clear . Your alias saves a few keystrokes, sure. But, if you frequently move between Windows and Linux command line, you can find yourself typing the Windows <code>cls</code> command on a Linux machine that doesn\u2019t know what you mean. Now it will know.</p> <p>Aliases can be much more intricate than that simple example. Here\u2019s an alias called <code>pf</code> (for process find) that is just a little more complex. Note the use of quotation marks around the command sequence. This is required if the command sequence has spaces in it. This alias uses the <code>ps</code> command to list the running processes and then pipes them through the <code>grep</code> command. The grep command looks for entries in the output from ps that match the command line parameter <code>$1</code>.</p> <pre><code>alias pf=\"ps -e | grep $1\"\n</code></pre> <p>If you wanted to discover the process ID (PID) of the shutter process\u2014or to find out if shutter was even running\u2014you could use the alias like this. Type <code>pf</code> followed by a space, and the name of the process you are intere$$sted in:</p> <p><pre><code>pf shutter\n</code></pre> alias command in terminal window</p> <p>Warn</p> <p>Aliases defined on the command line will die with the terminal window. When you close it, they are gone. To make your aliases always be available to you, add them to the <code>.bash_aliases</code> file in your home directory such as <code>.bash_profile</code> and <code>.bashrc</code></p>"},{"location":"technical_stuff/linux/linuxCommands/#cat","title":"cat","text":"<p>The cat command (short for \u201cconcatenate\u201d) lists the contents of files to the terminal window. This is faster than opening the file in an editor, and there\u2019s no chance you can accidentally alter the file. To read the contents of your <code>.bash_log_out</code> file, type the following command while the home directory is your current working directory, as it is by default.</p> <pre><code>cat .bash_logout\n</code></pre> <p>With files longer than the number of lines in your terminal window, the text will whip past too fast for you to read. You can pipe the output from cat through less to make the process more manageable.  With <code>less</code> you can scroll forward and backward through the file using the Up and Down Arrow keys, the PgUp and PgDn keys, and the Home and End keys. Type <code>q</code> to quit from <code>less</code>.</p> <pre><code>cat .bashrc | less\ncat .bashrc | Less in a terminal window\n</code></pre>"},{"location":"technical_stuff/linux/linuxCommands/#cd","title":"cd","text":"<p>what is cd command?</p> <p>The cd command changes your current directory. In other words, it moves you to a new place in the filesystem.If you are changing to a directory that is within your current directory, you can simply type cd and the name of the other directory.</p> <pre><code>cd work\n</code></pre> <p>If you are changing to a directory elsewhere within the filesystem directory tree, provide the path to the directory with a leading <code>/</code></p> <p><pre><code>cd /usr/local/bin\n</code></pre> To quickly return to your home directory, use the ~ (tilde) character as the directory name.</p> <pre><code>cd ~\n</code></pre> <p>Tip</p> <p>You can use the double dot symbol <code>..</code> to represent the parent of the current directory. You can type the following command to go up a directory:</p> <pre><code>cd ..\n</code></pre> <p>Imagine you are in a directory. The parent directory has other directories in it, as well as the directory you\u2019re currently in. To change into one of those other directories, you can use the <code>..</code> symbol to shorten what you have to type.</p> <pre><code>cd ../games\n</code></pre>"},{"location":"technical_stuff/linux/linuxCommands/#chmod","title":"chmod","text":"<p>The chmod command sets the file permissions flags on a file or folder. The flags define who can read, write to or execute the file. When you list files with the <code>-l</code> (long format) option you\u2019ll see a string of characters that look like</p> <p><pre><code>-rwxrwxrwx\n</code></pre> If the first character is a <code>-</code> the item is a file. if it is a <code>d</code> the item is a directory.</p> <p>The rest of the string is three sets of three characters. From the left, the first three represent the file permissions of the <code>owner</code>, the middle three represent the file permissions of the <code>group</code> and the rightmost three characters represent the permissions for <code>others</code>. In each set:-</p> <ol> <li><code>r</code> stands for read</li> <li><code>w</code> stands for write</li> <li><code>x</code> stands for execute.</li> </ol> <p>If the r, w, or x character is present that file permission is granted. If the letter is not present and a <code>-</code> appears instead, that file permission is not granted.</p> <p>Permission Levels</p> <p>One way to use <code>chmod</code> is to provide the permissions you wish to give to the owner, group, and others as a 3 digit number.  The leftmost digit represents the owner. The middle digit represents the group. The rightmost digit represents the others. The digits you can use and what they represent are listed here:</p> <pre><code>0: No permission\n1: Execute permission\n2: Write permission\n3: Write and execute permissions\n4: Read permission\n5: Read and execute permissions\n6: Read and write permissions\n7: Read, write and execute permissions\n</code></pre> <p>Looking at our <code>example.txt</code> file, we can see that all three sets of characters are <code>rwx</code>. That means everyone has read, write and execute rights with the file.</p> <p>To set the permission to be read, write, and execute (7 from our list) for the owner; read and write (6 from our list) for the group; and read and execute (5 from our list) for the others we\u2019d need to use the digits 765 with the chmod command: <pre><code>chmod -R 765 example.txt\nchmod command in a terminal window\n</code></pre> To set the permission to be read, write and execute (7 from our list) for the owner, and read and write (6 from our list) for the group and for the others we\u2019d need to use the digits 766 with the chmod command:</p> <pre><code>chmod 766 example.txt\n</code></pre>"},{"location":"technical_stuff/linux/linuxCommands/#chown","title":"chown","text":"<p>The <code>chown</code> command allows you to change the owner and group owner of a file. Listing our <code>example.txt</code> file with <code>ls -l</code> we can see <code>amar amar</code> in the file description. </p> <ol> <li>The first of these indicates the name of the file owner, which in this case is the user amar. </li> <li>The second entry shows that the name of the group owner is also amar.  </li> </ol> <p>Each user has a default group created when the user is created. That user is the only member of that group. This shows that the file is not shared with any other groups of users.</p> <p>Tip</p> <p>You can use <code>chown</code> to change the owner or group, or both of a file. You must provide the name of the owner and the group, separated by a : character. You will need to use sudo. To retain amar as the owner of the file but to set mary as the group owner, use this command: <pre><code>sudo chown amar:mary example.txt\n</code></pre></p> <p>To change both the owner and the group owner to mary, you would use the following command;</p> <pre><code>sudo chown mary:mary example.txt\n</code></pre> <p>To change the file so that amar is once more the file owner and the group owner, use this command:</p> <pre><code>sudo chown amar:amar example.txt\n</code></pre>"},{"location":"technical_stuff/linux/linuxCommands/#curl","title":"curl","text":"<p>The <code>curl</code> command is a tool to retrieve information and files from Uniform Resource Locators (URLs) or internet addresses.</p> <p>Not able to find curl on your system?</p> <p>The curl command may not be provided as a standard part of your Linux distribution. Use <code>apt-get</code> to install this package onto your system if you\u2019re using Ubuntu or another Debian-based distribution. On other Linux distributions, use your Linux distribution\u2019s package management tool instead.</p> <pre><code>sudo apt-get install curl\n</code></pre> <p>Suppose you want to retrieve a single file from a GitHub repository. There is no officially supported way to this. You\u2019re forced to clone the entire repository. With curl however, we can retrieve the file we want on its own.</p> <p>This command retrieves the file for us. Note that you need to specify the name of the file to save it in, using the <code>-o</code> (output) option. </p> <p>Tip</p> <p>If you do not do this, the contents of the file are scrolled rapidly in the terminal window but not saved to your computer.</p> <pre><code>curl https://raw.githubusercontent.com/torvalds/linux/master/kernel/events/core.c -o core.c\n</code></pre> <p>If you don\u2019t want to see the download progress information use the <code>-s</code> (silent) option.</p> <pre><code>curl -s https://raw.githubusercontent.com/torvalds/linux/master/kernel/events/core.c -o core.c\n</code></pre>"},{"location":"technical_stuff/linux/linuxCommands/#df","title":"df","text":"<p>The <code>df</code> command shows the size, used space, and available space on the mounted filesystems of your computer.</p> <p>Two of the most useful options are the <code>-h</code> (human readable) and <code>-x</code> (exclude) options. The human-readable option displays the sizes in Mb or Gb instead of in bytes. The exclude option allows you to tell <code>df</code> to discount filesystems you are not interested in. For example, the squashfs pseudo-filesystems that are created when you install an application with the snap command.</p> <pre><code>df -h -x squashfs\n</code></pre>"},{"location":"technical_stuff/linux/linuxCommands/#diff","title":"diff","text":"<p>The <code>diff</code> command compares two text files and shows the differences between them. There are many options to tailor the display to your requirements.</p> <p>The <code>-y</code> (side by side) option shows the line differences side by side. The <code>-w</code> (width) option lets you specify the maximum line width to use to avoid wraparound lines. The two files are called <code>alpha1.txt</code> and <code>alpha2.txt</code> in this example. The <code>--suppress-common-lines</code> prevents diff from listing the matching lines, letting you focus on the lines which have differences.</p> <pre><code>diff -y -W 70 alpha1.txt alpha2.txt --suppress-common-lines\n</code></pre>"},{"location":"technical_stuff/linux/linuxCommands/#echo","title":"echo","text":"<p>The echo command prints (echoes) a string of text to the terminal window.</p> <p>The command below will print the words \u201cA string of text\u201d on the terminal window.</p> <pre><code>echo A string of text\n</code></pre> <p>The echo command can show the value of environment variables, for example, the $USER, $HOME, and $PATH environment variables. These hold the values of the name of the user, the user\u2019s home directory, and the path searched for matching commands when the user types something on the command line.</p> <pre><code>echo $USER\necho $HOME\necho $PATH\n</code></pre> <p>The following command will cause a bleep to be issued. The -e (escape code) option interprets the escaped a character as a \u2018bell\u2019 character.</p> <pre><code>echo -e \"\\a\"\n</code></pre> <p>The <code>echo</code> command is also invaluable in shell scripts. A script can use this command to generate visible output to indicate the progress or results of the script as it is executed.</p>"},{"location":"technical_stuff/linux/linuxCommands/#exit","title":"exit","text":"<p>The exit command will close a terminal window, end the execution of a shell script, or log you out of an SSH remote access session.</p> <pre><code>exit\n</code></pre>"},{"location":"technical_stuff/linux/linuxCommands/#find","title":"find","text":"<p>Use the find command to track down files that you know exist if you can\u2019t remember where you put them. You must tell find where to start searching from and what it is looking for. In this example, the . matches the current folder and the -name option tells find to look for files with a name that matches the search pattern.</p> <p>You can use wildcards, where <code>*</code> represents any sequence of characters and <code>?</code> represents any single character. We\u2019re using ones to match any file name containing the sequence \u201cones.\u201d This would match words like bones, stones, and lonesome.</p> <pre><code>find . -name *ones*\nfind command in a terminal window\n</code></pre> <p>As we can see, find has returned a list of matches. One of them is a directory called Ramones. We can tell find to restrict the search to files only. We do this using the -type option with the f parameter. The f parameter stands for files.</p> <p><pre><code>find . -type f -name *ones*\n</code></pre> If you want the search to be case insensitive use the -iname (insensitive name) option.</p> <pre><code>find . -iname *wild*\n</code></pre>"},{"location":"technical_stuff/linux/linuxCommands/#finger","title":"finger","text":"<p>The finger command gives you a short dump of information about a user, including the time of the user\u2019s last login, the user\u2019s home directory, and the user account\u2019s full name.</p>"},{"location":"technical_stuff/linux/linuxCommands/#free","title":"free","text":"<p>The free command gives you a summary of the memory usage with your computer. It does this for both the main Random Access Memory (RAM) and swap memory. The -h (human) option is used to provide human-friendly numbers and units. Without this option, the figures are presented in bytes.</p> <pre><code>free -h\n</code></pre>"},{"location":"technical_stuff/linux/linuxCommands/#grep","title":"grep","text":"<p>The grep utility searches for lines which contain a search pattern. When we looked at the alias command, we used grep to search through the output of another program, ps . The grep command can also search the contents of files. Here we\u2019re searching for the word \u201ctrain\u201d in all text files in the current directory.</p> <pre><code>grep train *.txt\n</code></pre> <p>The output lists the name of the file and shows the lines that match. The matching text is highlighted.</p> <p>grep command in a terminal window</p> <p>The functionality and sheer usefulness of grep definitely warrants you checking out its man page.</p>"},{"location":"technical_stuff/linux/linuxCommands/#groups","title":"groups","text":"<p>The groups command tells you which groups a user is a member of.</p> <pre><code>groups dave\ngroups mary\n</code></pre>"},{"location":"technical_stuff/linux/linuxCommands/#gzip","title":"gzip","text":"<p>The gzip command compresses files. By default, it removes the original file and leaves you with the compressed version. To retain both the original and the compressed version, use the -k (keep) option.</p> <pre><code>gzip -k core.c\n</code></pre>"},{"location":"technical_stuff/linux/linuxCommands/#head","title":"head","text":"<p>The head command gives you a listing of the first 10 lines of a file. If you want to see fewer or more lines, use the -n (number) option. In this example, we use head with its default of 10 lines. We then repeat the command asking for only five lines.</p> <pre><code>head -core.c\nhead -n 5 core.c\n</code></pre>"},{"location":"technical_stuff/linux/linuxCommands/#history","title":"history","text":"<p>The history command lists the commands you have previously issued on the command line. You can repeat any of the commands from your history by typing an exclamation point ! and the number of the command from the history list.</p> <p><pre><code>!188\n</code></pre> Typing two exclamation points repeats your previous command.</p> <pre><code>!!\n</code></pre>"},{"location":"technical_stuff/linux/linuxCommands/#kill","title":"kill","text":"<p>The kill command allows you to terminate a process from the command line. You do this by providing the process ID (PID) of the process to kill. Don\u2019t kill processes willy-nilly. You need to have a good reason to do so. In this example, we\u2019ll pretend the shutter program has locked up.</p> <p>To find the PID of shutter we\u2019ll use our ps and grep trick from the section about the alias command, above. We can search for the shutter process and obtain its PID as follows: <pre><code>ps -e | grep shutter.\n</code></pre> Once we have determined the PID\u20141692 in this case\u2014we can kill it as follows:</p> <p><pre><code>kill 1692\n</code></pre> kill command in a terminal window</p>"},{"location":"technical_stuff/linux/linuxCommands/#less","title":"less","text":"<p>The less command allows you to view files without opening an editor. It\u2019s faster to use, and there\u2019s no chance of you inadvertently modifying the file. With less you can scroll forward and backward through the file using the Up and Down Arrow keys, the PgUp and PgDn keys and the Home and End keys. Press the Q key to quit from less.</p> <p>To view a file provide its name to less as follows:</p> <p><pre><code>less core.c\n</code></pre> less command in a terminal window</p> <p>You can also pipe the output from other commands into less. To see the output from ls for a listing of your entire hard drive, use the following command:</p> <pre><code>ls -R / | less\n</code></pre> <p>less command in a terminal window</p> <p>Use / to search forward in the file and use ? to search backward.</p>"},{"location":"technical_stuff/linux/linuxCommands/#ls","title":"ls","text":"<p>This might be the first command the majority of Linux users meet. It lists the files and folders in the directory you specify. By default, ls looks in the current directory. There are a great many options you can use with ls , and we strongly advise reviewing its the man page. Some common examples are presented here.</p> <p>To list the files and folders in the current directory:</p> <pre><code>ls\n</code></pre> <p>To list the files and folders in the current directory with a detailed listing use the -l (long) option: <pre><code>ls -l\n</code></pre> To use human-friendly file sizes include the -h (human) option:</p> <p><pre><code>ls -lh\n</code></pre> To include hidden files use the -a (all files) option:</p> <p><pre><code>ls -lha\n</code></pre> ls command in a terminal window</p>"},{"location":"technical_stuff/linux/linuxCommands/#man","title":"man","text":"<p>The man command displays the \u201cman pages\u201d for a command in less . The man pages are the user manual for that command. Because man uses less to display the man pages, you can use the search capabilities of less.</p> <p>For example, to see the man pages for chown, use the following command:</p> <p><pre><code>man chown\n</code></pre> Use the Up and Down arrow or PgUp and PgDn keys to scroll through the document. Press q to quit the man page or pressh for help.</p> <p>man command in a terminal window</p>"},{"location":"technical_stuff/linux/linuxCommands/#mkdir","title":"mkdir","text":"<p>The mkdir command allows you to create new directories in the filesystem. You must provide the name of the new directory to mkdir. If the new directory is not going to be within the current directory, you must provide the path to the new directory.</p> <p>To create two new directories in the current directory called \u201cinvoices\u201d and \u201cquotes,\u201d use these two commands:</p> <pre><code>mkdir invoices\nmkdir quotes\n</code></pre> <p>To create a new directory called \u201c2019\u201d inside the \u201cinvoices\u201d directory, use this command:</p> <pre><code>mkdir invoices/2109\n</code></pre> <p>If you are going to create a directory, but its parent directory does not exist, you can use the -p (parents) option to have mkdir create all of the required parent directories too. In the following command, we are creating the \u201c2019\u201d directory inside the \u201cyearly\u201d directory inside the \u201cquotes\u201d directory. The \u201cyearly\u201d directory does not exist, but we can have mkdir create all the specified directories at once:</p> <p><pre><code>mkdir -p quotes/yearly/2019\n</code></pre> The \u201cyearly\u201d directory is also created.</p>"},{"location":"technical_stuff/linux/linuxCommands/#mv","title":"mv","text":"<p>The mv command allows you to move files and directories from directory to directory. It also allows you to rename files.</p> <p>To move a file you must tell mv where the file is and where you want it to be moved to. In this example, we\u2019re moving a file called apache.pdf from the <code>~/Document/Ukulele</code> directory and placing it in the current directory, represented by the single <code>.</code> character.</p> <p><pre><code>mv ~/Documents/Ukulele/Apache.pdf .\n</code></pre> To rename the file, you \u201cmove\u201d it into a new file with the new name.</p> <pre><code>mv Apache.pdf The_Shadows_Apache.pdf\n</code></pre> <p>The file move and rename action could have been achieved in one step:</p> <pre><code>mv ~/Documents/Ukulele/Apache.pdf ./The_Shadows_Apache.pdf\n</code></pre>"},{"location":"technical_stuff/linux/linuxCommands/#passwd","title":"passwd","text":"<p>The <code>passwd</code> command lets you change the password for a user. Just type passwd to change your own password.</p> <p>You can also change the password of another user account, but you must use sudo. You will be asked to enter the new password twice.</p> <pre><code>sudo passwd mary\n</code></pre>"},{"location":"technical_stuff/linux/linuxCommands/#ping","title":"ping","text":"<p>The ping command lets you verify that you have network connectivity with another network device. It is commonly used to help troubleshoot networking issues. To use ping, provide the IP address or machine name of the other device.</p> <pre><code>ping 192.168.4.18\n</code></pre> <p>The ping command will run until you stop it with <code>Ctrl+C</code>.</p> <p>Here\u2019s what\u2019s going on here:</p> <ul> <li>The device at IP address 192.168.4.18 is responding to our ping requests and is sending back packets of 64 bytes.</li> <li>The Internet Control Messaging Protocol (ICMP) sequence numbering allows us to check for missed responses (dropped packets).</li> <li>The TTL figure is the \u201ctime to live\u201d for a packet. Each time the packet goes through a router, it is (supposed to be) decremented by one. If it reaches zero the packet is thrown away. The aim of this is to prevent network loopback problems from flooding the network.</li> <li>The time value is the duration of the round trip from your computer to the device and back. Simply put, the lower this time, the better.</li> <li>To ask ping to run for a specific number of ping attempts, use the <code>-c</code> (count) option.</li> </ul> <p><pre><code>ping -c 5 192.168.4.18\n</code></pre> To hear a ping, use the <code>-a</code> (audible) option.</p> <pre><code>ping -a 192.168.4.18\n</code></pre>"},{"location":"technical_stuff/linux/linuxCommands/#ps","title":"ps","text":"<p>The ps command lists running processes. Using ps without any options causes it to list the processes running in the current shell.</p> <pre><code>ps\n</code></pre> <p>To see all the processes related to a particular user, use the -u (user) option. This is likely to be a long list, so for convenience pipe it through less.</p> <pre><code>ps -u dave | less\n</code></pre> <p>To see every process that is running, use the -e (every process) option:</p> <pre><code>ps -e | less\n</code></pre>"},{"location":"technical_stuff/linux/linuxCommands/#pwd","title":"pwd","text":"<p>Nice and simple, the pwd command prints the working directory (the current directory) from the root / directory.</p> <pre><code>pwd\n</code></pre> <p>pwd command in a terminal window</p>"},{"location":"technical_stuff/linux/linuxCommands/#shutdown","title":"shutdown","text":"<p>The shutdown command lets you shut down or reboot your Linux system.</p> <p>Using shutdown with no parameters will shut down your computer in one minute.</p> <pre><code>shutdown\n</code></pre> <p>To shut down immediately, use the now parameter.</p> <pre><code>shutdown now\n</code></pre> <p>You can also schedule a shutdown and inform any logged in users of the pending shutdown. To let the shutdown command know when you want it to shut down, you provide it with a time. This can be a set number of minutes from now, such as +90 or a precise time, like 23:00. Any text message you provide is broadcast to logged in users.</p> <pre><code>shutdown 23:00 Shutdown tonight at 23:00, save your work and log out before then!\n</code></pre> <p>To cancel a shutdown, use the <code>-c</code> (cancel) option. Here we have scheduled a shutdown for fifteen minutes time from now\u2014and then changed our minds.</p> <pre><code>shutdown +15 Shutting down in 15 minutes!\nshutdown -c\n</code></pre> <p>Shutdown -c cancel command</p>"},{"location":"technical_stuff/linux/linuxCommands/#ssh","title":"SSH","text":"<p>Use the ssh command to make a connection to a remote Linux computer and log into your account. To make a connection, you must provide your user name and the IP address or domain name of the remote computer. In this example, the user mary is logging into the computer at 192.168.4.23. Once the connection is established, she is asked for her password.</p> <pre><code>ssh mary@192.168.4.23\nssh command in a terminal window\n</code></pre> <p>Her user name and password are verified and accepted, and she is logged in. Notice that her prompt has changed from \u201cNostromo\u201d to \u201chowtogeek.\u201d</p> <p>Mary issues the w command to list the current users on \u201chowtogeek\u201d system. She is listed as being connected from pts/1, which is a pseudo-terminal slave. That is, it is not a terminal directly connected to the computer.</p> <p>To close the session, mary types exit and is returned to the shell on the \u201cNostromo\u201d computer.</p> <p><code>w</code> <code>exit</code> w and exit commands in a terminal window</p>"},{"location":"technical_stuff/linux/linuxCommands/#sudo","title":"sudo","text":"<p>The sudo command is required when performing actions that require root or superuser permissions, such as changing the password for another user.</p> <pre><code>sudo passwd mary\n</code></pre> <p>passwd command in a terminal window</p>"},{"location":"technical_stuff/linux/linuxCommands/#tail","title":"tail","text":"<p>The tail command gives you a listing of the last 10 lines of a file. If you want to see fewer or more lines, use the -n (number) option. In this example, we use tail with its default of 10 lines. We then repeat the command asking for only five lines.</p> <pre><code>tail core.c\ntail -n 5 core.c\ntail command in a terminal window\n</code></pre>"},{"location":"technical_stuff/linux/linuxCommands/#tar","title":"tar","text":"<p>With the tar command, you can create an archive file (also called a tarball) that can contain many other files. This makes it much more convenient to distribute a collection of files. You can also use tar to extract the files from an archive file. It is common to ask tar to compress the archive. If you do not ask for compression, the archive file is created uncompressed.</p> <p>To create an archive file, you need to tell tar which files to include in the archive file, and the name you wish the archive file to have.</p> <p>In this example, the user is going to archive all of the files in the Ukulele directory, which is in the current directory.</p> <p><code>ls</code> command in the terminal window</p> <p>They have used the <code>-c</code> (create) option and the <code>-v</code> (verbose) option. The verbose option gives some visual feedback by listing the files to the terminal window as they are added to the archive. The <code>-f</code> (filename) option is followed by the desired name of the archive. In this case, it is songs.tar.</p> <pre><code>tar -cvf songs.tar Ukulele/\n</code></pre> <p><code>tar -cvf</code> command in a terminal window</p> <p>The files are listed to the terminal window as they are added to the archive file.</p> <p>There are two ways to tell tar that you want the archive file to be compressed. The first is with the -z (gzip) option. This tells tar to use the gzip utility to compress the archive once it has been created.</p> <p>It is usual to add <code>.gz</code> as suffix to this type of archive. That allows anyone who is extracting files from it to know which commands to pass to tar to correctly retrieve the files.</p> <pre><code>tar -cvzf songs.tar.gz Ukulele/\n</code></pre> <p>The files are listed to the terminal window as they are added to the archive file as before, but the creation of the archive will take a little longer because of the time required for the compression.</p> <p>To create an archive file that is compressed using a superior compression algorithm giving a smaller archive file use the -j (bzip2) option.</p> <pre><code>tar -cvjf songs.tar.bz2 Ukulele/\n</code></pre> <p>Once again, the files are listed as the archive is created. The -j option is noticeably slower than the -z option.</p> <p>If you are archiving a great many files, you must choose between the -z option for decent compression and reasonable speed, or the -j option for better compression and slower speed.</p> <p>As can be seen in the screenshot below, the \u201c.tar\u201d file is the largest, the \u201c.tar.gz\u201d is smaller, and the \u201c.tar.bz2\u201d is the smallest of the archives.</p> <p>To extract files from an archive file use the -x (extract) option. The -v (verbose) and -f (filename) options behave as they do when creating archives. Use ls to confirm which type of archive you are going to extract the files from, then issue the following command.</p> <pre><code>ls\ntar -xvf songs.tar\n</code></pre> <p>ls and tar -xvf commands in a terminal window</p> <p>The files are listed as they are extracted. Note that the Ukulele directory is also recreated for you.</p> <p>To extract files from a \u201c.tar.gz\u201d archive, use the -z (gzip) option.</p> <p><pre><code>tar -xvzf songs.tar.gz\n</code></pre> tar -xvzf command in a terminal window</p> <p>Finally, to extract files from a \u201c.tar.bz2\u201d archive use the -j option instead of the -z (gzip) option.</p> <p><pre><code>tar -xvjf songs.tar.bz2\n</code></pre> tar -xvjf command in a terminal window</p> <p>RELATED: How to Extract Files From a .tar.gz or .tar.bz2 File on Linux</p>"},{"location":"technical_stuff/linux/linuxCommands/#top","title":"top","text":"<p>The top command shows you a real-time display of the data relating to your Linux machine. The top of the screen is a status summary.</p> <p>The first line shows you the time and how long your computer has been running for, how many users are logged into it, and what the load average has been over the past one, five, and fifteen minutes.</p> <p>The second line shows the number of tasks and their states: running, stopped, sleeping and zombie.</p> <p>The third line shows CPU information. Here\u2019s what the fields mean:</p> <p><pre><code>us: value is the CPU time the CPU spends executing processes for users, in \u201cuser space\u201d\nsy: value is the CPU time spent on running system \u201ckernel space\u201d processes\nni: value is the CPU time spent on executing processes with a manually set nice value\nid: is the amount of CPU idle time\nwa: value is the time the CPU spends waiting for I/O to complete\nhi: The CPU time spent servicing hardware interrupts\nsi: The CPU time spent servicing software interrupts\nst: The CPU time lost due to running virtual machines (\u201csteal time\u201d)\n</code></pre> The fourth line shows the total amount of physical memory, and how much is free, used and buffered or cached.</p> <p>The fifth line shows the total amount of swap memory, and how much is free, used and available  (taking into account memory that is expected to be recoverable from caches).</p> <p>top command in a terminal window</p> <p>The user has pressed the E key to change the display into more humanly digestible figures instead of long integers representing bytes.</p> <p>The columns in the main display are made up of:</p> <pre><code>PID: Process ID\nUSER: Name of the owner of the process\nPR: Process priority\nNI: The nice value of the process\nVIRT: Virtual memory used by the process\nRES: Resident memory used by the process\nSHR: Shared memory used by the process\nS: Status of the process. See the list below of the values this field can take\n%CPU: the share of CPU time used by the process since last update\n%MEM: share of physical memory used\nTIME+: total CPU time used by the task in hundredths of a second\nCOMMAND: command name or command line (name + options)\n</code></pre> <p>The status of the process can be one of:</p> <p><pre><code>D: Uninterruptible sleep\nR: Running\nS: Sleeping\nT: Traced (stopped)\nZ: Zombie\nPress the Q key to exit from top.\n</code></pre> RELATED: How to Set Process Priorities With nice and renice on Linux</p>"},{"location":"technical_stuff/linux/linuxCommands/#uname","title":"uname","text":"<p>You can obtain some system information regarding the Linux computer you\u2019re working on with the uname command.</p> <ul> <li>Use the <code>-a</code> (all) option to see everything.</li> <li>Use the <code>-s</code> (kernel name) option to see the type of kernel.</li> <li>Use the <code>-r</code> (kernel release) option to see the kernel release.</li> <li>Use the <code>-v</code> (kernel version) option to see the kernel version.</li> </ul> <pre><code>uname -a\nuname -s\nuname -r\nuname -v\n</code></pre> <p>uname command in a terminal window</p>"},{"location":"technical_stuff/linux/linuxCommands/#w","title":"w","text":"<p>The w command lists the currently logged in users.</p> <p><pre><code>w\n</code></pre> w command in a terminal window</p>"},{"location":"technical_stuff/linux/linuxCommands/#whoami","title":"whoami","text":"<p>Use <code>whoami</code> to find out who you are logged in as or who is logged into an unmanned Linux terminal.</p>"},{"location":"technical_stuff/linux/java_home/","title":"Java Home","text":"<p>If you are getting <code>java_home is not defined correctly in Ubuntu</code>, then you are at right place.</p> <p>I was getting this error as I was trying to update the Java home in <code>~\\.bashrc</code> and <code>~\\.bash_profile</code>. Follow these steps to solve this issue. </p> <ol> <li> <p>Open the <code>/etc/environment</code> file as</p> <pre><code>vim /etc/environment\n</code></pre> </li> <li> <p>Then set <code>java_home</code> using</p> <pre><code>JAVA_HOME=\"/usr/lib/jvm/java-8-oracle\"\nexport JAVA_HOME\n</code></pre> </li> <li> <p>Open bash profile as</p> <pre><code>vim ~\\.bash_profile\n</code></pre> </li> <li> <p>Then add the following,</p> <pre><code>.  /etc/environment\n</code></pre> <p>It will load the <code>/etc/environment</code> every time terminal is started</p> </li> <li> <p>Then confirm the path using</p> <pre><code>echo $JAVA_HOME\n</code></pre> </li> </ol>"},{"location":"technical_stuff/maven/downgrade_maven/","title":"Downgrading maven version in brew","text":"<p>Check current version of <code>maven</code> using</p> <pre><code>mvn -verison\n</code></pre> <p>This will give you current version and the installation path as well. In my case current version is 3.5.0 as shown below</p> <p></p> <p>Then search for available versions of maven using</p> <pre><code>brew search maven\n</code></pre> <p>This will show you all the available versions as shown below</p> <p></p> <p>If you want to install maven 3.0 for instance, then type</p> <pre><code>brew install maven@3.0\n</code></pre> <p>This will install the downgraded version and show some caveats as well</p> <p></p> <p>Now the next step is to unlink older maven version and then overwrite it with version you just installed</p> <pre><code>brew unlink maven\nbrew link --overwrite maven@3.0\n</code></pre> <p>Now check version of maven again. if everything went well ,then you will see that version has been downgraded.</p> <p></p>"},{"location":"technical_stuff/mysql/JdbcStatements/","title":"Difference between Prepared Statement and Statement in Java","text":"<p>JDBC API provides 3 types of statements for wrapping an SQL query and sending for execution to the database.</p> <ol> <li>Statement: It is used to execute normal SQL queries such as <code>select count(*) from Courses</code>. You can also use it to execute DDL, DML and DCL SQL statements. </li> <li>Prepared Statement: is specialized to execute parameterized queries such as <code>select * from Courses where courseId=?</code>, you can execute this SQL multiple times by just changing the course id parameters. They are compiled and cached at database end, hence quite fast for repeated execution.</li> <li>Callable Statement: it is used to execute or call stored procedures stored in the database.</li> </ol> <p>Each of the Statement class has a different purpose and you should use them for what they have designed for. It's very important to understand what they are and what is their purpose, along with how to use it correctly.</p> <p>In this article, we will focus on understanding the difference between <code>Statement</code> and <code>Prepared Statement</code> by asking the below questions.</p>"},{"location":"technical_stuff/mysql/JdbcStatements/#1-which-one-to-use-and-when","title":"1. Which one to use and when?","text":"<p>Prepared Statement's sole purpose is to execute bind queries. If you need to execute a query multiple times with just different data then use <code>Prepared Statement</code> and use a placeholder, the question mark sign (?) for the variable data.</p> <p>Why it is called a prepared statement?</p> <p>When you first execute the prepared SQL query, the database will compile it and cache it for future reuse, next time you call the same query but with a different parameter, then the database will return the result almost immediately. Because of this pre-compilation, this class is called Prepared Statement in Java.</p> <p>It's very useful to build search and insert queries e.g. if your application provides an interface to search some data such as course details, let's say by course, name, instructor, price, or topic. You can create <code>Prepared Statement</code> to handle that for better performance.</p> <p>On the other hand, the sole purpose of <code>Statement</code> object is to execute a SQL query. You give them any query and it will execute it, but unlike <code>Prepared Statement</code>, it will not provide pre-compilation.</p>"},{"location":"technical_stuff/mysql/JdbcStatements/#2-syntax-for-these-statements","title":"2. Syntax for these statements?","text":"<p>The syntax for <code>Statement</code> is same as SQL query, you can actually copy SQL from your favorite SQL editor and pass it as String to Statement for execution, but for <code>Prepared Statement</code>, you need to include placeholders i.e. questions mark (?) sign in SQL query e.g.</p> <p><pre><code>select count(*) from Books;             //  Statement \nselect * from Books where book_id=?;    // Prepared Statement\n</code></pre> The actual value is set before executing the query at runtime by using the various <code>setXXX()</code> methods e.g. if placeholder refers to a <code>varchar</code> column then you can use <code>setString(value)</code> to set the value. Similarly, if placeholder refers to an integer column then you can use <code>setInteger(value)</code> method.</p>"},{"location":"technical_stuff/mysql/JdbcStatements/#3-which-has-better-performance","title":"3. Which has better performance?","text":"<p><code>Prepared Statement</code> provides better performance than <code>Statement</code> object because of pre-compilation of SQL query on the database server. This is because when you use Prepared Statement, the query is compiled the first time but after that it is cached at the database server, making subsequent run faster.</p> <p>On the other hand, with the Statement object, even if you execute the same query again and again, they are always first compiled and then executed, making them slower compared to <code>Prepared Statement</code> queries.</p>"},{"location":"technical_stuff/mysql/JdbcStatements/#4-which-one-is-more-secure","title":"4. Which one is more secure?","text":"<p>The <code>Prepared Statement</code> also provides safety against SQL injection, but the incorrect use of Statement can cause SQL injection. If you remember, the cause of SQL injection is malicious SQL code which is injected by malicious users. For example, you could have written above query which returns a book after passing Id as below:</p> <pre><code>String id = getFromUser();\nString SQL = \"select * from Books where book_id=\" + id;\n</code></pre> <p>If you pass this SQL to Statement object then it can cause SQL injection if a user sends malicious SQL code in form of id e.g.  <code>1 == 1</code> OR id, which will return every single book from the database. Though books, may not sound a sensitive data it could happen with any sensitive user data as well. <code>Prepared Statement</code> guards against this.</p> <p>That's all about the difference between Statement and <code>Prepared Statement</code> in Java. You can use Statement to execute SQL queries but it's not recommended, especially if you can use <code>Prepared Statement</code>, which is more secure and fast approach to get the data from the database. If you have to pass parameters always use PreparedStatment, never create dynamic SQL queries by concatenating String, it's not safe and prone to SQL injection attack.</p>"},{"location":"technical_stuff/mysql/mysql_basics/","title":"Mysql 101","text":""},{"location":"technical_stuff/mysql/mysql_basics/#installing-mysql","title":"Installing MySQL","text":"<pre><code>brew install mysql\n</code></pre> <p>Now, you have installed your MySQL database without a root password. To secure it run the following</p> <pre><code>mysql_secure_installation\n</code></pre>"},{"location":"technical_stuff/mysql/mysql_basics/#connecting-to-mysql","title":"Connecting to MySQL","text":"<p>Enter the below command <pre><code>mysql -u root\n</code></pre></p> <p>Use brew to manage MySQL using following commands</p> <pre><code>brew services start mysql   # This will register to run it at bot\nbrew services run           # Run the service formula without registering to launch at login\nbrew services stop          # Stop service immediately and unregister it\n</code></pre> <p>To run as background process, by default run the following </p> <pre><code>mysql.server start  # start MySQL\nmysql.server status # check status\nmysql.server stop   # stop the server\n</code></pre> <p>To reset the password for MySQL you first must create a new file with the following contents</p> <pre><code>ALTER USER 'root'@'localhost' IDENTIFIED BY 'PASSWORD';\n</code></pre> <p>Now, login using the password by using the below command</p> <pre><code>mysql -u root -p\n</code></pre>"},{"location":"technical_stuff/mysql/mysql_basics/#forgot-mysql-password","title":"Forgot MySQL password?","text":"<p>perform the following steps</p> <ol> <li>Stop the server</li> <li>Start server using <code>sudo mysqld_safe -skip-grant-tables -skip-netwroking &amp;</code></li> <li>Connect to server using root as <code>mysql -u root</code>. Now, run the following commands in terminal     <pre><code>use mysql;\n\u200bupdate user set authentication_string=password('NEWPASSWORD') where user='root';\n\u200bflush privileges;\n\u200bquit\n</code></pre></li> <li>Login to MySQL server again and enter the new password.</li> </ol>"},{"location":"technical_stuff/mysql/mysql_basics/#change-database-using","title":"change database using","text":"<pre><code>show databases;\nuse yourDatabaseName; # replace yourDatabaseName with your database name\nshow tables; \n</code></pre>"},{"location":"technical_stuff/mysql/mysql_basics/#downloading-using-gui-preferred-option","title":"Downloading using GUI (preferred option)","text":"<p>Go to https://dev.mysql.com/downloads/mysql/ and download MySQL for your OS.</p>"},{"location":"technical_stuff/wget/wget/","title":"WGET","text":"<p>Mac provides <code>curl</code>, however sometime using <code>wget</code> is more handy. This post is for installing <code>wget</code> for Mac. by following the below mentined steps:</p>"},{"location":"technical_stuff/wget/wget/#method-1","title":"Method 1","text":"<p>In most of tutorials, installing <code>wget</code> using <code>curl</code> is recommended . However I ran into several issues using this approach. I will share it with you guys. First you need to install the <code>xcode command line tools</code>, easiest way to do so by running in the Terminal</p> <pre><code>xcode-select --install\n</code></pre> <ol> <li>Download <code>wget</code> using <code>curl</code></li> </ol> <pre><code>curl -O http://ftp.gnu.org/gnu/wget/wget-1.15.tar.gz\n</code></pre> <ol> <li>Unpack it using</li> </ol> <pre><code>tar -zxvf wget-1.15.tar.gz\n</code></pre> <ol> <li>cd to folder using</li> </ol> <p><pre><code>cd wget-1.15/\n</code></pre> 4. Then configure</p> <pre><code>./configure\n</code></pre> <p>I have got following error that</p> <p>Error</p> <p><code>configure: error: --with-ssl=gnutls was given, but GNUTLS is not available.</code></p> <p>we can skip this error using</p> <pre><code>./configure --with-ssl=openssl\n</code></pre> <p>However, this also leads to another error shown below</p> <pre><code>configure: error: --with-ssl=openssl was given, but SSL is not available\n</code></pre>"},{"location":"technical_stuff/wget/wget/#method-2","title":"Method 2","text":"<ol> <li>We  can easily go around aforementioned error as <code>brew</code> will automatically install dependencies for<code>wget</code> and <code>openssl</code>. Gor doing this you must have <code>brew</code> installed</li> </ol> <pre><code>brew install wget\n</code></pre> <p>as shown below, it will automatically download the dependency</p> <pre><code>==&gt; Installing dependencies for wget: openssl\n==&gt; Installing wget dependency: openssl\n==&gt; Downloading https://homebrew.bintray.com/bottles/openssl-1.0.2j.sierra.bottle.tar.gz\n######################################################################## 100.0%\n==&gt; Pouring openssl-1.0.2j.sierra.bottle.tar.gz\n</code></pre> <ol> <li>For checking is <code>wget</code> is installed successfully, please <code>cd</code> to paricular folder and enter</li> </ol> <pre><code>wget -O sample.txt http://www.gutenberg.org/cache/epub/1787/pg1787.txt\n</code></pre> <p>text will be generated from link <code>http://www.gutenberg.org/cache/epub/1787/pg1787.txt</code> and saved as <code>sample.txt</code></p> <p>Success</p> <p>Congrats, we have successfully installed wget on mac.If you have some issues or suggestions, please feel free to comment  below.</p>"},{"location":"technical_stuff/wso2/h2/wso2/","title":"Accessing the H2 Database for WSO2 Products","text":"<p>Most of the WSO2 products comes with the H2 database, I have been facing some issues in order to access this database. You can follow approach A or B, in my opinion B is easier. Common step is a must</p>"},{"location":"technical_stuff/wso2/h2/wso2/#common-step","title":"Common Step","text":"<p>Open the <code>carbon.xml</code> file to enable the access to <code>h2</code> database.</p> <p></p> <p>As I am using the WSO2 IoT server, this file is located in the <code>IoT_HOME/conf/carbon.xml</code> as shown above. Open this file and uncomment the following</p> <pre><code>&lt;H2DatabaseConfiguration&gt;\n &lt;property name=\u201dweb\u201d /&gt;\n &lt;property name=\u201dwebPort\u201d&gt;8082&lt;/property&gt;\n &lt;property name=\u201dwebAllowOthers\u201d /&gt;\n &lt;property name=\u201dwebSSL\u201d /&gt;\n &lt;property name=\u201dtcp\u201d /&gt;\n &lt;property name=\u201dtcpPort\u201d&gt;9092&lt;/property&gt;\n &lt;property name=\u201dtcpAllowOthers\u201d /&gt;\n &lt;property name=\u201dtcpSSL\u201d /&gt;\n &lt;property name=\u201dpg\u201d /&gt;\n &lt;property name=\u201dpgPort\u201d&gt;5435&lt;/property&gt;\n &lt;property name=\u201dpgAllowOthers\u201d /&gt;\n &lt;property name=\u201dtrace\u201d /&gt;\n &lt;property name=\u201dbaseDir\u201d&gt;${carbon.home}&lt;/property&gt;\n &lt;/H2DatabaseConfiguration&gt;\n</code></pre>"},{"location":"technical_stuff/wso2/h2/wso2/#approach-a-using-web-browser","title":"Approach A: Using Web Browser","text":"<ol> <li> <p>Install the <code>H2</code> database: If you are Mac user, then enter <code>brew install h2</code> in terminal to install h2 database and then type <code>h2</code> in terminal to start it .The h2 database will start and can be accessed from <code>http://192.168.0.16:8082/</code>. If you are using the windows or Linux then install <code>h2</code> from here</p> </li> <li> <p>Then copy the path for the database you want to open, In my case the database path I want to access is shown below </p> </li> </ol> <p>Watch</p> <p>Do not copy the <code>h2.db</code> part of the file. For example path I copied is</p> <pre><code>/Users/amar/Documents/ThesisCode/CEP_codes/wso2iot-3.3.0_new/wso2/broker/repository/database/WSO2MB_DB\n</code></pre> <p>Now go the <code>http://192.168.0.16:8082/</code> and choose the <code>generic H2</code> then enter the JDBC url . Append the <code>jdbc:h2:file:</code> in front of path as shown in below screen-shot. Then enter <code>username</code> and <code>password</code> as <code>wso2carbon</code>. Press connect</p>"},{"location":"technical_stuff/wso2/h2/wso2/#approach-b-using-intellij-idea","title":"Approach B: Using IntelliJ IDEA","text":"<p>Now the same can be done in IntelliJ Idea which also provides access to databases. For accessing databases maybe you need a ultimate version of it, which is free for students under University account. Open IntelliJ Idea and click on add new database as shown below</p> <p></p> <p>Select <code>H2</code> . You have to install the H2 drivers for first time.</p> <p>Info</p> <p>Before going further, you have to close all existing connections to the <code>H2</code>, and stop <code>H2</code> if its running in terminal. </p> <p>As shown above, first choose Embedded database type from drop down menu, then browse your file using <code>...</code> option. Make sure the <code>h2.db</code> path does not contain <code>h2.db</code>. Enter <code>username</code> and <code>password</code> as <code>wso2carbon</code>. Thats it!</p> <p></p> <p>Hope, it helped \ud83d\ude00</p>"},{"location":"technical_stuff/wso2/jms/","title":"Setting up JMS Queue in WSO2 IoT Server","text":"<p>I was struggling for few days to get JMS queue up-and-running in <code>WSO2 IoT Server</code>. In my experience, setting the queue is easy in programming, but when it comes to complicated systems, sometime small mistakes can consume lot of time. So, without any further ado, lets get going. Please make sure the <code>WSO2 IoT server</code> is in off state. I will let you know when to turn it on.</p>"},{"location":"technical_stuff/wso2/jms/#knowing-the-configuration-file-paths","title":"Knowing the configuration file paths","text":"<p>The WSO2 IoT server consists of 3 tiers: <code>Broker</code>, <code>Core</code> and <code>Analytics</code> as shown below</p> <p></p> <p>Now, for using the JMS there are 3 options in WSO2 IoT server, which are</p> <ol> <li>Using Apache ActiveMQ</li> <li>Using Apache Qpid</li> <li>Using WSO2 Message Broker (MB)</li> </ol> <p>Out of these, the first option worked for me. The important thing to understand is that, we need to enable the ActiveMQ configurations for all three tiers i.e. Broker, Core and Analytics. The most confusing part is the file structure for configurations, as WSO2 has so many products that even most of paths mentioned in documentation got me confused. So, I would advice you to write the paths down or mark the folders using color coding. Path for IoT Core is shown below </p> <p>Mark both <code>.xml</code> files which are selected in fugure. Next, the path for IoT Analytics as shown below </p> <p>The path of <code>IoT broker</code> is in <code>wso2/broker/conf/axis2</code> similar to IoT analytics.</p>"},{"location":"technical_stuff/wso2/jms/#setting-up-the-configuration-for-jms","title":"Setting up the configuration for JMS","text":"<p>Once you know all the paths, lets make the required changes in <code>axis2.xml</code> file as shown below</p>"},{"location":"technical_stuff/wso2/jms/#enable-transport-receiver","title":"Enable Transport Receiver","text":"<p>Please note that JMS queue has 2 components: receiver and sender. First we will add the receiver part. If you open the <code>axis.xml</code> file and search for <code>jms</code> you will find the three blocks of code related to receiver part. Please uncomment the code related to ApacheMQ as shown below.</p> <p><pre><code>&lt;transportReceiver name=\"jms\" class=\"org.apache.axis2.transport.jms.JMSListener\"&gt;\n        &lt;parameter name=\"myTopicConnectionFactory\"&gt;\n         &lt;parameter name=\"java.naming.factory.initial\"&gt;org.apache.activemq.jndi.ActiveMQInitialContextFactory&lt;/parameter&gt;\n         &lt;parameter name=\"java.naming.provider.url\" locked=\"false\"&gt;failover:tcp://localhost:61616&lt;/parameter&gt;\n            &lt;parameter name=\"transport.jms.ConnectionFactoryJNDIName\" locked=\"false\"&gt;TopicConnectionFactory&lt;/parameter&gt;\n            &lt;parameter name=\"transport.jms.ConnectionFactoryType\" locked=\"false\"&gt;topic&lt;/parameter&gt;\n             &lt;property name=\"userName\" value=\"admin\"/&gt;\n             &lt;property name=\"password\" value=\"admin\"/&gt;\n        &lt;/parameter&gt;\n&lt;parameter name=\"myQueueConnectionFactory\"&gt;\n         &lt;parameter name=\"java.naming.factory.initial\"&gt;org.apache.activemq.jndi.ActiveMQInitialContextFactory&lt;/parameter&gt;\n         &lt;parameter name=\"java.naming.provider.url\" locked=\"false\"&gt;failover:tcp://localhost:61616&lt;/parameter&gt;\n            &lt;parameter name=\"transport.jms.ConnectionFactoryJNDIName\" locked=\"false\"&gt;QueueConnectionFactory&lt;/parameter&gt;\n            &lt;parameter name=\"transport.jms.ConnectionFactoryType\" locked=\"false\"&gt;topic&lt;/parameter&gt;\n             &lt;property name=\"userName\" value=\"admin\"/&gt;\n             &lt;property name=\"password\" value=\"admin\"/&gt;\n\n        &lt;/parameter&gt;\n&lt;parameter name=\"default\"&gt;\n         &lt;parameter name=\"java.naming.factory.initial\"&gt;org.apache.activemq.jndi.ActiveMQInitialContextFactory&lt;/parameter&gt;\n         &lt;parameter name=\"java.naming.provider.url\" locked=\"false\"&gt;failover:tcp://localhost:61616&lt;/parameter&gt;\n            &lt;parameter name=\"transport.jms.ConnectionFactoryJNDIName\" locked=\"false\"&gt;TopicConnectionFactory&lt;/parameter&gt;\n            &lt;parameter name=\"transport.jms.ConnectionFactoryType\" locked=\"false\"&gt;topic&lt;/parameter&gt;\n             &lt;property name=\"userName\" value=\"admin\"/&gt;\n             &lt;property name=\"password\" value=\"admin\"/&gt;\n\n        &lt;/parameter&gt;\n&lt;/transportReceiver&gt;\n</code></pre> As you have noticed, I have made some additional changes in given code, one of which is to add failover in front of <code>tcp://localhost:61616</code> and other is to add both <code>username</code> and <code>password</code>. The failover helps to reestablish the connection when connection breaks which often happen in real world scenarios.</p>"},{"location":"technical_stuff/wso2/jms/#enable-transport-sender","title":"Enable Transport Sender","text":"<pre><code>&lt;transportSender name=\"jms\" class=\"org.apache.axis2.transport.jms.JMSSender\"/&gt;\n</code></pre> <p>Un-comment the above code which is also available in file. Please make sure that there is no error when you un-comment the code. If you think there is some issue, please go ahead and copy paste the below code.</p>"},{"location":"technical_stuff/wso2/jms/#making-changes-in-axis2_clientxml-file","title":"Making Changes in axis2_client.xml file","text":"<p>Just enable the JMS sender by un-commenting the given sender code as shown below. There are no settings for JMS receiver in this file.</p> <pre><code>&lt;transportSender name=\"jms\"\n    class=\"org.apache.axis2.transport.jms.JMSSender\"/&gt;\n</code></pre>"},{"location":"technical_stuff/wso2/jms/#enable-receiver-and-sender-for-broker-core-and-analytics","title":"Enable Receiver and Sender for Broker, Core and Analytics","text":"<p>Yes, you have to enable aforementioned settings for both <code>axis2.xml</code> and <code>axis2_client.xml</code> for all three tiers except the exeption mentioned below. Take some time to make sure that there is no mistake while un-commenting the code.</p> <p>Exception</p> <p>Please dot not enable Transport Sender in IoT_HOME/conf/axis2.xml and IoT_HOME/conf/axis2_client.xml. However, enable Transport sender for all the tiers. A summarized view is given in below table. </p>"},{"location":"technical_stuff/wso2/jms/#download-the-required-dependencies","title":"Download the required dependencies.","text":"<p>Caveat</p> <p>The ActiveMQ libraries need to be available in various classpath folder of IoT server. You need to copy the jars from the lib folder of the ActiveMQ as shown below. </p> <p>In case you use the dependency manager such as brew or apt-get to install ActiveMQ, then please find the right version of the ActiveMQ and download the binary version to get jars or go to installation folder. As I used ActiveMQ version 5.5.1 , thus these are the required jars.</p> <p>Download these jar files form maven central</p> <ol> <li><code>activemq-core-5.5.1.jar</code></li> <li><code>axis2-transport-all-1.0.0.jar</code></li> <li><code>geronimo-j2ee-management_1.0_spec-1.0.1.jar</code></li> <li><code>geronimo-jms_1.1_spec-1.1.1.jar</code></li> </ol> <p>Place them in following 3 locations.</p> <ol> <li><code>wso2_home/lib</code></li> <li><code>wso2_home/wso2/lib</code></li> <li><code>wso2_home/wso2/components/lib</code></li> </ol> <p>Here, the <code>wso2_home</code> is the main installation directory for IoT server. Congrats, we are halfway thorough. Now is time to do some installation</p>"},{"location":"technical_stuff/wso2/jms/#install-and-start-activemq","title":"Install and start ActiveMQ","text":"<p>If you are mac user, then type <code>brew install activemq</code> to download ActiveMQ. Further, start the ActiveMQ using <code>activemq start</code> . Now, check status using <code>activemq status</code>. Please use your favorite package manager such as <code>apt-get</code> or <code>choco</code> to download the ActiveMQ, if you are using Linux or Windows respectively.</p> <p>Caveat</p> <p>If you run activemq start multiple times, it will kick off multiple JVMs to run the broker. However since you did not specify any unique broker configuration, each instance will use the same default broker configuration from <code>conf/activemq.xml</code>. And that typically means each broker instance will compete for the lock on the default <code>KahaDB</code> store. Only one instance will get the lock on that store and fully start up (the master broker), the other instances will continue to compete for the lock (slave brokers).</p>"},{"location":"technical_stuff/wso2/jms/#start-the-iot-server-ie-broker-core-and-analytics","title":"Start the IoT Server i.e. broker, core and analytics.","text":"<p>While the products are starting, run a find search in terminal for jms keyword. This way you will know if there is some issue related to JMS. Message broker should say that jms sender started as shown below </p>"},{"location":"technical_stuff/wso2/jms/#make-pipeline","title":"Make pipeline","text":"<p>First make the basic pipeline so that you have the streaming data arriving at IoT server to a particular stream say <code>stream A</code>. Now we will add the <code>stream A</code> to JMS queue publisher. Then we need the JMS receiver to get the data from JMS publisher and further sends it to a logger. Lets understand it by below figure.</p> <p></p> <p>As shows in the above figure the JMS publisher receives the <code>stream A</code> and en-queues the tuples/events in <code>queue1</code>. Further the JMS Subscriber subscribes to queue1 and send it to <code>Stream C</code> . Further <code>Stream C</code> is published to Logger publisher which shows the logs in Analytics terminal console .</p> <p>Setting for JMS publisher is given below</p> <pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;eventPublisher name=\"jms_pub\" processing=\"enable\" statistics=\"enable\"\n  trace=\"enable\" xmlns=\"http://wso2.org/carbon/eventpublisher\"&gt;\n  &lt;from streamName=\"stream1_scep_timestamped\" version=\"1.0.0\"/&gt;\n  &lt;mapping customMapping=\"disable\" type=\"json\"/&gt;\n  &lt;to eventAdapterType=\"jms\"&gt;\n    &lt;property name=\"transport.jms.DestinationType\"&gt;queue&lt;/property&gt;\n    &lt;property name=\"transport.jms.Destination\"&gt;queue1&lt;/property&gt;\n    &lt;property name=\"transport.jms.ConcurrentPublishers\"&gt;allow&lt;/property&gt;\n    &lt;property name=\"java.naming.factory.initial\"&gt;org.apache.activemq.jndi.ActiveMQInitialContextFactory&lt;/property&gt;\n    &lt;property name=\"java.naming.provider.url\"&gt;tcp://localhost:61616&lt;/property&gt;\n    &lt;property name=\"transport.jms.ConnectionFactoryJNDIName\"&gt;QueueConnectionFactory&lt;/property&gt;\n    &lt;property name=\"transport.jms.UserName\"&gt;admin&lt;/property&gt;\n    &lt;property encrypted=\"true\" name=\"transport.jms.Password\"&gt;aa47+5/q7d9AvOHUyYAJDXrx0Q6GQmgzIKS/hOkzp6huHrxslJJk6Oqmv2mrW159DOTfJ7Rw2nBbfGWjGiMckTFAO9p9YVF3kDDHhiyirWEJPSESSSJeBB782qnwoXEDSAjgiiUYWSRuYIfxdibXUUZr3JPSmjaxvy+EVMjjWgouMrid51UQTW50wl3C0fX03/nak4P9+GWx14T1JGAb07fKQlgK/AwYtJ8esNyiV1j0Z2jgGM9OLpqgZ9gqjsA95htzdqy2DgC/U74qfhkUKISAXUWZdGS+rCEYBFaVzAj0aPKtXmRWTrC6OTDSTVLQCKZPfcHqnU652PUQZqqKCA==&lt;/property&gt;\n  &lt;/to&gt;\n&lt;/eventPublisher&gt;\n</code></pre> <p>The username and password are <code>admin</code> and <code>admin</code>. Only the setting you need form here are the configuration setting like url etc. Make sure to use a unique queue name like I used <code>queue1</code>. Take a note of it as we need to subscribe to it in JMS receiver. Also, one more important thing, there are 2 options in JMS i.e. <code>queue</code> or <code>topic</code>. Make sure you use the same in JMS publisher and JMS Receiver. Also as I am using the queue, hence I used <code>QueueConnectionFactory</code>. The code for JMS receiver is also similar as shown below</p> <p><pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;eventReceiver name=\"jms_sub\" statistics=\"disable\" trace=\"disable\" xmlns=\"http://wso2.org/carbon/eventreceiver\"&gt;\n    &lt;from eventAdapterType=\"jms\"&gt;\n        &lt;property name=\"transport.jms.DestinationType\"&gt;queue&lt;/property&gt;\n        &lt;property name=\"transport.jms.Destination\"&gt;queue1&lt;/property&gt;\n        &lt;property name=\"java.naming.factory.initial\"&gt;org.apache.activemq.jndi.ActiveMQInitialContextFactory&lt;/property&gt;\n        &lt;property name=\"java.naming.provider.url\"&gt;tcp://localhost:61616&lt;/property&gt;\n        &lt;property name=\"transport.jms.SubscriptionDurable\"&gt;false&lt;/property&gt;\n        &lt;property name=\"transport.jms.ConnectionFactoryJNDIName\"&gt;QueueConnectionFactory&lt;/property&gt;\n        &lt;property name=\"transport.jms.UserName\"&gt;admin&lt;/property&gt;\n        &lt;property encrypted=\"true\" name=\"transport.jms.Password\"&gt;aa47+5/q7d9AvOHUyYAJDXrx0Q6GQmgzIKS/hOkzp6huHrxslJJk6Oqmv2mrW159DOTfJ7Rw2nBbfGWjGiMckTFAO9p9YVF3kDDHhiyirWEJPSESSSJeBB782qnwoXEDSAjgiiUYWSRuYIfxdibXUUZr3JPSmjaxvy+EVMjjWgouMrid51UQTW50wl3C0fX03/nak4P9+GWx14T1JGAb07fKQlgK/AwYtJ8esNyiV1j0Z2jgGM9OLpqgZ9gqjsA95htzdqy2DgC/U74qfhkUKISAXUWZdGS+rCEYBFaVzAj0aPKtXmRWTrC6OTDSTVLQCKZPfcHqnU652PUQZqqKCA==&lt;/property&gt;\n    &lt;/from&gt;\n    &lt;mapping customMapping=\"disable\" type=\"json\"/&gt;\n    &lt;to streamName=\"stream1_scep_timestamped\" version=\"2.0.0\"/&gt;\n&lt;/eventReceiver&gt;\n</code></pre> Now add the logger and send some events to <code>stream A</code>. If all went right, you should see the output at analytics tier console.</p>"},{"location":"togaf/adm-phases/","title":"ADM Phases","text":""},{"location":"togaf/adm-phases/#mapping-of-adm-and-agile","title":"Mapping of ADM and Agile","text":""},{"location":"togaf/adm-phases/#adm-phases_1","title":"ADM Phases","text":"<p>More detailed view is shown below</p> <p> </p> <p>The TOGAF Architecture Development Method (ADM) is a central feature of the TOGAF standard. The ADM cycle describes an incremental and iterative method for designing Business, Data, Applications, and Technology architectures. It progresses from <code>high-level concept diagrams</code>, to detailed domain architectures, all the way to the development of solution architectures, architecture roadmaps and implementation plans.</p> <p> </p> <p>What is Architmate?</p> <p>The ArchiMate\u00ae language is an Open Group standard that provides an Enterprise Architecture modeling language. The Archimate\u00ae language views the model as a set of layers (Business, Application, and Technology) as well as some specialized extensions (Motivation, and Implementation and Migration</p> <p> </p>"},{"location":"togaf/adm-phases/#preliminary-phase","title":"Preliminary Phase","text":"<p><code>Requests for Architecture Work</code> is a document that is sent from the sponsoring organization to the architecture organization to trigger the start of an architecture development cycle. Requests for Architecture Work can be created as an output of the Preliminary Phase, a result of approved architecture Change Requests, or terms of reference for architecture work originating from migration planning.</p> <p>The <code>Statement of Architecture Work</code> defines the scope and approach that will be used to complete an architecture development cycle. The Statement of Architecture Work is typically the document against which successful execution of the architecture project will be measured and may form the basis for a contractual agreement between the supplier and consumer of architecture services.</p> <p>The steps within the Preliminary Phase</p> <p>These steps are:</p> <ol> <li>Scope the Enterprise Organizations Impacted</li> <li>Confirm Governance and Support Frameworks</li> <li>Define and Establish Enterprise Architecture Team and Organization</li> <li>Identify and Establish Architecture Principles</li> <li>Tailor the TOGAF Framework and, if any, Other Selected Architecture Framework(s)</li> <li>Develop a Strategy and Implementation Plan for Tools and Techniques</li> </ol>"},{"location":"togaf/adm-phases/#deliverables-of-phase-a","title":"Deliverables of Phase A","text":"<ol> <li>Tailored Architecture Framework</li> <li>Organizational Model for Enterprise Architecture</li> <li>Architecture Principles</li> <li>Business Principles, Goals, and Drivers</li> <li>Request for Architecture Work</li> </ol>"},{"location":"togaf/adm-phases/#phase-a-vision","title":"Phase A (Vision)","text":"<p>The Architecture Vision Phase (Phase A) focuses on defining the scope of the project, creating and embracing the vision, and obtaining approvals to move forward. It develops the foundation by:</p> <ul> <li>Ensuring recognition and endorsement of the project</li> <li>Validating the business principles, goals and drivers</li> <li>Prioritizing the Baseline Architecture effort</li> </ul> <p>The vision provides the first attempt to provide a high-level description of the Baseline and Target Architectures.</p> <p>The Architecture Vision is documented within the Statement of Architecture Work, which is signed by the sponsoring organization and provides the consensus required to move forward.</p> <ul> <li>During the Architecture Vision phase, new requirements generated for future architecture work within the scope of the selected requirements need to be documented within the <code>Architecture Requirements Specification</code></li> <li>A <code>Business Transformation Readiness Assessment</code> can be used to evaluate and quantify the organization's readiness to undergo a change. This assessment is based upon the determination and analysis/rating of a series of readiness factors.</li> <li>Understand that the <code>baseline</code> and <code>target</code> need not be described at the same level of detail. In many cases, the baseline is described at a higher level of abstraction, so more time is available to specify the target in sufficient detail.</li> <li>Based on the stakeholder concerns, business capability requirements, scope, constraints, and principles, create a high-level view of the Baseline and Target Architectures.</li> <li>The Architecture Vision typically covers the breadth of scope identified for the project, at a high level.</li> <li> <p>The Architecture Vision includes a first-cut, high-level description of the baseline and target environments, from both a business and a technical perspective. </p> <p>Objective of Phase A</p> <ul> <li>To ensure that this evolution of the architecture development cycle has proper recognition and endorsement from the corporate management of the enterprise, and the support and commitment of the necessary line management</li> <li>To validate the business principles, business goals, and strategic business drivers of the organization</li> <li>To define the scope of, and to identify and prioritize the components of, the Baseline Architecture effort</li> <li>To define the relevant stakeholders, and their concerns and objectives</li> <li>To define the key business requirements to be addressed in this architecture effort, and the constraints that must be dealt with</li> <li>To articulate an Architecture Vision that demonstrates a response to those requirements and constraints</li> <li>To secure formal approval to proceed</li> <li>To understand the impact on, and of, other enterprise architecture development cycles ongoing in parallel.</li> </ul> </li> </ul>"},{"location":"togaf/adm-phases/#deliverables-of-phase-a_1","title":"Deliverables of Phase A","text":"<ol> <li>Statement of Architecture Work</li> <li>Architecture Vision</li> <li>Communications Plan</li> <li>Capability Assessment</li> <li>Architecture Definition Document</li> </ol>"},{"location":"togaf/adm-phases/#stakeholder-mangement","title":"Stakeholder Mangement","text":"<p><code>Stakeholder management</code> provides a discipline for gaining support between architecture practitioners and benefits the enterprise by:</p> <ul> <li> <p>Identifying powerful stakeholders early for their input to shape the architecture.</p> </li> <li> <p>Obtaining support from powerful stakeholders to enable more resources to be available during engagement of architectures.</p> </li> <li> <p>Early and frequent communications with stakeholders allowing better understanding of the architecture process.</p> </li> <li> <p>Reaction to architecture models and reports can be more effectively anticipated </p> </li> </ul> <p>Stakeholder analysis is used in the <code>Architecture Vision phase (Phase A)</code> to identify the key players in the engagement and updated with each subsequent phase of the ADM. The complexity of architecture can be difficult to manage and obtain agreement from large numbers of stakeholders. TOGAF addresses these issues throughout the ADM using the concepts of:</p> <ul> <li>Stakeholders</li> <li>Concerns</li> <li>Views</li> <li>Viewpoints</li> </ul>"},{"location":"togaf/adm-phases/#view-and-viewpoints","title":"View and ViewPoints","text":"<p><code>Architecture Views</code> are formal representations of the overall architecture that hold some significance or meaning to one or more stakeholders. This allows a particular architecture to be communicated and understood by all stakeholders in order to facilitate that the system is addressing their concerns. The views chosen are usually at the discretion of the architecture.</p> <p>Some of the most common views to be developed within architecture are:</p> <ol> <li>Use Case View  \u2014 Find out various use cases that we need to work on for a project/phase/pod etc. </li> <li> <p>Business View  \u2014 This view is used to addresses the concerns of the users. It can consist of </p> <ul> <li><code>BPMN Diagrams</code> by BA and Business.</li> <li><code>Process Flow diagrams</code> (per process agreed with SME and BA)</li> <li><code>Project Scope</code> and <code>Timeline/Gantt Chart diagrams</code> by Delivery lead</li> </ul> </li> <li> <p>SHLC/VHLC/HLC View \u2014 Create a high level view of the system</p> </li> <li> <p>Logical View/s \u2014 addresses the VHLs, API VHL's (usually per API). Usually these consist of </p> <ul> <li>Sequence diagrams</li> <li>Class diagrams</li> <li>Component diagams</li> <li>State Machine Diagrams</li> <li>Activity Diagrams</li> </ul> </li> <li> <p>Network View \u2014 addresses the structuring of network and communication elements in order to simplify network design and planning.</p> </li> <li> <p>Deployment View \u2014 addresses the deployment view of the system.</p> </li> <li> <p>Data Flow View \u2014 addresses the data requirements of processing, storage, retrieval, archiving and security</p> </li> <li> <p>DevOps View \u2014 addresses the DevOps view for the complicated CI and CD's.</p> </li> <li> <p>HA and DR View \u2014 addresses the HA and DR requirements of the system.</p> </li> <li> <p>Implementation View \u2014 addresses the detailed implementation view of various components.</p> </li> <li> <p>Security View \u2014 addresses the security aspects of a system.</p> </li> <li> <p>Data View \u2014 addresses the data aspects of a system. Various diagrams used are:</p> </li> <li>Business data model</li> <li>Logical data model</li> <li>Data management process model</li> <li>Data Entity/Business Function matrix</li> <li> <p>Data Interoperability requirements</p> </li> <li> <p>Enterprise Manageability View \u2014 addresses the operations, administration, and management of a system</p> </li> </ol>"},{"location":"togaf/adm-phases/#phase-b-business","title":"Phase B (Business)","text":"<p>Objective of Phase B</p> <ul> <li>Establish a Baseline Business Architecture</li> <li>Develop the Target Business Architecture that describes how the enterprise needs to operate to achieve the business goals, and respond to the strategic drivers set out in the Architecture Vision, in a way that addresses the Request for Architecture Work and stakeholder concerns</li> <li>Analyze gaps between Baseline and Target</li> <li>Identify candidate Architecture Roadmap components based upon gaps between the Baseline and Target Business Architectures</li> </ul>"},{"location":"togaf/adm-phases/#deliverables-of-phase-b-c-and-d","title":"Deliverables of Phase B, C and D","text":"<ol> <li>Architecture Definition Document</li> <li>Architecture Requirements Specification</li> <li>Architecture Roadmap</li> <li>Architecture Building Blocks</li> </ol>"},{"location":"togaf/adm-phases/#architecture-defination-document","title":"Architecture Defination Document","text":"<p>The Architecture Definition Document is the deliverable container for the core architectural artifacts created during a project. The Architecture Definition Document spans all architecture domains (business, data, application, and technology) and also examines all relevant states of the architecture (baseline, interim state(s), and target).</p> <p>The Architecture Definition Document is a companion to the Architecture Requirements Specification, with a complementary objective:</p> <ul> <li>The Architecture Definition Document provides a qualitative view of the solution and aims to communicate the intent of the architects.</li> <li>The Architecture Requirements Specification provides a quantitative view of the solution, stating measurable criteria that must be met during the implementation of the architecture.</li> </ul> <p>It is suggested that this document reference the various deliverables in the container. For instance, the Architecture Principles will be documented in an Architecture Principles document and that document referenced here. It may be that this container is implemented using a wiki or as an intranet rather than a text-based document. Even better would be to use a licensed TOGAF tool that captures this output.</p>"},{"location":"togaf/adm-phases/#architecture-requirements-specification","title":"Architecture Requirements Specification","text":"<p>Why we need Architecture Requirements Specification? </p> <p>The <code>Architecture Requirements Specification</code> provides a set of quantitative statements that outline what an implementation project must do in order to comply with the architecture. An Architecture Requirements Specification will typically form a major component of an implementation contract or contract for more detailed Architecture Definition.</p> <p>As mentioned above, the Architecture Requirements Specification is a companion to the Architecture Definition Document, with a complementary objective:</p> <ul> <li>The Architecture Definition Document provides a qualitative view of the solution and aims to communicate the intent of the architect.</li> <li>The Architecture Requirements Specification provides a quantitative view of the solution, stating measurable criteria that must be met during the implementation of the architecture.</li> </ul>"},{"location":"togaf/adm-phases/#architecture-roadmap","title":"Architecture Roadmap","text":"<ul> <li>The Architecture Roadmap lists individual work packages that will realize the Target Architecture and lays them out on a timeline to show progression from the Baseline Architecture to the Target Architecture.</li> <li>The Architecture Roadmap highlights individual <code>work packages'</code> business value at each stage.</li> <li><code>Transition Architectures</code> necessary to effectively realize the Target Architecture are identified as intermediate steps.</li> <li>The Architecture Roadmap is incrementally developed throughout Phases E and F, and informed by readily identifiable roadmap components from Phase B, C, and D within the ADM.</li> </ul>"},{"location":"togaf/adm-phases/#phase-c-information-system","title":"Phase C (Information System)","text":"<p>The Information Systems Architecture Phase (Phase C) handles the development of the Data and Application aspects of the architecture, specifically creating Target Architectures for business processes supported by IT implementations</p> <p>Bottom up or Top Down</p> <p>Implementation of architecture is commonly approached by <code>top-down design</code> and performing <code>bottoms-up implementation</code>, though the steps for implementing can follow any order.</p>"},{"location":"togaf/adm-phases/#deliverables-of-phase-b-c-and-d_1","title":"Deliverables of Phase B, C and D","text":"<ol> <li>Architecture Definition Document</li> <li>Architecture Requirements Specification</li> <li>Architecture Building Blocks</li> </ol>"},{"location":"togaf/adm-phases/#phase-d-technology","title":"Phase D (Technology)","text":"<p>The Technology Architecture Phase (Phase D) focuses on the technical aspect of the architecture, specifically those available within the Architecture Continuum.</p> <p>The decisions made in previous phases of the Architecture Development Method may have implications on the technology components and platform, particularly those decisions around service granularity and service boundaries.</p> <p>The areas of impact within the Technology Architecture include:</p> <ul> <li>Hardware, Software and communication technology: All techs needs to be looked at, their relationships to each other and the environment.</li> <li>Performance \u2014 platform service requirements can contain services with several functionality units with varying non-functional requirements and more services than required by the requesting system</li> <li>Maintainability \u2014 if service granularity is too general, the introduction of change to the system may be too difficult and costly</li> <li>Latency \u2014 inter-service communication may be impacted by the inappropriate setting of service boundaries and granularity</li> <li>Availability \u2014 when defining service composition and service granularity, high availability concerns may be a key determiner</li> </ul>"},{"location":"togaf/adm-phases/#deliverables-of-phase-b-c-and-d_2","title":"Deliverables of Phase B, C and D","text":"<ol> <li>Architecture Definition Document</li> <li>Architecture Requirements Specification</li> <li>Architecture Roadmap</li> <li>Architecture Building Blocks</li> </ol>"},{"location":"togaf/adm-phases/#phase-e-opportunities-solutions","title":"Phase E (Opportunities &amp; Solutions)","text":"<p>The Opportunities and Solutions phase is where the architecture team starts to be concerned with the actual implementation of the Target Architecture, looking into the best path for implementing the architecture, from both the corporate business and technical perspectives. </p> <p>The activities are logically grouped into <code>project work packages</code>.</p> <p>This phase can use Transition Architecture</p> <p><code>Transition/Tactical/Band-aid Architectures</code> allow changes to architecture without too extensive of an impact on the organization in any single increment. It also allows simultaneous work on several architectures to be conducted on different levels of detail.</p>"},{"location":"togaf/adm-phases/#deliverables-of-phase-e","title":"Deliverables of Phase E","text":"<ol> <li>Architecture Definition Document</li> <li>Architecture Building Blocks</li> <li>Architecture Roadmap</li> <li>Solution Building Blocks</li> <li>Implementation and Migration Plan</li> <li>Transition Architecture</li> <li>Implementation Governance Model</li> </ol>"},{"location":"togaf/adm-phases/#objective-for-e","title":"Objective for E","text":"<ul> <li>Generate the initial complete version of the Architecture Roadmap, based upon the gap analysis and candidate Architecture Roadmap components from Phases B, C, and D.</li> <li>Phase E is the initial step on the creation of the <code>Implementation and Migration Plan</code> which is completed in Phase F. It provides the basis of a well considered Implementation and Migration Plan that is integrated into the enterprise's portfolio in Phase F.</li> </ul> <p>ABB's to SBB's</p> <p>Finally, in <code>Phase E</code> the SBB's (building blocks) become more <code>implementation-specific as SBBs</code>, and their interfaces become the detailed architecture specification. The output of Phase E is the building block architecture, both in ABB (i.e., <code>functionally defined</code>) and SBB (i.e., <code>product-specific</code>) forms.</p>"},{"location":"togaf/adm-phases/#phase-f-migration-planning","title":"Phase F (Migration Planning)","text":"<p>The primary focus of the <code>Migration Planning</code> approach is to create a viable Implementation and Migration Plan with the assigned portfolio and project managers. This includes assessing the dependencies, costs, and benefits of the transition architecture and migration projects</p> <p>Generally, there are 3 basic approaches: - Greenfield \u2014 starting from the beginning - Revolutionary \u2014 radical change to the environment - Evolutionary \u2014 phased approach to introduce capabilities</p>"},{"location":"togaf/adm-phases/#abb","title":"ABB","text":"<p>Architecture Building Blocks (ABBs) relate to the Architecture Continuum, and are defined or selected as a result of the application of the ADM. Characteristics are:</p> <ul> <li>Capture architecture requirements, e.g., business, data, application, and technology requirements.</li> <li>Direct and guide the development of SBBs.</li> </ul>"},{"location":"togaf/adm-phases/#architecture-contract","title":"Architecture Contract","text":"<p>Architecture Contracts are the joint agreements between development partners and sponsors on the deliverables, quality, and fitness-for-purpose of an architecture. Successful implementation of these agreements will be delivered through effective architecture governance (see TOGAF Part VII, Architecture Governance). By implementing a governed approach to the management of contracts, the following will be ensured:</p> <ul> <li>A system of continuous monitoring to check integrity, changes, decision-making, and audit of all architecture-related activities within the organization</li> <li>Adherence to the principles, standards, and requirements of the existing or developing architectures</li> <li>Identification of risks in all aspects of the development and implementation of the architecture(s) covering the internal development against accepted standards, policies, technologies, and products as well as the operational aspects of the architectures such that the organization can continue its business within a resilient environment</li> <li>A set of processes and practices that ensure accountability, responsibility, and discipline with regard to the development and usage of all architectural artifacts</li> <li>A formal understanding of the governance organization responsible for the contract, their level of authority, and scope of the architecture under the governance of this body This is a signed statement of intent to conform with the enterprise architecture, issued by enterprise business users. When the enterprise architecture has been implemented (at the end of Phase F), an Architecture Contract will normally be drawn up between the architecting function (or the IT governance function, subsuming the architecting function) and the business users who will subsequently be building and deploying application systems in the architected environment.</li> </ul>"},{"location":"togaf/adm-phases/#implementation-governance-model","title":"Implementation Governance Model","text":"<p>Why do we need the implementation governance model?</p> <p>Once an architecture has been defined, it is necessary to plan how the <code>Transition Architecture</code> that implements the architecture will be governed through implementation. Within organizations that have established architecture functions, there is likely to be a governance framework already in place, but specific processes, organizations, roles, responsibilities, and measures may need to be defined on a project-by-project basis.</p> <p>The Implementation Governance Model ensures that a project transitioning into implementation also smoothly transitions into appropriate architecture governance.</p>"},{"location":"togaf/adm-phases/#phase-g-implementation-governance","title":"Phase G (Implementation &amp; Governance)","text":"<p>Phase G focuses on providing an oversight of the implementation, ensuring that there is adherence to the defined architecture during the development and deployment of solutions. This phase involves managing and governing the implementation process, thus maintaining alignment with the architectural vision and requirements.</p> <ul> <li>Conformance &amp; Compliance Review is in Phase G.</li> <li>Phase G establishes the connection between architecture and implementation organization, through the Architecture Contract.</li> </ul>"},{"location":"togaf/adm-phases/#compliance-assessment","title":"Compliance Assessment","text":"<p><code>Compliance Assessment</code>: Once an architecture has been defined, it is necessary to govern that architecture through implementation to ensure that the original Architecture Vision is appropriately realized and that any implementation learnings are fed back into the architecture process. </p> <p>Periodic compliance reviews of implementation projects provide a mechanism to review project progress and ensure that the design and implementation is proceeding in line with the strategic and architectural objectives.</p>"},{"location":"togaf/adm-phases/#sbb","title":"SBB","text":"<p>Solution Building Blocks (SBBs) relate to the Solutions Continuum and may be either procured or developed.SBB characteristics are:</p> <ul> <li>Define what products and components will implement the functionality.</li> <li>Define the implementation.</li> <li>Fulfil business requirements.</li> <li>Be product or vendor-aware</li> </ul>"},{"location":"togaf/adm-phases/#phase-h-change-management","title":"Phase H (Change Management)","text":"<p>The objective of Phase H is to establish an continual monitoring and architecture change management process for the new enterprise architecture baseline that is achieved with completion of Phase G. </p> <p>3 types of change managements</p> <p>The approach is based on classifying required architectural changes into one of three categories:</p> <ol> <li><code>Simplification change</code>: This change to an architecture is often driven by a requirement to reduce investment. A simplification change can normally be handled via change management techniques.</li> <li><code>Incremental change</code>: This change is  driven by a requirement to derive additional value from existing investment. An incremental change may be capable of being handled via change management techniques, or it may require partial re-architecting, depending on the nature of the change.</li> <li><code>Re-architecting change</code>: This change is driven by a requirement to increase investment in order to create new value for exploitation. A re-architecting change requires putting the whole architecture through the architecture development cycle again.</li> </ol> <p>Steps done are: - Provide analysis for architecture <code>Change Management</code> - Conduct Enterprise Architecture <code>performance reviews</code> with service management. - Assess Change Requests and reporting to ensure that the expected value realization and <code>Service-Level Agreement (SLA)</code> expectations of the customers are met. - Undertake a <code>Gap Analysis</code> of the performance of the Enterprise Architecture. - Ensure change management requests adhere to the Enterprise Architecture Governance and framework. -  If the change is at an infrastructure level \u2014 for example, ten systems reduced or changed to one system \u2014 this may not change the architecture above the physical layer, but it will change the Baseline Description of the Technology Architecture; this would be a simplification change handled via change management techniques. - Establish value realization process - Deploy monitoring tools - Manage risks - Provide analysis for architecture change management - Develop change requirements to meet performance targets - Manage governance process - Activate the process to implement change</p>"},{"location":"togaf/adm-phases/#change-request","title":"Change Request","text":"<p>During the implementation of an architecture, as more facts become known, it is possible that the original architecture definition and requirements are not suitable or are not sufficient to complete the implementation of a solution.</p> <p>In these circumstances, it is necessary for implementation projects to either deviate from the suggested architectural approach or to request scope extensions. Additionally, external factors \u2013 such as market factors, changes in business strategy, and new technology opportunities, may open up opportunities to extend and refine the architecture.</p> <p>In these circumstances, a Change Request may be submitted in order to kick-start a further cycle of architecture work.Typical contents of a Change Request are:</p> <ul> <li>Description of the proposed change</li> <li>Rationale for the proposed change</li> <li>Impact assessment of the proposed change, including:</li> </ul>"},{"location":"togaf/adm-phases/#requirements-management","title":"Requirements Management","text":"<p>Note</p> <p>It is important to note that the Requirements Management circle denotes not a static set of requirements, but a dynamic process whereby requirements for enterprise architecture and subsequent changes to those requirements are identified, stored, and fed into and out of the relevant ADM phases, and also between cycles of the ADM.</p> <p>Note also that the requirements management process itself does not dispose of, address, or prioritize any requirements: this is done within the relevant phase of the ADM. It is merely the process for managing requirements throughout the overall ADM.</p>"},{"location":"togaf/adm-phases/#deliverables-of-requirement-phase","title":"Deliverables of Requirement Phase","text":"<ol> <li>Architecture Requirement Spec Document</li> <li>Requirements Impace Assessment</li> </ol>"},{"location":"togaf/adm-phases/#requirements-impace-assessment","title":"Requirements Impace Assessment","text":"<p>Throughout the ADM, new information is collected relating to an architecture. As this information is gathered, new facts may come to light that invalidate existing aspects of the architecture. </p> <p>A Requirements Impact Assessment assesses the current architecture requirements and specification to identify changes that should be made and the implications of those changes.</p>"},{"location":"togaf/continums/","title":"Architecture Continum's","text":""},{"location":"togaf/continums/#enterprise-continum","title":"Enterprise Continum","text":"<ul> <li>The Enterprise Continuum enables the organization of re-usable architecture artifacts and solution assets to maximize the Enterprise Architecture investment opportunities.</li> <li>The Enterprise Continuum provides methods for classifying architecture and solution artifacts, both internal and external to the Architecture Repository, as they evolve from <code>Generic Foundation Architectures</code> to <code>Organization-Specific Architectures</code>.</li> </ul> <p>Example</p> <p>The simplest way of thinking of the Enterprise Continuum is as a view of the repository of all the architecture assets. It can contain Architecture Descriptions, models, building blocks, patterns, architecture viewpoints, and other artifacts - that exist both within the enterprise and in the IT industry at large, which the enterprise considers to have available for the development of architectures for the enterprise.</p> <p>The Enterprise Continuum is a combination of two concepts:</p> <ul> <li>The Architecture Continuum \u2014 It provides a method of defining and understanding the rules, representations, and relationships present in an information system.</li> <li>The Solutions Continuum \u2014 It supports the Architecture Continuum by providing a method to describe and understand the implementation of rules, representations, and relationships found in the Architecture Continuum</li> </ul>"},{"location":"togaf/continums/#architecture-continum","title":"Architecture Continum","text":"<p>The TOGAF <code>Architecture Development Method</code> is a process for moving from the TOGAF Foundation Architecture to an Enterprise/Org specific architecture. </p> <p> </p> <p>The enterprise needs and business requirements are addressed in increasing detail from left to right (as shown in the above diagram). The architect will typically look to find re-usable architectural elements toward the left of the continuum. When elements are not found, the requirements for the missing elements are passed to the left of the continuum for incorporation. Those implementing architectures within their own organizations can use the same continuum models specialized for their business.</p> <p>TOGAF provides 2 reference models that could be included in an organization\u2019s <code>Enterprise Continuum</code>:</p> <ul> <li>TOGAF Foundation Architecture \u2014 a set of generic services and functions that provide a foundation for more specific architectures to be developed</li> <li>Integrated Information Infrastructure Reference Model (III-RM) \u2014 It is based on the TOGAF Foundation Architecture to enable and support a Boundaryless Information Flow vision</li> </ul>"},{"location":"togaf/continums/#foundational-architecture","title":"Foundational Architecture","text":"<p>A Foundation Architecture consists of generic components, inter-relationships, principles, and guidelines that provide a foundation on which more specific architectures can be built. The TOGAF ADM is a process that would support specialization of such Foundation Architectures in order to create organization-specific models.</p> <p>For The Open Group, this Foundation Architecture is the <code>Technical Reference Model (TRM)</code> and <code>Standards Information Base (SIB)</code>. </p>"},{"location":"togaf/continums/#trm","title":"TRM","text":"<p>The <code>Technical Reference Model (TRM)</code> is a component of the TOGAF Foundation Architecture, which provides a model and taxonomy of generic platform services. It is comprised of taxonomy and a graphic</p>"},{"location":"togaf/continums/#sib","title":"SIB","text":"<p>The Standards Information Base (SIB) is a database of industry standards used for:</p> <ul> <li>Architecture Development</li> <li>Acquisition and Procurement</li> <li>General Information</li> </ul> <p>The content of the SIB is a collection of works from various sources, including IEEE, ISO, ISACA, WWW Consortium, or the Object Management Group. The content includes guidelines, technical processes, product standards, and other documentation relevant to widely accepted best practices. The SIB is managed by The Open Group.</p> <p>To be considered for inclusion in the SIB, a specific standard must meet some criteria, including: - Ability to be implemented in a non-discriminatory way - Available of dependent products or services to interested parties. - Implementation of the standard is commercially available. - Organizations are free to develop a practical solution that supports or utilizes the standard. - Future versions of the standard remain available.</p>"},{"location":"togaf/continums/#iii-rm","title":"III-RM","text":"<p>The Integrated Information Infrastructure Reference Model (III-RM) is a component and extension of the TOGAF Technical Reference Model, which addresses the ability of an enterprise to enable Boundaryless Information Flow. Like other components of the TOGAF, the IIS-RM is comprised of taxonomy and an associated graphic representing the taxonomy.</p> <p>The concept of Boundaryless Information Flow has its roots in the modern enterprise\u2019s growing need for speed, flexibility, and responsiveness in the organization\u2019s ability to work together. The solution is the creation of an infrastructure that integrates the information requirements of the organization and provides integrated access to that information for all members of the organization.</p> <p>The core components of an III-RM at a high-level are: - Business Applications (BA) - Infrastructure Applications (IA) - Application Platform - Interfaces - Qualities</p>"},{"location":"togaf/continums/#common-systems-architectures","title":"Common Systems Architectures","text":"<p>Common Systems Architectures guide the selection and integration of specific services from the Foundation Architecture to create an architecture useful for building common (i.e., highly re-usable) solutions across a wide number of relevant domains.</p> <p>Examples of Common Systems Architectures include: a <code>security architecture</code>, a <code>management architecture</code>, a <code>network architecture</code>, an <code>operations architecture</code>, etc. </p> <p>Each is incomplete in terms of overall system functionality, but is complete in terms of a particular problem domain (security, manageability, networking, operations, etc.), so that solutions implementing the architecture constitute re-usable building blocks for the creation of functionally complete operating states of the enterprise.</p> <p>The TOGAF <code>Integrated Information Infrastructure Reference Model (III-RM)</code> - is a reference model that supports describing Common Systems Architecture in the Application Domain that focuses on the requirements, building blocks, and standards relating to the vision of Boundaryless Information Flow.</p>"},{"location":"togaf/continums/#industry-architectures","title":"Industry Architectures","text":"<p>It integrates Common systems components with Industry specific components to create solutions for target customer problems.</p>"},{"location":"togaf/continums/#organization-architectures","title":"Organization Architectures","text":"<p>It represents the deployed solutions for a particular enterprise.</p>"},{"location":"togaf/continums/#solution-continum","title":"Solution Continum","text":"<p>The <code>Solutions Continuum</code> represents the detailed specification and construction of the architectures at the corresponding levels of the Architecture Continuum. At each level, the Solutions Continuum is a population of the architecture with reference building blocks - either purchased products or built components - that represent a solution to the enterprise's business need expressed at that level.</p>"},{"location":"togaf/interview_questions/","title":"INterview questions","text":"<ul> <li>Hadoop: </li> <li>Diff between monolithic and microservices architecture</li> <li>3 advantages of MS arch</li> <li>Role of service mesh/Registry in Micro-services</li> <li>ANS: Istio and Linkerd</li> </ul> <p>Polling project example ui \u2192 api gw \u2192 labmbda \u2192 APIM \u2192 azure function \u2192 pod </p> <ul> <li> <p>You found that the databricks token is used by Azure function and stored in configs, what will you suggest</p> </li> <li> <p>example of microservice orchestraton </p> </li> <li> <p>Difference between VM and container? Why container is lightweight?</p> </li> <li> <p>In which case will you go for GraphQL over REST?</p> </li> <li>Overfetching</li> <li> <p>Frontend devs have more control over what to fetch</p> </li> <li> <p>Why financial institutes prefer SOAP over REST?</p> </li> <li> <p>What is eventual consistency? https://blog.bytebytego.com/p/ep114-7-must-know-strategies-to-scale</p> </li> <li> <p>You have Storage account queue , max size is 64 KB and we need to send images?</p> </li> <li> <p>Saga Pattern question</p> </li> <li> <p>Book Flight, Book Hotel, Book Restraunt</p> </li> <li> <p>Exponential backoff pattern?</p> </li> <li> <p>BFF pattern </p> </li> <li> <p>What is benefit of making a service stateless?</p> </li> <li> <p>Use App load balancer or Network Load balancer.</p> </li> <li> <p>Few things you will do to make sure API security.</p> </li> <li> <p>We are doing consistent long polling to make sure we get real time updates but its costing us, what will you suggest? </p> </li> <li> <p>The payload is larage, and your teammate suggested to use GRPC which use Protoubf, can you tell me why Protobuf was suggested?</p> </li> <li> <p>We have a database of 2M rows, and 1000 columns and we need to do analytics on top of that, in which database type you will save that. Your query involves aggregation type operations </p> </li> <li>Key value</li> <li>Document DB</li> <li>Columnar DB</li> <li> <p>Graph DB</p> </li> <li> <p>Your users are experienceing latency globaly, what will you suggest?</p> </li> </ul>"},{"location":"togaf/tog-basics/","title":"TOGAF Basics","text":"<ul> <li><code>TOGAF</code>, an acronym for <code>The Open Group Architecture Framework</code>, is intended to be a standard way to design and implement architectures for very large computer systems. Today, 80% of Global 50 companies use TOGAF. The TOGAF Standard applies to all Enterprise Architecture practices.</li> <li>It does not matter whether your architecture will support strategy, portfolio, project, or solution delivery, or whether it is about embarking on a Digital Transformation or legacy simplification </li> </ul> <p>The <code>TOGAF</code> Standard is a framework for Enterprise Architecture. Put simply, it is a standard approach for developing, approving, using, and maintaining Enterprise Architectures. It applies to all Enterprise Architecture practices. It is based on an iterative process model supported by best practices and a re-usable set of existing architectural assets.</p> <p>What is tactical architecture?</p> <p>Tactical architecture as \u201ctemporary\u201d architecture that gets the business to a reasonable target state given a set of significant constraints that demand the architecture address a relatively immediate need without a significant amount of resources (money, time, workers)</p>"},{"location":"togaf/tog-basics/#diagram-examples","title":"Diagram examples","text":""},{"location":"togaf/tog-basics/#techniques","title":"Techniques","text":"<ul> <li>Contextual: Why</li> <li>Conceptual: What</li> <li>Logical: How</li> <li>Physical: Actual solution</li> </ul>"},{"location":"togaf/tog-basics/#terms","title":"Terms","text":""},{"location":"togaf/tog-basics/#cross-cutting-concern","title":"Cross Cutting Concern","text":"<p>Security Architecture is a cross-cutting concern pervasive through the whole Enterprise Architecture. It can be described as a coherent collection of views, viewpoints, and artifacts, including security, privacy, and operational risk perspectives, along with related topics like security objectives and security services.</p> <p> </p> <p>The Security Architecture is more than a dataset; it is based on the Information Security Management (ISM) and Enterprise Risk Management (ERM) processes.</p>"},{"location":"togaf/tog-basics/#architecture-board","title":"Architecture Board","text":"<p>An Architecture Board is responsible for operational items and must be capable of making decisions in situations of possible conflict and be accountable for taking those decisions. It should therefore be a representation of all the key stakeholders in the architecture, and will typically comprise a group of executives responsible for the review and maintenance of the overall architecture. It is important that the members of the Architecture Board cover architecture, business, and program management areas.</p>"},{"location":"togaf/tog-basics/#abbs-and-sbbs","title":"ABB's and SBB's","text":"<p>For instance, at an early stage, a building block can simply consist of a grouping of functionality such as a customer database and some retrieval tools. Building blocks at this functional level of definition are described in TOGAF as Architecture Building Blocks (ABBs). Later on, real products or specific custom developments replace these simple definitions of functionality, and the building blocks are then described as Solution Building Blocks (SBBs).</p> <p> </p> <p>A building block's boundary and specification should be <code>loosely coupled</code> to its implementation; i.e., it should be possible to realize a building block in several different ways without impacting the boundary or specification of the building block.</p>"},{"location":"togaf/tog-basics/#abbs","title":"ABB's","text":"<p>Architecture Building Blocks (ABB) are related to the Architecture Continuum and define which functionality will be implemented through the capture of business and technical requirements. An ABB is technology aware and is used to direct and guide the development of Solution Building Blocks (SBB).</p> <p>The fundamental functionality and attributes of an ABB are semantic and unambiguous. Their interfaces are either chosen or supplied</p>"},{"location":"togaf/tog-basics/#sbbs","title":"SBB's","text":"<p>Solution Building Blocks (SBBs) relate to the Solutions Continuum. They are implementation choices of the architectures identified in the enterprise\u2019s Architecture Continuum and may be either procured or developed.</p> <p>SBBs appear in Phase E of the ADM where product-specific building blocks are considered for the first time. SBBs define what products and components will implement the functionality, thereby defining the implementation.</p> <p>They fulfill business requirements and are product or vendor-aware. The content of an SBB specification includes the following as a minimum:</p> <ul> <li>Specific functionality and attributes.</li> <li>Interfaces; the implemented set.</li> <li>Required SBBs used with required functionality and names of the interfaces used.</li> <li>Mapping from the SBBs to the IT topology and operational policies.</li> <li>Specifications of attributes shared such as security, manageability, localizability, scalability.</li> <li>Performance, configurability.</li> <li>Design drivers and constraints, including the physical architecture.</li> <li>Relationships between the SBBs and ABBs.</li> </ul>"},{"location":"togaf/tog-basics/#architecture-principles","title":"Architecture Principles","text":"<p>Shortut</p> <p>N-SIR: Name, Statement, Implication and Rationale</p> <p>Remember that Arch principles are added in Vision Phase. It may be that the Architecture Principles are documented using a <code>wiki</code> or as an <code>intranet</code> rather than a text-based document. Even better would be to use a licensed TOGAF tool that captures this output.</p> <ol> <li><code>Name</code>: Should both represent the essence of the rule as well as be easy to remember. Specific technology platforms should not be mentioned in the name or statement of a principle.</li> <li><code>Statement</code>: Should succinctly and unambiguously communicate the fundamental rule. For the most part, the principles statements for managing information are similar from one organization to the next. It is vital that the principles statement is unambiguous.</li> <li><code>Rationale</code>: Tellls about benefits (Why)<ol> <li>The Rationale should highlight the business benefits of adhering to the principle, using business terminology. Point to the similarity of information and technology principles to the principles governing business operations. </li> <li>Also describe the relationship to other principles, and the intentions regarding a balanced interpretation.</li> <li>Describe situations where one principle would be given precedence or carry more weight than another for making a decision.</li> </ol> </li> <li><code>Implications</code>: Talks about requriements mostly<ol> <li>The Implications should highlight the requirements, both for the business and IT, for carrying out the principle \u2013 in terms of resources, costs, and activities/tasks.</li> <li>It will often be apparent that current systems, standards, or practices would be incongruent with the principle upon adoption.</li> <li>The impact to the business and consequences of adopting a principle should be clearly stated. The reader should readily discern the answer to: \u201cHow does this affect me?\u201d It is important not to oversimplify, trivialize, or judge the merit of the impact. </li> <li>Some of the implications will be identified as potential impacts only, and may be speculative rather than fully analyzed. </li> </ol> </li> </ol>"},{"location":"togaf/tog-basics/#architecture-capability","title":"Architecture Capability","text":"<p>Creating architecture for an enterprise requires the organization to have the business capability to support the architecture through structures, roles, responsibilities, skills, and processes.</p> <p> </p> <p>TOGAF architecture capability builds on the Architecture Repository and Enterprise Continuum by identifying the architecture components providing the capability and their relationships to each other.</p> <p>The components include:</p> <ul> <li>Skilled Resource Pool</li> <li>Roles and Responsibilities</li> <li>Contracts</li> <li>Projects and Portfolios</li> <li>Governance of Projects and Portfolios</li> <li>Business Operations</li> <li>Governance Bodies</li> </ul>"},{"location":"togaf/tog-basics/#architecture-repository","title":"Architecture Repository","text":"<p>Relationship between Architecture Repository and Entperprise Continum</p> <p>The Enterprise Continuum is a <code>view of the Architecture Repository</code> that provides methods for classifying architecture and solution artifacts as they evolve from generic Foundation Architectures to Organization-Specific Architectures. The Enterprise Continuum comprises two complementary concepts: </p> <ul> <li>Architecture Continuum </li> <li>Solutions Continuum</li> </ul> <p>Architecture Repository can be used to store different classes of architectural output at different levels of abstraction, created by the ADM.</p> <p> </p> <p>At a high level, six classes of architectural information are expected to be held within an Architecture Repository:</p> <ul> <li><code>Architecture Metamodel</code>: It describes the organizationally tailored application of an architecture framework, including a method for architecture development and a metamodel for architecture content.</li> <li><code>Architecture Capability</code>: It defines the parameters, structures, and processes that support governance of the Architecture Repository.</li> <li><code>Architecture Landscape</code>: It shows an architectural view of the building blocks that are in use within the organization today (e.g., a list of the live applications). The landscape is likely to exist at multiple levels of granularity to suit different architecture objectives.</li> <li><code>Standards Information Base</code>: It captures the standards with which new architectures must comply, which may include industry standards, selected products and services from suppliers, or shared services already deployed within the organizations</li> <li><code>Reference Library</code>: It provides guidelines, templates, patterns, and other forms of reference material that can be leveraged in order to accelerate the creation of new architectures for the enterprise.</li> <li><code>Governance Log</code>: It provides a record of governance activity across the enterprise.</li> </ul>"},{"location":"togaf/tog-basics/#enterprise-continum","title":"Enterprise Continum","text":"<p>A model for structuring a virtual repository and methods for classifying architecture and solution artifacts.</p>"},{"location":"togaf/tog-basics/#architecture-landscape","title":"Architecture Landscape","text":"<p>The Architecture Landscape holds architectural views of the state of the enterprise at particular points in time. Due to the sheer volume and the diverse stakeholder needs throughout an entire enterprise, the Architecture Landscape is divided into three levels of granularity</p> <p> </p> <ol> <li> <p>Strategic Architecture provides an organizing framework for operational and change activity and allows for direction setting at an executive level.</p> </li> <li> <p>Segment Architecture provides an organizing framework for operational and change activity and allows for direction setting and the development of effective architecture roadmaps at a program or portfolio level.</p> </li> <li> <p>Capability Architecture provides an organizing framework for change activity and the development of effective architecture roadmaps realizing capability increments.</p> </li> </ol> <p> </p>"},{"location":"togaf/tog-basics/#content-metamodel","title":"Content MetaModel","text":"<p>The content metamodel provides a definition of all the types of building blocks that may exist within an architecture, showing how these building blocks can be described and related to one another. For example, when creating an architecture, an architect will identify applications, \"data entities\" held within applications, and technologies that implement those applications. These applications will in turn support particular groups of business user or actor, and will be used to fulfil \"business services\".</p> <p> </p>"},{"location":"togaf/tog-basics/#content-framework","title":"Content Framework","text":"<p>Architects executing the ADM will produce a number of outputs as a result of their efforts, such as process flows, architectural requirements, project plans, or project compliance assessments.</p> <p>The TOGAF Content Framework is intended to:</p> <ul> <li>Provide a detailed model of architectural work products</li> <li>Drive consistency in the outputs created when following the ADM</li> <li>Provide a comprehensive checklist of architecture output that could be created</li> <li>Reduce the risk of gaps within the final architecture deliverable set</li> <li>Help an enterprise mandate standard architecture concepts, terms, and deliverables</li> </ul> <p>The <code>Content Framework</code> provided supports the use of the TOGAF framework as a stand-alone framework for architecture within an enterprise. However, other Content Frameworks exist (for example, that provided by the ArchiMate Specification) and it is expected that some enterprises will use an external framework in conjunction with the ADM instead. In such cases, the TOGAF Content Framework provides a useful reference and starting point for TOGAF content to be mapped to the metamodels of other frameworks.</p> <p> </p>"},{"location":"togaf/tog-basics/#organizational-model-for-ea","title":"Organizational Model for EA","text":"<p>The Organizational Model for Enterprise Architecture demonstrates the organization, roles and responsibility within the enterprise.</p>"},{"location":"togaf/tog-basics/#implementation-governance-model","title":"Implementation Governance Model","text":"<p>Once an architecture has been defined, it is necessary to plan how the architecture will be governed through implementation. Within organizations that have established architecture functions, there is likely to be a governance framework already in place, but specific processes, organizations, roles, responsibilities, and measures may need to be defined on a project-by-project basis.</p> <p>When is this produced?</p> <p>The Implementation Governance Model produced as an output of <code>Phase F</code> ensures that a project transitioning into implementation also smoothly transitions into appropriate Architecture Governance (for <code>Phase G</code>)</p>"},{"location":"togaf/tog-basics/#request-for-architecture-work","title":"Request for Architecture Work","text":"<p>The Request for Architecture Work is sent from sponsoring organizations to the architecture organization to trigger the start of an architecture development cycle</p> <p>When is request of Architecture work created?</p> <p>It is produced with the assistance of the architecture organization as an output of the <code>Preliminary Phase</code>. Requests for Architecture Work can also be created as a result of approved architecture Change Requests, or terms of reference for architecture work originating from migration planning.</p>"},{"location":"togaf/tog-basics/#requirements-impact-assessment","title":"Requirements Impact Assessment","text":"<p>The Requirements Impact Assessment assesses the current architecture requirements and specifications to identify changes to be made and the implications of those changes.</p>"},{"location":"togaf/tog-basics/#statement-of-architecture-work","title":"Statement of Architecture Work","text":"<p>The Statement of Architecture Work defines the scope and approach used to complete an architecture project.</p>"},{"location":"togaf/tog-basics/#transition-architecture","title":"Transition Architecture","text":"<p>Where the scope of change to implement the Target Architecture requires an incremental approach, one or more Transition Architectures are defined within the Architecture Definition Document output from Phase E.</p> <p>A Transition Architecture shows the enterprise at an architecturally significant state between the Baseline and Target Architectures. Transition Architectures are used to describe <code>interim architectures</code> necessary for the effective realization of the Target Architecture. </p>"},{"location":"togaf/tog-basics/#communication-plan","title":"Communication Plan","text":"<p>Tldr</p> <p>The Communications Plan provides a basis for communicating within a planned and managed process to stakeholders.</p> <p>Effective communication of targeted information to the right stakeholders at the right time is a <code>Critical Success Factor (CSF)</code> for Enterprise Architecture. Development of a Communications Plan for architecture in <code>Phase A</code> allows for this communication to be carried out within a planned and managed process.</p> <p>Typical contents of a Communications Plan are:</p> <ul> <li>Identification of stakeholders and grouping by communication requirements</li> <li>Identification of communication needs, key messages in relation to the Architecture Vision, communication risks, and CSFs</li> <li>Identification of mechanisms that will be used to communicate with stakeholders and allow access to architecture information, such as meetings, newsletters, repositories, etc.</li> </ul>"},{"location":"togaf/tog-basics/#architecture-roadmap","title":"Architecture Roadmap","text":"<p>The Architecture Roadmap is a listing of individual increments of change and shows the progression from Baseline Architecture to the Target Architecture.</p>"},{"location":"togaf/tog-basics/#architecture-requirement-spec","title":"Architecture Requirement Spec","text":"<p>A set of quantitative statements, which outline what actions an implementation project must take to comply with the architecture.</p> <p>Spec and Defination Document difference</p> <p>The Architecture Definition Document is a companion to the Architecture Requirements Specification where:</p> <ul> <li>The <code>Architecture Requirements Specification</code> provides a quantitative view of the solution, stating measurable criteria that must be met during the implementation of solutions supporting the architecture.</li> <li>The <code>Architecture Definition Document</code> provides a qualitative view of the solution and aims to communicate the intent of the architects.</li> </ul>"},{"location":"togaf/tog-basics/#architecture-definition-document","title":"Architecture Definition Document","text":"<p>A deliverable container for the core architectural artifacts created during a project. The Architecture Definition Document is a companion document to the Architecture Requirements Specification.</p> <p>When it is created and updated?</p> <p>The Architecture Definition Document spans all architecture domains (Business, Data, Application, and Technology) and also examines all relevant states of the architecture (Baseline, Transition, and Target).</p> <p>It is first created in <code>Phase A</code>, where it is populated with artifacts created to support the Architecture Vision. It is updated in <code>Phase B</code>, with Business Architecture-related material, and subsequently updated with Information Systems Architecture material in <code>Phase C</code>, and then with Technology Architecture material in <code>Phase D</code>. Where the scope of change to implement the Target Architecture requires an incremental approach, the Architecture Definition Document will be updated to include one or more Transition Architectures in <code>Phase E</code>.</p>"},{"location":"togaf/tog-basics/#architecture-content-framework","title":"Architecture Content Framework","text":"<p>A detailed model of architectural work products, including deliverables, artifacts within deliverables, and the Architecture Building Blocks (ABBs) that deliverables represent</p>"},{"location":"togaf/tog-basics/#ea-capability-model","title":"EA Capability Model","text":"<p>An Enterprise Architecture Capability is the ability to develop, use, and maintain the architecture of a particular enterprise, and use the architecture to govern change.</p> <p>The term \u201ccapability\u201d is defined differently by different practitioners, most commonly when it is used as part of a formal analysis technique when the definition must be precise and constrained. The term EA Capability is used as a management concept that facilitates planning improvements in the ability to do something that leads to enhanced outcomes enabled by the capability.</p> <p>While every organization can benefit from an EA Capability, each organization will require a different EA Capability.</p> <p> </p>"},{"location":"togaf/tog-basics/#architecture-capability-framework","title":"Architecture Capability Framework","text":"<p>A structured definition of the organisations, skills, roles and responsibilities to establish and operate an Enterprise Architecture</p>"},{"location":"togaf/tog-basics/#capability-assessment","title":"Capability Assessment","text":"<p>Before embarking upon a detailed architecture definition, it is valuable to understand the baseline and target capability level of the enterprise. This Capability Assessment is first carried out in <code>Phase A</code>, and updated in <code>Phase E</code></p>"},{"location":"togaf/tog-basics/#stakeholder-management","title":"Stakeholder Management","text":"<p>Stakeholder management is an important discipline that successful architects can use to win support from others. It helps them ensure that their projects succeed where others fail. The technique should be used during Phase A to identify the key players in the engagement, and also be updated throughout each phase. The output of this process forms the start of the Communications Plan.</p>"},{"location":"togaf/tog-basics/#gap-analysis","title":"Gap Analysis","text":"<p>Done at end of every phase</p> <p>The Gap Analysis technique is usually the final step within a phase. </p> <p>The basic premise is to highlight a shortfall between the Baseline Architecture and the Target Architecture; that is, items that have been deliberately omitted, accidentally left out, or not yet defined.</p>"},{"location":"togaf/tog-basics/#architecture-tradeoff-method","title":"Architecture Tradeoff Method","text":"<p>There is often more than one possible Target Architecture that would conform to the Architecture Vision, Architecture Principles, and Requirements.</p> <p>It is important to identify alternative Target Architectures and build understanding of different possibilities and identify trade-offs between the alternatives. Creating an architecture normally requires trade-offs among competing forces.</p> <p>Presenting different alternatives and trade-offs to stakeholders helps architects to extract hidden agendas, principles, and requirements that could impact the final Target Architecture. Below Figure illustrates the architecture trade-off method.</p> <p> </p>"},{"location":"togaf/tog-basics/#value-stream","title":"Value Stream","text":"<p>A value stream is depicted as an end-to-end collection of value-adding activities that create an overall result for a customer, stakeholder, or end user. In modeling terms, those value-adding activities are known as value stream stages, each of which creates and adds incremental stakeholder value from one stage to the next.</p> <p> </p> <p>Value in a value stream, is achieved through a series of sequential and/or parallel actions, known as value stream stages, that incrementally create and add stakeholder value from one stage to the next.</p> <p> </p>"},{"location":"togaf/tog-basics/#business-capability-model","title":"Business Capability Model","text":"<p>A business capability model represents the currently active, stable set of business capabilities (along with all of their defining characteristics) that cover the business, enterprise, or organizational unit in question.</p> <p>The first task of business capability modeling is to capture and document all of the business capabilities that represent the full scope of what the business segment under consideration does today (irrespective of how well it does it) or what it desires to be able to do in the future. The second task is to organize that information in a logical manner.</p>"},{"location":"togaf/tog-basics/#gap-analysis_1","title":"GAP Analysis","text":"<p>The Gap Analysis technique is usually the final step within a phase. The basic premise is to highlight a shortfall between the Baseline Architecture and the Target Architecture; that is, items that have been deliberately omitted, accidentally left out, or not yet defined.</p>"},{"location":"togaf/tog-basics/#architecture-scope","title":"Architecture Scope","text":"<p>Note</p> <p>Four dimensions are typically used in order to define and limit the scope of an architecture:</p> <ul> <li><code>Breadth</code> - what is the full extent of the enterprise (i.e., Enterprise Focus)</li> <li><code>Depth</code> - To what level of detail should the architecting effort go (i.e., Level of detail)</li> <li><code>Time Period</code> - what is the time period that needs to be articulated for the Architecture Vision</li> <li><code>Architecture Domains</code>: a complete Enterprise Architecture description should contain all four architecture domains (Business, Data, Application, Technology).</li> </ul>"},{"location":"togaf/togaf-notes/","title":"TOGAF Notes","text":""},{"location":"togaf/togaf-notes/#key-points","title":"Key Points","text":"<ol> <li> <p>The Integrated Information Infrastructure Model was created to address issue of Boundryless information flow.</p> <ul> <li>The III-RM is a subset of the TOGAF TRM in terms of its overall scope, but it also expands certain parts of the TRM - in particular, the business applications and infrastructure applications parts - in order to provide help in addressing one of the key challenges facing the enterprise architect today: the need to design an integrated information infrastructure to enable Boundaryless Information Flow.</li> <li>The <code>Boundaryless Information Flow</code> problem space is one that is shared by many customer members of The Open Group, and by many similar organizations worldwide. It is essentially the problem of getting information to the right people at the right time in a secure, reliable manner, in order to support the operations that are core to the extended enterprise.</li> </ul> <p>There are two levels of risk</p> <ul> <li><code>Initial Level of Risk</code>: Risk categorization prior to determining and implementing mitigating actions</li> <li><code>Residual Level of Risk</code>: Risk categorization after implementation of mitigating actions (if any)</li> </ul> </li> <li> <p>Organizations today recognize that they need not abandon functional or departmental organization altogether. They can enable the right people to come together in cross-functional teams so that all the skills, knowledge, and expertise can be brought to bear on any specific problem or business opportunity.</p> </li> <li> <p>The <code>TOGAF TRM</code> is an example of a <code>Foundation/Fundamental Architecture</code>. It is a fundamental architecture upon which other, more specific architectures can be based.</p> </li> <li> <p>A <code>Foundation Architecture</code> consists of generic components, inter-relationships, principles, and guidelines that provide a foundation on which more specific architectures can be built. The <code>TOGAF ADM</code> is a process that would support specialization of such Foundation Architectures in order to create organization-specific models.</p> </li> <li> <p><code>Business Transformation Readiness Assessment</code>, used for evaluating and quantifying an organization's readiness to undergo change.</p> </li> <li> <p><code>Practitioner and implementer</code> are directed, and both are controlled by the stakeholder.</p> </li> <li> <p><code>Phaes G</code> ensures that implementation projects conform to the defined architecture.</p> </li> <li> <p>The Requirements Management Phase stores requirements and manage their flow to relevant ADM Phases.</p> </li> <li> <p>One of the objective of the preliminary phase is to define the framework and methodologies to be used.</p> </li> <li> <p>According to TOGAF, when creating views for a particular architecture, the recommended step is to refer to the existing libraries for viewpoints, to identify for re-use.</p> </li> <li> <p>Architecture Vision document contains a high-level description of the baseline and target architectures.</p> </li> <li> <p>Enterprise Continuum is used to structure re-usable architecture and solution assets.</p> </li> <li> <p><code>Request of Architecture Work</code> is sent from the sponsoring organization to the architecture organization to trigger the start of an ADM cycle in phase A.</p> </li> <li> <p>Architecture Governance is a practice by which enterprise architectures are controlled at an enterprise-wide level.</p> </li> <li> <p>Reference Library is the component within the Architecture Repository holds best practice or template materials that can be used to construct architectures.</p> </li> <li> <p>Compliance Assessment is used to govern the architecture throughout its implementation process,</p> </li> <li> <p>A key objective of the Technology Architecture Phase is to define technology components into a set of technology platforms.</p> </li> <li> <p>ADM should be adapted to suit the specific needs of the enterprise.</p> </li> <li> <p>In <code>Phase E</code> of the TOGAF ADM are Gap Analysis results from earlier phases (B, C and D) consolidated.</p> </li> <li> <p>Purpose of a business scenario is to help identify and understand the business requirements that an architecture must address.</p> </li> <li> <p>When multiple server systems are being consolidated to a single system can be considered as <code>Simplificaiton Change</code></p> </li> <li> <p>One of the Purpose of <code>Enterprise Architecture</code> is optimise an enterprise into an environment that is responsive to business needs.</p> </li> <li> <p>Architecture Principles are used to guide decision making within the enterprise.</p> </li> <li> <p>Building Blocks become more implementation-specific in Phase E.</p> </li> <li> <p>One of the objective of Phase A is to secure formal approval to proceed.</p> </li> <li> <p>TOGAF defines an enterprise as any collection of organizations that has a common set of goals.</p> </li> <li> <p><code>Rationale</code> talks about the business benefits of an principle.</p> </li> <li> <p>As per TOGAF, view is the representation of a system from the perspective of a related set of concerns.</p> </li> <li> <p>TOGAF Architecture Governance Framework includes a model for governance including process, content and context.</p> </li> <li> <p>Foundation, Common Systems, Industry, Organization-Specific (FCIO) is the right order</p> </li> <li> <p>One of the purpose of Phase E is to define the initial implementation plans.</p> </li> <li> <p>A building block's boundary and specification should be loosely coupled to its implementation; i.e., it should be possible to realize a building block in several different ways without impacting the boundary or specification of the building block.</p> </li> <li> <p>An initial assessment of business transformation readiness occur in Phase A</p> </li> <li> <p>If the business case for doing architecture at all is not well recognized, then creating an Architecture Vision is almost always essential; and a detailed Business Architecture often needs to come next, in order to underpin the Architecture Vision, detail the business case for remaining architecture work, and secure the active participation of key stakeholders in that work.</p> </li> <li> <p>Business Transformation Readiness Assessment is used for evaluating the status of an organization to undergo change?</p> </li> <li> <p>The Architecture Development Method produces content to be stored in the Repository, which is classified according to the <code>Enterprise Continuum</code>.</p> </li> <li> <p>Incremental change in phase H : A change driven by a requirement to derive additional value from the existing investment </p> </li> <li> <p>A viewpoint is used as a template to create a view.</p> </li> <li> <p>Application Architecture and Data Architecture may be developed in either sequence.</p> </li> <li> <p>The Architecture Repository is used to store different classes of architectural output created by the ADM </p> </li> <li> <p>Preliminary Phase of the ADM establishes a set of Principles.</p> </li> <li> <p>Business scenarios figure most prominently in the phase A (Architecture Vision), when they are used to define relevant business requirements, and to build consensus with business management and other stakeholders.</p> </li> <li> <p>Phase G establishes the connection between architecture and implementation organization, through the Architecture Contract.</p> </li> <li> <p>The Requiremetns Management phase manages the flow of requirements, storing them, and feeding them in and out of the relevant ADM phases.</p> </li> <li> <p>Implicatons highlight the requirements for carrying out the principle</p> </li> <li> <p><code>Select reference models, viewpoints and tools</code> are the steps in Phases B, C, and D which occur before development of the baseline or target architectures.</p> </li> <li> <p>In phase H, we conduct Enterprise Architecture performance reviews with service management Assess Change Requests and reporting to ensure that the expected value realization and Service-Level Agreement (SLA) expectations of the customers are met.</p> </li> <li> <p>The Solutions Continuum represents the detailed specification and construction of the architectures at the corresponding levels of the Architecture Continuum. At each level, the Solutions Continuum is a population of the architecture with reference building blocks - either purchased products or built components - that represent a solution to the enterprise's business need expressed at that level.</p> </li> <li> <p>Architecture Principle provides a foundation for making architecture and planning decisions, framing policies, procedures, and standards, and supporting resolution of contradictory situations.</p> </li> <li> <p>Purpose of the Architecture Roadmap is to show progression of change from the <code>Baseline Architecture</code> to the <code>Target Architecture</code>.</p> </li> <li> <p>Capability based planning focuses on achieving business outcomes rather than just technical deliverables.</p> </li> <li> <p>In Phase E, the building blocks become implementation-specific.</p> </li> <li> <p>Preparing architecture review reports is NOT done by Arch board, but by an Arch</p> </li> <li> <p>Part IV: Architecture Content Framework contains a structured metamodel for architectural artifacts.</p> </li> <li> <p><code>Foundation Architecture</code> contains building blocks and their corresponding standards. </p> </li> <li> <p>Statement of Architecture Work defines the scope and approach to complete an architecture project.</p> </li> <li> <p>One of the goals of the Preliminary phase is to select and implement supporting tools and other infrastructure to support the architecture activity.</p> </li> <li> <p>The TOGAF Integrated Information Infrastructure Reference Model (III-RM) \u2014 is a reference model that supports describing Common Systems Architecture in the Application</p> </li> <li> <p><code>Re-architecting change</code> is the change driven by a requirement to increase investment in order to create new value for exploitation.</p> </li> <li> <p>One of the objective of Phase F is to ensure that the Implementation and Migration Plan is co-ordinated with the various management frameworks in use within the enterprise.</p> </li> <li> <p>Artifacts are generally classified as <code>catalogs</code> (lists of things), <code>matrices</code> (showing relationships between things), and <code>diagrams</code> (pictures of things). Examples include a requirements catalog, business interaction matrix, and a use-case diagram. An architectural deliverable may contain many artifacts and artifacts will form the content of the Architecture Repository.</p> </li> <li> <p><code>Implications</code>: Should highlight the requirements, both for the business and IT, for carrying out the principle - in terms of resources, costs, and activities/tasks. It will often be apparent that current systems, standards, or practices would be incongruent with the principle upon adoption. The impact to the business and consequences of adopting a principle should be clearly stated. The reader should readily discern the answer to: \"How does this affect me?\"</p> </li> <li> <p>The Technology Architecture that depicts the architecture practice's infrastructure requirements and deployment in support of the architecture applications and Enterprise Continuum.</p> </li> <li> <p>The Reference Library provides guidelines, templates, patterns, and other forms of reference material that can be leveraged in order to accelerate the creation of new architectures for the enterprise.</p> </li> <li> <p>Qualities of Principles (SURCC)</p> <ul> <li>Understandable: the underlying tenets can be quickly grasped and understood by individuals throughout the organization</li> <li>Robust: enable good quality decisions about architectures and plans to be made, and enforceable policies and standards to be created</li> <li>Complete: the principles cover every situation perceived</li> <li>Consistent: strict adherence to one principle may require a loose interpretation of another principle</li> <li>Stable: principles should be enduring, yet able to accommodate changes</li> </ul> </li> <li> <p>An \"architecture view\" is a representation of a system from the perspective of a related set of concerns. It consists of one or more architecture models of the system.</p> </li> <li> <p>The Standards Information Base provides a repository area to hold a set of specifications, to which architectures must conform. Establishment of a Standards Information Base provides an unambiguous basis for Architecture Governance because:</p> <ul> <li>The standards are easily accessible to projects and therefore the obligations of the project can be understood and planned for</li> <li>Standards are stated in a clear and unambiguous manner, so that compliance can be objectively assessed</li> </ul> </li> <li> <p>Patterns offer the promise of helping the architect to identify combinations of Architecture and/or Solution Building Blocks (<code>ABBs/SBBs</code>) that have been proven to deliver effective solutions in the past, and may provide the basis for effective solutions in the future.</p> </li> <li> <p>\u201cArchitecture\u201d has two meanings depending upon its contextual usage:</p> <ul> <li>A formal description of a system, or a detailed plan of the system at component level to guide its implementation.</li> <li>The structure of components, their interrelationships, and the principles and guidelines governing their design </li> </ul> </li> <li> <p><code>Architecture Capability Framework</code> provides a set of reference materials for how to establish such an architecture function.</p> </li> <li> <p>The objectives of <code>Phase A</code> are to:</p> <ul> <li>Develop a high-level aspirational vision of the capabilities and business value to be delivered as a result of the proposed Enterprise Architecture.</li> <li>Obtain approval for a Statement of Architecture Work that defines a program of works to develop and deploy the architecture outlined in the Architecture Vision.</li> </ul> </li> <li> <p>The TOGAF <code>Foundation Architecture</code> is an architecture of generic services and functions that provides a foundation on which more specific architectures and architectural components can be built. This Foundation Architecture has two main elements:</p> <ul> <li>Technical Reference Model (TRM) which provides a model and taxonomy of generic platform services</li> <li>Standards Information Base (SIB) which provides a database of standards that can be used to define the particular services and other components of an organization-specific architecture that is derived from the TOGAF Foundation Architecture.</li> </ul> <p>About SIB</p> <p>The Standards Information Base provides a repository area to hold a set of specifications, to which architectures must conform.</p> </li> <li> <p>Change types:</p> <ul> <li><code>Simplification change</code>- reduce investment</li> <li><code>Incremental change</code> - additional value from existing investment</li> <li><code>Re-architecting change</code> -  create new value for exploitation</li> </ul> </li> <li> <p>Steps for phase B, C and D are</p> <ul> <li>Select Reference Models, Viewpoints, and Tools</li> <li>Develop Baseline Business Architecture Description</li> <li>Develop Target Business Architecture Description</li> <li>Perform Gap Analysis</li> <li>Define Candidate Roadmap Components</li> <li>Resolve Impacts Across the Architecture Landscape</li> <li>Conduct Formal Stakeholder Review</li> <li>Finalize the Business Architecture</li> <li>Create the Architecture Definition Document</li> </ul> </li> <li> <p>Architecture Governance Framework is the approach to ensure the effectiveness of an organization's architectures.</p> </li> <li> <p>The technique known as <code>gap analysis</code> is widely used in the TOGAF Architecture Development Method (ADM) to validate an architecture that is being developed. The basic premise is to highlight a shortfall between the Baseline Architecture and the Target Architecture; that is, items that have been deliberately omitted, accidentally left out, or not yet defined.</p> </li> <li> <p>The goal of an architecture change management process is to ensure that the architecture achieves its original target business value. </p> </li> <li> <p>ADM is the step-by-step approach to developing an Enterprise Architecture.</p> </li> <li> <p>Architecture is a formal description of a system, or a detailed plan of the system at component level to guide its implementation</p> </li> <li> <p>Rationale: Should highlight the business benefits of adhering to the principle, using business terminology. Point to the similarity of information and technology principles to the principles governing business operations. Also describe the relationship to other principles, and the intentions regarding a balanced interpretation. Describe situations where one principle would be given precedence or carry more weight than another for making a decision.</p> </li> <li> <p><code>Version 1.0</code> indicates a formally reviewed, detailed architecture</p> </li> <li> <p><code>Capability-based planning</code>: business planning technique that focuses on business outcomes. </p> </li> <li> <p><code>View</code>: A representation of a system from the perspective of a related set of concerns.</p> </li> <li> <p>The <code>Architecture Content Framework</code> uses the following 3 categories to describe the type of architectural work product within the context of use:</p> <ol> <li>A <code>deliverable</code> </li> <li>An <code>artifact</code> </li> <li>A <code>a building block</code></li> </ol> </li> <li> <p><code>Implications</code>: Should highlight the requirements, both for the business and IT, for carrying out the principle - in terms of resources, costs, and activities/tasks. It will often be apparent that current systems, standards, or practices would be incongruent with the principle upon adoption. The impact to the business and consequences of adopting a principle should be clearly stated. The reader should readily discern the answer to: \"How does this affect me?\". It is important not to oversimplify, trivialize, or judge the merit of the impact. Some of the implications will be identified as potential impacts only, and may be speculative rather than fully analyzed.</p> </li> <li> <p>Purpose of <code>Architecture Compliance Review</code> is to Identify key criteria for procurement activities (e.g., for inclusion in Commercial Off-The-Shelf (COTS) product RFI/RFP documents).</p> </li> <li> <p>The <code>Architecture Board</code> is typically made responsible, and accountable, for achieving some or all of the following goals:</p> <ul> <li>Providing the basis for all decision-making with regard to the architectures</li> <li>Consistency between sub-architectures</li> <li>Establishing targets for re-use of components</li> <li>Flexibility of the Enterprise Architecture:</li> <li>To meet changing business needs</li> <li>To leverage new technologies</li> <li>Enforcement of Architecture Compliance</li> <li>Improving the maturity level of architecture discipline within the organization</li> <li>Ensuring that the discipline of architecture-based development is adopted</li> <li>Supporting a visible escalation capability for out-of-bounds decisions</li> </ul> </li> <li> <p><code>Version 0.1</code> indicates that a high-level outline of the architecture is in place.</p> </li> <li> <p>Architecture Capability Framework: This part discusses the organization, processes, skills, roles, and responsibilities required to establish and operate an architecture function within an enterprise.</p> </li> <li> <p>An architecture view is a representation of a system from the perspective of a related set of concerns. </p> </li> <li> <p>In <code>Phase A</code> the earliest building block definitions start as relatively abstract entities within the Architecture Vision.</p> </li> <li> <p>The goals of an Architecture Compliance review include some or all of the following:</p> <ul> <li>First and foremost, catch errors in the project architecture early, and thereby reduce the cost and risk of changes required later in the lifecycle. This in turn means that the overall project time is shortened, and that the business gets the bottom-line benefit of the architecture development faster.</li> <li>Ensure the application of best practices to architecture work.</li> <li>Provide an overview of the compliance of an architecture to mandated enterprise standards.</li> <li>Identify where the standards themselves may require modification.</li> </ul> </li> <li> <p><code>Concerns</code> are the key interests that are crucially important to the stakeholders in the system, and determine the acceptability of the system. Concerns may pertain to any aspect of the system's functioning, development, or operation, including considerations such as performance, reliability, security, distribution, and evolvability.</p> </li> <li> <p>The major information areas managed by a governance repository should contain the following types of information:</p> <ul> <li> <p>Reference Data (collateral from the organization's own repositories/Enterprise Continuum, including external data; e.g., COBIT, the IT4IT Reference Architecture): used for guidance and instruction during project implementation</p> </li> <li> <p>This includes the details of information outlined above. The reference data includes a description of the governance procedures themselves. </p> </li> <li> <p>Process Status: all information regarding the state of any governance processes will be managed</p> </li> <li> <p>Examples of this include outstanding compliance requests, dispensation requests, and compliance assessments investigations.</p> </li> <li> <p>Audit Information: this will record all completed governance process actions and will be used to support:</p> </li> <li> <p>Key decisions and responsible personnel for any architecture project that has been sanctioned by the governance process</p> </li> <li> <p>A reference for future architectural and supporting process developments, guidance, and precedence    </p> </li> </ul> </li> <li> <p>Foundation Architecture = Generic Building Blocks</p> </li> <li> <p>Business Architecture is done before data, application and techology as it provides prerequisite knowledge for undertaking architecture work in the other domains.</p> </li> <li> <p>The Architecture Requirements Specification provides a set of quantitative statements that outline what an implementation project must do in order to comply with the architecture. An Architecture Requirements Specification will typically form a major component of an implementation contract or contract for more detailed Architecture Definition.</p> <p>TOGAF Document Categorization Model</p> <p>TOGAF document categorization model exists to structure the release management of the TOGAF specification. It is NOT intended to serve as an implementation guide for practitioners. Within the model, the content of the TOGAF document is categorized according to the following 4 categories:</p> <ul> <li> <p>TOGAF Core consists of the fundamental concepts that form the essence of TOGAF.</p> </li> <li> <p>TOGAF Mandated consists of the normative parts of the TOGAF specification. These elements of TOGAF are central to its usage and without them the framework would not be recognizably TOGAF. Strong consideration must be given to these elements when applying TOGAF.</p> </li> <li> <p>TOGAF Recommended consists of a pool of resources that are specifically referenced in TOGAF as ways in which the TOGAF Core and Mandated processes can be accomplished (e.g., the SEI Architecture Trade-Off Analysis Method or business scenarios).</p> </li> <li> <p>TOGAF Supporting consists of additional resources that are not referenced in the other three TOGAF categories itself but provide valuable assistance.</p> </li> </ul> </li> <li> <p>The business imperatives behind the Enterprise Architecture work drive the requirements and performance metrics for the architecture work.</p> </li> <li> <p>Developing Arch Capability: Establishing a sustainable architecture practice within an organization can be achieved by adhering to the same approach that is used to establish any other capability - such as a business process management capability - within an organization. The ADM is an ideal method to be used to architect and govern the implementation of such a capability. Applying the ADM with the <code>specific Architecture Vision</code> to establish an architecture practice within the organization would achieve this objective. This shouldn't be seen as a phase of an architecture project, or a one-off project, but rather as an ongoing practice that provides the context, environment, and resources to govern and enable architecture delivery to the organization.</p> </li> <li> <p>The Architecture Definition Document provides a <code>qualitative view</code> of the solution and aims to communicate the intent of the architects.</p> </li> <li> <p>III-RM is fundamentally an Application Architecture reference model </p> </li> <li> <p>The Architecture Capability defines the parameters, structures, and processes that support governance of the Architecture Repository.</p> </li> <li> <p><code>Phase G</code> establishes the connection between architecture and implementation organization, through the Architecture Contract.</p> </li> <li> <p>Requirements for Architecture Work: The business imperatives behind the enterprise architecture work drive the requirements and performance metrics for the architecture work. They should be sufficiently clear so that this phase may scope the business outcomes and resource requirements, and define the outline enterprise business information requirements and associated strategies of the enterprise architecture work to be done. For example, these may include:</p> <ul> <li>Business requirements</li> <li>Cultural aspirations</li> <li>Organization intents</li> <li>Strategic intent</li> <li>Forecast financial requirements</li> </ul> </li> <li> <p>Risk is pervasive in any Enterprise Architecture activity and is present in all phases within the <code>Architecture Development Method (ADM)</code></p> </li> <li> <p>Capability-based Planning: a business planning technique that focuses on business outcomes.</p> </li> <li> <p>Business scenarios are an appropriate and useful technique to discover and document business requirements, and to articulate an architectural vision (in Phase A) that responds to those requirements.</p> </li> <li> <p>Ensuring the compliance of individual projects with the enterprise architecture is an essential aspect of architecture governance.</p> </li> <li> <p>One of the purpose of an <code>Architecture Compliance Review</code> is to communicate the technical readiness of the project.</p> </li> <li> <p>Consider 3 classes of building blocks:</p> <ul> <li>Re-usable building blocks, such as legacy items</li> <li>Building blocks to be the subject of development, such as new applications</li> <li>Building blocks to be the subject of purchase; i.e., Commercial Off-The-Shelf (COTS) applications</li> </ul> </li> <li> <p>Deliverables are specified as contractual outputs from a project, whereas artifacts are not.</p> </li> <li> <p>TOGAF must be adapted to satisfy organization specific requirements.</p> </li> <li> <p>The Technology Architecture describes the logical software and hardware capabilities that are required to support the deployment of business, data, and application services; this includes IT infrastructure, middleware, networks, communications, processing, standards, etc.</p> </li> <li> <p>An architecture framework is a foundational structure, or set of structures, which can be used for developing a broad range of different architectures. It should describe a method for designing a target state of the enterprise in terms of a set of building blocks, and for showing how the building blocks fit together. It should contain a set of tools and provide a common vocabulary. It should also include a list of recommended standards and compliant products that can be used to implement the building blocks.</p> </li> <li> <p>In phase F the Transition Architectures is finalized.</p> </li> <li> <p>The normal approach to <code>Target Architecture development</code> is top-down. In the Baseline Description, however, the analysis of the <code>current state</code> often has to be done bottom-up.</p> </li> <li> <p>Phase E is the first phase which is directly concerned with the structure of how the Target Architecture will be implemented.</p> </li> <li> <p>Actions arising from the Business Transformation Readiness Assessment technique should be incorporated in the Implementation and Migration Plan</p> </li> <li> <p>Objective of Phase E is to generate the initial complete version of the Architecture Roadmap.</p> </li> <li> <p>Reference Library provides guidelines, templates, patterns, and other forms of reference material that can be leveraged in order to accelerate the creation of new architectures for the enterprise.</p> </li> <li> <p>The objectives of Phase G are to:</p> <ol> <li>Ensure conformance with the Target Architecture by implementation projects</li> <li>Perform appropriate Architecture Governance functions for the solution and any implementation-driven architecture Change Requests.</li> </ol> </li> <li> <p>Artifacts are generally classified as catalogs (<code>lists of things</code>), matrices (<code>showing relationships between things</code>), and diagrams (<code>pictures of things</code>).</p> </li> <li> <p>Value streams provide valuable stakeholder context into why the organization needs business capabilities.</p> </li> <li> <p>The objectives of the Preliminary Phase are to:</p> <ul> <li>Determine the Architecture Capability desired by the organization:</li> <li>Review the organizational context for conducting Enterprise Architecture</li> <li>Identify and scope the elements of the enterprise organizations affected by the Architecture Capability</li> <li>Identify the established frameworks, methods, and processes that intersect with the Architecture Capability</li> <li>Establish Capability Maturity target</li> <li>Establish the Architecture Capability:</li> <li>Define and establish the Organizational Model for Enterprise Architecture</li> <li>Define and establish the detailed process and resources for Architecture Governance</li> <li>Select and implement tools that support the Architecture Capability</li> <li>Define the Architecture Principles.</li> </ul> </li> <li> <p>The <code>TOGAF Library</code> is a reference library containing guidelines, templates, patterns, and other forms of reference material to accelerate the creation of new architectures for the enterprise. Library resources are organized into 4 sections:</p> <ul> <li>Foundation Documents</li> <li>Generic Guidance and Techniques</li> <li>Industry-Specific Guidance and Techniques</li> <li>Organization-Specific Guidance and Techniques</li> </ul> </li> <li> <p>The <code>business capability map</code> found or developed in the Architecture Vision phase provides a self-contained view of the business that is independent of the current organizational structure, business processes, information systems and applications.</p> </li> <li>Mapping in business<ul> <li>Capability Map exposes what a business does </li> <li>Value stream Map exposes how it delivers value to specific stakeholders</li> <li>Organization Map identities the business units or third parties that possess or use those capabilities and which participate in the value streams.</li> </ul> </li> <li>Check classes of arch engagement here</li> <li>Arch Development approaches</li> </ol> <p>Dev approached for Architecture</p> <p>2 approaches can be adopted within the ADM for the development of architectures:</p> <ul> <li> <p><code>Baseline First</code>: in this style, an assessment of the baseline landscape is used to identify problem areas and improvement opportunities. This process is most suitable when the baseline is complex, not clearly understood, or agreed upon. This approach is common where organizational units have had a high degree of autonomy.</p> </li> <li> <p><code>Target First</code>: in this style, the target solution is elaborated in detail and then mapped back to the baseline, in order to identify change activity. This process is suitable when a target state is agreed at a high level and where the enterprise wishes to effectively transition to the target model.</p> </li> </ul> <ol> <li>The <code>Business Value Assessment</code> is used to identify the benefits, costs, and risks of executing the architecture project, which will help in determining if the proposed architecture can produce sufficient value to warrant the attendant risks. This is directly addressing the concerns raised by the senior management.</li> <li>The business scenario technique is a method for deriving business requirements that address business concerns and issues. It involves understanding and documenting the business context, events triggering the scenario, and the desired outcomes. This approach would help to identify and address the concerns of the stakeholders, and to clarify the requirements for the new system.</li> </ol>"},{"location":"togaf/togaf-notes/#shortcuts","title":"Shortcuts","text":"<ul> <li><code>SSC</code>: Stragegy, Segment and Capability arch (in Arch Landscape)</li> <li><code>FICO</code>: Foundation, Industry, Common Systems, and Org-specific arch (in Reference Arch)</li> <li><code>BDAT</code>: Business, Data, Application and Technology Phase (For ADM)</li> <li><code>BDAT</code>: Breadth, Depth, Architecture Domain, Time Period (For scope of architecture)</li> <li><code>SIR</code>: Simplification change, Incremental change, Re-architecting change (in change manamgement)</li> <li><code>NSIR</code>: Name, Statement, Implication, Rationale (for Architecture Principle)</li> <li><code>ACF</code>: Architecture Capability Framework  and Architecture Content Framework </li> <li><code>SURCC</code>: Stable, Understandable, Robust, Complete, Consistent (5 criterias of principles)</li> <li><code>BTRA</code>: Business Transformation Readiness Assessment</li> </ul>"},{"location":"util/util-info/","title":"utils","text":"<p>commands are <code>config</code>, <code>experiment</code>, <code>example</code>, <code>beta</code>, <code>abstract</code>, <code>bug</code>, <code>tip</code>, <code>quote</code></p>"},{"location":"util/util-info/#image","title":"Image","text":"<p> Sample Caption <p></p>"},{"location":"util/util-info/#table","title":"Table","text":"Method Description <code>GET</code>      Fetch resource <code>PUT</code>  Update resource <code>DELETE</code>      Delete resource"},{"location":"blog/archive/2024/","title":"2024","text":""},{"location":"blog/archive/2023/","title":"2023","text":""},{"location":"blog/category/embeddings/","title":"Embeddings","text":""},{"location":"blog/category/genai/","title":"GenAI","text":""},{"location":"blog/category/llm/","title":"LLM","text":""},{"location":"blog/category/quantization/","title":"Quantization","text":""},{"location":"blog/category/aiml/","title":"AI/ML","text":""},{"location":"blog/category/migration/","title":"Migration","text":""},{"location":"blog/category/architecture/","title":"Architecture","text":""},{"location":"blog/category/prompt-engineering/","title":"Prompt Engineering","text":""},{"location":"blog/category/llms/","title":"LLMS","text":""},{"location":"blog/category/sagemaker/","title":"SageMaker","text":""},{"location":"blog/category/ai/","title":"AI","text":""},{"location":"blog/category/rag/","title":"RAG","text":""},{"location":"blog/category/databricks/","title":"Databricks","text":""},{"location":"blog/category/feature-engineering/","title":"Feature Engineering","text":""}]}