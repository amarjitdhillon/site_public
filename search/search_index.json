{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"Home"},{"location":"about/","text":"Hey, my name is Amar Dhillon \ud83d\udc4b On this blog I speak in my name and my name only. Opinions stated here are my own and not my employer\u2019s. Checkout my channel at amar_dhillon_studio and channel at Fully Simplified Feel free to reach out on My LinkedIn Profile Working as Enterprise Software Architect: AI at Air Canada \ud83d\udeeb I exploring new places \ud83d\udeb4\ud83c\udffb \ud83c\udfd6 Aspiring content creator \ud83d\udcf8 Music lover \ud83c\udfb8 \ud83c\udfb9 PB29 to \ud83c\udde8\ud83c\udde6","title":"About me"},{"location":"AI/azureml/","text":"Azure ML \u00b6 AML Workspace \u00b6 The workspace is the top-level resource for Azure Machine Learning, providing a centralized place to work with all the artifacts you create. When you create a new workspace, it automatically creates below Azure resources: Storage account : It used to store files used by the workspace as well as data for experiments and model training . It Is used as the default datastore for the workspace. Jupyter notebooks that are used with your Azure Machine Learning compute instances are stored here as well. Application Insights : It is used to monitor predictive services in the workspace. Key Vault instance : Stores secrets that are used by compute targets and other sensitive information that's needed by the workspace. Container/Model registry , used to manage containers for deployed models. VMs : provide computing power for your AzureML workspace and are an integral part in deploying and training models. Load Balancer : a network load balancer is created for each compute instance and compute cluster to manage traffic even while the compute instance/cluster is stopped. Virtual Network : these help Azure resources communicate with one another, the internet, and other on-premises networks. Other components: \u00b6 Compute Instance \u00b6 It is a managed cloud-based workstation for data scientists. Each compute instance has only one owner, although you can share files between multiple compute instances. They can be used for dev and test purposes. Compute Target \u00b6 it is a designated compute resource or environment where you run your training script or host your service deployment. This location might be your local machine or a cloud-based compute resource. Types of Compute Targets Local \u2014 This is used to run the experiment on the same compute target as the code used to initiate the experiment. Training Cluster \u2014 for high scalable training requirements \u2014 distributed computes, CPU/GPU are enabled and scaled on-demand. Inference Cluster \u2014 containerized clusters to deploy the inference of the trained model as an overall application module. Attached Compute \u2014 to attach already acquired Azure ML VM or Databricks machine Training Cluster \u00b6 AML compute instance AML compute cluster AKS Databricks Batch Inference Cluster \u00b6 Docker container is created before inference When performing inference, AML creates a Docker container that hosts the model and associated resources needed to use it. This container is then used in a compute target . Various inference compute target can be: AML endpoints : Fully managed computes for real-time (managed online endpoints) and batch scoring (batch endpoints) on serverless compute. AKS : use managed k8s ACI : Run docker container without orchestrator AML Concepts \u00b6 Model deployment \u00b6 After you train an ML model, you need to deploy the model so that others can use it to do inferencing. In Azure Machine Learning, you can use endpoints and deployments to do so. Endpoint \u00b6 Info An endpoint is an HTTPS endpoint that clients can call to receive the inferencing (scoring) output of a trained model. It provides: - Authentication using \"key & token\" based auth - SSL termination - A stable scoring URI (endpoint-name.region.inference.ml.azure.com) Deployment \u00b6 A deployment is a set of resources required for hosting the model that does the actual inferencing. Remember A single endpoint can contain multiple deployments. Endpoints and deployments are independent Azure Resource Manager resources that appear in the Azure portal. Real-time/batch scoring \u00b6 Batch scoring , or batch inferencing, involves invoking an endpoint with a reference to data. The batch endpoint runs jobs asynchronously to process data in parallel on compute clusters and store the data for further analysis Real-time scoring , or online inferencing, involves invoking an endpoint with one or more model deployments and receiving a response in near-real-time via HTTPs. Traffic can be split across multiple deployments, allowing for testing new model versions by diverting some amount of traffic initially and increasing once confidence in the new model is established. CLI V2 \u00b6 The AML CLI v2 is the latest extension for the Azure CLI. The CLI v2 provides commands in the format az ml <noun> <verb> <options> to create and maintain Azure ML assets and workflows. The assets or workflows themselves are defined using a YAML file . The YAML file defines the configuration of the asset or workflow \u2013 what is it, where should it run, and so on. AML commands examples az ml job create -- file my_job_definition . yaml az ml environment update -- name my - env -- file my_updated_env_definition . yaml az ml model list az ml compute show -- name my_compute AML Pipelines \u00b6 Pipelines are workflows of complete machine learning tasks that can be run independently. The Azure Machine Learning pipeline service automatically orchestrates all the dependencies between pipeline steps. You can create pipelines without using components, but components offer better amount of flexibility and reuse . Azure ML Pipelines may be defined in YAML and run from the CLI, authored in Python, or composed in Azure ML Studio Designer with a drag-and-drop UI. Experiment \u00b6 An Experiment is a container of trials that represent multiple model runs. Environment \u00b6 AMl environments are an encapsulation of the environment where your ML training happens. They specify the Python packages, environment variables, and software settings around your training and scoring scripts. They also specify runtimes (Python, Spark, or Docker). Environments can broadly be divided into three categories: Curated : They are provided by Azure Machine Learning and are available in your workspace by default. Intended to be used as is, they contain collections of Python packages and settings to help you get started with various machine learning frameworks User-managed : In this you're responsible for setting up your environment and installing every package that your training script needs on the compute target. System-managed : Conda will manage env. build the env to docker AML builds environment definitions into Docker images and conda environments . It also caches the environments, so they can be reused in subsequent training jobs and service endpoint deployments. AML Studio \u00b6 AML Designer \u00b6 We can use the designer to train and deploy ML models without writing any code. Drag and drop datasets and components to create ML pipelines. Component \u00b6 An Azure Machine Learning component is a self-contained piece of code that does one step in a machine learning pipeline. Components are the building blocks of advanced machine learning pipelines. Components can do tasks such as data processing, model training, model scoring, and so on. A component is analogous to a function - it has a name, parameters, expects input, and returns output. AMl Compute \u00b6 A compute is a designated compute resource where you run your job or host your endpoint. Azure Machine learning supports the following types of compute: Compute cluster - a managed-compute infrastructure that allows you to easily create a cluster of CPU or GPU compute nodes in the cloud. Compute instance - a fully configured and managed development environment in the cloud. You can use the instance as a training or inference compute for development and testing. It's similar to a virtual machine on the cloud Inference cluster - used to deploy trained machine learning models to Azure Kubernetes Service. You can create an Azure Kubernetes Service (AKS) cluster from your Azure ML workspace, or attach an existing AKS cluster. Attached compute - You can attach your own compute resources to your workspace and use them for training and inference. Datastore \u00b6 Azure Machine Learning datastores securely keep the connection information to your data storage on Azure, so you don't have to code it in your scripts. You can register and create a datastore to easily connect to your storage account, and access the data in your underlying storage service. The CLI v2 and SDK v2 support the following types of cloud-based storage services: Azure Blob Container Azure File Share Azure Data Lake Azure Data Lake Gen2 MLFlow \u00b6 MLflow is an open-source framework that's designed to manage the complete ML lifecycle. Its ability to train and serve models on different platforms allows you to use a consistent set of tools regardless of where your experiments are running: locally on your computer, on a remote compute target, on a virtual machine, or on an AML compute instance. Model \u00b6 Azure machine learning models consist of the binary file(s) that represent a machine learning model and any corresponding metadata. Models can be created from a local or remote file or directory. For remote locations https, wasbs and azureml locations are supported . The created model will be tracked in the workspace under the specified name and version. Azure ML supports 3 types of storage format for models: custom_model mlflow_model triton_model Difference between Artifacts and Models Any file generated (and captured) from an experiment's run or job is an artifact. It may represent a model serialized as a Pickle file , the weights of a PyTorch or TensorFlow model, or even a text file containing the coefficients of a linear regression ONNX format \u00b6 It is the Open Neural Network Exchange format. Who created ONNX and why? Microsoft and a community of partners created ONNX as an open standard for representing ML models. Models from many frameworks including TensorFlow, PyTorch, SciKit-Learn, Keras, Chainer, MXNet, MATLAB, and SparkML can be exported or converted to the standard ONNX format. Once the models are in the ONNX format, they can be run on a variety of platforms and devices. ONNX Runtime : It is a high-performance inference engine for deploying ONNX models to production. It's optimized for both cloud and edge and works on Linux, Windows, and Mac. MLOPS \u00b6 DevOps for machine learning models, often called MLOps , is a process for developing models for production. A model's lifecycle from training to deployment must be auditable if not reproducible. Model training lifecycle \u00b6 The Azure training lifecycle consists of: Zipping the files in your project folder, ignoring those specified in .amlignore or .gitignore Scaling up your compute cluster Building or downloading the dockerfile to the compute node The system calculates a hash of: The base image Custom docker steps The conda definition YAML The system uses this hash as the key in a lookup of the workspace Azure Container Registry (ACR) If it is not found, it looks for a match in the global ACR If it is not found, the system builds a new image (which will be cached and registered with the workspace ACR ) Downloading your zipped project file to temporary storage on the compute node . Unzipping the project file. The compute node executing python <entry script> <arguments> Saving logs, model files, and other files written to ./outputs to the storage account associated with the workspace. Scaling down compute, including removing temporary storage.","title":"Azure ML"},{"location":"AI/azureml/#azure-ml","text":"","title":"Azure ML"},{"location":"AI/azureml/#aml-workspace","text":"The workspace is the top-level resource for Azure Machine Learning, providing a centralized place to work with all the artifacts you create. When you create a new workspace, it automatically creates below Azure resources: Storage account : It used to store files used by the workspace as well as data for experiments and model training . It Is used as the default datastore for the workspace. Jupyter notebooks that are used with your Azure Machine Learning compute instances are stored here as well. Application Insights : It is used to monitor predictive services in the workspace. Key Vault instance : Stores secrets that are used by compute targets and other sensitive information that's needed by the workspace. Container/Model registry , used to manage containers for deployed models. VMs : provide computing power for your AzureML workspace and are an integral part in deploying and training models. Load Balancer : a network load balancer is created for each compute instance and compute cluster to manage traffic even while the compute instance/cluster is stopped. Virtual Network : these help Azure resources communicate with one another, the internet, and other on-premises networks.","title":"AML Workspace"},{"location":"AI/azureml/#other-components","text":"","title":"Other components:"},{"location":"AI/azureml/#compute-instance","text":"It is a managed cloud-based workstation for data scientists. Each compute instance has only one owner, although you can share files between multiple compute instances. They can be used for dev and test purposes.","title":"Compute Instance"},{"location":"AI/azureml/#compute-target","text":"it is a designated compute resource or environment where you run your training script or host your service deployment. This location might be your local machine or a cloud-based compute resource. Types of Compute Targets Local \u2014 This is used to run the experiment on the same compute target as the code used to initiate the experiment. Training Cluster \u2014 for high scalable training requirements \u2014 distributed computes, CPU/GPU are enabled and scaled on-demand. Inference Cluster \u2014 containerized clusters to deploy the inference of the trained model as an overall application module. Attached Compute \u2014 to attach already acquired Azure ML VM or Databricks machine","title":"Compute Target"},{"location":"AI/azureml/#training-cluster","text":"AML compute instance AML compute cluster AKS Databricks Batch","title":"Training Cluster"},{"location":"AI/azureml/#inference-cluster","text":"Docker container is created before inference When performing inference, AML creates a Docker container that hosts the model and associated resources needed to use it. This container is then used in a compute target . Various inference compute target can be: AML endpoints : Fully managed computes for real-time (managed online endpoints) and batch scoring (batch endpoints) on serverless compute. AKS : use managed k8s ACI : Run docker container without orchestrator","title":"Inference Cluster"},{"location":"AI/azureml/#aml-concepts","text":"","title":"AML Concepts"},{"location":"AI/azureml/#model-deployment","text":"After you train an ML model, you need to deploy the model so that others can use it to do inferencing. In Azure Machine Learning, you can use endpoints and deployments to do so.","title":"Model deployment"},{"location":"AI/azureml/#endpoint","text":"Info An endpoint is an HTTPS endpoint that clients can call to receive the inferencing (scoring) output of a trained model. It provides: - Authentication using \"key & token\" based auth - SSL termination - A stable scoring URI (endpoint-name.region.inference.ml.azure.com)","title":"Endpoint"},{"location":"AI/azureml/#deployment","text":"A deployment is a set of resources required for hosting the model that does the actual inferencing. Remember A single endpoint can contain multiple deployments. Endpoints and deployments are independent Azure Resource Manager resources that appear in the Azure portal.","title":"Deployment"},{"location":"AI/azureml/#real-timebatch-scoring","text":"Batch scoring , or batch inferencing, involves invoking an endpoint with a reference to data. The batch endpoint runs jobs asynchronously to process data in parallel on compute clusters and store the data for further analysis Real-time scoring , or online inferencing, involves invoking an endpoint with one or more model deployments and receiving a response in near-real-time via HTTPs. Traffic can be split across multiple deployments, allowing for testing new model versions by diverting some amount of traffic initially and increasing once confidence in the new model is established.","title":"Real-time/batch scoring"},{"location":"AI/azureml/#cli-v2","text":"The AML CLI v2 is the latest extension for the Azure CLI. The CLI v2 provides commands in the format az ml <noun> <verb> <options> to create and maintain Azure ML assets and workflows. The assets or workflows themselves are defined using a YAML file . The YAML file defines the configuration of the asset or workflow \u2013 what is it, where should it run, and so on. AML commands examples az ml job create -- file my_job_definition . yaml az ml environment update -- name my - env -- file my_updated_env_definition . yaml az ml model list az ml compute show -- name my_compute","title":"CLI V2"},{"location":"AI/azureml/#aml-pipelines","text":"Pipelines are workflows of complete machine learning tasks that can be run independently. The Azure Machine Learning pipeline service automatically orchestrates all the dependencies between pipeline steps. You can create pipelines without using components, but components offer better amount of flexibility and reuse . Azure ML Pipelines may be defined in YAML and run from the CLI, authored in Python, or composed in Azure ML Studio Designer with a drag-and-drop UI.","title":"AML Pipelines"},{"location":"AI/azureml/#experiment","text":"An Experiment is a container of trials that represent multiple model runs.","title":"Experiment"},{"location":"AI/azureml/#environment","text":"AMl environments are an encapsulation of the environment where your ML training happens. They specify the Python packages, environment variables, and software settings around your training and scoring scripts. They also specify runtimes (Python, Spark, or Docker). Environments can broadly be divided into three categories: Curated : They are provided by Azure Machine Learning and are available in your workspace by default. Intended to be used as is, they contain collections of Python packages and settings to help you get started with various machine learning frameworks User-managed : In this you're responsible for setting up your environment and installing every package that your training script needs on the compute target. System-managed : Conda will manage env. build the env to docker AML builds environment definitions into Docker images and conda environments . It also caches the environments, so they can be reused in subsequent training jobs and service endpoint deployments.","title":"Environment"},{"location":"AI/azureml/#aml-studio","text":"","title":"AML Studio"},{"location":"AI/azureml/#aml-designer","text":"We can use the designer to train and deploy ML models without writing any code. Drag and drop datasets and components to create ML pipelines.","title":"AML Designer"},{"location":"AI/azureml/#component","text":"An Azure Machine Learning component is a self-contained piece of code that does one step in a machine learning pipeline. Components are the building blocks of advanced machine learning pipelines. Components can do tasks such as data processing, model training, model scoring, and so on. A component is analogous to a function - it has a name, parameters, expects input, and returns output.","title":"Component"},{"location":"AI/azureml/#aml-compute","text":"A compute is a designated compute resource where you run your job or host your endpoint. Azure Machine learning supports the following types of compute: Compute cluster - a managed-compute infrastructure that allows you to easily create a cluster of CPU or GPU compute nodes in the cloud. Compute instance - a fully configured and managed development environment in the cloud. You can use the instance as a training or inference compute for development and testing. It's similar to a virtual machine on the cloud Inference cluster - used to deploy trained machine learning models to Azure Kubernetes Service. You can create an Azure Kubernetes Service (AKS) cluster from your Azure ML workspace, or attach an existing AKS cluster. Attached compute - You can attach your own compute resources to your workspace and use them for training and inference.","title":"AMl Compute"},{"location":"AI/azureml/#datastore","text":"Azure Machine Learning datastores securely keep the connection information to your data storage on Azure, so you don't have to code it in your scripts. You can register and create a datastore to easily connect to your storage account, and access the data in your underlying storage service. The CLI v2 and SDK v2 support the following types of cloud-based storage services: Azure Blob Container Azure File Share Azure Data Lake Azure Data Lake Gen2","title":"Datastore"},{"location":"AI/azureml/#mlflow","text":"MLflow is an open-source framework that's designed to manage the complete ML lifecycle. Its ability to train and serve models on different platforms allows you to use a consistent set of tools regardless of where your experiments are running: locally on your computer, on a remote compute target, on a virtual machine, or on an AML compute instance.","title":"MLFlow"},{"location":"AI/azureml/#model","text":"Azure machine learning models consist of the binary file(s) that represent a machine learning model and any corresponding metadata. Models can be created from a local or remote file or directory. For remote locations https, wasbs and azureml locations are supported . The created model will be tracked in the workspace under the specified name and version. Azure ML supports 3 types of storage format for models: custom_model mlflow_model triton_model Difference between Artifacts and Models Any file generated (and captured) from an experiment's run or job is an artifact. It may represent a model serialized as a Pickle file , the weights of a PyTorch or TensorFlow model, or even a text file containing the coefficients of a linear regression","title":"Model"},{"location":"AI/azureml/#onnx-format","text":"It is the Open Neural Network Exchange format. Who created ONNX and why? Microsoft and a community of partners created ONNX as an open standard for representing ML models. Models from many frameworks including TensorFlow, PyTorch, SciKit-Learn, Keras, Chainer, MXNet, MATLAB, and SparkML can be exported or converted to the standard ONNX format. Once the models are in the ONNX format, they can be run on a variety of platforms and devices. ONNX Runtime : It is a high-performance inference engine for deploying ONNX models to production. It's optimized for both cloud and edge and works on Linux, Windows, and Mac.","title":"ONNX format"},{"location":"AI/azureml/#mlops","text":"DevOps for machine learning models, often called MLOps , is a process for developing models for production. A model's lifecycle from training to deployment must be auditable if not reproducible.","title":"MLOPS"},{"location":"AI/azureml/#model-training-lifecycle","text":"The Azure training lifecycle consists of: Zipping the files in your project folder, ignoring those specified in .amlignore or .gitignore Scaling up your compute cluster Building or downloading the dockerfile to the compute node The system calculates a hash of: The base image Custom docker steps The conda definition YAML The system uses this hash as the key in a lookup of the workspace Azure Container Registry (ACR) If it is not found, it looks for a match in the global ACR If it is not found, the system builds a new image (which will be cached and registered with the workspace ACR ) Downloading your zipped project file to temporary storage on the compute node . Unzipping the project file. The compute node executing python <entry script> <arguments> Saving logs, model files, and other files written to ./outputs to the storage account associated with the workspace. Scaling down compute, including removing temporary storage.","title":"Model training lifecycle"},{"location":"AI/intro_to_stat/","text":"Stat 101 concepts \u00b6 Descriptive and Inferential Statistics \u00b6 Descriptive Statistics \u00b6 It provides summary statistics of the existing data. Some examples are: Mean Median Standard deviation Co-relation Inferential Statistics \u00b6 It is the concept of deriving the conclusions about population using the sample data. It can be said as an extension of the descriptive statistics. It is used to make the business decisions. It helps us think beyond data. Descriptiive Statistics can answer questions like: who were our customers while Infrential Statistics can answer questions like Will it snow in Ottawa at 2 am on a certain day? Who can be our customers in next 2 years for a particular region? We need to calculate the likeliness for this. How many customers will we gain if we gain/loose if we take some decision? Generally speaking, the Descriptive Statistics is done on the histogram whereas the Infrential Statistics is done using the underlying distribution Random variable \u00b6 Discrete random variable \u00b6 the sum of all the probabilities should be one If all the probabilities for the variables can be listed, then we can call it as discrete random variable The term associated with this is probability mass function Continuous random variable \u00b6 We cannot specify all the probabilities in this case. The term associated with this is probability density function Various distributions \u00b6 Bernoulli Distribution \u00b6 It is associated with yes and no. It is a special case of Binomial distribution . The Bernoulli distribution is a discrete distribution having two possible outcomes labelled by n=0 and n=1 in which n=1 (\"success\") occurs with probability p and n=0 (\"failure\") occurs with probability q=1-p , where 0<p<1. It therefore has probability density function \\[ P(n) = \\Bigg\\{ \\begin{matrix} 1-p & n=0\\\\ p & n=1 \\end{matrix} \\] Examples: - Stock prices are going up or down? - Item sold or not sold? Bernoulli Trial \u00b6 It\u2019s an experiment where you can have one of two possible outcomes. For example, \u201cYes\u201d and \u201cNo\u201d or \u201cHeads\u201d and \u201cTails.\u201d A few examples: Coin tosses : record how many coins land heads up and how many land tails up? Births : how many boys and how many girls are born each day? Rolling Dice : the probability of a roll of two die resulting in a double six? Binomial Distribution \u00b6 It is associated with yes and no. A binomial distribution can be thought of as simply the probability of a SUCCESS or FAILURE outcome in an experiment or survey that is repeated multiple times. The binomial is a type of distribution that has two possible outcomes (the prefix \u201cbi\u201d means two, or twice). For example, a coin toss has only two possible outcomes: heads or tails and taking a test could have two possible outcomes: pass or fail. $$ P(X=x)=\\binom{n}{x} (p)^x (1\u2212p)^{n\u2212x} $$ The first variable in the binomial formula, n, stands for the number of times the experiment runs. The second variable, x, represents the probability of one specific outcome. Note Assumptions for this distribution are: - One trial is independent of another - There are 2 possible outcomes of each trial are possible - Probability of success is p and is same for all trials - Number of trials are fixed The diff between binomial and Bernoulli distribution is explained below: Tip The Bernoulli distribution is the discrete probability distribution and has only two possible outcomes. The Binomial distribution can be said as discrete probability distribution of the number of times an event is likely to occur within a given period of time. Uniform Distribution \u00b6 There is equal likeliness of events happening. Normal Distribution \u00b6 It is associated with yes and no. Z-score \u00b6 Hypothesis testing \u00b6 Central limit theorm \u00b6 Area under curve \u00b6 The integral of \\(f(x)\\) corresponds to the computation of the area under the graph of \\(f(x)\\) . Area between points \\(a\\) and \\(b\\) is given as \\[ A(a,b) = \\int_{a}^{b} f(x) dx \\] variability \u00b6 While the central tendency, or average, tells you where most of your points lie, variability summarizes how far apart they are. Low variability is ideal because it means that you can better predict information about the population based on sample data. High variability means that the values are less consistent, so it\u2019s harder to make predictions. What is a test statistic? \u00b6 A test statistic is a number calculated by a statistical test. It describes how far your observed data is from the null hypothesis of no relationship between variables or no difference among sample groups. The test statistic tells you how different two or more groups are from the overall population mean, or how different a linear slope is from the slope predicted by a null hypothesis. Different test statistics are used in different statistical tests. Nominal and Ordinal data \u00b6 Nominal data is classified without a natural order or rank, whereas ordinal data has a predetermined or natural order What is the difference between ordinal, interval and ratio variables? Why should I care? https://www.graphpad.com/support/faq/what-is-the-difference-between-ordinal-interval-and-ratio-variables-why-should-i-care/ Histogram \u00b6 Central tendency of data? \u00b6 using Arithmetic mean","title":"STAT 101"},{"location":"AI/intro_to_stat/#stat-101-concepts","text":"","title":"Stat 101 concepts"},{"location":"AI/intro_to_stat/#descriptive-and-inferential-statistics","text":"","title":"Descriptive and Inferential Statistics"},{"location":"AI/intro_to_stat/#descriptive-statistics","text":"It provides summary statistics of the existing data. Some examples are: Mean Median Standard deviation Co-relation","title":"Descriptive Statistics"},{"location":"AI/intro_to_stat/#inferential-statistics","text":"It is the concept of deriving the conclusions about population using the sample data. It can be said as an extension of the descriptive statistics. It is used to make the business decisions. It helps us think beyond data. Descriptiive Statistics can answer questions like: who were our customers while Infrential Statistics can answer questions like Will it snow in Ottawa at 2 am on a certain day? Who can be our customers in next 2 years for a particular region? We need to calculate the likeliness for this. How many customers will we gain if we gain/loose if we take some decision? Generally speaking, the Descriptive Statistics is done on the histogram whereas the Infrential Statistics is done using the underlying distribution","title":"Inferential Statistics"},{"location":"AI/intro_to_stat/#random-variable","text":"","title":"Random variable"},{"location":"AI/intro_to_stat/#discrete-random-variable","text":"the sum of all the probabilities should be one If all the probabilities for the variables can be listed, then we can call it as discrete random variable The term associated with this is probability mass function","title":"Discrete random variable"},{"location":"AI/intro_to_stat/#continuous-random-variable","text":"We cannot specify all the probabilities in this case. The term associated with this is probability density function","title":"Continuous random variable"},{"location":"AI/intro_to_stat/#various-distributions","text":"","title":"Various distributions"},{"location":"AI/intro_to_stat/#bernoulli-distribution","text":"It is associated with yes and no. It is a special case of Binomial distribution . The Bernoulli distribution is a discrete distribution having two possible outcomes labelled by n=0 and n=1 in which n=1 (\"success\") occurs with probability p and n=0 (\"failure\") occurs with probability q=1-p , where 0<p<1. It therefore has probability density function \\[ P(n) = \\Bigg\\{ \\begin{matrix} 1-p & n=0\\\\ p & n=1 \\end{matrix} \\] Examples: - Stock prices are going up or down? - Item sold or not sold?","title":"Bernoulli Distribution"},{"location":"AI/intro_to_stat/#bernoulli-trial","text":"It\u2019s an experiment where you can have one of two possible outcomes. For example, \u201cYes\u201d and \u201cNo\u201d or \u201cHeads\u201d and \u201cTails.\u201d A few examples: Coin tosses : record how many coins land heads up and how many land tails up? Births : how many boys and how many girls are born each day? Rolling Dice : the probability of a roll of two die resulting in a double six?","title":"Bernoulli Trial"},{"location":"AI/intro_to_stat/#binomial-distribution","text":"It is associated with yes and no. A binomial distribution can be thought of as simply the probability of a SUCCESS or FAILURE outcome in an experiment or survey that is repeated multiple times. The binomial is a type of distribution that has two possible outcomes (the prefix \u201cbi\u201d means two, or twice). For example, a coin toss has only two possible outcomes: heads or tails and taking a test could have two possible outcomes: pass or fail. $$ P(X=x)=\\binom{n}{x} (p)^x (1\u2212p)^{n\u2212x} $$ The first variable in the binomial formula, n, stands for the number of times the experiment runs. The second variable, x, represents the probability of one specific outcome. Note Assumptions for this distribution are: - One trial is independent of another - There are 2 possible outcomes of each trial are possible - Probability of success is p and is same for all trials - Number of trials are fixed The diff between binomial and Bernoulli distribution is explained below: Tip The Bernoulli distribution is the discrete probability distribution and has only two possible outcomes. The Binomial distribution can be said as discrete probability distribution of the number of times an event is likely to occur within a given period of time.","title":"Binomial Distribution"},{"location":"AI/intro_to_stat/#uniform-distribution","text":"There is equal likeliness of events happening.","title":"Uniform Distribution"},{"location":"AI/intro_to_stat/#normal-distribution","text":"It is associated with yes and no.","title":"Normal Distribution"},{"location":"AI/intro_to_stat/#z-score","text":"","title":"Z-score"},{"location":"AI/intro_to_stat/#hypothesis-testing","text":"","title":"Hypothesis testing"},{"location":"AI/intro_to_stat/#central-limit-theorm","text":"","title":"Central limit theorm"},{"location":"AI/intro_to_stat/#area-under-curve","text":"The integral of \\(f(x)\\) corresponds to the computation of the area under the graph of \\(f(x)\\) . Area between points \\(a\\) and \\(b\\) is given as \\[ A(a,b) = \\int_{a}^{b} f(x) dx \\]","title":"Area under curve"},{"location":"AI/intro_to_stat/#variability","text":"While the central tendency, or average, tells you where most of your points lie, variability summarizes how far apart they are. Low variability is ideal because it means that you can better predict information about the population based on sample data. High variability means that the values are less consistent, so it\u2019s harder to make predictions.","title":"variability"},{"location":"AI/intro_to_stat/#what-is-a-test-statistic","text":"A test statistic is a number calculated by a statistical test. It describes how far your observed data is from the null hypothesis of no relationship between variables or no difference among sample groups. The test statistic tells you how different two or more groups are from the overall population mean, or how different a linear slope is from the slope predicted by a null hypothesis. Different test statistics are used in different statistical tests.","title":"What is a test statistic?"},{"location":"AI/intro_to_stat/#nominal-and-ordinal-data","text":"Nominal data is classified without a natural order or rank, whereas ordinal data has a predetermined or natural order What is the difference between ordinal, interval and ratio variables? Why should I care? https://www.graphpad.com/support/faq/what-is-the-difference-between-ordinal-interval-and-ratio-variables-why-should-i-care/","title":"Nominal and Ordinal data"},{"location":"AI/intro_to_stat/#histogram","text":"","title":"Histogram"},{"location":"AI/intro_to_stat/#central-tendency-of-data","text":"using Arithmetic mean","title":"Central tendency of data?"},{"location":"Linux/grepContent/","text":"Find if content exists in some file \u00b6 Simple Search \u00b6 Lets say we have to sarch if pluign exists in ~/.bashrc file, then we can use grep -r plugin ~/.bashrc the output is output /Users/amar/.bashrc:# Which plugins would you like to load? ( plugins can be found in ~/.oh-my-bash/plugins/* ) /Users/amar/.bashrc:# Custom plugins may be added to ~/.oh-my-bash/custom/plugins/ Recursive Search \u00b6 grep -R \"domain\" /etc/apache2/httpd.conf # (1) /etc/apache2/httpd.conf:# as error documents. e.g. admin@your-domain.com Output of the grep command Finding exact text \u00b6 Lets imagine we have a file named file.txt with below contents File contents $ cat file.txt Ottawa is an awesome place places to too live Below will find all words which have to Simple search $ grep to file.txt to too Finding the exact to Find exact text $grep -w to file.txt to","title":"Grep content"},{"location":"Linux/grepContent/#find-if-content-exists-in-some-file","text":"","title":"Find if content exists in some file"},{"location":"Linux/grepContent/#simple-search","text":"Lets say we have to sarch if pluign exists in ~/.bashrc file, then we can use grep -r plugin ~/.bashrc the output is output /Users/amar/.bashrc:# Which plugins would you like to load? ( plugins can be found in ~/.oh-my-bash/plugins/* ) /Users/amar/.bashrc:# Custom plugins may be added to ~/.oh-my-bash/custom/plugins/","title":"Simple Search"},{"location":"Linux/grepContent/#recursive-search","text":"grep -R \"domain\" /etc/apache2/httpd.conf # (1) /etc/apache2/httpd.conf:# as error documents. e.g. admin@your-domain.com Output of the grep command","title":"Recursive Search"},{"location":"Linux/grepContent/#finding-exact-text","text":"Lets imagine we have a file named file.txt with below contents File contents $ cat file.txt Ottawa is an awesome place places to too live Below will find all words which have to Simple search $ grep to file.txt to too Finding the exact to Find exact text $grep -w to file.txt to","title":"Finding exact text"},{"location":"Linux/updatePaths/","text":"Update $path in Mac \u00b6 Let's imagine we have to update the Python path in PATH variable Paths are available in /etc/paths . Update then using sudo bash vim /etc/paths Enter the path of the Python install directory at the end of this list. Reload bashrc using source ~/.bashrc","title":"Update Paths"},{"location":"Linux/updatePaths/#update-path-in-mac","text":"Let's imagine we have to update the Python path in PATH variable Paths are available in /etc/paths . Update then using sudo bash vim /etc/paths Enter the path of the Python install directory at the end of this list. Reload bashrc using source ~/.bashrc","title":"Update $path in Mac"},{"location":"aws/acl/","text":"Default ACL allows all the inbound and outbound traffic. Custom ACL denies all inbound and outbound traffic. We can block IP\u2019s using ACL but not SG.","title":"ACL \u26d4\ufe0f"},{"location":"aws/api_gw/","text":"What is Lambda authorizer? \u00b6 TLDR A Lambda authorizer (formerly known as a custom authorizer) is an API Gateway feature that uses a Lambda function to control access to your API. A Lambda authorizer is useful if you want to implement a custom authorization scheme that uses a bearer token authentication strategy such as OAuth or SAML, or that uses request parameters to determine the caller's identity. When a client makes a request to one of your API's methods, API Gateway calls your Lambda authorizer, which takes the caller's identity as input and returns an IAM policy as output. Types of Lambda authorizers \u00b6 Info There are two types of Lambda authorizers: A token-based Lambda authorizer (also called a TOKEN authorizer) receives the caller's identity in a bearer token, such as a JSON Web Token (JWT) or an OAuth token. For an example application, see Open Banking Brazil - Authorization Samples on GitHub. A request parameter-based Lambda authorizer (also called a REQUEST authorizer) receives the caller's identity in a combination of headers, query string parameters, stageVariables, and $context variables. For WebSocket APIs, only request parameter-based authorizers are supported. Using custom authorizer It is possible to use an AWS Lambda function from an AWS account that is different from the one in which you created your API. For more information, Authorization workflow \u00b6 The client calls a method on an API Gateway API method, passing a bearer token or request parameters. What is Bearer token? Bearer authentication (also called token authentication) is an HTTP authentication scheme that involves security tokens called bearer tokens. The name \u201cBearer authentication\u201d can be understood as \u201cgive access to the bearer of this token.\u201d The bearer token is a cryptic string, usually generated by the server in response to a login request. The client must send this token in the Authorization header when making requests to protected resources API Gateway checks whether a Lambda authorizer is configured for the method. If it is, API Gateway calls the Lambda function. The Lambda function authenticates the caller by means such as the following: Calling out to an OAuth provider to get an OAuth access token Calling out to a SAML provider to get a SAML assertion. Generating an IAM policy based on the request parameter values. Retrieving credentials from a database. If the call succeeds, the Lambda function grants access by returning an output object containing at least an IAM policy and a principal identifier . API Gateway evaluates the policy. If access is denied, API Gateway returns a suitable HTTP status code, such as 403 ACCESS_DENIED . If access is allowed, API Gateway executes the method. If caching is enabled in the authorizer settings, API Gateway also caches the policy so that the Lambda authorizer function doesn't need to be invoked again.","title":"API Gateway \ud83c\udf09"},{"location":"aws/api_gw/#what-is-lambda-authorizer","text":"TLDR A Lambda authorizer (formerly known as a custom authorizer) is an API Gateway feature that uses a Lambda function to control access to your API. A Lambda authorizer is useful if you want to implement a custom authorization scheme that uses a bearer token authentication strategy such as OAuth or SAML, or that uses request parameters to determine the caller's identity. When a client makes a request to one of your API's methods, API Gateway calls your Lambda authorizer, which takes the caller's identity as input and returns an IAM policy as output.","title":"What is Lambda authorizer?"},{"location":"aws/api_gw/#types-of-lambda-authorizers","text":"Info There are two types of Lambda authorizers: A token-based Lambda authorizer (also called a TOKEN authorizer) receives the caller's identity in a bearer token, such as a JSON Web Token (JWT) or an OAuth token. For an example application, see Open Banking Brazil - Authorization Samples on GitHub. A request parameter-based Lambda authorizer (also called a REQUEST authorizer) receives the caller's identity in a combination of headers, query string parameters, stageVariables, and $context variables. For WebSocket APIs, only request parameter-based authorizers are supported. Using custom authorizer It is possible to use an AWS Lambda function from an AWS account that is different from the one in which you created your API. For more information,","title":"Types of Lambda authorizers"},{"location":"aws/api_gw/#authorization-workflow","text":"The client calls a method on an API Gateway API method, passing a bearer token or request parameters. What is Bearer token? Bearer authentication (also called token authentication) is an HTTP authentication scheme that involves security tokens called bearer tokens. The name \u201cBearer authentication\u201d can be understood as \u201cgive access to the bearer of this token.\u201d The bearer token is a cryptic string, usually generated by the server in response to a login request. The client must send this token in the Authorization header when making requests to protected resources API Gateway checks whether a Lambda authorizer is configured for the method. If it is, API Gateway calls the Lambda function. The Lambda function authenticates the caller by means such as the following: Calling out to an OAuth provider to get an OAuth access token Calling out to a SAML provider to get a SAML assertion. Generating an IAM policy based on the request parameter values. Retrieving credentials from a database. If the call succeeds, the Lambda function grants access by returning an output object containing at least an IAM policy and a principal identifier . API Gateway evaluates the policy. If access is denied, API Gateway returns a suitable HTTP status code, such as 403 ACCESS_DENIED . If access is allowed, API Gateway executes the method. If caching is enabled in the authorizer settings, API Gateway also caches the policy so that the Lambda authorizer function doesn't need to be invoked again.","title":"Authorization workflow"},{"location":"aws/appsync/","text":"AWS AppSync provides a robust, scalable GraphQL interface for application developers to combine data from multiple sources, including Amazon DynamoDB, AWS Lambda, and HTTP APIs. Features \u00b6 Powerful GraphQL schema editing through the AWS AppSync console, including automatic GraphQL schema generation from DynamoDB Efficient data caching Integration with Amazon Cognito user pools for fine-grained access control at a per-field level Arcitecture \u00b6 Concepts \u00b6 GraphQL Proxy \u00b6 A component that runs the GraphQL engine for processing requests and mapping them to logical functions for data operations or triggers. The data resolution process performs a batching process (called the Data Loader) to your data sources. This component also manages conflict detection and resolution strategies. Operation \u00b6 AWS AppSync supports the three GraphQL operations: query (read-only fetch), mutation (write followed by a fetch), and subscription (long-lived requests that receive data in response to events). Action \u00b6 There is one action that AWS AppSync defines. This action is a notification to connected subscribers, which is the result of a mutation. Clients become subscribers through a handshake process following a GraphQL subscription. Data Source \u00b6 A persistent storage system or a trigger, along with credentials for accessing that system or trigger. Your application state is managed by the system or trigger defined in a data source. Examples of data sources include NoSQL databases, relational databases, AWS Lambda functions, and HTTP APIs. Resolver \u00b6 A function that converts the GraphQL payload to the underlying storage system protocol and executes if the caller is authorized to invoke it. Resolvers are comprised of request and response mapping templates, which contain transformation and execution logic. Unit Resolver \u00b6 A unit resolver is a resolver that performs a single operation against a predefined data source. Pipeline Resolver \u00b6 A pipeline resolver is a resolver that allows executing multiple operations against one or more data sources. A pipeline resolver is composed of a list of functions. Each function is executed in sequence and can execute a single operation against a predefined data source. Function \u00b6 A function defines a single operation that can be used across pipeline resolvers. Functions can be reused to perform redundant logic throughout the GraphQL Proxy. Functions are comprised of a request and a response mapping template, a data source name, and a version. Identity \u00b6 A representation of the caller based on a set of credentials, which must be sent along with every request to the GraphQL proxy. It includes permissions to invoke resolvers. Identity information is also passed as context to a resolver and the conflict handler to perform additional checks. AWS AppSync Client \u00b6 The location where GraphQL operations are defined. The client performs appropriate authorization wrapping of request statements before submitting to the GraphQL proxy. Responses are persisted in an offline store and mutations are made in a write-through pattern.","title":"App Sync \u267b\ufe0f"},{"location":"aws/appsync/#features","text":"Powerful GraphQL schema editing through the AWS AppSync console, including automatic GraphQL schema generation from DynamoDB Efficient data caching Integration with Amazon Cognito user pools for fine-grained access control at a per-field level","title":"Features"},{"location":"aws/appsync/#arcitecture","text":"","title":"Arcitecture"},{"location":"aws/appsync/#concepts","text":"","title":"Concepts"},{"location":"aws/appsync/#graphql-proxy","text":"A component that runs the GraphQL engine for processing requests and mapping them to logical functions for data operations or triggers. The data resolution process performs a batching process (called the Data Loader) to your data sources. This component also manages conflict detection and resolution strategies.","title":"GraphQL Proxy"},{"location":"aws/appsync/#operation","text":"AWS AppSync supports the three GraphQL operations: query (read-only fetch), mutation (write followed by a fetch), and subscription (long-lived requests that receive data in response to events).","title":"Operation"},{"location":"aws/appsync/#action","text":"There is one action that AWS AppSync defines. This action is a notification to connected subscribers, which is the result of a mutation. Clients become subscribers through a handshake process following a GraphQL subscription.","title":"Action"},{"location":"aws/appsync/#data-source","text":"A persistent storage system or a trigger, along with credentials for accessing that system or trigger. Your application state is managed by the system or trigger defined in a data source. Examples of data sources include NoSQL databases, relational databases, AWS Lambda functions, and HTTP APIs.","title":"Data Source"},{"location":"aws/appsync/#resolver","text":"A function that converts the GraphQL payload to the underlying storage system protocol and executes if the caller is authorized to invoke it. Resolvers are comprised of request and response mapping templates, which contain transformation and execution logic.","title":"Resolver"},{"location":"aws/appsync/#unit-resolver","text":"A unit resolver is a resolver that performs a single operation against a predefined data source.","title":"Unit Resolver"},{"location":"aws/appsync/#pipeline-resolver","text":"A pipeline resolver is a resolver that allows executing multiple operations against one or more data sources. A pipeline resolver is composed of a list of functions. Each function is executed in sequence and can execute a single operation against a predefined data source.","title":"Pipeline Resolver"},{"location":"aws/appsync/#function","text":"A function defines a single operation that can be used across pipeline resolvers. Functions can be reused to perform redundant logic throughout the GraphQL Proxy. Functions are comprised of a request and a response mapping template, a data source name, and a version.","title":"Function"},{"location":"aws/appsync/#identity","text":"A representation of the caller based on a set of credentials, which must be sent along with every request to the GraphQL proxy. It includes permissions to invoke resolvers. Identity information is also passed as context to a resolver and the conflict handler to perform additional checks.","title":"Identity"},{"location":"aws/appsync/#aws-appsync-client","text":"The location where GraphQL operations are defined. The client performs appropriate authorization wrapping of request statements before submitting to the GraphQL proxy. Responses are persisted in an offline store and mutations are made in a write-through pattern.","title":"AWS AppSync Client"},{"location":"aws/athnea/","text":"It Interactive query service which allows analyzing the S3 database using the SQL. Athena is serverless, so there is no infrastructure to manage, and you pay only for the queries that you run. It will work with a number of data formats including \"JSON\", \"Apache Parquet\", \"Apache ORC\" amongst others, but \"XML\" is not a format that is supported. No need to set up the ETL processes Athena is used to Generate Query log files in s3. Generate reports from s3.","title":"Athnea"},{"location":"aws/autoScaling/","text":"A placement group is concerned primarily with network throughput and reducing latency among EC2 instances within a single availability zone. AWS does support a placement group spanning multiple AZs via spread placement groups, but unless \u201cspread\u201d is specifically mentioned, you should assume the question references a \u201cnormal\u201d (or \u201ccluster\u201d) placement group. Cluster placement group is the default type of placement group. Spread placement groups can span availability zones and support up to 7 instances per zone.","title":"Auto Scaling \ud83d\udccf"},{"location":"aws/cloudfront/","text":"It is a CDN which have 2 types of distributions Web distribution : fFor static and dynamic content, media files. RTMP : It is used to speed up the distribution of streaming media files using Adobe Flash\u2019s RTMP (Real-Time Messaging Protocol). Using this the user can begin playing - the file before it\u2019s downloaded from the server. Origin is the actual location where the resource is (it can be S3), Edge is where the user. Edge location will cache the data for some period called TTL ( Time To Live ) Distribution is the name given to the CDN, which is the collection of edge locations. \ud83d\udca1 You can clear the contents in the cache, but you will be charged for this. Invalidation removes the objects from the edge cache. /* will invalidate everything. Creating and deleting distribution takes some time. Remember The invalidation API is the fastest way to remove a file or object, although it will typically incur additional costs. First, remove the file from the origin servers; then set the expiration time on the CloudFront distribution to 0 to remove the file from the CloudFront. RDS instance can be an origin server. You can read and write objects directly to an edge location. You cannot delete or update them directly; only the CloudFront service can handle that. CloudFront allows interaction via CloudFormation, the AWS CLI, the AWS console, the AWS CLI, the AWS APIs, and the various SDKs that AWS provides. CloudFront can front several AWS services: AWS Shield, S3, ELBs (including ALBs), and EC2 instances. CloudFront automatically provides AWS Shield (standard) to protect from DDoS, and it also can integrate with AWS WAF and AWS Shield advanced. These combine to secure content at the edge. CloudFront is easy to set up and lets you create a global content delivery network without contracts. It\u2019s also a mechanism for distributing content at low latency. When you create a CloudFront distribution, you register a domain name for your static and dynamic content. This domain should then be used by clients. There is no charge associated with data moving from any region to a CloudFront edge location. CloudFront supports a variety of origin servers, including a non-AWS origin server. It supports EC2 regardless of region, as well. It does not support RDS or SNS. Edge locations can be set to have a 0-second expiration period, which effectively means no caching occurs. CloudFront can distribute content from an ELB, rather than directly interfacing with S3, and can do the same with a Route 53 record set. These allow the content to come from multiple instances. CloudFront serves content from origin servers, usually static files, and dynamic responses. These origin servers are often S3 buckets for static content and EC2 instances for dynamic content.","title":"CloudFront \ud83c\udf27"},{"location":"aws/cloudwatch/","text":"Cloudwatch monitors performance. Standard monitoring is 5 mins while detailed monitoring is 1 minute Cloudwatch can monitor - EC2 - Autoscaling groups - Elastic Load Balancers - Route 53 checks - EBS - CloudFront CloudTrain and Cloudwatch difference? CloudTrail is different from CloudWatch as it records what API calls are made to AWS management console and which actions are taken. We can see who made the call, what was the IP, and when calls were made.","title":"Cloud Trail"},{"location":"aws/concepts/","text":"Bastion Host \u00b6 A bastion host is a special-purpose computer on a network specifically designed and configured to withstand attacks. The computer generally hosts a single application, for example, a proxy server, and all other services are removed or limited to reduce the threat to the computer. It is hardened In AWS, a bastion host is a server whose purpose is to provide access to a private network from an external network, such as the Internet. Because of its exposure to potential attacks, a bastion host must minimize the chances of penetration. For users to talk to a private instance, we place a bastion host in a public subnet that is connected to the instance in a private subnet Pre Signed URL \u00b6 A user who does not have AWS credentials or permission to access an S3 object can be granted temporary access by using a pre-signed URL. A pre-signed URL is generated by an AWS user who has access to the object. The generated URL is then given to the unauthorized user. The pre-signed URL can be entered in a browser or used by a program or HTML webpage. The credentials used by the pre-signed URL are those of the AWS user who generated the URL Dedicated instances \u00b6 Instances run on hardware that is dedicated to a host. Dedicated Host \u00b6 In this case, the whole server is dedicated to a particular host. It is used in case, where a company has a security policy. VPC peering \u00b6 A VPC peering connection is a networking connection between two VPCs that enables you to route traffic between them using private IPv4 addresses or IPv6 addresses. Instances in either VPC can communicate with each other as if they are within the same network. You can create a VPC peering connection between your own VPCs, or with a VPC in another AWS account. The VPCs can be in different regions (also known as an inter-region VPC peering connection). Inter region VPC peering : You can establish peering relationships between VPCs across different AWS Regions (also called Inter-Region VPC Peering). This allows VPC resources including EC2 instances, Amazon RDS databases, and Lambda functions that run in different AWS Regions to communicate with each other using private IP addresses, without requiring gateways, VPN connections, or separate network appliances. The traffic remains in the private IP space. All inter-region traffic is encrypted with no single point of failure, or bandwidth bottleneck. A VPC peering connection is a one to one relationship between two VPCs. You can create multiple VPC peering connections for each VPC that you own, but transitive peering relationships are not supported. NAT Gateway \u00b6 NAT gateway is a managed NAT service. We create NAT instance in a public subnet so that it can talk to the internet. Lift and shift \u00b6 \"Lift and shift\" is a strategy for migrating a workload to the cloud without redesigning the application or making code changes. Also called rehosting.","title":"Basic Concepts"},{"location":"aws/concepts/#bastion-host","text":"A bastion host is a special-purpose computer on a network specifically designed and configured to withstand attacks. The computer generally hosts a single application, for example, a proxy server, and all other services are removed or limited to reduce the threat to the computer. It is hardened In AWS, a bastion host is a server whose purpose is to provide access to a private network from an external network, such as the Internet. Because of its exposure to potential attacks, a bastion host must minimize the chances of penetration. For users to talk to a private instance, we place a bastion host in a public subnet that is connected to the instance in a private subnet","title":"Bastion Host"},{"location":"aws/concepts/#pre-signed-url","text":"A user who does not have AWS credentials or permission to access an S3 object can be granted temporary access by using a pre-signed URL. A pre-signed URL is generated by an AWS user who has access to the object. The generated URL is then given to the unauthorized user. The pre-signed URL can be entered in a browser or used by a program or HTML webpage. The credentials used by the pre-signed URL are those of the AWS user who generated the URL","title":"Pre Signed URL"},{"location":"aws/concepts/#dedicated-instances","text":"Instances run on hardware that is dedicated to a host.","title":"Dedicated instances"},{"location":"aws/concepts/#dedicated-host","text":"In this case, the whole server is dedicated to a particular host. It is used in case, where a company has a security policy.","title":"Dedicated Host"},{"location":"aws/concepts/#vpc-peering","text":"A VPC peering connection is a networking connection between two VPCs that enables you to route traffic between them using private IPv4 addresses or IPv6 addresses. Instances in either VPC can communicate with each other as if they are within the same network. You can create a VPC peering connection between your own VPCs, or with a VPC in another AWS account. The VPCs can be in different regions (also known as an inter-region VPC peering connection). Inter region VPC peering : You can establish peering relationships between VPCs across different AWS Regions (also called Inter-Region VPC Peering). This allows VPC resources including EC2 instances, Amazon RDS databases, and Lambda functions that run in different AWS Regions to communicate with each other using private IP addresses, without requiring gateways, VPN connections, or separate network appliances. The traffic remains in the private IP space. All inter-region traffic is encrypted with no single point of failure, or bandwidth bottleneck. A VPC peering connection is a one to one relationship between two VPCs. You can create multiple VPC peering connections for each VPC that you own, but transitive peering relationships are not supported.","title":"VPC peering"},{"location":"aws/concepts/#nat-gateway","text":"NAT gateway is a managed NAT service. We create NAT instance in a public subnet so that it can talk to the internet.","title":"NAT Gateway"},{"location":"aws/concepts/#lift-and-shift","text":"\"Lift and shift\" is a strategy for migrating a workload to the cloud without redesigning the application or making code changes. Also called rehosting.","title":"Lift and shift"},{"location":"aws/dynamo/","text":"Dynamo DB \u00b6 DynamoDB is a database that supports key-value and document-based table design. It is fully-managed and provides high performance at scale. Interestingly, its performance improves with scale. Scaling in Dynamo \u00b6 It works better on large scale due to consistent hashing. Schema \u00b6 The only schema constraint you have when inserting an item in a table is that the item should have a unique primary key. As long as we provide a unique primary key, we can insert whatever data we want to insert. Scaling \u00b6 Push-button scaling: This is just a very fancy term to say you can scale your instance resources using a graphical user interface. To be precise, it means that you can scale the size of an instance(memory, CPU, PIOPS, disk, etc) either up or down, with the click of a button. Partitioning and re-partitioning \u00b6 AS we know that data is horizontally scaled across multiple servers in Dynamo. This means that the data needs to be partitioned. Also, servers are added and removed all the time, which means that they have to be re-partitioned frequently. You don\u2019t have to worry about that this either, since the partitioning and repartitioning processes are managed by AWS without downtime. Storing large files in Dynamo Although we can save up to 400 KB of data in one item, if we are trying to save larger files in DynamoDB, we will be at a loss in terms of cost. It would be better for us to use AWS S3 to store the files and have links to the files in our database \ud83d\ude04 Tip DynamoDb is running on SSD and have minimum of 3 instances. Dynamo Data Model \u00b6 Items \u00b6 Items in DynamoDB are similar to the rows in relational databases. An item belongs to a table and can contain multiple attributes. An item in DynamoDB can also be represented as a JSON object (a collection of key-value pairs). Attributes \u00b6 Each individual key-value pair of an item is known as an attribute. An item is built from multiple attributes. We can think of attributes as the properties of the item when we think of the item as a JSON object. Values of attributes can have many scalar and composite data types Primary key \u00b6 Each table in DynamoDB contains a primary key . A primary key is a special set of attributes. Its value is unique for every item and is used to identify the item in the database. Under the hood, it is used to partition and store the data in order. There are two types of primary keys: Partition key : Here, we have a unique key of scalar type (string, number, boolean), which determines the storage partition the item will go into. Partition key and Sort key : Here, we have two keys. The partition key determines the partition where the item goes into the storage and the sort key determines the rank of the item in the partition. Neither of these two keys need to be unique. However, their combination should be unique. Designing No-SQL Schema \u00b6 NoSQL design requires a different mindset than RDBMS design. For an RDBMS, you can go ahead and create a normalized data model without thinking about access patterns. In particular, it is important to understand three fundamental properties of your application's access patterns before you begin: Data size : Knowing how much data will be stored and requested at one time will help determine the most effective way to partition the data. Data shape : Instead of reshaping data when a query is processed (as an RDBMS system does), a NoSQL database organizes data so that its shape in the database corresponds with what will be queried. This is a key factor in increasing speed and scalability. Data velocity : DynamoDB scales by increasing the number of physical partitions that are available to process queries, and by efficiently distributing data across those partitions. Knowing in advance what the peak query loads will be might help determine how to partition data to best use I/O capacity. No-SQL best practices for NFR's \u00b6 Keep related data together \u00b6 The single most important factor in speeding up response time: keeping related data together in one place. Tip As a general rule, you should maintain as few tables as possible in a DynamoDB application.Exceptions are cases where high-volume time series data are involved, or datasets that have very different access patterns. A single table with inverted indexes can usually enable simple queries to create and retrieve the complex hierarchical data structures required by your application. Use sort order \u00b6 Related items can be grouped together and queried efficiently if their key design causes them to sort together. Distribute queries \u00b6 It is also important that a high volume of queries not be focused on one part of the database, where they can exceed I/O capacity. Instead, you should design data keys to distribute traffic evenly across partitions as much as possible, avoiding \"hot spots.\" Use global secondary indexes \u00b6 By creating specific global secondary indexes, you can enable different queries than your main table can support, and that are still fast and relatively inexpensive.","title":"Dynamo DB"},{"location":"aws/dynamo/#dynamo-db","text":"DynamoDB is a database that supports key-value and document-based table design. It is fully-managed and provides high performance at scale. Interestingly, its performance improves with scale.","title":"Dynamo DB"},{"location":"aws/dynamo/#scaling-in-dynamo","text":"It works better on large scale due to consistent hashing.","title":"Scaling in Dynamo"},{"location":"aws/dynamo/#schema","text":"The only schema constraint you have when inserting an item in a table is that the item should have a unique primary key. As long as we provide a unique primary key, we can insert whatever data we want to insert.","title":"Schema"},{"location":"aws/dynamo/#scaling","text":"Push-button scaling: This is just a very fancy term to say you can scale your instance resources using a graphical user interface. To be precise, it means that you can scale the size of an instance(memory, CPU, PIOPS, disk, etc) either up or down, with the click of a button.","title":"Scaling"},{"location":"aws/dynamo/#partitioning-and-re-partitioning","text":"AS we know that data is horizontally scaled across multiple servers in Dynamo. This means that the data needs to be partitioned. Also, servers are added and removed all the time, which means that they have to be re-partitioned frequently. You don\u2019t have to worry about that this either, since the partitioning and repartitioning processes are managed by AWS without downtime. Storing large files in Dynamo Although we can save up to 400 KB of data in one item, if we are trying to save larger files in DynamoDB, we will be at a loss in terms of cost. It would be better for us to use AWS S3 to store the files and have links to the files in our database \ud83d\ude04 Tip DynamoDb is running on SSD and have minimum of 3 instances.","title":"Partitioning and re-partitioning"},{"location":"aws/dynamo/#dynamo-data-model","text":"","title":"Dynamo Data Model"},{"location":"aws/dynamo/#items","text":"Items in DynamoDB are similar to the rows in relational databases. An item belongs to a table and can contain multiple attributes. An item in DynamoDB can also be represented as a JSON object (a collection of key-value pairs).","title":"Items"},{"location":"aws/dynamo/#attributes","text":"Each individual key-value pair of an item is known as an attribute. An item is built from multiple attributes. We can think of attributes as the properties of the item when we think of the item as a JSON object. Values of attributes can have many scalar and composite data types","title":"Attributes"},{"location":"aws/dynamo/#primary-key","text":"Each table in DynamoDB contains a primary key . A primary key is a special set of attributes. Its value is unique for every item and is used to identify the item in the database. Under the hood, it is used to partition and store the data in order. There are two types of primary keys: Partition key : Here, we have a unique key of scalar type (string, number, boolean), which determines the storage partition the item will go into. Partition key and Sort key : Here, we have two keys. The partition key determines the partition where the item goes into the storage and the sort key determines the rank of the item in the partition. Neither of these two keys need to be unique. However, their combination should be unique.","title":"Primary key"},{"location":"aws/dynamo/#designing-no-sql-schema","text":"NoSQL design requires a different mindset than RDBMS design. For an RDBMS, you can go ahead and create a normalized data model without thinking about access patterns. In particular, it is important to understand three fundamental properties of your application's access patterns before you begin: Data size : Knowing how much data will be stored and requested at one time will help determine the most effective way to partition the data. Data shape : Instead of reshaping data when a query is processed (as an RDBMS system does), a NoSQL database organizes data so that its shape in the database corresponds with what will be queried. This is a key factor in increasing speed and scalability. Data velocity : DynamoDB scales by increasing the number of physical partitions that are available to process queries, and by efficiently distributing data across those partitions. Knowing in advance what the peak query loads will be might help determine how to partition data to best use I/O capacity.","title":"Designing No-SQL Schema"},{"location":"aws/dynamo/#no-sql-best-practices-for-nfrs","text":"","title":"No-SQL best practices for NFR's"},{"location":"aws/dynamo/#keep-related-data-together","text":"The single most important factor in speeding up response time: keeping related data together in one place. Tip As a general rule, you should maintain as few tables as possible in a DynamoDB application.Exceptions are cases where high-volume time series data are involved, or datasets that have very different access patterns. A single table with inverted indexes can usually enable simple queries to create and retrieve the complex hierarchical data structures required by your application.","title":"Keep related data together"},{"location":"aws/dynamo/#use-sort-order","text":"Related items can be grouped together and queried efficiently if their key design causes them to sort together.","title":"Use sort order"},{"location":"aws/dynamo/#distribute-queries","text":"It is also important that a high volume of queries not be focused on one part of the database, where they can exceed I/O capacity. Instead, you should design data keys to distribute traffic evenly across partitions as much as possible, avoiding \"hot spots.\"","title":"Distribute queries"},{"location":"aws/dynamo/#use-global-secondary-indexes","text":"By creating specific global secondary indexes, you can enable different queries than your main table can support, and that are still fast and relatively inexpensive.","title":"Use global secondary indexes"},{"location":"aws/ebs/","text":"We can think them of to be like virtual hard drives They can only be used with EC2 Can only be tied to single AZ Officially, instances can have up to 28 EBS attachments. One of those attachments is the network interface attachment, leaving 27 attachments available for EBS volumes. However, the better approach is to remember that an instance can attach to a root volume and several more volumes (more than two). As shown in the image below, the EC2 instance storage is locked to EC2 instance while EBS are not locked to an EC2. We can also create snapshot from EBS !!! question 'Why do we need snapshots?' - Cost effective backup stragegy - To share data-sets with others users/accounts - Migrate a system to new AZ/Region - They are used to convert unencrypted volume to an encrypted one Each EBS is replicated in it\u2019s AZ by default. Encryption \u00b6 EBS volumes can be encrypted when they are created. You cannot encrypt an existing EBS volume \u201con the fly.\u201d You must create a snapshot and then encrypt that snapshot as you copy it to another, encrypted snapshot. You can then restore from that new snapshot. What all is encrypted in EBS? There are four types of data encrypted when an EBS volume is encrypted: - Data at rest on the volume. - Data moving between the volume and the instance. - Any snapshots created from the volume. - Any volumes created from those snapshots. Points to remember You must make a copy of an unencrypted snapshot to apply encryption. You cannot encrypt an existing EBS volume. You cannot encrypt a snapshot that is unencrypted. You can encrypt a copy of a snapshot and restore an encrypted snapshot to a volume that is encrypted. Different types of EBS \u00b6 General-purpose SSD Provisioned IOPS SSD Throughput optimized HDD Cold HDD: Cold HDD is the lowest cost HDD EBS magnetic HDD Move the EBS from one AZ to another \u00b6 Option A \u00b6 creates a snapshot from a volume. create an AMI from a snapshot. Option B \u00b6 Copy the AMI to a different region. Then go to that region and choose the AZ AZ\u2019s can be changed when we choose from the subnets Types of root volumes \u00b6 2 types of root volumes are there: Instance storage : They are called Ephemeral storage as we are going to lose all the data if it\u2019s stopped. They can not be stopped. EBS backed : they can be stopped, they are persisted as well. Remember By default, root volumes do get deleted when the associated instance terminates. However, you can configure this to not be the case. We can create AMI from both snapshots and volumes. For an EBS volume that is used as a root volume, if you want to take a consistent - snapshot then stop the instance as it is writing the data to EBS. ENI (Elastic Network Interface): EN (Enhanced Networking): This has a high speed of 100 Gbps and is used for high - performance. It uses the ENA (Enhanced network adapter). EFA (Elastic Fiber Adapter): use for HPC and ML. Warning If we close the instance, then the root volume is deleted but the persisted data - remains. Volumes exist on EBS whereas the snapshots exist on S3 Snapshot is a delta (incremental). Taking the first snapshot may take a lot of time. We can change the EBS volume size on the fly. Volumes are always in the same AZ as the instance is. We can create AMI from snapshots. Move EC2 to at new AZ take a snapshot \u2192 to create AMI \u2192 use AMI to launch in new AZ Move EC2 to a new region Take a snapshot \u2192 to create AMI \u2192 copy AMI to new region \u2192 use AMI to launch in new - AZ","title":"EBS \u25a3"},{"location":"aws/ebs/#encryption","text":"EBS volumes can be encrypted when they are created. You cannot encrypt an existing EBS volume \u201con the fly.\u201d You must create a snapshot and then encrypt that snapshot as you copy it to another, encrypted snapshot. You can then restore from that new snapshot. What all is encrypted in EBS? There are four types of data encrypted when an EBS volume is encrypted: - Data at rest on the volume. - Data moving between the volume and the instance. - Any snapshots created from the volume. - Any volumes created from those snapshots. Points to remember You must make a copy of an unencrypted snapshot to apply encryption. You cannot encrypt an existing EBS volume. You cannot encrypt a snapshot that is unencrypted. You can encrypt a copy of a snapshot and restore an encrypted snapshot to a volume that is encrypted.","title":"Encryption"},{"location":"aws/ebs/#different-types-of-ebs","text":"General-purpose SSD Provisioned IOPS SSD Throughput optimized HDD Cold HDD: Cold HDD is the lowest cost HDD EBS magnetic HDD","title":"Different types of EBS"},{"location":"aws/ebs/#move-the-ebs-from-one-az-to-another","text":"","title":"Move the EBS from one AZ to another"},{"location":"aws/ebs/#option-a","text":"creates a snapshot from a volume. create an AMI from a snapshot.","title":"Option A"},{"location":"aws/ebs/#option-b","text":"Copy the AMI to a different region. Then go to that region and choose the AZ AZ\u2019s can be changed when we choose from the subnets","title":"Option B"},{"location":"aws/ebs/#types-of-root-volumes","text":"2 types of root volumes are there: Instance storage : They are called Ephemeral storage as we are going to lose all the data if it\u2019s stopped. They can not be stopped. EBS backed : they can be stopped, they are persisted as well. Remember By default, root volumes do get deleted when the associated instance terminates. However, you can configure this to not be the case. We can create AMI from both snapshots and volumes. For an EBS volume that is used as a root volume, if you want to take a consistent - snapshot then stop the instance as it is writing the data to EBS. ENI (Elastic Network Interface): EN (Enhanced Networking): This has a high speed of 100 Gbps and is used for high - performance. It uses the ENA (Enhanced network adapter). EFA (Elastic Fiber Adapter): use for HPC and ML. Warning If we close the instance, then the root volume is deleted but the persisted data - remains. Volumes exist on EBS whereas the snapshots exist on S3 Snapshot is a delta (incremental). Taking the first snapshot may take a lot of time. We can change the EBS volume size on the fly. Volumes are always in the same AZ as the instance is. We can create AMI from snapshots. Move EC2 to at new AZ take a snapshot \u2192 to create AMI \u2192 use AMI to launch in new AZ Move EC2 to a new region Take a snapshot \u2192 to create AMI \u2192 copy AMI to new region \u2192 use AMI to launch in new - AZ","title":"Types of root volumes"},{"location":"aws/ec2/","text":"EC2 instances, as well as ECS containers, can both be scaled up and down by - Auto Scaling A launch configuration contains an AMI ID, key pair, instance type, security - groups, and possibly a block device mapping. There are a number of valid scaling policies for Auto Scaling: Maintain current instance levels : use to ensure that a specific number of - instances is running at all times. Manual scaling : Manual scaling allows you to specify a minimum and a maximum - number of instances as well as the desired capacity. The Auto Scaling policy - then handles maintaining that capacity. Schedule-based scaling : Demand-based scaling : Demand-based scaling allows you to specify parameters to - control scaling. One of those parameters can be CPU utilization InService and Standby are valid states for an instance in an auto-scaling group. You have to create a launch configuration first, then an Auto Scaling group, - and then you can verify your configuration and group. A launch configuration needs a single AMI ID to use for all instances it - launches. Default security groups prevent all incoming traffic and allow all traffic out - whereas the new SG allows all the outgoing traffic and blocks all the incoming - traffic. It is generally better to allow AWS to handle encryption in cases where you - want to ensure all encryption is the same across a data store. A bastion host is a publicly accessible host that allows traffic to connect to - it. Then, an additional connection is made from the bastion host into a private - subnet and the hosts within that subnet. The security of the bastion must be different from the hosts in the private - subnet. The bastion host should be hardened significantly as it is public, but - also accessible; this is in many ways the opposite of the security requirements - of hosts within a private subnet. For private subnet instances, you need a route out to a NAT gateway, and that - NAT gateway must be in a public subnet\u2014otherwise, it would not itself be able - to provide outbound traffic access to the Internet. NAT gateway is essentially a managed service and a NAT instance as an instance (- which you manage) for networking. all custom NACLs disallow all inbound and outbound traffic. It is only a VPC\u2019s - default NACL that has an \u201callow all\u201d policy. Security groups only contain allow rules, not deny rules. A security group can actually have no inbound or outbound rules. A security group does require a name and description, though. A security group can be attached to multiple constructs, like an EC2 instance, - but is ultimately associated with a network interface, which in turn is - attached to individual instances. by default SG has Outbound 0.0.0.0/0 for all protocols allowed for IPV4 Outbound ::/0 for all protocols allowed for IPV6 Security group rules have a protocol and a description. They do not have a - subnet, although they can have CIDR blocks or single IP addresses. Instances - can associate with a security group, but a security group does not itself refer - to a specific instance. Client-side encryption involves the client (you, in this example) managing the - entire encryption and decryption process. AWS only provides storage. SSE-S3, SSE-KMS, and SSE-C are all valid approaches to server-side S3 - encryption. SSE-KMS provides a very good audit trail and security. SSE-S3 requires that Amazon S3 manage the data and master encryption keys while - SSE-KMS requires that AWS manage the data key but you manage the customer - master key (CMK) in AWS KMS. An instance has a primary network interface in all cases but can have - additional network interfaces attached. You can only assign a single role to an instance. By default, root volumes are terminated on instance deletion, and by default, additional EBS volumes attached to an instance are not. Boot volumes \u00b6 This is easiest to remember by noting that HDD types are not available to use as boot volumes. General SSD and Provisioned IOPS are. EC2 Placement groups \u00b6 When you launch a new EC2 instance, the EC2 service attempts to place the instance in such a way that all of your instances are spread out across underlying hardware to minimize correlated failures. You can use placement groups to influence the placement of a group of interdependent instances to meet the needs of your workload. Depending on the type of workload, you can create a placement group using one of the following placement strategies: Cluster : It means placing the instances in the same rack and AZ so that there is low latency and high throughput. It packs instances close together inside an Availability Zone. This strategy enables workloads to achieve the low-latency network performance necessary for tightly-coupled node-to-node communication that is typical of HPC applications. Spread : A spread placement group is a group of instances that are each placed on distinct racks, with each rack having its own network and power source. The following image shows seven instances in a single Availability Zone that are placed into a spread placement group. The seven instances are placed on seven different racks. Spread placement groups are recommended for applications that have a small number of critical instances that should be kept separate from each other. Partition : Same as Spread but with a difference that in this, we can have multiple instances in the same partition. In this case, the different racks have a different power source. Use case: HDFS, Cassandra, etc. Partition placement groups help reduce the likelihood of correlated hardware failures for your application. When using partition placement groups, Amazon EC2 divides each group into logical segments called partitions. Amazon EC2 ensures that each partition within a placement group has its own set of racks. Each rack has its own network and power source. No two partitions within a placement group share the same racks, allowing you to isolate the impact of a hardware failure within your application. The following image is a simple visual representation of a partition placement group in a single Availability Zone. It shows instances that are placed into a partition placement group with three partitions\u2014Partition 1, Partition 2, and Partition 3. Each partition comprises multiple instances. The instances in a partition do not share racks with the instances in the other partitions, allowing you to contain the impact of a single hardware failure to only the associated partition.","title":"EC2 \ud83d\udda5"},{"location":"aws/ec2/#boot-volumes","text":"This is easiest to remember by noting that HDD types are not available to use as boot volumes. General SSD and Provisioned IOPS are.","title":"Boot volumes"},{"location":"aws/ec2/#ec2-placement-groups","text":"When you launch a new EC2 instance, the EC2 service attempts to place the instance in such a way that all of your instances are spread out across underlying hardware to minimize correlated failures. You can use placement groups to influence the placement of a group of interdependent instances to meet the needs of your workload. Depending on the type of workload, you can create a placement group using one of the following placement strategies: Cluster : It means placing the instances in the same rack and AZ so that there is low latency and high throughput. It packs instances close together inside an Availability Zone. This strategy enables workloads to achieve the low-latency network performance necessary for tightly-coupled node-to-node communication that is typical of HPC applications. Spread : A spread placement group is a group of instances that are each placed on distinct racks, with each rack having its own network and power source. The following image shows seven instances in a single Availability Zone that are placed into a spread placement group. The seven instances are placed on seven different racks. Spread placement groups are recommended for applications that have a small number of critical instances that should be kept separate from each other. Partition : Same as Spread but with a difference that in this, we can have multiple instances in the same partition. In this case, the different racks have a different power source. Use case: HDFS, Cassandra, etc. Partition placement groups help reduce the likelihood of correlated hardware failures for your application. When using partition placement groups, Amazon EC2 divides each group into logical segments called partitions. Amazon EC2 ensures that each partition within a placement group has its own set of racks. Each rack has its own network and power source. No two partitions within a placement group share the same racks, allowing you to isolate the impact of a hardware failure within your application. The following image is a simple visual representation of a partition placement group in a single Availability Zone. It shows instances that are placed into a partition placement group with three partitions\u2014Partition 1, Partition 2, and Partition 3. Each partition comprises multiple instances. The instances in a partition do not share racks with the instances in the other partitions, allowing you to contain the impact of a single hardware failure to only the associated partition.","title":"EC2 Placement groups"},{"location":"aws/efs/","text":"Elastic File System \u00b6 Sun created NFS (which is a distributed File System) EFS is an implementation for NFS file share The storage capacity is elastic. It is a great way of sharing the files between the two instances. EFS uses the NFS file system. Unlike the EBS where we have to define/provision space, in the EFS we do not need to - do that as they are backed by S3. It can scale up to Petabytes. It can support thousands of concurrent EFS connections. Data is stored across multiple AZ\u2019s in a region. Read after write consistency. When to use which one? \u00b6 EFS: for Linux based systems that need the distributed storage. Amazon FSx for Windows: When you need storage for windows-based systems for Sharepoint, SQL Server, etc. Amazon FSx for Lustre: for HPC and ML in windows env. It can store data on S3.","title":"EFS"},{"location":"aws/efs/#elastic-file-system","text":"Sun created NFS (which is a distributed File System) EFS is an implementation for NFS file share The storage capacity is elastic. It is a great way of sharing the files between the two instances. EFS uses the NFS file system. Unlike the EBS where we have to define/provision space, in the EFS we do not need to - do that as they are backed by S3. It can scale up to Petabytes. It can support thousands of concurrent EFS connections. Data is stored across multiple AZ\u2019s in a region. Read after write consistency.","title":"Elastic File System"},{"location":"aws/efs/#when-to-use-which-one","text":"EFS: for Linux based systems that need the distributed storage. Amazon FSx for Windows: When you need storage for windows-based systems for Sharepoint, SQL Server, etc. Amazon FSx for Lustre: for HPC and ML in windows env. It can store data on S3.","title":"When to use which one?"},{"location":"aws/eks/","text":"CLI's used \u00b6 kubectl \u2013 A command line tool for working with Kubernetes clusters. eksctl \u2013 A command line tool for working with EKS clusters that automates many individual tasks. Types of nodes in EKS \u00b6 Fargate \u2013 Linux : Select this type of node if you want to run Linux applications on AWS Fargate. Fargate is a serverless compute engine that lets you deploy Kubernetes pods without managing Amazon EC2 instances. self-managed nodes \u2013 Linux : Select this type of node if you want to run Amazon Linux applications on Amazon EC2 instances. eksctl create cluster --name my-cluster --region region-code --fargate kubectl get nodes -o wide # view nodes","title":"EKS"},{"location":"aws/eks/#clis-used","text":"kubectl \u2013 A command line tool for working with Kubernetes clusters. eksctl \u2013 A command line tool for working with EKS clusters that automates many individual tasks.","title":"CLI's used"},{"location":"aws/eks/#types-of-nodes-in-eks","text":"Fargate \u2013 Linux : Select this type of node if you want to run Linux applications on AWS Fargate. Fargate is a serverless compute engine that lets you deploy Kubernetes pods without managing Amazon EC2 instances. self-managed nodes \u2013 Linux : Select this type of node if you want to run Amazon Linux applications on Amazon EC2 instances. eksctl create cluster --name my-cluster --region region-code --fargate kubectl get nodes -o wide # view nodes","title":"Types of nodes in EKS"},{"location":"aws/elasticache/","text":"In general, pulling an image from a cache is far faster than performing a database read. ElastiCache uses shards as a grouping mechanism for individual Redis nodes. So a single node is part of a shard, which in turn is part of a cluster. Consider ElastiCache as only useful for storing transient data. Further, it\u2019s not a persistent store; therefore, it\u2019s great for caching data from a message queue or providing very fast ephemeral storage.\u00d8","title":"Elasti Cache"},{"location":"aws/elasticip/","text":"Elastic IP \u00b6 An Elastic IP address is a static and public IPv4 address designed for dynamic cloud computing. An Elastic IP address is associated with your AWS account. With an Elastic IP address, you can mask the failure of an instance or software by rapidly remapping the address to another instance in your account. An Elastic IP address is a reserved public IP address that you can assign to any EC2 instance in a particular region until you choose to release it. When you associate an Elastic IP address with an instance or its primary network interface, the instance's public IPv4 address (if it had one) is released back into Amazon's pool of public IPv4 addresses. You cannot reuse a public IPv4 address, and you cannot convert a public IPv4 address to an Elastic IP address. When you associate an Elastic IP address with an instance that previously had a public IPv4 address, the public DNS hostname of the instance changes to match the Elastic IP address. To use an elastic IP, you must: Allocate it for use in a VPC. Associate it with an instance in that VPC. Remember By default, all AWS accounts are limited to 5 Elastic IP addresses per Region, because public (IPv4) internet addresses are a scarce public resource.","title":"Elastic IP"},{"location":"aws/elasticip/#elastic-ip","text":"An Elastic IP address is a static and public IPv4 address designed for dynamic cloud computing. An Elastic IP address is associated with your AWS account. With an Elastic IP address, you can mask the failure of an instance or software by rapidly remapping the address to another instance in your account. An Elastic IP address is a reserved public IP address that you can assign to any EC2 instance in a particular region until you choose to release it. When you associate an Elastic IP address with an instance or its primary network interface, the instance's public IPv4 address (if it had one) is released back into Amazon's pool of public IPv4 addresses. You cannot reuse a public IPv4 address, and you cannot convert a public IPv4 address to an Elastic IP address. When you associate an Elastic IP address with an instance that previously had a public IPv4 address, the public DNS hostname of the instance changes to match the Elastic IP address. To use an elastic IP, you must: Allocate it for use in a VPC. Associate it with an instance in that VPC. Remember By default, all AWS accounts are limited to 5 Elastic IP addresses per Region, because public (IPv4) internet addresses are a scarce public resource.","title":"Elastic IP"},{"location":"aws/eni/","text":"An Elastic Network Interface (ENI) is virtual and can have: multiple IPv4 and IPv6 addresses security groups a MAC address a source/destination check flag. Tip Traffic follows the network interface rather than sticking to any particular instance. So in the case when ENI is moved from one instance to another, the traffic is redirected to the new instance but stays targeted at the elastic network interface An elastic network interface can only be attached to a single instance at one time but can be moved from one instance to another. We can attach multiple elastic network interfaces to an instance An instance\u2019s primary ENI cannot be detached.","title":"ENI"},{"location":"aws/failover/","text":"If all your clients are in one region, then there is little advantage to adding read replicas to additional regions. Instead, providing replicas in the same region as the clients gives them the fastest access. You also cannot failover to a read replica. You can convert it to a stand-alone instance, but that is a manual process that is not a failover.","title":"Failover"},{"location":"aws/glacier/","text":"AWS Glacier \ud83d\uddfb \u00b6 It is called as cold storage It is used by AWS storage gateway Virtual Tape library Used in S3 via lifecycle management Tip Glacier is a service in itself. You don't need to use S3 to use Glacier.","title":"Glacier \ud83d\uddfb"},{"location":"aws/glacier/#aws-glacier","text":"It is called as cold storage It is used by AWS storage gateway Virtual Tape library Used in S3 via lifecycle management Tip Glacier is a service in itself. You don't need to use S3 to use Glacier.","title":"AWS Glacier \ud83d\uddfb"},{"location":"aws/glue/","text":"Waht is Glue \u00b6 AWS Glue is a fully managed ETL (extract, transform, and load) service that makes it simple and cost-effective to categorize your data, clean it, enrich it, and move it reliably between various data stores and data streams. AWS Glue consists of a central metadata repository known as the AWS Glue Data Catalog, an ETL engine that automatically generates Python or Scala code, and a flexible scheduler that handles dependency resolution, job monitoring, and retries AWS Glue is serverless, so there\u2019s no infrastructure to set up or manage. How it works \u00b6 AWS Glue is designed to work with semi-structured data. It introduces a component called a dynamic frame, which you can use in your ETL scripts. A dynamic frame is similar to an Apache Spark dataframe, which is a data abstraction used to organize data into rows and columns, except that each record is self-describing so no schema is required initially. With dynamic frames, you get schema flexibility and a set of advanced transformations specifically designed for dynamic frames. You can convert between dynamic frames and Spark dataframes, so that you can take advantage of both AWS Glue and Spark transformations to do the kinds of analysis that you want. Use cases \u00b6 You can use the AWS Glue console to discover data, transform it, and make it available for search and querying. The console calls the underlying services to orchestrate the work required to transform your data. You can also use the AWS Glue API operations to interface with AWS Glue services. Edit, debug, and test your Python or Scala Apache Spark ETL code using a familiar development environment.","title":"Glue"},{"location":"aws/glue/#waht-is-glue","text":"AWS Glue is a fully managed ETL (extract, transform, and load) service that makes it simple and cost-effective to categorize your data, clean it, enrich it, and move it reliably between various data stores and data streams. AWS Glue consists of a central metadata repository known as the AWS Glue Data Catalog, an ETL engine that automatically generates Python or Scala code, and a flexible scheduler that handles dependency resolution, job monitoring, and retries AWS Glue is serverless, so there\u2019s no infrastructure to set up or manage.","title":"Waht is Glue"},{"location":"aws/glue/#how-it-works","text":"AWS Glue is designed to work with semi-structured data. It introduces a component called a dynamic frame, which you can use in your ETL scripts. A dynamic frame is similar to an Apache Spark dataframe, which is a data abstraction used to organize data into rows and columns, except that each record is self-describing so no schema is required initially. With dynamic frames, you get schema flexibility and a set of advanced transformations specifically designed for dynamic frames. You can convert between dynamic frames and Spark dataframes, so that you can take advantage of both AWS Glue and Spark transformations to do the kinds of analysis that you want.","title":"How it works"},{"location":"aws/glue/#use-cases","text":"You can use the AWS Glue console to discover data, transform it, and make it available for search and querying. The console calls the underlying services to orchestrate the work required to transform your data. You can also use the AWS Glue API operations to interface with AWS Glue services. Edit, debug, and test your Python or Scala Apache Spark ETL code using a familiar development environment.","title":"Use cases"},{"location":"aws/iam/","text":"IAM \ud83d\udd11 \u00b6 IAM is universal service and it does not apply to a single region; it is cross-region. The root account is the account created for the first time and it has full access (also called as God mode \ud83e\uddda ). New users have no permissions when created Access key id and secret access key are required for programmatic access to AWS. They can\u2019t be used to login to the console. Use MFA in the root account. We can use the Google Auth app for this \ud83d\udcf1 IAM is concerned with the raw AWS resources, not access to running web applications. Tip Setting up a cross-account IAM role is currently the only method that will allow IAM users to access cross-account S3 buckets both programmatically and via the AWS console. Types of policies in IAM \u00b6 There are four types of policies in IAM:- Identity-based Resource-based Organization SCPs Access control lists (ACLs) You can provide console access and programmatic access via IAM. Programmatic access includes API and CLI access. Remember IAM is not the managed service for handling MFA Delete setup on S3 buckets Users, groups, roles, permissions, and similar constructs are part of IAM Organizations and organizational units are part of AWS Organizations, a different facility. User policies are not part of IAM but permissions are. IAM policies are written in JSON. IAM policies can be attached to users, groups, and roles in the case of identity-based policies, and AWS services and components via resource-based policies. Restoring revoked permissions for a user and changing the support options need the root user access. IAM changes apply immediately to all users across the system; there is no lag, and no need to log out and back in. Power-user access is a predefined policy that allows access to all AWS services with the exception of group or user management within IAM Warning AWS strongly recommends you delete your root user access keys and create IAM users for everyday use. IAM root user account is needed for very privileged access; in this case, that\u2019s creating a CloudFront key pair, which essentially provides signed access to applications and is a very trusted action. AWS firmly believes that root account access should be highly limited, but also not confined to a single user. Having a very small group of engineers (ideally AWS certified) is the best approach to reducing root account level access as much as possible. You will always need to provide non-root sign-in URLs for new users. New users have no access to AWS services. They are \u201cbare\u201d or \u201cempty\u201d or \u201cnaked\u201d users, as they can merely login to the AWS console (if a URL is provided). They cannot make any changes to AWS services or even view services. AWS usernames have to be unique across the AWS account in which that user exists \ud83d\udd11 If you have an external Active Directory, you\u2019d want to federate those users into AWS. This allows you to use the existing user base, not re-create each individual user.","title":"IAM"},{"location":"aws/iam/#iam","text":"IAM is universal service and it does not apply to a single region; it is cross-region. The root account is the account created for the first time and it has full access (also called as God mode \ud83e\uddda ). New users have no permissions when created Access key id and secret access key are required for programmatic access to AWS. They can\u2019t be used to login to the console. Use MFA in the root account. We can use the Google Auth app for this \ud83d\udcf1 IAM is concerned with the raw AWS resources, not access to running web applications. Tip Setting up a cross-account IAM role is currently the only method that will allow IAM users to access cross-account S3 buckets both programmatically and via the AWS console.","title":"IAM \ud83d\udd11"},{"location":"aws/iam/#types-of-policies-in-iam","text":"There are four types of policies in IAM:- Identity-based Resource-based Organization SCPs Access control lists (ACLs) You can provide console access and programmatic access via IAM. Programmatic access includes API and CLI access. Remember IAM is not the managed service for handling MFA Delete setup on S3 buckets Users, groups, roles, permissions, and similar constructs are part of IAM Organizations and organizational units are part of AWS Organizations, a different facility. User policies are not part of IAM but permissions are. IAM policies are written in JSON. IAM policies can be attached to users, groups, and roles in the case of identity-based policies, and AWS services and components via resource-based policies. Restoring revoked permissions for a user and changing the support options need the root user access. IAM changes apply immediately to all users across the system; there is no lag, and no need to log out and back in. Power-user access is a predefined policy that allows access to all AWS services with the exception of group or user management within IAM Warning AWS strongly recommends you delete your root user access keys and create IAM users for everyday use. IAM root user account is needed for very privileged access; in this case, that\u2019s creating a CloudFront key pair, which essentially provides signed access to applications and is a very trusted action. AWS firmly believes that root account access should be highly limited, but also not confined to a single user. Having a very small group of engineers (ideally AWS certified) is the best approach to reducing root account level access as much as possible. You will always need to provide non-root sign-in URLs for new users. New users have no access to AWS services. They are \u201cbare\u201d or \u201cempty\u201d or \u201cnaked\u201d users, as they can merely login to the AWS console (if a URL is provided). They cannot make any changes to AWS services or even view services. AWS usernames have to be unique across the AWS account in which that user exists \ud83d\udd11 If you have an external Active Directory, you\u2019d want to federate those users into AWS. This allows you to use the existing user base, not re-create each individual user.","title":"Types of policies in IAM"},{"location":"aws/lambda/","text":"Lambda basics \u00b6 Serverless computing allows you to build and run applications and services without thinking about servers \ud83d\udda5\ufe0f With serverless computing, your application still runs on servers, but all the server management is done by AWS \u2601\ufe0f It scales out instead of scaling up \u2b06\ufe0f with the number of requests. Lambda functions are independent, meaning that they will get replicated with each event \ud83d\udd25 Note Using AWS and its Serverless Platform, you can build and deploy applications on cost-effective services that provide built-in application availability and flexible scaling capabilities. This lets you focus on your application code instead of worrying about provisioning, configuring, and managing servers. Which services are serverless? \u00b6 Lambda is serverless. RDS is not serverless as it has downtime. Aurora is serverless. Dynamo, S3 is serverless. Concepts \u00b6 Function \u00b6 A function is a resource that you can invoke to run your code in Lambda. A function has code to process the events that you pass into the function or that other AWS services send to the function. Trigger \u00b6 A trigger is a resource or configuration that invokes a Lambda function. Triggers include AWS services that you can configure to invoke a function and event source mappings. Tip An event source mapping is a resource in Lambda that reads items from a stream or queue and invokes a function. Event \u00b6 An event is a JSON-formatted document that contains data for a Lambda function to process. The runtime converts the event to an object and passes it to your function code. When you invoke a function, you determine the structure and contents of the event. Example custom event \u2013 Weather data { \"TemperatureK\" : 281 , \"WindKmh\" : -3 , \"HumidityPct\" : 0.55 , \"PressureHPa\" : 1020 } When an AWS service invokes your function, the service defines the shape of the event. Example service event \u2013 Amazon SNS notification { \"Records\" : [ { \"Sns\" : { \"Timestamp\" : \"2019-01-02T12:45:07.000Z\" , \"Signature\" : \"tcc6faL2yUC6dgZdmrwh1Y4cGa/ebXEkAi6RibDsvpi+tE/1+82j...65r==\" , \"MessageId\" : \"95df01b4-ee98-5cb9-9903-4c221d41eb5e\" , \"Message\" : \"Hello from SNS!\" , ... Execution environment \u00b6 An execution environment provides a secure and isolated runtime environment for your Lambda function. An execution environment manages the processes and resources that are required to run the function. Instruction set architecture \u00b6 The instruction set architecture determines the type of computer processor that Lambda uses to run the function. Lambda provides a choice of instruction set architectures: arm64 \u2013 64-bit ARM architecture, for the AWS Graviton2 processor. x86_64 \u2013 64-bit x86 architecture, for x86-based processors. Deployment package \u00b6 You deploy your Lambda function code using a deployment package. Lambda supports two types of deployment packages: A .zip file archive that contains your function code and its dependencies. Lambda provides the operating system and runtime for your function. A container image that is compatible with the Open Container Initiative (OCI) specification. You add your function code and dependencies to the image. You must also include the operating system and a Lambda runtime. Runtime \u00b6 The runtime provides a language-specific environment that runs in an execution environment. The runtime relays invocation events, context information, and responses between Lambda and the function. You can use runtimes that Lambda provides, or build your own. If you package your code as a .zip file archive, you must configure your function to use a runtime that matches your programming language. For a container image, you include the runtime when you build the image. Layer \u00b6 A Lambda layer is a .zip file archive that can contain additional code or other content. A layer can contain libraries, a custom runtime, data, or configuration files. Remember Layers provide a convenient way to package libraries and other dependencies that you can use with your Lambda functions. Using layers reduces the size of uploaded deployment archives and makes it faster to deploy your code. Layers also promote code sharing and separation of responsibilities so that you can iterate faster on writing business logic. You can include up to five layers per function. Layers count towards the standard Lambda deployment size quotas. When you include a layer in a function, the contents are extracted to the /opt directory in the execution environment. By default, the layers that you create are private to your AWS account. You can choose to share a layer with other accounts or to make the layer public. If your functions consume a layer that a different account published, your functions can continue to use the layer version after it has been deleted, or after your permission to access the layer is revoked. However, you cannot create a new function or update functions using a deleted layer version. Tldr Functions deployed as a container image do not use layers. Instead, you package your preferred runtime, libraries, and other dependencies into the container image when you build the image. Destination \u00b6 Destinations are AWS resources that receive a record of an invocation after success or failure. You can configure Lambda to send invocation records when your function is invoked asynchronously, or if your function processes records from a stream. The contents of the invocation record and supported destination services vary by source. Extension \u00b6 Lambda extensions enable you to augment your functions. For example, you can use extensions to integrate your functions with your preferred monitoring, observability, security, and governance tools. Concurrency \u00b6 Concurrency is the number of requests that your function is serving at any given time. When your function is invoked, Lambda provisions an instance of it to process the event. When the function code finishes running, it can handle another request. If the function is invoked again while a request is still being processed, another instance is provisioned, increasing the function's concurrency. Throtlling \u00b6 what is Throtlling? At the highest level, throttling just means that Lambda will intentionally reject one of your requests and so what we see from the user side is that when making a client call, Lambda will throw a throttling exception, which you need to handle. Typically, people handle this by backing off for some time and retrying. But there are also some different mechanisms that you can use, so that\u2019s interesting, Lambda will reject your request. Why does it occur? Throttling occurs when your concurrent execution count > concurrency limit . Now, just as a reminder, if this wasn\u2019t clear, Lambda can handle multiple instance invocations at the same time and the sum of all of those invocations amounts to your concurrency execution count. So, assume we\u2019re at a particular instant in time if you have more invocations that are running that exceed your configured limit, all new requests to your Lambda function will get a throttling exception. What are the configure limits? Lambda has a default 1000 concurrency limit that\u2019s specified per region within an account. But it does get a little bit more complicated in terms of how this rule applies when you have multiple Lambda functions in the same region and the same account. Function URL \u00b6 A function URL is a dedicated HTTP(S) endpoint for your Lambda function. You can create and configure a function URL through the Lambda console or the Lambda API. When you create a function URL, Lambda automatically generates a unique URL endpoint for you. Once you create a function URL, its URL endpoint never changes. Function URL endpoints have the following format: h tt ps : //<url-id>.lambda-url.<region>.on.aws Info Function URLs are dual stack-enabled, supporting IPv4 and IPv6. Code editor \u00b6 The code editor supports languages that do not require compiling, such as Node.js and Python . The code editor suppports only .zip archive deployment packages, and the size of the deployment package must be less than 3 MB.","title":"Lambda \u2a10"},{"location":"aws/lambda/#lambda-basics","text":"Serverless computing allows you to build and run applications and services without thinking about servers \ud83d\udda5\ufe0f With serverless computing, your application still runs on servers, but all the server management is done by AWS \u2601\ufe0f It scales out instead of scaling up \u2b06\ufe0f with the number of requests. Lambda functions are independent, meaning that they will get replicated with each event \ud83d\udd25 Note Using AWS and its Serverless Platform, you can build and deploy applications on cost-effective services that provide built-in application availability and flexible scaling capabilities. This lets you focus on your application code instead of worrying about provisioning, configuring, and managing servers.","title":"Lambda basics"},{"location":"aws/lambda/#which-services-are-serverless","text":"Lambda is serverless. RDS is not serverless as it has downtime. Aurora is serverless. Dynamo, S3 is serverless.","title":"Which services are serverless?"},{"location":"aws/lambda/#concepts","text":"","title":"Concepts"},{"location":"aws/lambda/#function","text":"A function is a resource that you can invoke to run your code in Lambda. A function has code to process the events that you pass into the function or that other AWS services send to the function.","title":"Function"},{"location":"aws/lambda/#trigger","text":"A trigger is a resource or configuration that invokes a Lambda function. Triggers include AWS services that you can configure to invoke a function and event source mappings. Tip An event source mapping is a resource in Lambda that reads items from a stream or queue and invokes a function.","title":"Trigger"},{"location":"aws/lambda/#event","text":"An event is a JSON-formatted document that contains data for a Lambda function to process. The runtime converts the event to an object and passes it to your function code. When you invoke a function, you determine the structure and contents of the event. Example custom event \u2013 Weather data { \"TemperatureK\" : 281 , \"WindKmh\" : -3 , \"HumidityPct\" : 0.55 , \"PressureHPa\" : 1020 } When an AWS service invokes your function, the service defines the shape of the event. Example service event \u2013 Amazon SNS notification { \"Records\" : [ { \"Sns\" : { \"Timestamp\" : \"2019-01-02T12:45:07.000Z\" , \"Signature\" : \"tcc6faL2yUC6dgZdmrwh1Y4cGa/ebXEkAi6RibDsvpi+tE/1+82j...65r==\" , \"MessageId\" : \"95df01b4-ee98-5cb9-9903-4c221d41eb5e\" , \"Message\" : \"Hello from SNS!\" , ...","title":"Event"},{"location":"aws/lambda/#execution-environment","text":"An execution environment provides a secure and isolated runtime environment for your Lambda function. An execution environment manages the processes and resources that are required to run the function.","title":"Execution environment"},{"location":"aws/lambda/#instruction-set-architecture","text":"The instruction set architecture determines the type of computer processor that Lambda uses to run the function. Lambda provides a choice of instruction set architectures: arm64 \u2013 64-bit ARM architecture, for the AWS Graviton2 processor. x86_64 \u2013 64-bit x86 architecture, for x86-based processors.","title":"Instruction set architecture"},{"location":"aws/lambda/#deployment-package","text":"You deploy your Lambda function code using a deployment package. Lambda supports two types of deployment packages: A .zip file archive that contains your function code and its dependencies. Lambda provides the operating system and runtime for your function. A container image that is compatible with the Open Container Initiative (OCI) specification. You add your function code and dependencies to the image. You must also include the operating system and a Lambda runtime.","title":"Deployment package"},{"location":"aws/lambda/#runtime","text":"The runtime provides a language-specific environment that runs in an execution environment. The runtime relays invocation events, context information, and responses between Lambda and the function. You can use runtimes that Lambda provides, or build your own. If you package your code as a .zip file archive, you must configure your function to use a runtime that matches your programming language. For a container image, you include the runtime when you build the image.","title":"Runtime"},{"location":"aws/lambda/#layer","text":"A Lambda layer is a .zip file archive that can contain additional code or other content. A layer can contain libraries, a custom runtime, data, or configuration files. Remember Layers provide a convenient way to package libraries and other dependencies that you can use with your Lambda functions. Using layers reduces the size of uploaded deployment archives and makes it faster to deploy your code. Layers also promote code sharing and separation of responsibilities so that you can iterate faster on writing business logic. You can include up to five layers per function. Layers count towards the standard Lambda deployment size quotas. When you include a layer in a function, the contents are extracted to the /opt directory in the execution environment. By default, the layers that you create are private to your AWS account. You can choose to share a layer with other accounts or to make the layer public. If your functions consume a layer that a different account published, your functions can continue to use the layer version after it has been deleted, or after your permission to access the layer is revoked. However, you cannot create a new function or update functions using a deleted layer version. Tldr Functions deployed as a container image do not use layers. Instead, you package your preferred runtime, libraries, and other dependencies into the container image when you build the image.","title":"Layer"},{"location":"aws/lambda/#destination","text":"Destinations are AWS resources that receive a record of an invocation after success or failure. You can configure Lambda to send invocation records when your function is invoked asynchronously, or if your function processes records from a stream. The contents of the invocation record and supported destination services vary by source.","title":"Destination"},{"location":"aws/lambda/#extension","text":"Lambda extensions enable you to augment your functions. For example, you can use extensions to integrate your functions with your preferred monitoring, observability, security, and governance tools.","title":"Extension"},{"location":"aws/lambda/#concurrency","text":"Concurrency is the number of requests that your function is serving at any given time. When your function is invoked, Lambda provisions an instance of it to process the event. When the function code finishes running, it can handle another request. If the function is invoked again while a request is still being processed, another instance is provisioned, increasing the function's concurrency.","title":"Concurrency"},{"location":"aws/lambda/#throtlling","text":"what is Throtlling? At the highest level, throttling just means that Lambda will intentionally reject one of your requests and so what we see from the user side is that when making a client call, Lambda will throw a throttling exception, which you need to handle. Typically, people handle this by backing off for some time and retrying. But there are also some different mechanisms that you can use, so that\u2019s interesting, Lambda will reject your request. Why does it occur? Throttling occurs when your concurrent execution count > concurrency limit . Now, just as a reminder, if this wasn\u2019t clear, Lambda can handle multiple instance invocations at the same time and the sum of all of those invocations amounts to your concurrency execution count. So, assume we\u2019re at a particular instant in time if you have more invocations that are running that exceed your configured limit, all new requests to your Lambda function will get a throttling exception. What are the configure limits? Lambda has a default 1000 concurrency limit that\u2019s specified per region within an account. But it does get a little bit more complicated in terms of how this rule applies when you have multiple Lambda functions in the same region and the same account.","title":"Throtlling"},{"location":"aws/lambda/#function-url","text":"A function URL is a dedicated HTTP(S) endpoint for your Lambda function. You can create and configure a function URL through the Lambda console or the Lambda API. When you create a function URL, Lambda automatically generates a unique URL endpoint for you. Once you create a function URL, its URL endpoint never changes. Function URL endpoints have the following format: h tt ps : //<url-id>.lambda-url.<region>.on.aws Info Function URLs are dual stack-enabled, supporting IPv4 and IPv6.","title":"Function URL"},{"location":"aws/lambda/#code-editor","text":"The code editor supports languages that do not require compiling, such as Node.js and Python . The code editor suppports only .zip archive deployment packages, and the size of the deployment package must be less than 3 MB.","title":"Code editor"},{"location":"aws/lb/","text":"Load Balancer \u00b6 Classic LB \u00b6 Classic load balancers support both IPv4 and IPv6. They support HTTP/1 and HTTP/1.1, but only application load balancers support HTTP/2. Further, you must register individual instances, rather than target groups, with classic load balancers; registering target groups is a functionality only available with application load balancers. An ELB is an elastic load balancer and generally refers to a classic load balancer. An ALB is an application load balancer. Classic load balancers operate at both the connection (Level 4) and the request (Level 7) layer of the TCP stack. Application LB \u00b6 An ALB offers SSL termination and makes the SSL offload process very simple through tight integration with SSL processes. While an ELB will handle SSL termination, it does not offer the management features that ALBs do. Network LB \u00b6","title":"Load Balancers \u2696\ufe0f"},{"location":"aws/lb/#load-balancer","text":"","title":"Load Balancer"},{"location":"aws/lb/#classic-lb","text":"Classic load balancers support both IPv4 and IPv6. They support HTTP/1 and HTTP/1.1, but only application load balancers support HTTP/2. Further, you must register individual instances, rather than target groups, with classic load balancers; registering target groups is a functionality only available with application load balancers. An ELB is an elastic load balancer and generally refers to a classic load balancer. An ALB is an application load balancer. Classic load balancers operate at both the connection (Level 4) and the request (Level 7) layer of the TCP stack.","title":"Classic LB"},{"location":"aws/lb/#application-lb","text":"An ALB offers SSL termination and makes the SSL offload process very simple through tight integration with SSL processes. While an ELB will handle SSL termination, it does not offer the management features that ALBs do.","title":"Application LB"},{"location":"aws/lb/#network-lb","text":"","title":"Network LB"},{"location":"aws/macie/","text":"Amazon Macie is a security service that uses machine learning to automatically discover, classify, and protect sensitive data in AWS. Amazon Macie recognizes sensitive data such as personally identifiable information (PII) or intellectual property and provides you with dashboards and alerts that give visibility into how this data is being accessed or moved. The fully managed service continuously monitors data access activity for anomalies and generates detailed alerts when it detects the risk of unauthorized access or inadvertent data leaks. Amazon Macie is available to protect data stored in Amazon S3. Used AI to analyze data in S3 to find sensitive data. Analyze cloud trail logs to find suspicious activity. It has dashboards and reporting features. Used to prevent ID theft.","title":"Macie"},{"location":"aws/natgw/","text":"What is NAT Gateway \u00b6 A NAT gateway is a Network Address Translation (NAT) service. You can use a NAT gateway so that instances in a private subnet can connect to services outside your VPC but external services cannot initiate a connection with those instances. NAT Gateway is a highly available AWS managed service that makes it easy to connect to the Internet from instances within a private subnet in an Amazon Virtual Private Cloud (Amazon VPC). Previously, you needed to launch a NAT instance to enable NAT for instances in a private subnet. Remember A NAT gateway is preferable to a NAT instance because it is managed by AWS rather than you, the architect.","title":"NAT GW"},{"location":"aws/natgw/#what-is-nat-gateway","text":"A NAT gateway is a Network Address Translation (NAT) service. You can use a NAT gateway so that instances in a private subnet can connect to services outside your VPC but external services cannot initiate a connection with those instances. NAT Gateway is a highly available AWS managed service that makes it easy to connect to the Internet from instances within a private subnet in an Amazon Virtual Private Cloud (Amazon VPC). Previously, you needed to launch a NAT instance to enable NAT for instances in a private subnet. Remember A NAT gateway is preferable to a NAT instance because it is managed by AWS rather than you, the architect.","title":"What is NAT Gateway"},{"location":"aws/natinstance/","text":"","title":"NAT Instance"},{"location":"aws/orgs/","text":"It is used to centrally manage the AWS account using the Org Units (OU). You can get one bill per AWS account. Here the star account \u2b50 is the master account. Enable/ Disable the services on AWS using the Service Control Policies (SCP). Bucket policies are at the bucket level only. ACLs go down to the object level. Danger Do not use the paying account to deploy the resources.","title":"Orgs \ud83c\udfdb"},{"location":"aws/rds/","text":"Multi-AZ RDS instances use synchronous replication to push changes. You cannot encrypt a running RDS instance; you have to create the instance with encryption enabled. Amazon RDS backups can be retained for up to 35 days, and no longer.","title":"RDS"},{"location":"aws/readReplica/","text":"Read Replicas \u00b6 A multi-AZ instance can not be promoted to a read-replica. There is no charge for primary-to-secondary data replication. Although it\u2019s not particularly common, you can set up a read replica in an on-premises instance. Additionally, read replicas are often created in separate regions from the primary instance, to improve performance for clients closer to different regions than the primary instance. Currently, read replicas in RDS are only supported by MariaDB, MySQL, and PostgreSQL. Not for DynamoDB Read replicas are updated via asynchronous replication\u2014 the most performant approach. Read replica setup only allows for five read replicas. This is not a limit that can be raised by AWS. Read replica has its own database engine active so that it can be promoted to a primary DB. Multi-AZ setup is focused on disaster recovery and fault tolerance, while read replicas provide performance and scalability. You can manually promote (not automatically ) a read replica instance to a stand-alone instance if you have to. Read replicas do not create automatic backups, but the primary database instance must have automatic backups enabled to create read replicas. Tip There is no particular performance increase in a Multi-AZ deployment unless read replicas are also turned on.","title":"Read Replics"},{"location":"aws/readReplica/#read-replicas","text":"A multi-AZ instance can not be promoted to a read-replica. There is no charge for primary-to-secondary data replication. Although it\u2019s not particularly common, you can set up a read replica in an on-premises instance. Additionally, read replicas are often created in separate regions from the primary instance, to improve performance for clients closer to different regions than the primary instance. Currently, read replicas in RDS are only supported by MariaDB, MySQL, and PostgreSQL. Not for DynamoDB Read replicas are updated via asynchronous replication\u2014 the most performant approach. Read replica setup only allows for five read replicas. This is not a limit that can be raised by AWS. Read replica has its own database engine active so that it can be promoted to a primary DB. Multi-AZ setup is focused on disaster recovery and fault tolerance, while read replicas provide performance and scalability. You can manually promote (not automatically ) a read replica instance to a stand-alone instance if you have to. Read replicas do not create automatic backups, but the primary database instance must have automatic backups enabled to create read replicas. Tip There is no particular performance increase in a Multi-AZ deployment unless read replicas are also turned on.","title":"Read Replicas"},{"location":"aws/route53/","text":"Route 53 \u00b6 Route 53 offers a number of different routing policies: Simple Failover Geolocation Geo proximity Latency-based Multivalue answer Weighted Route 53 supports up to 50 domain names by default, but this limit can be raised if requested. A naked domain is a domain name server (DNS) name that can't be a canonical name record (CNAME). An example is hello.com, without the www subdomain tips on DNS The A record maps a name to one or more IP addresses when the IP is known and stable. The CNAME record maps a name to another name. It should only be used when there are no other records on that name. The ALIAS record maps a name to another name but can coexist with other records on that name. The URL record redirects the name to the target name using the HTTP 301 status code. Some rules: The A, CNAME, and ALIAS records cause a name to resolve to an IP. Conversely, the URL record redirects the name to a destination. The URL record is a simple and effective way to apply a redirect for one name to another name, for example redirecting www.example.com to example.com. The A name must resolve to an IP. The CNAME and ALIAS records must point to a name. Some imp records are A : They are the most basic type of DNS record and are used to point a domain or subdomain to an IP address. CNAME Alias MX : A mail exchanger record specifies the mail server responsible for accepting email messages on behalf of a domain name. SOA : it means Start Of Authority PTR : Getting reverse DNS going is done by finding the PTR records in use by a DNS server. NS : NS stands for 'name server' and this record indicates which DNS server is authoritative for that domain (which server contains the actual DNS records). A domain will often have multiple NS records which can indicate primary and backup name servers for that domain.","title":"Route 53 \ud83d\udee3"},{"location":"aws/route53/#route-53","text":"Route 53 offers a number of different routing policies: Simple Failover Geolocation Geo proximity Latency-based Multivalue answer Weighted Route 53 supports up to 50 domain names by default, but this limit can be raised if requested. A naked domain is a domain name server (DNS) name that can't be a canonical name record (CNAME). An example is hello.com, without the www subdomain tips on DNS The A record maps a name to one or more IP addresses when the IP is known and stable. The CNAME record maps a name to another name. It should only be used when there are no other records on that name. The ALIAS record maps a name to another name but can coexist with other records on that name. The URL record redirects the name to the target name using the HTTP 301 status code. Some rules: The A, CNAME, and ALIAS records cause a name to resolve to an IP. Conversely, the URL record redirects the name to a destination. The URL record is a simple and effective way to apply a redirect for one name to another name, for example redirecting www.example.com to example.com. The A name must resolve to an IP. The CNAME and ALIAS records must point to a name. Some imp records are A : They are the most basic type of DNS record and are used to point a domain or subdomain to an IP address. CNAME Alias MX : A mail exchanger record specifies the mail server responsible for accepting email messages on behalf of a domain name. SOA : it means Start Of Authority PTR : Getting reverse DNS going is done by finding the PTR records in use by a DNS server. NS : NS stands for 'name server' and this record indicates which DNS server is authoritative for that domain (which server contains the actual DNS records). A domain will often have multiple NS records which can indicate primary and backup name servers for that domain.","title":"Route 53"},{"location":"aws/s3/","text":"S3 \u00b6 Generic info \u00b6 It is object-based storage. Files can be from 0 bytes to 5 TB. A bucket is a folder. The object has a key, value (5 Tb max size), version id, metadata, subresources (such as ACL and bucket policy) S3 has a flat structure but we can create a directory by using the prefixes. It has read after write consistency. Eventual consistency for overwriting. It has a simple web service interface Buckets are created in a region , not in an AZ . It means that S3 is a region-based and fully managed service. We can use tags in objects to group various objects and later retrieve them S3 is a restful web service. Tags can be used in cloud-trail , cloud-watch , and lifecycle management , etc. The lifecycle of tiers means we can move them from one tier to another tier (this can be automated as well) Use MFA for delete to make sure someone else does not delete data in bucket. ACL is used to grant access at the fine-grain level on bucket. First-byte latency is the time between requesting an object from the service and when that data starts to arrive. With S3 (for example) that time is measured in milliseconds and in many cases could be considered instant. Cross-region replication will replicate the data across the regions. Users can save data to the edge locations in case of transfer acceleration . We can not install an operating system on S3. We can create up to 100 buckets by default. Buckets have sub-resources ; which are the resources that can not exist on its own. What are we geting charged for? Object tagging Storage Requests Transfer acceleration Remember Standard IA will charge every time we are going to use the data. It is usually used for backups etc. Glacier has minimum storage for 3 months and Glacier deep archive has it for a minimum of 6 months. For Intelligent tiering , it will put the data automatically to IA if it is not used for 30 days and there will be no archival feel associated with it as well (special case). Object size less than 128 kb will not be moved by using the Intelligent tiering, it will remain in the standard tier. S3 is a universal namespace, so it has to be unique. S3 storage tiers \u00b6 Standard : Standard S3 is the most expensive tier. Infrequently Accessed (IA) One zone (less cost) as we use the only 1 zone to keep data. One zone -IA : S3 One Zone-IA is intended for use cases with infrequently accessed data that is re-creatable, such as storing secondary backup copies of on-premises data or for storage that is already replicated in another AWS Region for compliance or disaster recovery purposes. Intelligent tiering (it will use ML to move data between tiers) Glacier : It is used for data archival, mainly for compliance reasons. Retrival time is from minutes to hours Glacier deep archive : Long term data archieve with retrival time < 12 hours Tip All other tiers except one-zone replicate data in 3 or more AZ's Lifecycle Management \u00b6 An S3 Lifecycle configuration is a set of rules that define actions that Amazon S3 applies to a group of objects. There are two types of actions: Transition actions \u2013 These actions define when objects transition to another storage class. For example, you might choose to transition objects to the S3 Standard-IA storage class 30 days after creating them, or archive objects to the S3 Glacier Flexible Retrieval storage class one year after creating them. Expiration actions \u2013 These actions define when objects expire. Amazon S3 deletes expired objects on your behalf. Lifecycle expiration costs depend on when you choose to expire objects. Structure of S3 \u00b6 S3 consist of the following: Key: it is the object name. Value: it is the actual data we are storing inside the object. Version id Metadata Sub-resources ACL Torrents Consistency in S3 \u00b6 Read after write : you can read after writing. Eventual consistency for update and delete. In S3-IA, we are charged a retrieval fee. In S3 Glacier, we can configure the retrieval time from minutes to hours. Prices are S3 > S3 IA > S3 intelligent tiering > S3 one zone > S3 glacier > S3 glacier deep archive Policies \u00b6 2 types of policies are Resource-based policy \u00b6 Access control list: used for objects Bucket policy(JSON) and bucket ACL (XML) User-based \u00b6 ACL is legacy while bucket policy is new Root user does not have the IAM policy Deny access always wins. You pay for storage, data replication, requests, management (monitoring) Bucket policy work at bucket level which the ACL works at the object level. Security \u00b6 Encryption in transit is done using SSL or TLS Encryption in transit is optional. Server-side S3 managed keys, SSE- S3 (USE AES-256) Key management service (KMS) The customer provided keys. Client-side - use your own Once versioning is enabled, It can not be disabled but can only be suspended. If we delete the files in the S3 bucket with versioning turned on, then it will place a delete marker. We can restore the files if we delete the delete marker. Lifecycle rules can be used in conjunction with versioning. File replication \u00b6 Cross-region replication needs the versioning to be enabled for both the source and destination buckets. The existing files in the bucket which were added before the replication are not replicated, so we have to replicate them manually. The subsequently updated files will be replicated automatically. For the replicated files, if you put a delete marker/or delete a file, then the file is not deleted on the replicated. S3 Transfer Acceleration \u00b6 It uses the CloudFront to fasten the process of uploading. The users will first upload to the edge location from where the files are uploaded to bucked using Amazon\u2019s backbone network. !!! question 'Which policies should I use?' We should use the User policy or the bucket policy as it will help in providing us in access at a much fine-grained level. ACL\u2019s are the legacy tech. Access Control List \u00b6 They are XML docs use to give access to both objects and the bucket. Each bucket and the object has the ACL attached to it in the sub-resources part. The default ACL provides full access to a resource owner A grantee can be an AWS account or one of the predefined Amazon S3 groups. When an object is created, then the only ACL is created not the user policy or bucket policy. ACL\u2019s can be used to grant permissions to pre-defined groups but not to an IAM user. With ACL we can NOT provide the deny rules and conditional access. All we can do is provide the basic read/write permissions. Canned ACL : Amazon S3 supports a set of predefined grants, known as canned ACLs. Each canned ACL has a predefined set of grantees and permissions . They are an easy way to grant access. S3 pre-defined groups : Amazon S3 has a set of predefined groups. When granting account access to a group, you specify one of our URIs instead of a canonical user ID. We provide the following predefined groups: Authenticated Users group: all AWS accounts All Users group: authenticated and anonymous users. Log Delivery group Tip The canonical user ID is an alpha-numeric identifier, such as 79a59df900b949e55d96 , that is an obfuscated form of the AWS account ID. You can use this ID to identify an AWS account when granting cross-account access to buckets and objects using Amazon S3. Server-side encryption \u00b6 Server-side encryption is the encryption of data at its destination by the application or service that receives it. Amazon S3 encrypts your data at the object level as it writes it to disks in its data centers and decrypts it for you when you access it. SSE-S3 \u00b6 Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3): When you use Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3), each object is encrypted with a unique - key. As an additional safeguard, it encrypts the key itself with a master key that it regularly rotates. All the objects are encrypted using different keys We can not manage keys in this case SSE-KMS \u00b6 Server-Side Encryption with Customer Master Keys (CMKs) Stored in AWS Key Management Service (SSE-KMS): Server-Side Encryption with Customer Master Keys (CMKs) Stored in AWS Key - Management Service (SSE-KMS) is similar to SSE-S3, but with some additional benefits and charges for using this service. There are separate permissions for the use of a CMK that - provides added protection against unauthorized access of your objects in Amazon S3. We can create and use data keys, master keys, and rotate keys as well. It gives us a lot of control as a user can choose a key to encrypt the object. We have access to data keys and master keys. AWS does not have access to the keys in this case. We can audit the use of KMS using Cloudtrail that shows when your CMK was used and by whom. SSE-C \u00b6 Server-Side Encryption with Customer-Provided Keys (SSE-C): With Server-Side Encryption with Customer-Provided Keys (SSE-C), you manage the encryption keys and Amazon S3 manages - the encryption, as it writes to disks and decryption when you access your objects. The Key is managed by the user. A user generates the key and uploads it with data. Must use HTTPS to upload the objects If we lose the key, so we lose the data. S3 will discard the key after using it in this case. S3 and EBS difference \u00b6 S3 (Simple Storage Service) and EBS (Elastic Block Store) are two file storage services provided by Amazon. The main difference between them is with what they can be used with. EBS is specifically meant for EC2 (Elastic Computing Cloud) instances and is not accessible unless mounted to one. On the other hand, S3 is not limited to EC2. The files within an S3 bucket can be retrieved using HTTP protocols and even with BitTorrent. Many sites use S3 to hold most of their files because of its accessibility to HTTP clients; web browsers for example. As already stated above, you need some type of software in order to read or write information with S3. With EBS, a volume can be mounted on an EC2 instance and it would appear just like a hard disk partition. It can be formatted with any file system and files can be written or read by the EC2 instance just like it would to a hard drive. When it comes to the total amount that you can store, S3 still has the upper hand. EBS has a standard limit of 20 volumes with each volume holding up to 1TB of data. With S3, the standard limit is at 100 buckets with each bucket having an unlimited data capacity. S3 users do not need to worry about filling a bucket and the only concern is having enough buckets for your needs. A limitation of EBS is its inability to be used by multiple instances at once. Once it is mounted by an instance, no other instance can use it. S3 can have multiple images of its contents so it can be used by many at the same time. An interesting side-effect of this capability is something called \u2018eventual consistency\u2019. With EBS, data read or write occurs almost instantly. With S3, the changes are not written immediately so if you write something, it may not be the data that a read operation returns. Summary \u00b6 EBS can only be used with EC2 instances while S3 can be used outside EC2. EBS appears as a mountable volume while the S3 requires software to read and write data. EBS can accommodate a smaller amount of data than S3. EBS can only be used by one EC2 instance at a time while S3 can be used by multiple instances. S3 typically experiences write delays while EBS does not as EBS is attached to an instance. Limits \u00b6 Until 2018 there was a hard limit on S3 puts of 100 PUTs per second. To achieve this care needed to be taken with the structure of the name Key to ensuring parallel processing. As of July 2018, the limit was raised to 3500 and the need for the Key design was basically eliminated. Notes \u00b6 The policy is a JSON doc. IAM is Universal. S3 is not suitable to install OS or a database as it is object-based. By default, the buckets are private and we have to make them public. We can log the requests made to the S3 and then later these logs can be sent to another account as well. S3 supports bittorrent protocol to retrieve any publicaly available object using torrent files (peer to peer) Some points to remember Data is encrypted by the client and the encrypted data is uploaded in case of Client-side encryption. HTTP and HTTPS are both enabled in S3 by default, but we can disable the HTTP by using the bucket policy. HTTPS uses asymmetric encryption. We can apply lifecycle rules to a whole bucket or a subset. We can have 1000 lifecycle policies per bucket. Lifecycle is defined as XML and is stored in the sub-resources section. Lifecycle configuration on multi-factor authentication (MFA)-enabled buckets is NOT supported. This is because the MFA needs human intervention. For glacier, we are charged for at least 90 days(3 months) and 180 days(6 months) for the deep archive. The difference between Security Group and ACLs is that the Security Group acts as a firewall for associated Amazon EC2 instances, controlling both inbound and outbound traffic at the instance level, while ACLs act as a firewall for associated subnets, controlling both inbound and outbound traffic at the subnet level. A particular folder cannot be tagged separately from other folders; only an entire bucket can be tagged. With the exception of Glacier, retrieving data from the various S3 storage classes should be virtually identical (network issues notwithstanding).","title":"S3 \ud83e\udea3"},{"location":"aws/s3/#s3","text":"","title":"S3"},{"location":"aws/s3/#generic-info","text":"It is object-based storage. Files can be from 0 bytes to 5 TB. A bucket is a folder. The object has a key, value (5 Tb max size), version id, metadata, subresources (such as ACL and bucket policy) S3 has a flat structure but we can create a directory by using the prefixes. It has read after write consistency. Eventual consistency for overwriting. It has a simple web service interface Buckets are created in a region , not in an AZ . It means that S3 is a region-based and fully managed service. We can use tags in objects to group various objects and later retrieve them S3 is a restful web service. Tags can be used in cloud-trail , cloud-watch , and lifecycle management , etc. The lifecycle of tiers means we can move them from one tier to another tier (this can be automated as well) Use MFA for delete to make sure someone else does not delete data in bucket. ACL is used to grant access at the fine-grain level on bucket. First-byte latency is the time between requesting an object from the service and when that data starts to arrive. With S3 (for example) that time is measured in milliseconds and in many cases could be considered instant. Cross-region replication will replicate the data across the regions. Users can save data to the edge locations in case of transfer acceleration . We can not install an operating system on S3. We can create up to 100 buckets by default. Buckets have sub-resources ; which are the resources that can not exist on its own. What are we geting charged for? Object tagging Storage Requests Transfer acceleration Remember Standard IA will charge every time we are going to use the data. It is usually used for backups etc. Glacier has minimum storage for 3 months and Glacier deep archive has it for a minimum of 6 months. For Intelligent tiering , it will put the data automatically to IA if it is not used for 30 days and there will be no archival feel associated with it as well (special case). Object size less than 128 kb will not be moved by using the Intelligent tiering, it will remain in the standard tier. S3 is a universal namespace, so it has to be unique.","title":"Generic info"},{"location":"aws/s3/#s3-storage-tiers","text":"Standard : Standard S3 is the most expensive tier. Infrequently Accessed (IA) One zone (less cost) as we use the only 1 zone to keep data. One zone -IA : S3 One Zone-IA is intended for use cases with infrequently accessed data that is re-creatable, such as storing secondary backup copies of on-premises data or for storage that is already replicated in another AWS Region for compliance or disaster recovery purposes. Intelligent tiering (it will use ML to move data between tiers) Glacier : It is used for data archival, mainly for compliance reasons. Retrival time is from minutes to hours Glacier deep archive : Long term data archieve with retrival time < 12 hours Tip All other tiers except one-zone replicate data in 3 or more AZ's","title":"S3 storage tiers"},{"location":"aws/s3/#lifecycle-management","text":"An S3 Lifecycle configuration is a set of rules that define actions that Amazon S3 applies to a group of objects. There are two types of actions: Transition actions \u2013 These actions define when objects transition to another storage class. For example, you might choose to transition objects to the S3 Standard-IA storage class 30 days after creating them, or archive objects to the S3 Glacier Flexible Retrieval storage class one year after creating them. Expiration actions \u2013 These actions define when objects expire. Amazon S3 deletes expired objects on your behalf. Lifecycle expiration costs depend on when you choose to expire objects.","title":"Lifecycle Management"},{"location":"aws/s3/#structure-of-s3","text":"S3 consist of the following: Key: it is the object name. Value: it is the actual data we are storing inside the object. Version id Metadata Sub-resources ACL Torrents","title":"Structure of S3"},{"location":"aws/s3/#consistency-in-s3","text":"Read after write : you can read after writing. Eventual consistency for update and delete. In S3-IA, we are charged a retrieval fee. In S3 Glacier, we can configure the retrieval time from minutes to hours. Prices are S3 > S3 IA > S3 intelligent tiering > S3 one zone > S3 glacier > S3 glacier deep archive","title":"Consistency in S3"},{"location":"aws/s3/#policies","text":"2 types of policies are","title":"Policies"},{"location":"aws/s3/#resource-based-policy","text":"Access control list: used for objects Bucket policy(JSON) and bucket ACL (XML)","title":"Resource-based policy"},{"location":"aws/s3/#user-based","text":"ACL is legacy while bucket policy is new Root user does not have the IAM policy Deny access always wins. You pay for storage, data replication, requests, management (monitoring) Bucket policy work at bucket level which the ACL works at the object level.","title":"User-based"},{"location":"aws/s3/#security","text":"Encryption in transit is done using SSL or TLS Encryption in transit is optional. Server-side S3 managed keys, SSE- S3 (USE AES-256) Key management service (KMS) The customer provided keys. Client-side - use your own Once versioning is enabled, It can not be disabled but can only be suspended. If we delete the files in the S3 bucket with versioning turned on, then it will place a delete marker. We can restore the files if we delete the delete marker. Lifecycle rules can be used in conjunction with versioning.","title":"Security"},{"location":"aws/s3/#file-replication","text":"Cross-region replication needs the versioning to be enabled for both the source and destination buckets. The existing files in the bucket which were added before the replication are not replicated, so we have to replicate them manually. The subsequently updated files will be replicated automatically. For the replicated files, if you put a delete marker/or delete a file, then the file is not deleted on the replicated.","title":"File replication"},{"location":"aws/s3/#s3-transfer-acceleration","text":"It uses the CloudFront to fasten the process of uploading. The users will first upload to the edge location from where the files are uploaded to bucked using Amazon\u2019s backbone network. !!! question 'Which policies should I use?' We should use the User policy or the bucket policy as it will help in providing us in access at a much fine-grained level. ACL\u2019s are the legacy tech.","title":"S3 Transfer Acceleration"},{"location":"aws/s3/#access-control-list","text":"They are XML docs use to give access to both objects and the bucket. Each bucket and the object has the ACL attached to it in the sub-resources part. The default ACL provides full access to a resource owner A grantee can be an AWS account or one of the predefined Amazon S3 groups. When an object is created, then the only ACL is created not the user policy or bucket policy. ACL\u2019s can be used to grant permissions to pre-defined groups but not to an IAM user. With ACL we can NOT provide the deny rules and conditional access. All we can do is provide the basic read/write permissions. Canned ACL : Amazon S3 supports a set of predefined grants, known as canned ACLs. Each canned ACL has a predefined set of grantees and permissions . They are an easy way to grant access. S3 pre-defined groups : Amazon S3 has a set of predefined groups. When granting account access to a group, you specify one of our URIs instead of a canonical user ID. We provide the following predefined groups: Authenticated Users group: all AWS accounts All Users group: authenticated and anonymous users. Log Delivery group Tip The canonical user ID is an alpha-numeric identifier, such as 79a59df900b949e55d96 , that is an obfuscated form of the AWS account ID. You can use this ID to identify an AWS account when granting cross-account access to buckets and objects using Amazon S3.","title":"Access Control List"},{"location":"aws/s3/#server-side-encryption","text":"Server-side encryption is the encryption of data at its destination by the application or service that receives it. Amazon S3 encrypts your data at the object level as it writes it to disks in its data centers and decrypts it for you when you access it.","title":"Server-side encryption"},{"location":"aws/s3/#sse-s3","text":"Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3): When you use Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3), each object is encrypted with a unique - key. As an additional safeguard, it encrypts the key itself with a master key that it regularly rotates. All the objects are encrypted using different keys We can not manage keys in this case","title":"SSE-S3"},{"location":"aws/s3/#sse-kms","text":"Server-Side Encryption with Customer Master Keys (CMKs) Stored in AWS Key Management Service (SSE-KMS): Server-Side Encryption with Customer Master Keys (CMKs) Stored in AWS Key - Management Service (SSE-KMS) is similar to SSE-S3, but with some additional benefits and charges for using this service. There are separate permissions for the use of a CMK that - provides added protection against unauthorized access of your objects in Amazon S3. We can create and use data keys, master keys, and rotate keys as well. It gives us a lot of control as a user can choose a key to encrypt the object. We have access to data keys and master keys. AWS does not have access to the keys in this case. We can audit the use of KMS using Cloudtrail that shows when your CMK was used and by whom.","title":"SSE-KMS"},{"location":"aws/s3/#sse-c","text":"Server-Side Encryption with Customer-Provided Keys (SSE-C): With Server-Side Encryption with Customer-Provided Keys (SSE-C), you manage the encryption keys and Amazon S3 manages - the encryption, as it writes to disks and decryption when you access your objects. The Key is managed by the user. A user generates the key and uploads it with data. Must use HTTPS to upload the objects If we lose the key, so we lose the data. S3 will discard the key after using it in this case.","title":"SSE-C"},{"location":"aws/s3/#s3-and-ebs-difference","text":"S3 (Simple Storage Service) and EBS (Elastic Block Store) are two file storage services provided by Amazon. The main difference between them is with what they can be used with. EBS is specifically meant for EC2 (Elastic Computing Cloud) instances and is not accessible unless mounted to one. On the other hand, S3 is not limited to EC2. The files within an S3 bucket can be retrieved using HTTP protocols and even with BitTorrent. Many sites use S3 to hold most of their files because of its accessibility to HTTP clients; web browsers for example. As already stated above, you need some type of software in order to read or write information with S3. With EBS, a volume can be mounted on an EC2 instance and it would appear just like a hard disk partition. It can be formatted with any file system and files can be written or read by the EC2 instance just like it would to a hard drive. When it comes to the total amount that you can store, S3 still has the upper hand. EBS has a standard limit of 20 volumes with each volume holding up to 1TB of data. With S3, the standard limit is at 100 buckets with each bucket having an unlimited data capacity. S3 users do not need to worry about filling a bucket and the only concern is having enough buckets for your needs. A limitation of EBS is its inability to be used by multiple instances at once. Once it is mounted by an instance, no other instance can use it. S3 can have multiple images of its contents so it can be used by many at the same time. An interesting side-effect of this capability is something called \u2018eventual consistency\u2019. With EBS, data read or write occurs almost instantly. With S3, the changes are not written immediately so if you write something, it may not be the data that a read operation returns.","title":"S3 and EBS difference"},{"location":"aws/s3/#summary","text":"EBS can only be used with EC2 instances while S3 can be used outside EC2. EBS appears as a mountable volume while the S3 requires software to read and write data. EBS can accommodate a smaller amount of data than S3. EBS can only be used by one EC2 instance at a time while S3 can be used by multiple instances. S3 typically experiences write delays while EBS does not as EBS is attached to an instance.","title":"Summary"},{"location":"aws/s3/#limits","text":"Until 2018 there was a hard limit on S3 puts of 100 PUTs per second. To achieve this care needed to be taken with the structure of the name Key to ensuring parallel processing. As of July 2018, the limit was raised to 3500 and the need for the Key design was basically eliminated.","title":"Limits"},{"location":"aws/s3/#notes","text":"The policy is a JSON doc. IAM is Universal. S3 is not suitable to install OS or a database as it is object-based. By default, the buckets are private and we have to make them public. We can log the requests made to the S3 and then later these logs can be sent to another account as well. S3 supports bittorrent protocol to retrieve any publicaly available object using torrent files (peer to peer) Some points to remember Data is encrypted by the client and the encrypted data is uploaded in case of Client-side encryption. HTTP and HTTPS are both enabled in S3 by default, but we can disable the HTTP by using the bucket policy. HTTPS uses asymmetric encryption. We can apply lifecycle rules to a whole bucket or a subset. We can have 1000 lifecycle policies per bucket. Lifecycle is defined as XML and is stored in the sub-resources section. Lifecycle configuration on multi-factor authentication (MFA)-enabled buckets is NOT supported. This is because the MFA needs human intervention. For glacier, we are charged for at least 90 days(3 months) and 180 days(6 months) for the deep archive. The difference between Security Group and ACLs is that the Security Group acts as a firewall for associated Amazon EC2 instances, controlling both inbound and outbound traffic at the instance level, while ACLs act as a firewall for associated subnets, controlling both inbound and outbound traffic at the subnet level. A particular folder cannot be tagged separately from other folders; only an entire bucket can be tagged. With the exception of Glacier, retrieving data from the various S3 storage classes should be virtually identical (network issues notwithstanding).","title":"Notes"},{"location":"aws/sg/","text":"There are slight differences between a normal 'new' Security Group and a 'default' security group in the default VPC. For a 'new' security group nothing is allowed in by default. Security groups evaluate all the rules on the group before deciding how to handle the traffic. Security groups only provide for allow rules. Security groups operate at the instance level.","title":"Security Group \ud83d\udd10"},{"location":"aws/snowball/","text":"Used for large data transfer It has less cost, fast and is secure. A SnowBall edge connects to your existing applications and infrastructure using the standard interfaces.","title":"Snow Ball \u2744\ufe0f"},{"location":"aws/sqs/","text":"SQS \u00b6 SQS will guarantee that a message is delivered at least once, but that message may be redelivered. SQS queues only make an \u201cattempt\u201d to deliver messages in order (more or less a FIFO approach) but do not guarantee FIFO. If strict FIFO is needed, that option can be selected. SNS \u00b6 SNS manages notifications and SQS manages messages. SNS is a push-based system while SQS is a pull-based system.","title":"SQS"},{"location":"aws/sqs/#sqs","text":"SQS will guarantee that a message is delivered at least once, but that message may be redelivered. SQS queues only make an \u201cattempt\u201d to deliver messages in order (more or less a FIFO approach) but do not guarantee FIFO. If strict FIFO is needed, that option can be selected.","title":"SQS"},{"location":"aws/sqs/#sns","text":"SNS manages notifications and SQS manages messages. SNS is a push-based system while SQS is a pull-based system.","title":"SNS"},{"location":"aws/storagegw/","text":"AWS Storage Gateway is a hybrid cloud storage service that gives you on-premises access to virtually unlimited cloud storage. Customers use Storage Gateway to simplify storage management and reduce costs for key hybrid cloud storage use cases. Types based on installation \u00b6 They are of two types 1. It can be installed as VM on a hypervisor. 1. It comes as a physical box. Sharing Protocols \u00b6 Two types of file sharing protocols are: SMB : Made by IBM originally and then taken by Microsoft. It is best used for Windows. NFS : Made by Sun. It can be used both with SMB and NFS. Types of Storage GW's \u00b6 3 different types of fo storage gateways are: File gateway (using NFS) : The File Gateway presents a file interface that enables you to store files as objects in Amazon S3 using the industry-standard NFS and SMB file protocols, and access those files via NFS and SMB from your datacenter or Amazon EC2, or access those files as objects with the S3 API. Volume Gateway : The Volume Gateway presents your applications block storage volumes using the iSCSI protocol. Data written to these volumes can be asynchronously backed up as point-in-time snapshots of your volumes, and stored in the cloud as Amazon EBS snapshots. Tape Gateway: Used to backup data into the glacier. Cached volumes : Entire dataset is stored on S3 and the most frequently used data is cached on-prem Notes \u00b6 Snapshots are incremental backups (delta) which will only capture the changed blocks. In the case of stored volume all the data is stored on the data center (on-prem) and the data is replicated to the backend server in an Async fashion VM is used to replicate the data","title":"Storage Gateway \u2ed4"},{"location":"aws/storagegw/#types-based-on-installation","text":"They are of two types 1. It can be installed as VM on a hypervisor. 1. It comes as a physical box.","title":"Types based on installation"},{"location":"aws/storagegw/#sharing-protocols","text":"Two types of file sharing protocols are: SMB : Made by IBM originally and then taken by Microsoft. It is best used for Windows. NFS : Made by Sun. It can be used both with SMB and NFS.","title":"Sharing Protocols"},{"location":"aws/storagegw/#types-of-storage-gws","text":"3 different types of fo storage gateways are: File gateway (using NFS) : The File Gateway presents a file interface that enables you to store files as objects in Amazon S3 using the industry-standard NFS and SMB file protocols, and access those files via NFS and SMB from your datacenter or Amazon EC2, or access those files as objects with the S3 API. Volume Gateway : The Volume Gateway presents your applications block storage volumes using the iSCSI protocol. Data written to these volumes can be asynchronously backed up as point-in-time snapshots of your volumes, and stored in the cloud as Amazon EBS snapshots. Tape Gateway: Used to backup data into the glacier. Cached volumes : Entire dataset is stored on S3 and the most frequently used data is cached on-prem","title":"Types of Storage GW's"},{"location":"aws/storagegw/#notes","text":"Snapshots are incremental backups (delta) which will only capture the changed blocks. In the case of stored volume all the data is stored on the data center (on-prem) and the data is replicated to the backend server in an Async fashion VM is used to replicate the data","title":"Notes"},{"location":"aws/subnet/","text":"AWS reserves both the first 4 and last one IP addresses in each subnet\u2019s CIDR block. In total, AWS reserves 5 IP\u2019s for your subnet. We can have only one IG per subnet. When we create a custom VPC, no subnet or IGW is created.","title":"101 Networking"},{"location":"aws/vpc/","text":"VPC \u00b6 Elements in VPC \u00b6 Elastic network interface \u00b6 It is a logical networking component in a VPC that represents a virtual network card. Subnet \u00b6 A range of IP addresses in your VPC. You can add AWS resources to a specified subnet. Use a public subnet for resources that must connect to the internet, and a private subnet for resources that don't connect to the internet. Security group \u00b6 We can use security groups to control access to the AWS resources in each subnet. Access control list (ACL) \u00b6 It uses a network ACL to provide additional security in a subnet. The default subnet ACL allows all inbound and outbound traffic. A subnet can only be associated with a single NACL at a time. NACL rules have rule number, protocol, choice of ALLOW or DENY, CIDR range port or port range for inbound and outbound traffic. Route table \u00b6 contains a set of routes that AWS uses to direct the network traffic for your VPC. You can explicitly associate a subnet with a particular route table. Remember By default, the subnet is associated with the main route table. Route \u00b6 Each route in a route table specifies a range of IP addresses and the destination where Lambda sends the traffic for that range. The route also specifies a target, which is the gateway, network interface, or connection through which to send the traffic. NAT gateway \u00b6 An AWS Network Address Translation (NAT) service that controls access from a private VPC private subnet to the Internet. VPC endpoints \u00b6 You can use an Amazon VPC endpoint to create private connectivity to services hosted in AWS, without requiring access over the internet or through a NAT device, VPN connection, or AWS Direct Connect connection. VPC flow logs \u00b6 VPC flow logs: We can use them for troubleshooting to see if there is some attack on the VPC. Also, they can let us know why the communication between the VPC\u2019s is not working out. VPC Flow Logs is a feature that enables you to capture information about the IP traffic going to and from network interfaces in your VPC. You can create a flow log for a VPC, a subnet, or a network interface. If you create a flow log for a subnet or VPC, each network interface in that subnet or VPC is monitored. Notes \u00b6 Some notes on VPC are NACL applies to all instances in an associated subnet. NACLs are always evaluated first because they exist at the border of a subnet. As security groups are attached to instances, they are not processed until traffic passes through the NACL and into the instance\u2019s subnet. NACLs are associated with subnets, not VPC. Each rule in an NACL has a number, and those rules are evaluated using those numbers, moving from low to high. Each subnet in your VPC must be associated with an NACL. An NACL is associated with a subnet, not an instance or VPC. It can be associated with a single subnet or multiple subnets. A VPC spans all the availability zones in a region. You must always select a region to create a VPC, and you must always provide a CIDR block. VPCs span all the AZs in a region, so that is not required. While you can add secondary IPv4 CIDR blocks, you cannot add additional CIDR blocks for IPv6 at this time. When creating a VPC, you can specify an option name (no description needed), a required IPv4 CIDR block, and an optional IPv6 CIDR block. A VPN-only subnet routes traffic through a virtual private gateway rather than an internet gateway At a minimum, a VPC-only subnet must have a routing table routing traffic and a virtual private gateway to which traffic is routed. You can only create 5 VPCs per region by default. Creating more requires a request to AWS. You can create 200 subnets per VPC. You\u2019re allowed 5 elastic IP addresses per region unless you have the default limits raised by AWS. A security group denies all traffic unless explicitly allowed. This means it functions as a whitelist. A VPC endpoint is a connection to an AWS service and explicitly does not use internet gateways or VPN connections. Bastion hosts should be in a public subnet so that they can be accessed via the public Internet. They can then route traffic into a private subnet. Bastion hosts are also sometimes called jump servers because they allow a connection to \u201cjump\u201d to the bastion and then into a private subnet. Internet gateways scale horizontally, not vertically. They are also redundant and highly available automatically. Default VPC has an internet gateway automatically attached. Load Balancers Application load balancers operate at the Application layer, which is layer 7 of the OSI model. ELBs (classic load balancers) operate at the Transport layer, layer 4, as well as layer 7. Network load balancers operate at layer 4 as well A NAT device\u2014network address translation\u2014provides routing for instances to an internet gateway but can prevent undesired inbound traffic. A site-to-site connection is going to require a private subnet on the AWS side with private instances within it. An egress-only gateway is for use with IPv6 traffic and only allows outbound traffic. They are stateful. The default VPC has an internet gateway attached by default while custom VPC do not The default VPC has a CIDR block of /16 , but the default subnet in each AZ is a /20 . Default VPC does get a subnet automatically (as well as an internet gateway). While the default VPC automatically creates a subnet, while custom VPCs do not create a subnet. With custom VPC, You do automatically get a security group, route table, and NACL. A VPC endpoint can connect to S3 and DynamoDB, as well as a host of additional AWS services. It does not require an internet gateway or a VPN connection and does not route traffic over the public Internet. A VPC endpoint is a virtual device that provides redundancy via AWS (and automatically). A VPC endpoint provides a connection over the Amazon network between your VPC and service, such as S3. This avoids leaving the network and routing over the public Internet, which inherently provides greater security for the traffic involved VPCs can peer with other VPCs, in the same account or different ones","title":"VPC"},{"location":"aws/vpc/#vpc","text":"","title":"VPC"},{"location":"aws/vpc/#elements-in-vpc","text":"","title":"Elements in VPC"},{"location":"aws/vpc/#elastic-network-interface","text":"It is a logical networking component in a VPC that represents a virtual network card.","title":"Elastic network interface"},{"location":"aws/vpc/#subnet","text":"A range of IP addresses in your VPC. You can add AWS resources to a specified subnet. Use a public subnet for resources that must connect to the internet, and a private subnet for resources that don't connect to the internet.","title":"Subnet"},{"location":"aws/vpc/#security-group","text":"We can use security groups to control access to the AWS resources in each subnet.","title":"Security group"},{"location":"aws/vpc/#access-control-list-acl","text":"It uses a network ACL to provide additional security in a subnet. The default subnet ACL allows all inbound and outbound traffic. A subnet can only be associated with a single NACL at a time. NACL rules have rule number, protocol, choice of ALLOW or DENY, CIDR range port or port range for inbound and outbound traffic.","title":"Access control list (ACL)"},{"location":"aws/vpc/#route-table","text":"contains a set of routes that AWS uses to direct the network traffic for your VPC. You can explicitly associate a subnet with a particular route table. Remember By default, the subnet is associated with the main route table.","title":"Route table"},{"location":"aws/vpc/#route","text":"Each route in a route table specifies a range of IP addresses and the destination where Lambda sends the traffic for that range. The route also specifies a target, which is the gateway, network interface, or connection through which to send the traffic.","title":"Route"},{"location":"aws/vpc/#nat-gateway","text":"An AWS Network Address Translation (NAT) service that controls access from a private VPC private subnet to the Internet.","title":"NAT gateway"},{"location":"aws/vpc/#vpc-endpoints","text":"You can use an Amazon VPC endpoint to create private connectivity to services hosted in AWS, without requiring access over the internet or through a NAT device, VPN connection, or AWS Direct Connect connection.","title":"VPC endpoints"},{"location":"aws/vpc/#vpc-flow-logs","text":"VPC flow logs: We can use them for troubleshooting to see if there is some attack on the VPC. Also, they can let us know why the communication between the VPC\u2019s is not working out. VPC Flow Logs is a feature that enables you to capture information about the IP traffic going to and from network interfaces in your VPC. You can create a flow log for a VPC, a subnet, or a network interface. If you create a flow log for a subnet or VPC, each network interface in that subnet or VPC is monitored.","title":"VPC flow logs"},{"location":"aws/vpc/#notes","text":"Some notes on VPC are NACL applies to all instances in an associated subnet. NACLs are always evaluated first because they exist at the border of a subnet. As security groups are attached to instances, they are not processed until traffic passes through the NACL and into the instance\u2019s subnet. NACLs are associated with subnets, not VPC. Each rule in an NACL has a number, and those rules are evaluated using those numbers, moving from low to high. Each subnet in your VPC must be associated with an NACL. An NACL is associated with a subnet, not an instance or VPC. It can be associated with a single subnet or multiple subnets. A VPC spans all the availability zones in a region. You must always select a region to create a VPC, and you must always provide a CIDR block. VPCs span all the AZs in a region, so that is not required. While you can add secondary IPv4 CIDR blocks, you cannot add additional CIDR blocks for IPv6 at this time. When creating a VPC, you can specify an option name (no description needed), a required IPv4 CIDR block, and an optional IPv6 CIDR block. A VPN-only subnet routes traffic through a virtual private gateway rather than an internet gateway At a minimum, a VPC-only subnet must have a routing table routing traffic and a virtual private gateway to which traffic is routed. You can only create 5 VPCs per region by default. Creating more requires a request to AWS. You can create 200 subnets per VPC. You\u2019re allowed 5 elastic IP addresses per region unless you have the default limits raised by AWS. A security group denies all traffic unless explicitly allowed. This means it functions as a whitelist. A VPC endpoint is a connection to an AWS service and explicitly does not use internet gateways or VPN connections. Bastion hosts should be in a public subnet so that they can be accessed via the public Internet. They can then route traffic into a private subnet. Bastion hosts are also sometimes called jump servers because they allow a connection to \u201cjump\u201d to the bastion and then into a private subnet. Internet gateways scale horizontally, not vertically. They are also redundant and highly available automatically. Default VPC has an internet gateway automatically attached. Load Balancers Application load balancers operate at the Application layer, which is layer 7 of the OSI model. ELBs (classic load balancers) operate at the Transport layer, layer 4, as well as layer 7. Network load balancers operate at layer 4 as well A NAT device\u2014network address translation\u2014provides routing for instances to an internet gateway but can prevent undesired inbound traffic. A site-to-site connection is going to require a private subnet on the AWS side with private instances within it. An egress-only gateway is for use with IPv6 traffic and only allows outbound traffic. They are stateful. The default VPC has an internet gateway attached by default while custom VPC do not The default VPC has a CIDR block of /16 , but the default subnet in each AZ is a /20 . Default VPC does get a subnet automatically (as well as an internet gateway). While the default VPC automatically creates a subnet, while custom VPCs do not create a subnet. With custom VPC, You do automatically get a security group, route table, and NACL. A VPC endpoint can connect to S3 and DynamoDB, as well as a host of additional AWS services. It does not require an internet gateway or a VPN connection and does not route traffic over the public Internet. A VPC endpoint is a virtual device that provides redundancy via AWS (and automatically). A VPC endpoint provides a connection over the Amazon network between your VPC and service, such as S3. This avoids leaving the network and routing over the public Internet, which inherently provides greater security for the traffic involved VPCs can peer with other VPCs, in the same account or different ones","title":"Notes"},{"location":"azure/aad/","text":"Azure Active Directory \ud83d\udd10 \u00b6 It is global IAM service for Azure We can use it for B2B and B2C (for support) We can do monitoring with Azure AD. Example : check if user is logging in from another location based on the distance. AAD vs Windows server AD You must use Windows Server Active Directory to update the identity, contact info, or job info for users whose source of authority is Windows Server Active Directory . Usage location is an Azure property that can only be modified from Azure AD (for all users including Windows Server AD users synced via Azure AD Connect). Security basics \u00b6 User Principal : The one accessing the account Service Principal : Identity created by system to access applications Audience : The Audience is the applicaton id for API which your App has permissions on Role and Scope : Scopes are typically associated with API access. An API defines what scopes are available (what services it provides). For example a user account management API might define scopes like read:user, create:user, update:user . These are the capabilities the API provides, but not necessarily what any given user can do. In the \u201cRole & Scope\u201d model, Roles are defined, and users are given a Role. Individual Scopes are associated with a given Role, combining all these elements together. For example, you might have: Role : Audit, API : user_manager, Scopes : read:user Role : Access Control, API : user_manager, Scopes : create:user, update:user And maybe Amar has the Audit role, while Bob has the Access Control role. When you request a scope, the Authorization Server (Auth0/AAD) will decide whether you get that scope or not. You can request anything you like, but the Authorization Server sends back a token with only the scopes it has decided you are allowed to have. Role defination example { \"value\" : [ { \"properties\" : { \"roleName\" : \"Billing Reader Plus\" , \"type\" : \"CustomRole\" , \"description\" : \"Read billing data and download invoices\" , \"assignableScopes\" : [ \"/subscriptions/{subscriptionId1}\" ], \"permissions\" : [ { \"actions\" : [ \"Microsoft.Authorization/*/read\" , \"Microsoft.Billing/*/read\" , \"Microsoft.Commerce/*/read\" , \"Microsoft.Consumption/*/read\" , \"Microsoft.Management/managementGroups/read\" , \"Microsoft.CostManagement/*/read\" , \"Microsoft.Billing/invoices/download/action\" , \"Microsoft.CostManagement/exports/*\" ], \"notActions\" : [ \"Microsoft.CostManagement/exports/delete\" ], \"dataActions\" : [], \"notDataActions\" : [] } ], \"createdOn\" : \"2021-05-22T21:57:23.5764138Z\" , \"updatedOn\" : \"2021-05-22T21:57:23.5764138Z\" , \"createdBy\" : \"{createdByObjectId1}\" , \"updatedBy\" : \"{updatedByObjectId1}\" }, \"id\" : \"/subscriptions/{subscriptionId1}/providers/Microsoft.Authorization/roleDefinitions/{roleDefinitionId1}\" , \"type\" : \"Microsoft.Authorization/roleDefinitions\" , \"name\" : \"{roleDefinitionId1}\" } ] } What is Azure AD tenant? \u00b6 Tldr Tenant is an instance of AAD An Azure AD tenant is a reserved Azure AD service instance that an organization receives and owns once it signs up for a Microsoft cloud service such as Azure, Microsoft Intune, or Microsoft 365. Remember Each tenant represents an organization, and is distinct and separate from other Azure AD tenants. SCP (Service Connection Point) \u00b6 The Active Directory Schema defines a Service Connection Point (SCP) object class to make it easy for a service to publish service-specific data in the directory. Clients of the service use the data in an SCP to locate, connect to, and authenticate an instance of your service. Service Principal \u00b6 An Azure service principal is an identity created for use with applications, hosted services, and automated tools to access Azure resources. This access is restricted by the roles assigned to the service principal, giving you control over which resources can be accessed and at which level. For security reasons, it's always recommended to use service principals with automated tools rather than allowing them to log in with a user identity. There are two types of authentication available for service principals: Password-based authentication Certificate-based authentication. SP VS Managed Identity \u00b6 SP and Managed Identity The difference between managed identities and service principals is subtle. Service principals are no longer recommended for services that support managed identities. Managed identities are effectively \u201cmanaged service principals\u201d and remove the need for storing credentials in the application\u2019s configuration and instead inject certificates into the resource that the managed identity is created into. Active Directory vs Azure AD \u00b6 They are not competing services. Think of them as 2 separate services. Acrive Directory Azure AD Organizational Units Administrative Units Kerberos, LDAP SAML, OAuth Hierarchial Flat structure On Premise Cloud based and Global Default Tenant \u00b6 The default tenant is created when we create first subscription. Later, we can create another tenant and will have option to switch between various tenants. Below you can see how we can create a new tenant. Users \u00b6 Users are set of JSON properties Types of users: Admins Members: Assigned : Specifically assign a user to a group Dynamic user : Rules are created which automate group membership via user attributes Guests Communication bw diff tenants \u00b6 The associated service principal in tenant 1 will be used to authenticate to resources within the service's own subscription. A separate associated service principal which resides in tenant 2 will be used to authenticate to resources in subscriptions 2 and 3. Core Concepts \u00b6 Groups \u00b6 Type of groups: Security Group : Azure AD Security Groups are analogous to Security Groups in on-prem Windows Active Directory. They are Security Principals, which means they can be used to secure objects in Azure AD. They can be created natively in Azure AD, or synced from Windows AD with Azure AD Connect. Their membership can be static, or it can be generated dynamically with rules. Microsoft 365 : Microsoft 365 Groups are a membership object in Microsoft 365 that eases the task of ensuring a group of people have consistent permissions to a group of related resources. You may see them referred to as Microsoft 365 Groups or Unified Groups. Dynamic groups \u00b6 Dynamic group membership adds and removes group members automatically using membership rules based on member attributes. You can set up a rule for dynamic membership on security groups or Microsoft 365 groups. Remember When the attributes of a user or a device change, the system evaluates all dynamic group rules in a directory to see if the change would trigger any group adds or removes. If a user or device satisfies a rule on a group, they're added as a member of that group. If they no longer satisfy the rule, they're removed. You can't manually add or remove a member of a dynamic group. Rule Builder \u00b6 Azure AD provides a rule builder to create and update your important rules more quickly. The rule builder supports the construction of up to five expressions. The rule builder makes it easier to form a rule with a few simple expressions, however, it can't be used to reproduce every rule. If the rule builder doesn't support the rule you want to create, you can use the text box. Administrative Units \u00b6 An administrative unit is an Azure AD resource that can be a container for other Azure AD resources. An administrative unit can contain only users, groups, or devices. Use of Admin Units Administrative units restrict permissions in a role to any portion of your organization that you define. You could, for example, use administrative units to delegate the Helpdesk Administrator role to regional support specialists, so they can manage users only in the region that they support. A use case scenario It can be useful to restrict administrative scope by using administrative units in organizations that are made up of independent divisions of any kind. Consider the example of a large university that's made up of many autonomous schools (School of Business, School of Engineering, and so on). Each school has a team of IT admins who control access, manage users, and set policies for their school. A central administrator could: Create an AU for the School of Business. Populate the AU with only students and staff within the School of Business. Create a role with administrative permissions over only Azure AD users in the School of Business AU. Add the business school IT team to the role, along with its scope. AAD Connect \u00b6 Azure AD Connect is an on-premises Microsoft application that's designed to meet and accomplish your hybrid identity goals. If you're evaluating how to best meet your goals, you should also consider the cloud-managed solution Azure AD Connect cloud sync. What is AAD Connect Health? Azure Active Directory (Azure AD) Connect Health provides robust monitoring of your on-premises identity infrastructure. It enables you to maintain a reliable connection to Microsoft 365 and Microsoft Online Services. Azure AD Connect sync \u00b6 Azure AD Connect sync synchronize changes occurring in your on-premises directory using a scheduler . There are two scheduler processes: - one for password sync - another for object/attribute sync and maintenance tasks When to Sync? By default every 30 minutes a synchronization cycle is run. If you have modified the synchronization cycle you will need to make sure that a synchronization cycle is run at least once every 7 days. Tip Best way is either a Synchronization being executed through the Azure AD Connect , in the Portal or using the command Start-ADSyncSyncCycle -PolicyType Delta . Running a full sync cycle can be very time-consuming, so if you need to replicate the user information to Azure AD immediately then run Start-ADSyncSyncCycle -PolicyType Delta . Licence \u00b6 Many AAD services require you to license each of your users or groups (and associated members) for that service. Only users with active licenses will be able to access and use the licensed Azure AD services for which that's true. Licenses are applied per tenant and don't transfer to other tenants. Configuring SSPR (Self Service Password Reset) \u00b6 Azure Active Directory (Azure AD) self-service password reset (SSPR) gives users the ability to change or reset their password, with no administrator or help desk involvement. If Azure AD locks a user's account, or they forget their password, they can follow prompts to unblock themselves and get back to work. This ability reduces help desk calls and loss of productivity when a user can't sign in to their device or an application. SSPR can be set for All users None Selected Users Remember Security questions as an authentication method is not available for admins Admins are always enabled for SSPR and need to have 2 security methods to reset their password. Various options for SSPR are Mobile app notification Mobile app code Email Mobile phone call Office phone call Security questions (least recommended) Device Management \u00b6 We can register the devices which are not in our organization Azure AD registered devices are signed in to using a local account like a Microsoft account on a Windows 10 or newer device. These devices have an Azure AD account for access to organizational resources. Access to resources in the organization can be limited based on that Azure AD account and Conditional Access policies applied to the device identity. MDM Administrators can secure and further control these Azure AD registered devices using Mobile Device Management (MDM) tools like Microsoft Intune. MDM provides a means to enforce organization-required configurations like requiring storage to be encrypted, password complexity, and security software kept updated. Registration options \u00b6 We have 3 options: - Azure AD registered : This is the least restrictive and allows BYOD. Also supportgs iOS and macOS - Azure AD joined : Azure AD joined devices are signed in to using an organizational Azure AD account. In this case the device is owned by organization. - Hybrid Azure AD joined : Joined to on-premises AD and Azure AD requiring organizational account to sign in to the device Conditional Access Policy \u00b6 Info Organizations who have deployed Microsoft Intune can use the information returned from their devices to identify devices that meet compliance requirements such as: Requiring a PIN to unlock Requiring device encryption Requiring a minimum or maximum operating system version Requiring a device isn't jailbroken or rooted RBAC and AAD roles \u00b6 Roles have descending effect as shown in below figure RBAC Vs Azure AD? Azure (RBAC) and Azure AD roles are independent. AD roles do not grant access to resources and Azure roles do not grant access to Azure AD. However, a Global Administrator in AD can elevate access to all subscriptions and will be User Access Administrator in Azure root scope. Elevated access for global admin \u00b6 When you set the toggle to Yes , you are assigned the User Access Administrator role in Azure RBAC at root scope / . This grants you permission to assign roles in all Azure subscriptions and management groups associated with this Azure AD directory . This toggle is only available to users who are assigned the Global Administrator role in Azure AD. When you set the toggle to No , the User Access Administrator role in Azure RBAC is removed from your user account. You can no longer assign roles in all Azure subscriptions and management groups that are associated with this Azure AD directory. You can view and manage only the Azure subscriptions and management groups to which you have been granted access. Azure RBAC roles \u00b6 Owner : Grants full access to manage all resources, including the ability to assign roles in Azure RBAC. Contributor : Grants full access to manage all resources, but does NOT allow you to assign roles in Azure RBAC. (you cannot add users or changes their rights) User Access Administrator : Lets you manage user access to Azure resources. Reader : View all resources, but does not allow you to make any changes. Security Admin : View and update permissions for Security Center. Same permissions as the Security Reader role and can also update the security policy and dismiss alerts and recommendations. Network Contributor : Lets you manage networks, but not access to them. (so you can add VNET, subnet, etc) Azure AD roles \u00b6 Global admin role : Can manage azure AD resources Billing admin role : Can perform billing tasks User admin role : Can manage users and groups Helpdesk admin : Can reset passwords Scope of roles Scope of AD roles is at tenant level whereas RBAC roles scope can be at different levels MFA Provider \u00b6 Info Authentication providers can be found in the Azure portal > Azure Active Directory > Security > MFA > Providers Two-step verification is available by default for Global Administrators who have Azure Active Directory, and Microsoft 365 users. However, if you wish to take advantage of advanced features then you should purchase the full version of Azure AD Multi-Factor Authentication (MFA). An Azure AD Multi-Factor Auth Provider is used to take advantage of features provided by Azure AD Multi-Factor Authentication for users who do not have licenses. Warning You can't change the usage model (per enabled user or per authentication) after an MFA provider is created. Per-user \u00b6 The per-user option calculates the number of users who are eligible to perform MFA, which is all users in Azure AD, and all enabled users in MFA Server. This option is best if some users have licenses, but you need to extend MFA to more users beyond your licensing limits. Per authentication \u00b6 The per-authentication option calculates the number of authentications performed against your tenant in a month. This option is best if some users authenticate only occasionally 3 rd party access \u00b6 Catalog \u00b6 A catalog is a container of resources and access packages. You create a catalog when you want to group related resources and access packages. An administrator can create a catalog Create a new catalog using Azure Active Directory --> Identity Governance Access package \u00b6 An access package enables you to do a one-time setup of resources and policies that automatically administers access for the life of the access package All access packages must be put in a container called a catalog. A catalog defines what resources you can add to your access package. If you don't specify a catalog, your access package will be put into the general catalog. Currently, you can't move an existing access package to a different catalog. Access package assignments \u00b6 In entitlement management, you can see who has been assigned to access packages, their policy, and status. Application Proxy \u00b6 Application Proxy is a feature of Azure AD that enables users to access on-premises web applications from a remote client. Application Proxy includes both the Application Proxy service which runs in the cloud, and the Application Proxy connector which runs on an on-premises server. Azure AD, the Application Proxy service, and the Application Proxy connector work together to securely pass the user sign-on token from Azure AD to the web application.","title":"Active Directory \ud83d\udd10"},{"location":"azure/aad/#azure-active-directory","text":"It is global IAM service for Azure We can use it for B2B and B2C (for support) We can do monitoring with Azure AD. Example : check if user is logging in from another location based on the distance. AAD vs Windows server AD You must use Windows Server Active Directory to update the identity, contact info, or job info for users whose source of authority is Windows Server Active Directory . Usage location is an Azure property that can only be modified from Azure AD (for all users including Windows Server AD users synced via Azure AD Connect).","title":"Azure Active Directory \ud83d\udd10"},{"location":"azure/aad/#security-basics","text":"User Principal : The one accessing the account Service Principal : Identity created by system to access applications Audience : The Audience is the applicaton id for API which your App has permissions on Role and Scope : Scopes are typically associated with API access. An API defines what scopes are available (what services it provides). For example a user account management API might define scopes like read:user, create:user, update:user . These are the capabilities the API provides, but not necessarily what any given user can do. In the \u201cRole & Scope\u201d model, Roles are defined, and users are given a Role. Individual Scopes are associated with a given Role, combining all these elements together. For example, you might have: Role : Audit, API : user_manager, Scopes : read:user Role : Access Control, API : user_manager, Scopes : create:user, update:user And maybe Amar has the Audit role, while Bob has the Access Control role. When you request a scope, the Authorization Server (Auth0/AAD) will decide whether you get that scope or not. You can request anything you like, but the Authorization Server sends back a token with only the scopes it has decided you are allowed to have. Role defination example { \"value\" : [ { \"properties\" : { \"roleName\" : \"Billing Reader Plus\" , \"type\" : \"CustomRole\" , \"description\" : \"Read billing data and download invoices\" , \"assignableScopes\" : [ \"/subscriptions/{subscriptionId1}\" ], \"permissions\" : [ { \"actions\" : [ \"Microsoft.Authorization/*/read\" , \"Microsoft.Billing/*/read\" , \"Microsoft.Commerce/*/read\" , \"Microsoft.Consumption/*/read\" , \"Microsoft.Management/managementGroups/read\" , \"Microsoft.CostManagement/*/read\" , \"Microsoft.Billing/invoices/download/action\" , \"Microsoft.CostManagement/exports/*\" ], \"notActions\" : [ \"Microsoft.CostManagement/exports/delete\" ], \"dataActions\" : [], \"notDataActions\" : [] } ], \"createdOn\" : \"2021-05-22T21:57:23.5764138Z\" , \"updatedOn\" : \"2021-05-22T21:57:23.5764138Z\" , \"createdBy\" : \"{createdByObjectId1}\" , \"updatedBy\" : \"{updatedByObjectId1}\" }, \"id\" : \"/subscriptions/{subscriptionId1}/providers/Microsoft.Authorization/roleDefinitions/{roleDefinitionId1}\" , \"type\" : \"Microsoft.Authorization/roleDefinitions\" , \"name\" : \"{roleDefinitionId1}\" } ] }","title":"Security basics"},{"location":"azure/aad/#what-is-azure-ad-tenant","text":"Tldr Tenant is an instance of AAD An Azure AD tenant is a reserved Azure AD service instance that an organization receives and owns once it signs up for a Microsoft cloud service such as Azure, Microsoft Intune, or Microsoft 365. Remember Each tenant represents an organization, and is distinct and separate from other Azure AD tenants.","title":"What is Azure AD tenant?"},{"location":"azure/aad/#scp-service-connection-point","text":"The Active Directory Schema defines a Service Connection Point (SCP) object class to make it easy for a service to publish service-specific data in the directory. Clients of the service use the data in an SCP to locate, connect to, and authenticate an instance of your service.","title":"SCP (Service Connection Point)"},{"location":"azure/aad/#service-principal","text":"An Azure service principal is an identity created for use with applications, hosted services, and automated tools to access Azure resources. This access is restricted by the roles assigned to the service principal, giving you control over which resources can be accessed and at which level. For security reasons, it's always recommended to use service principals with automated tools rather than allowing them to log in with a user identity. There are two types of authentication available for service principals: Password-based authentication Certificate-based authentication.","title":"Service Principal"},{"location":"azure/aad/#sp-vs-managed-identity","text":"SP and Managed Identity The difference between managed identities and service principals is subtle. Service principals are no longer recommended for services that support managed identities. Managed identities are effectively \u201cmanaged service principals\u201d and remove the need for storing credentials in the application\u2019s configuration and instead inject certificates into the resource that the managed identity is created into.","title":"SP VS Managed Identity"},{"location":"azure/aad/#active-directory-vs-azure-ad","text":"They are not competing services. Think of them as 2 separate services. Acrive Directory Azure AD Organizational Units Administrative Units Kerberos, LDAP SAML, OAuth Hierarchial Flat structure On Premise Cloud based and Global","title":"Active Directory vs Azure AD"},{"location":"azure/aad/#default-tenant","text":"The default tenant is created when we create first subscription. Later, we can create another tenant and will have option to switch between various tenants. Below you can see how we can create a new tenant.","title":"Default Tenant"},{"location":"azure/aad/#users","text":"Users are set of JSON properties Types of users: Admins Members: Assigned : Specifically assign a user to a group Dynamic user : Rules are created which automate group membership via user attributes Guests","title":"Users"},{"location":"azure/aad/#communication-bw-diff-tenants","text":"The associated service principal in tenant 1 will be used to authenticate to resources within the service's own subscription. A separate associated service principal which resides in tenant 2 will be used to authenticate to resources in subscriptions 2 and 3.","title":"Communication bw diff tenants"},{"location":"azure/aad/#core-concepts","text":"","title":"Core Concepts"},{"location":"azure/aad/#groups","text":"Type of groups: Security Group : Azure AD Security Groups are analogous to Security Groups in on-prem Windows Active Directory. They are Security Principals, which means they can be used to secure objects in Azure AD. They can be created natively in Azure AD, or synced from Windows AD with Azure AD Connect. Their membership can be static, or it can be generated dynamically with rules. Microsoft 365 : Microsoft 365 Groups are a membership object in Microsoft 365 that eases the task of ensuring a group of people have consistent permissions to a group of related resources. You may see them referred to as Microsoft 365 Groups or Unified Groups.","title":"Groups"},{"location":"azure/aad/#dynamic-groups","text":"Dynamic group membership adds and removes group members automatically using membership rules based on member attributes. You can set up a rule for dynamic membership on security groups or Microsoft 365 groups. Remember When the attributes of a user or a device change, the system evaluates all dynamic group rules in a directory to see if the change would trigger any group adds or removes. If a user or device satisfies a rule on a group, they're added as a member of that group. If they no longer satisfy the rule, they're removed. You can't manually add or remove a member of a dynamic group.","title":"Dynamic groups"},{"location":"azure/aad/#rule-builder","text":"Azure AD provides a rule builder to create and update your important rules more quickly. The rule builder supports the construction of up to five expressions. The rule builder makes it easier to form a rule with a few simple expressions, however, it can't be used to reproduce every rule. If the rule builder doesn't support the rule you want to create, you can use the text box.","title":"Rule Builder"},{"location":"azure/aad/#administrative-units","text":"An administrative unit is an Azure AD resource that can be a container for other Azure AD resources. An administrative unit can contain only users, groups, or devices. Use of Admin Units Administrative units restrict permissions in a role to any portion of your organization that you define. You could, for example, use administrative units to delegate the Helpdesk Administrator role to regional support specialists, so they can manage users only in the region that they support. A use case scenario It can be useful to restrict administrative scope by using administrative units in organizations that are made up of independent divisions of any kind. Consider the example of a large university that's made up of many autonomous schools (School of Business, School of Engineering, and so on). Each school has a team of IT admins who control access, manage users, and set policies for their school. A central administrator could: Create an AU for the School of Business. Populate the AU with only students and staff within the School of Business. Create a role with administrative permissions over only Azure AD users in the School of Business AU. Add the business school IT team to the role, along with its scope.","title":"Administrative Units"},{"location":"azure/aad/#aad-connect","text":"Azure AD Connect is an on-premises Microsoft application that's designed to meet and accomplish your hybrid identity goals. If you're evaluating how to best meet your goals, you should also consider the cloud-managed solution Azure AD Connect cloud sync. What is AAD Connect Health? Azure Active Directory (Azure AD) Connect Health provides robust monitoring of your on-premises identity infrastructure. It enables you to maintain a reliable connection to Microsoft 365 and Microsoft Online Services.","title":"AAD Connect"},{"location":"azure/aad/#azure-ad-connect-sync","text":"Azure AD Connect sync synchronize changes occurring in your on-premises directory using a scheduler . There are two scheduler processes: - one for password sync - another for object/attribute sync and maintenance tasks When to Sync? By default every 30 minutes a synchronization cycle is run. If you have modified the synchronization cycle you will need to make sure that a synchronization cycle is run at least once every 7 days. Tip Best way is either a Synchronization being executed through the Azure AD Connect , in the Portal or using the command Start-ADSyncSyncCycle -PolicyType Delta . Running a full sync cycle can be very time-consuming, so if you need to replicate the user information to Azure AD immediately then run Start-ADSyncSyncCycle -PolicyType Delta .","title":"Azure AD Connect sync"},{"location":"azure/aad/#licence","text":"Many AAD services require you to license each of your users or groups (and associated members) for that service. Only users with active licenses will be able to access and use the licensed Azure AD services for which that's true. Licenses are applied per tenant and don't transfer to other tenants.","title":"Licence"},{"location":"azure/aad/#configuring-sspr-self-service-password-reset","text":"Azure Active Directory (Azure AD) self-service password reset (SSPR) gives users the ability to change or reset their password, with no administrator or help desk involvement. If Azure AD locks a user's account, or they forget their password, they can follow prompts to unblock themselves and get back to work. This ability reduces help desk calls and loss of productivity when a user can't sign in to their device or an application. SSPR can be set for All users None Selected Users Remember Security questions as an authentication method is not available for admins Admins are always enabled for SSPR and need to have 2 security methods to reset their password. Various options for SSPR are Mobile app notification Mobile app code Email Mobile phone call Office phone call Security questions (least recommended)","title":"Configuring SSPR (Self Service Password Reset)"},{"location":"azure/aad/#device-management","text":"We can register the devices which are not in our organization Azure AD registered devices are signed in to using a local account like a Microsoft account on a Windows 10 or newer device. These devices have an Azure AD account for access to organizational resources. Access to resources in the organization can be limited based on that Azure AD account and Conditional Access policies applied to the device identity. MDM Administrators can secure and further control these Azure AD registered devices using Mobile Device Management (MDM) tools like Microsoft Intune. MDM provides a means to enforce organization-required configurations like requiring storage to be encrypted, password complexity, and security software kept updated.","title":"Device Management"},{"location":"azure/aad/#registration-options","text":"We have 3 options: - Azure AD registered : This is the least restrictive and allows BYOD. Also supportgs iOS and macOS - Azure AD joined : Azure AD joined devices are signed in to using an organizational Azure AD account. In this case the device is owned by organization. - Hybrid Azure AD joined : Joined to on-premises AD and Azure AD requiring organizational account to sign in to the device","title":"Registration options"},{"location":"azure/aad/#conditional-access-policy","text":"Info Organizations who have deployed Microsoft Intune can use the information returned from their devices to identify devices that meet compliance requirements such as: Requiring a PIN to unlock Requiring device encryption Requiring a minimum or maximum operating system version Requiring a device isn't jailbroken or rooted","title":"Conditional Access Policy"},{"location":"azure/aad/#rbac-and-aad-roles","text":"Roles have descending effect as shown in below figure RBAC Vs Azure AD? Azure (RBAC) and Azure AD roles are independent. AD roles do not grant access to resources and Azure roles do not grant access to Azure AD. However, a Global Administrator in AD can elevate access to all subscriptions and will be User Access Administrator in Azure root scope.","title":"RBAC and AAD roles"},{"location":"azure/aad/#elevated-access-for-global-admin","text":"When you set the toggle to Yes , you are assigned the User Access Administrator role in Azure RBAC at root scope / . This grants you permission to assign roles in all Azure subscriptions and management groups associated with this Azure AD directory . This toggle is only available to users who are assigned the Global Administrator role in Azure AD. When you set the toggle to No , the User Access Administrator role in Azure RBAC is removed from your user account. You can no longer assign roles in all Azure subscriptions and management groups that are associated with this Azure AD directory. You can view and manage only the Azure subscriptions and management groups to which you have been granted access.","title":"Elevated access for global admin"},{"location":"azure/aad/#azure-rbac-roles","text":"Owner : Grants full access to manage all resources, including the ability to assign roles in Azure RBAC. Contributor : Grants full access to manage all resources, but does NOT allow you to assign roles in Azure RBAC. (you cannot add users or changes their rights) User Access Administrator : Lets you manage user access to Azure resources. Reader : View all resources, but does not allow you to make any changes. Security Admin : View and update permissions for Security Center. Same permissions as the Security Reader role and can also update the security policy and dismiss alerts and recommendations. Network Contributor : Lets you manage networks, but not access to them. (so you can add VNET, subnet, etc)","title":"Azure RBAC roles"},{"location":"azure/aad/#azure-ad-roles","text":"Global admin role : Can manage azure AD resources Billing admin role : Can perform billing tasks User admin role : Can manage users and groups Helpdesk admin : Can reset passwords Scope of roles Scope of AD roles is at tenant level whereas RBAC roles scope can be at different levels","title":"Azure AD roles"},{"location":"azure/aad/#mfa-provider","text":"Info Authentication providers can be found in the Azure portal > Azure Active Directory > Security > MFA > Providers Two-step verification is available by default for Global Administrators who have Azure Active Directory, and Microsoft 365 users. However, if you wish to take advantage of advanced features then you should purchase the full version of Azure AD Multi-Factor Authentication (MFA). An Azure AD Multi-Factor Auth Provider is used to take advantage of features provided by Azure AD Multi-Factor Authentication for users who do not have licenses. Warning You can't change the usage model (per enabled user or per authentication) after an MFA provider is created.","title":"MFA Provider"},{"location":"azure/aad/#per-user","text":"The per-user option calculates the number of users who are eligible to perform MFA, which is all users in Azure AD, and all enabled users in MFA Server. This option is best if some users have licenses, but you need to extend MFA to more users beyond your licensing limits.","title":"Per-user"},{"location":"azure/aad/#per-authentication","text":"The per-authentication option calculates the number of authentications performed against your tenant in a month. This option is best if some users authenticate only occasionally","title":"Per authentication"},{"location":"azure/aad/#3rd-party-access","text":"","title":"3rd party access"},{"location":"azure/aad/#catalog","text":"A catalog is a container of resources and access packages. You create a catalog when you want to group related resources and access packages. An administrator can create a catalog Create a new catalog using Azure Active Directory --> Identity Governance","title":"Catalog"},{"location":"azure/aad/#access-package","text":"An access package enables you to do a one-time setup of resources and policies that automatically administers access for the life of the access package All access packages must be put in a container called a catalog. A catalog defines what resources you can add to your access package. If you don't specify a catalog, your access package will be put into the general catalog. Currently, you can't move an existing access package to a different catalog.","title":"Access package"},{"location":"azure/aad/#access-package-assignments","text":"In entitlement management, you can see who has been assigned to access packages, their policy, and status.","title":"Access package assignments"},{"location":"azure/aad/#application-proxy","text":"Application Proxy is a feature of Azure AD that enables users to access on-premises web applications from a remote client. Application Proxy includes both the Application Proxy service which runs in the cloud, and the Application Proxy connector which runs on an on-premises server. Azure AD, the Application Proxy service, and the Application Proxy connector work together to securely pass the user sign-on token from Azure AD to the web application.","title":"Application Proxy"},{"location":"azure/aci/","text":"Azure container Instance \u00b6 Azure Container Instances (ACI) is a managed service that allows you to run containers directly on the Microsoft Azure public cloud, without requiring the use of virtual machines (VMs). Azure Container Instances offers the fastest and simplest way to run a container in Azure, without having to manage any virtual machines and without having to adopt a higher-level service such as AKS. When to use AKS? Azure Container Instances is a great solution for any scenario that can operate in isolated containers, including simple applications, task automation, and build jobs. For scenarios where you need full container orchestration, including service discovery across multiple containers, automatic scaling, and coordinated application upgrades, Azure Kubernetes Service (AKS) is recommended How to ACI? \u00b6 Azure Container Instances enables exposing your container groups directly to the internet with an IP address and a fully qualified domain name (FQDN). When you create a container instance, you can specify a custom DNS name label so your application is reachable at customlabel.azureregion.azurecontainer.io Features of ACI \u00b6 Support for both Linux and Windows containers Ability to launch new containers through the Azure portal or command line interface (CLI) Support for standard Docker images and the use of public container registries, such as Docker Hub , as well as Azure Container Registry Ability to provide access to containers over Internet using a fully qualified domain name and IP address Ability to specify the number of CPU cores and memory required for container instances Support for persistent storage by mounting Azure file shares to the container. Container Group/Pod \u00b6 A container group is a collection of containers that get scheduled on the same host machine. The containers in a container group share a lifecycle, resources, local network, and storage volumes. It's similar in concept to a pod in Kubernetes. No Autoscaling \u00b6 No autoscaling is available in ACI and manual redeployment of container group is required. Restart Policy \u00b6 It can be Always Never OnFailure Persistent storage \u00b6 Azure Files are used for this purpose. Linux and Windows containers \u00b6 Azure Container Instances can schedule both Windows and Linux containers with the same API. Simply specify the OS type when you create your container groups.","title":"ACI \ud83d\uddf3"},{"location":"azure/aci/#azure-container-instance","text":"Azure Container Instances (ACI) is a managed service that allows you to run containers directly on the Microsoft Azure public cloud, without requiring the use of virtual machines (VMs). Azure Container Instances offers the fastest and simplest way to run a container in Azure, without having to manage any virtual machines and without having to adopt a higher-level service such as AKS. When to use AKS? Azure Container Instances is a great solution for any scenario that can operate in isolated containers, including simple applications, task automation, and build jobs. For scenarios where you need full container orchestration, including service discovery across multiple containers, automatic scaling, and coordinated application upgrades, Azure Kubernetes Service (AKS) is recommended","title":"Azure container Instance"},{"location":"azure/aci/#how-to-aci","text":"Azure Container Instances enables exposing your container groups directly to the internet with an IP address and a fully qualified domain name (FQDN). When you create a container instance, you can specify a custom DNS name label so your application is reachable at customlabel.azureregion.azurecontainer.io","title":"How to ACI?"},{"location":"azure/aci/#features-of-aci","text":"Support for both Linux and Windows containers Ability to launch new containers through the Azure portal or command line interface (CLI) Support for standard Docker images and the use of public container registries, such as Docker Hub , as well as Azure Container Registry Ability to provide access to containers over Internet using a fully qualified domain name and IP address Ability to specify the number of CPU cores and memory required for container instances Support for persistent storage by mounting Azure file shares to the container.","title":"Features of ACI"},{"location":"azure/aci/#container-grouppod","text":"A container group is a collection of containers that get scheduled on the same host machine. The containers in a container group share a lifecycle, resources, local network, and storage volumes. It's similar in concept to a pod in Kubernetes.","title":"Container Group/Pod"},{"location":"azure/aci/#no-autoscaling","text":"No autoscaling is available in ACI and manual redeployment of container group is required.","title":"No Autoscaling"},{"location":"azure/aci/#restart-policy","text":"It can be Always Never OnFailure","title":"Restart Policy"},{"location":"azure/aci/#persistent-storage","text":"Azure Files are used for this purpose.","title":"Persistent storage"},{"location":"azure/aci/#linux-and-windows-containers","text":"Azure Container Instances can schedule both Windows and Linux containers with the same API. Simply specify the OS type when you create your container groups.","title":"Linux and Windows containers"},{"location":"azure/acr/","text":"Azure Container Registry is a managed registry service based on the open-source Docker Registry 2.0. Create and maintain Azure container registries to store and manage your container images and related artifacts. Use Azure container registries with your existing container development and deployment pipelines, or use Azure Container Registry Tasks to build container images in Azure. Build on demand, or fully automate builds with triggers such as source code commits and base image updates.","title":"Container Registry \ud83d\udcd6"},{"location":"azure/adf/","text":"ADF \u00b6 Tldr Azure Data Factory is Azure's cloud ETL service for scale-out serverless data integration and data transformation.It offers a code-free UI for intuitive authoring and single-pane-of-glass monitoring and management. Concepts \u00b6 Integration Runtime (IR) \u00b6 This is a configuration object stored in the ADF metastore that defines the location and type of compute that you\u2019ll use for parts of your pipeline that require computation. This can mean VMs for copying data, executing SSIS (SQL Server Integration Services) packages, or cluster size and type for Mapping Data Flows. Self-Hosted IR \u00b6 Another approach to executing ADF pipeline activities in a private network or to connect to on-premises data is by using the self-hosted integration runtime or SHIR. Pipeline \ud83d\udeb0 \u00b6 The primary unit of work in ADF is pipelines. Pipelines drive all of the actions that your data integration and ETL jobs perform. A pipeline is essentially a collection of activities that you connect together in a meaningful pattern to create a workflow . All actions in ADF are scheduled through a pipeline execution, including the Mapping Data Flows . You deploy and schedule the pipeline instead of the activities independently. Pipeline run \u00b6 A pipeline run in Azure Data Factory defines an instance of a pipeline execution. For example , say you have a pipeline that executes at 8:00 AM, 9:00 AM, and 10:00 AM. In this case, there are three separate runs of the pipeline or pipeline runs. Each pipeline run has a unique pipeline run ID. A run ID is a GUID that uniquely defines that particular pipeline run. On-demand execution \u00b6 The manual execution of a pipeline is also referred to as on-demand execution. Activities \u00b6 Pipelines are constructed from individual activities. Activities define the individual action that you wish to perform . There are many activities that you can use to compose a pipeline. Examples of activities include copying data (Copy activity), transforming data (Mapping Data Flows), \u201cFor Each,\u201d \u201cIf Then,\u201d and other control flow activities. How to call external activities? You can also call out to external compute activities like Databricks Notebook and Azure Functions to execute custom code. Copy activity \u00b6 Copy Activity in Data Factory copies data from a source data store to a sink data store. Data Flow \u23f3 \u00b6 Mapping Data Flows provide a way to transform data at scale without any coding required. You can design a data transformation job in the data flow designer by constructing a series of transformations. Start with any number of source transformations followed by data transformation steps. Then, complete your data flow with sink to land your results in a destination. Triggers \u00b6 Triggers allow you to set the conditions for your pipeline to execute. You can create schedule triggers, tumbling window, storage events, and custom events. Triggers can be shared across pipelines inside your factory, and there is a separate monitoring view organized by trigger Schedule Triggers \u00b6 They allow you to set the execute frequency and times for your pipeline. Tumbling windows \u00b6 Tumbling window allows for time intervals. ADF will establish windows of time for the recurrence that you choose starting on the date that you choose. Storage Events \u00b6 Storage events will allow you to trigger your pipeline when a file arrives or is deleted from a storage account. Custom Events \u00b6 You can create custom topics in Azure Event Grid and then subscribe to those events. When a specific event is received by your custom event trigger, your pipeline will be triggered automatically. Linked Service \u00b6 Tldr They are used to store credentials, location, and authentication mechanisms to connect to your data. Linked services are used by datasets and activities in ADF pipelines so that it can be determined where and how to connect to your data. You can share linked service definitions across objects in your factory. Linked services are much like connection strings, which define the connection information needed for the service to connect to external resources. Think of it this way; the dataset represents the structure of the data within the linked data stores, and the linked service defines the connection to the data source. For example , an Azure Storage linked service links a storage account to the service. An Azure Blob dataset represents the blob container and the folder within that Azure Storage account that contains the input blobs to be processed Datasets \ud83d\udcc0 \u00b6 Datasets define the shape of your data. In ADF, datasets do not contain or hold any data. Instead, they point to the data and provide ADF information about the schema for your data. In ADF, your data does not require schema. You can work with data in a schema-less manner. When you build ETL jobs using schema-less datasets, you will build data flows that are known as \u201clate binding\u201d and working with \u201cschema drift.\u201d Data Drift \u00b6 Similar to metadata schema drift , data drift occurs when values inside of existing columns begin to arrive outside of a set domain or boundaries. In ADF, you can establish Assert expectations that define data ranges. When those domains or ranges of metadata rules are breached in the data, you can fail the job or tag the rows as data quality errors and make downstream decisions on how to handle those errors.","title":"Data Factory \ud83c\udfed"},{"location":"azure/adf/#adf","text":"Tldr Azure Data Factory is Azure's cloud ETL service for scale-out serverless data integration and data transformation.It offers a code-free UI for intuitive authoring and single-pane-of-glass monitoring and management.","title":"ADF"},{"location":"azure/adf/#concepts","text":"","title":"Concepts"},{"location":"azure/adf/#integration-runtime-ir","text":"This is a configuration object stored in the ADF metastore that defines the location and type of compute that you\u2019ll use for parts of your pipeline that require computation. This can mean VMs for copying data, executing SSIS (SQL Server Integration Services) packages, or cluster size and type for Mapping Data Flows.","title":"Integration Runtime (IR)"},{"location":"azure/adf/#self-hosted-ir","text":"Another approach to executing ADF pipeline activities in a private network or to connect to on-premises data is by using the self-hosted integration runtime or SHIR.","title":"Self-Hosted IR"},{"location":"azure/adf/#pipeline","text":"The primary unit of work in ADF is pipelines. Pipelines drive all of the actions that your data integration and ETL jobs perform. A pipeline is essentially a collection of activities that you connect together in a meaningful pattern to create a workflow . All actions in ADF are scheduled through a pipeline execution, including the Mapping Data Flows . You deploy and schedule the pipeline instead of the activities independently.","title":"Pipeline \ud83d\udeb0"},{"location":"azure/adf/#pipeline-run","text":"A pipeline run in Azure Data Factory defines an instance of a pipeline execution. For example , say you have a pipeline that executes at 8:00 AM, 9:00 AM, and 10:00 AM. In this case, there are three separate runs of the pipeline or pipeline runs. Each pipeline run has a unique pipeline run ID. A run ID is a GUID that uniquely defines that particular pipeline run.","title":"Pipeline run"},{"location":"azure/adf/#on-demand-execution","text":"The manual execution of a pipeline is also referred to as on-demand execution.","title":"On-demand execution"},{"location":"azure/adf/#activities","text":"Pipelines are constructed from individual activities. Activities define the individual action that you wish to perform . There are many activities that you can use to compose a pipeline. Examples of activities include copying data (Copy activity), transforming data (Mapping Data Flows), \u201cFor Each,\u201d \u201cIf Then,\u201d and other control flow activities. How to call external activities? You can also call out to external compute activities like Databricks Notebook and Azure Functions to execute custom code.","title":"Activities"},{"location":"azure/adf/#copy-activity","text":"Copy Activity in Data Factory copies data from a source data store to a sink data store.","title":"Copy activity"},{"location":"azure/adf/#data-flow","text":"Mapping Data Flows provide a way to transform data at scale without any coding required. You can design a data transformation job in the data flow designer by constructing a series of transformations. Start with any number of source transformations followed by data transformation steps. Then, complete your data flow with sink to land your results in a destination.","title":"Data Flow \u23f3"},{"location":"azure/adf/#triggers","text":"Triggers allow you to set the conditions for your pipeline to execute. You can create schedule triggers, tumbling window, storage events, and custom events. Triggers can be shared across pipelines inside your factory, and there is a separate monitoring view organized by trigger","title":"Triggers"},{"location":"azure/adf/#schedule-triggers","text":"They allow you to set the execute frequency and times for your pipeline.","title":"Schedule Triggers"},{"location":"azure/adf/#tumbling-windows","text":"Tumbling window allows for time intervals. ADF will establish windows of time for the recurrence that you choose starting on the date that you choose.","title":"Tumbling windows"},{"location":"azure/adf/#storage-events","text":"Storage events will allow you to trigger your pipeline when a file arrives or is deleted from a storage account.","title":"Storage Events"},{"location":"azure/adf/#custom-events","text":"You can create custom topics in Azure Event Grid and then subscribe to those events. When a specific event is received by your custom event trigger, your pipeline will be triggered automatically.","title":"Custom Events"},{"location":"azure/adf/#linked-service","text":"Tldr They are used to store credentials, location, and authentication mechanisms to connect to your data. Linked services are used by datasets and activities in ADF pipelines so that it can be determined where and how to connect to your data. You can share linked service definitions across objects in your factory. Linked services are much like connection strings, which define the connection information needed for the service to connect to external resources. Think of it this way; the dataset represents the structure of the data within the linked data stores, and the linked service defines the connection to the data source. For example , an Azure Storage linked service links a storage account to the service. An Azure Blob dataset represents the blob container and the folder within that Azure Storage account that contains the input blobs to be processed","title":"Linked Service"},{"location":"azure/adf/#datasets","text":"Datasets define the shape of your data. In ADF, datasets do not contain or hold any data. Instead, they point to the data and provide ADF information about the schema for your data. In ADF, your data does not require schema. You can work with data in a schema-less manner. When you build ETL jobs using schema-less datasets, you will build data flows that are known as \u201clate binding\u201d and working with \u201cschema drift.\u201d","title":"Datasets \ud83d\udcc0"},{"location":"azure/adf/#data-drift","text":"Similar to metadata schema drift , data drift occurs when values inside of existing columns begin to arrive outside of a set domain or boundaries. In ADF, you can establish Assert expectations that define data ranges. When those domains or ranges of metadata rules are breached in the data, you can fail the job or tag the rows as data quality errors and make downstream decisions on how to handle those errors.","title":"Data Drift"},{"location":"azure/adsl/","text":"ADLS - Gen2 \u00b6 A fundamental part of Data Lake Storage Gen2 is the addition of a hierarchical namespace to Blob storage. The hierarchical namespace organizes objects/files into a hierarchy of directories for efficient data access. A common object store naming convention uses slashes in the name to mimic a hierarchical directory structure. This structure becomes real with Data Lake Storage Gen2. Key features \u00b6 Can be used with Hadoop, Spark, Databricks Allow us to use POSIX and ACL permissions Data Lake Storage Gen2 is very cost effective because it's built on top of the low-cost Azure Blob Storage. The extra features further lower the total cost of ownership for running big data analytics on Azure. Lifecycle policy \u00b6 Lifecycle management offers a rich, rule-based policy for general purpose v2 and blob storage accounts. Use the policy to transition your data to the appropriate access tiers or expire at the end of the data's lifecycle.","title":"ADLS-Gen2"},{"location":"azure/adsl/#adls-gen2","text":"A fundamental part of Data Lake Storage Gen2 is the addition of a hierarchical namespace to Blob storage. The hierarchical namespace organizes objects/files into a hierarchy of directories for efficient data access. A common object store naming convention uses slashes in the name to mimic a hierarchical directory structure. This structure becomes real with Data Lake Storage Gen2.","title":"ADLS - Gen2"},{"location":"azure/adsl/#key-features","text":"Can be used with Hadoop, Spark, Databricks Allow us to use POSIX and ACL permissions Data Lake Storage Gen2 is very cost effective because it's built on top of the low-cost Azure Blob Storage. The extra features further lower the total cost of ownership for running big data analytics on Azure.","title":"Key features"},{"location":"azure/adsl/#lifecycle-policy","text":"Lifecycle management offers a rich, rule-based policy for general purpose v2 and blob storage accounts. Use the policy to transition your data to the appropriate access tiers or expire at the end of the data's lifecycle.","title":"Lifecycle policy"},{"location":"azure/aks/","text":"AKS \u00b6 Azure Kubernetes Service (AKS) simplifies deploying a managed Kubernetes cluster in Azure by offloading the operational overhead to Azure. As a hosted Kubernetes service, Azure handles critical tasks, like health monitoring and maintenance. Who manages control plane? When you create an AKS cluster, a control plane is automatically created and configured by Microsoft and you pay for only the number of nodes you add to the cluster Core components \u00b6 Core Kubernetes infrastructure components: Control plane ; Azure-managed nodes are the masters of the cluster, and they\u2019re responsible for managing the cluster. When you create an AKS cluster, this node is automatically provisioned and configured with all the management-layer components. As this layer is managed by Azure, customers will not be able to access or make configuration changes to this one. Nodes : They are the VMs in which the containerized applications are running. Node pools : Nodes of the same configuration are grouped together into node pools . A Kubernetes cluster contains at least one node pool . The initial number of nodes and size are defined when you create an AKS cluster, which creates a default node pool . This default node pool in AKS contains the underlying VMs that run your agent nodes . Example You could create a pool with Windows VMs for running Windows containers and a pool with Linux VM for running Linux containers windows support To run an AKS cluster that supports node pools for Windows Server containers , your cluster needs to use a network policy that uses Azure CNI (advanced) network plugin. Also, Windows Containers need their own Node pool as default AKS configuration is for Linux containers . Pods : It represent the smallest execution unit in Kubernetes. A pod encapsulates one or more containers AKS Features \u00b6 IAM : You can authenticate, authorize, secure, and control access to Kubernetes clusters in 2 ways: Using Kubernetes role-based access control (Kubernetes RBAC), you can grant users, groups, and service accounts access to only the resources they need. Here you can use: Roles ClusterRoles RoleBindings ClusterRoleBindings Service Accounts Azure Active Directory and Azure RBAC . Registry support Autoscaling of Node pools Networking using CNI plugin AKS Namespaces \u00b6 When you create an AKS cluster, the following namespaces are available: Default : Where pods and deployments are created by default when none is provided. When you interact with the Kubernetes API, such as with kubectl get pods, the default namespace is used when none is specified. Kube-system : Where core resources exist, such as network features like DNS and proxy , or the Kubernetes dashboard . You typically don't deploy your own applications into this namespace Kube-public : Typically not used, but can be used for resources to be visible across the whole cluster, and can be viewed by any user . Network Plugins \u00b6 Kubenet \u00b6 NAT is used in Kubenet but not on CNI Kubenet is a very basic, simple network plugin, on Linux only . It does not, of itself, implement more advanced features like cross-node networking or network policy. It is typically used together with a cloud provider that sets up routing rules for communication between nodes, or in single-node environments. Nodes receive an IP address from the Azure virtual network subnet. Pods receive an IP address from a logically different address space than the nodes' Azure virtual network subnet. Network address translation (NAT) is then configured so that the pods can reach resources on the Azure virtual network. The source IP address of the traffic is translated to the node's primary IP address. CNI \u00b6 With Azure Container Networking Interface (CNI), every pod gets an IP address from the subnet and can be accessed directly. These IP addresses must be planned in advance and unique across your network space. Carefully plan for the IP's Each node has a configuration parameter for the maximum number of pods it supports. The equivalent number of IP addresses per node are then reserved up front. This approach can lead to IP address exhaustion or the need to rebuild clusters in a larger subnet as your application demands grow, so it's important to plan properly. Network Policy \u00b6 If you want to control traffic flow at the IP address or port level (OSI layer 3 or 4), then you might consider using Kubernetes NetworkPolicies for particular applications in your cluster. Dependency on Network Plugin Network policies are implemented by the network plugin . To use network policies, you must be using a networking solution which supports NetworkPolicy . Creating a NetworkPolicy resource without a controller that implements it will have no effect. Volumes \u00b6 In AKS, data volumes can be created using Azure Files or Azure Disks Disks \u00b6 Azure Disks can be referenced in the YAML file as a DataDisk resource. Both Azure Standard storage (HDD) and Premium storage (SSD) is supported. Since there is premium storage with high-performance SSD, this is ideal for production workloads that demand high IOPS. Warning Azure Disk is mounted as ReadWriteOnce , so it will be available to a single node. If you are planning to implement shared storage, then Azure Files is the right choice Files \u00b6 Azure Files uses SMB 3.0 and lets you mount the same storage to multiple nodes and pods. In Azure Files also you have support for Standard storage and Premium storage. These files can be accessed by multiple nodes and pods at the same time, making it ideal for shared storage scenarios. Auto-Scaling \u00b6 Cluster Autoscaling \u00b6 Cluster autoscaler is typically used alongside the horizontal pod autoscaler. When combined, the horizontal pod autoscaler increases or decreases the number of pods based on application demand, and the cluster autoscaler adjusts the number of nodes as needed to run those additional pods accordingly. HPA \u00b6 Kubernetes uses the horizontal pod autoscaler (HPA) to monitor the resource demand and automatically scale the number of replicas. By default, the horizontal pod autoscaler checks the Metrics API every 15 seconds for any required changes in replica count, but the Metrics API retrieves data from the Kubelet every 60 seconds. Effectively, the HPA is updated every 60 seconds. When changes are required, the number of replicas is increased or decreased accordingly Configuration required When you configure the HPA, you can decide on the minimum number of instances,the maximum number of instances, and the metrics that need to be monitored.","title":"AKS \u2388"},{"location":"azure/aks/#aks","text":"Azure Kubernetes Service (AKS) simplifies deploying a managed Kubernetes cluster in Azure by offloading the operational overhead to Azure. As a hosted Kubernetes service, Azure handles critical tasks, like health monitoring and maintenance. Who manages control plane? When you create an AKS cluster, a control plane is automatically created and configured by Microsoft and you pay for only the number of nodes you add to the cluster","title":"AKS"},{"location":"azure/aks/#core-components","text":"Core Kubernetes infrastructure components: Control plane ; Azure-managed nodes are the masters of the cluster, and they\u2019re responsible for managing the cluster. When you create an AKS cluster, this node is automatically provisioned and configured with all the management-layer components. As this layer is managed by Azure, customers will not be able to access or make configuration changes to this one. Nodes : They are the VMs in which the containerized applications are running. Node pools : Nodes of the same configuration are grouped together into node pools . A Kubernetes cluster contains at least one node pool . The initial number of nodes and size are defined when you create an AKS cluster, which creates a default node pool . This default node pool in AKS contains the underlying VMs that run your agent nodes . Example You could create a pool with Windows VMs for running Windows containers and a pool with Linux VM for running Linux containers windows support To run an AKS cluster that supports node pools for Windows Server containers , your cluster needs to use a network policy that uses Azure CNI (advanced) network plugin. Also, Windows Containers need their own Node pool as default AKS configuration is for Linux containers . Pods : It represent the smallest execution unit in Kubernetes. A pod encapsulates one or more containers","title":"Core components"},{"location":"azure/aks/#aks-features","text":"IAM : You can authenticate, authorize, secure, and control access to Kubernetes clusters in 2 ways: Using Kubernetes role-based access control (Kubernetes RBAC), you can grant users, groups, and service accounts access to only the resources they need. Here you can use: Roles ClusterRoles RoleBindings ClusterRoleBindings Service Accounts Azure Active Directory and Azure RBAC . Registry support Autoscaling of Node pools Networking using CNI plugin","title":"AKS Features"},{"location":"azure/aks/#aks-namespaces","text":"When you create an AKS cluster, the following namespaces are available: Default : Where pods and deployments are created by default when none is provided. When you interact with the Kubernetes API, such as with kubectl get pods, the default namespace is used when none is specified. Kube-system : Where core resources exist, such as network features like DNS and proxy , or the Kubernetes dashboard . You typically don't deploy your own applications into this namespace Kube-public : Typically not used, but can be used for resources to be visible across the whole cluster, and can be viewed by any user .","title":"AKS Namespaces"},{"location":"azure/aks/#network-plugins","text":"","title":"Network Plugins"},{"location":"azure/aks/#kubenet","text":"NAT is used in Kubenet but not on CNI Kubenet is a very basic, simple network plugin, on Linux only . It does not, of itself, implement more advanced features like cross-node networking or network policy. It is typically used together with a cloud provider that sets up routing rules for communication between nodes, or in single-node environments. Nodes receive an IP address from the Azure virtual network subnet. Pods receive an IP address from a logically different address space than the nodes' Azure virtual network subnet. Network address translation (NAT) is then configured so that the pods can reach resources on the Azure virtual network. The source IP address of the traffic is translated to the node's primary IP address.","title":"Kubenet"},{"location":"azure/aks/#cni","text":"With Azure Container Networking Interface (CNI), every pod gets an IP address from the subnet and can be accessed directly. These IP addresses must be planned in advance and unique across your network space. Carefully plan for the IP's Each node has a configuration parameter for the maximum number of pods it supports. The equivalent number of IP addresses per node are then reserved up front. This approach can lead to IP address exhaustion or the need to rebuild clusters in a larger subnet as your application demands grow, so it's important to plan properly.","title":"CNI"},{"location":"azure/aks/#network-policy","text":"If you want to control traffic flow at the IP address or port level (OSI layer 3 or 4), then you might consider using Kubernetes NetworkPolicies for particular applications in your cluster. Dependency on Network Plugin Network policies are implemented by the network plugin . To use network policies, you must be using a networking solution which supports NetworkPolicy . Creating a NetworkPolicy resource without a controller that implements it will have no effect.","title":"Network Policy"},{"location":"azure/aks/#volumes","text":"In AKS, data volumes can be created using Azure Files or Azure Disks","title":"Volumes"},{"location":"azure/aks/#disks","text":"Azure Disks can be referenced in the YAML file as a DataDisk resource. Both Azure Standard storage (HDD) and Premium storage (SSD) is supported. Since there is premium storage with high-performance SSD, this is ideal for production workloads that demand high IOPS. Warning Azure Disk is mounted as ReadWriteOnce , so it will be available to a single node. If you are planning to implement shared storage, then Azure Files is the right choice","title":"Disks"},{"location":"azure/aks/#files","text":"Azure Files uses SMB 3.0 and lets you mount the same storage to multiple nodes and pods. In Azure Files also you have support for Standard storage and Premium storage. These files can be accessed by multiple nodes and pods at the same time, making it ideal for shared storage scenarios.","title":"Files"},{"location":"azure/aks/#auto-scaling","text":"","title":"Auto-Scaling"},{"location":"azure/aks/#cluster-autoscaling","text":"Cluster autoscaler is typically used alongside the horizontal pod autoscaler. When combined, the horizontal pod autoscaler increases or decreases the number of pods based on application demand, and the cluster autoscaler adjusts the number of nodes as needed to run those additional pods accordingly.","title":"Cluster Autoscaling"},{"location":"azure/aks/#hpa","text":"Kubernetes uses the horizontal pod autoscaler (HPA) to monitor the resource demand and automatically scale the number of replicas. By default, the horizontal pod autoscaler checks the Metrics API every 15 seconds for any required changes in replica count, but the Metrics API retrieves data from the Kubelet every 60 seconds. Effectively, the HPA is updated every 60 seconds. When changes are required, the number of replicas is increased or decreased accordingly Configuration required When you configure the HPA, you can decide on the minimum number of instances,the maximum number of instances, and the metrics that need to be monitored.","title":"HPA"},{"location":"azure/api_gateway/","text":"APIM \u00b6 Tldr API gateway is the entry point for clients. Instead of calling services directly, clients call the API gateway, which forwards the call to the appropriate services on the back end. Features Cross cutting: The API Gateway can perform other cross-cutting functions such as authentication, logging, SSL termination, and load balancing. Caching Inbound and outbound policies API testing API observability Consolidate various API's to one endpoint APIM Components \u00b6 APIM includes API Gateway Managed : Default setting Self Hosted : Optional and containerized version for on-prem Management Portal/Plane Developer portal: The open-source developer portal. URL's for APIM endpoints? We can have 3 URLs - Gateway URL - Management URL - Developer Portal URL Kinds of APIM \u00b6 On cloud Serverless Self hosted and Federated API Gateway \u00b6 Facade : Accepts API calls and routes them to your backend APIs. AuthT and AuthZ : Verifies API keys, JWT tokens, certificates , and other credentials. API Throttling : Enforces usage quotas and rate limits by Denial of Service (DOS) Caching : Caches backend responses where set up. Logging : Logs for monitoring and reporting. Policy enforcement : Transforms your API on the fly using policy statements Management plane \u00b6 API providers interact with the service through the management plane, which provides full access to the API Management service capabilities. Customers interact with the management plane through Azure tools including the Azure portal, Azure PowerShell, Azure CLI, a Visual Studio Code extension, or client SDKs in several popular programming languages. Use of Management portal? Use the management plane to: Provision and configure API Management service settings Define or import API schemas from a wide range of sources, including OpenAPI specifications, - Azure compute services, or WebSocket or GraphQL backends Package APIs into products Set up policies like quotas or transformations on the APIs Get insights from analytics Manage users Developer Portal \u00b6 API providers can customize the look and feel of the developer portal by adding custom content, customizing styles, and adding their branding. Extend the developer portal further by self-hosting. Using the developer portal, developers can: Read API documentation Call an API via the interactive console Create an account and subscribe to get API keys Access analytics on their own usage Download API definitions Manage API keys Core concepts \u00b6 Policy \u00b6 With policies, an API publisher can change the behavior of an API through configuration. Policies are a collection of statements that are executed sequentially on the request or response of an API. Popular statements include format conversion from XML to JSON and call-rate limiting to restrict the number of incoming calls from a developer. The policy XML configuration is divided into inbound, backend, outbound, and on-error sections. This series of specified policy statements is executed in order for a request and a response. Policy definaiton <policies> <inbound> <!-- statements to be applied to the request go here --> </inbound> <backend> <!-- statements to be applied before the request is forwarded to the backend service go here --> </backend> <outbound> <!-- statements to be applied to the response go here --> </outbound> <on-error> <!-- statements to be applied if there is an error condition go here --> </on-error> </policies> The policies can be applied at various scopes, which determine the affected APIs or operations and dynamically configured using policy expressions Backend API \u00b6 A service, most commonly HTTP-based, that implements an API and its operations. Sometimes backend APIs are referred to simply as backends. For more information, see Backends. Frontend API \u00b6 API Management serves as mediation layer over the backend APIs. Frontend API is an API that is exposed to API consumers from API Management . You can customize the shape and behavior of a frontend API in API Management without making changes to the backend API(s) that it represents Products \u00b6 Products are how APIs are surfaced to developers. Products in API Management have one or more APIs, and can be open or protected. Protected products require a subscription key, while open products can be consumed freely. Version \u00b6 A version is a distinct variant of existing frontend API that differs in shape or behavior from the original. Versions give customers a choice of sticking with the original API or upgrading to a new version at the time of their choosing. Versions are a mechanism for releasing breaking changes without impacting API consumers. APIM with AKS \u00b6 APIM can be used with AKS in various use cases Should we connect to pods? In a Kubernetes cluster, containers are deployed in Pods, which are ephemeral and have a lifecycle. When a worker node dies, the Pods running on the node are lost. Therefore, the IP address of a Pod can change anytime. We cannot rely on it to communicate with the pod. To solve this problem, Kubernetes introduced the concept of Services . A Kubernetes Service is an abstraction layer which defines a logic group of Pods and enables external traffic exposure, load balancing and service discovery for those Pods. Direct \u00b6 Services in an AKS cluster can be exposed publicly using Service types of NodePort, LoadBalancer, or ExternalName. In this case, Services are accessible directly from public internet. After deploying API Management in front of the cluster, we need to ensure all inbound traffic goes through API Management by applying authentication in the microservices. Using Ingress \u00b6 If an API Management instance does not reside in the cluster VNet, Mutual TLS authentication (mTLS) is a robust way of ensuring the traffic is secure and trusted in both directions between an API Management instance and an AKS cluster. Mutual TLS authentication is natively supported by API Management and can be enabled in Kubernetes by installing an Ingress Controller . As a result, authentication will be performed in the Ingress Controller , which simplifies the microservices. Additionally, you can add the IP addresses of API Management to the allowed list by Ingress to make sure only API Management has access to the cluster. Revision and Version \u00b6 Version : They are for breaking changes Revision : They are for non breaking changes","title":"APIM Gateway \u2fa8"},{"location":"azure/api_gateway/#apim","text":"Tldr API gateway is the entry point for clients. Instead of calling services directly, clients call the API gateway, which forwards the call to the appropriate services on the back end. Features Cross cutting: The API Gateway can perform other cross-cutting functions such as authentication, logging, SSL termination, and load balancing. Caching Inbound and outbound policies API testing API observability Consolidate various API's to one endpoint","title":"APIM"},{"location":"azure/api_gateway/#apim-components","text":"APIM includes API Gateway Managed : Default setting Self Hosted : Optional and containerized version for on-prem Management Portal/Plane Developer portal: The open-source developer portal. URL's for APIM endpoints? We can have 3 URLs - Gateway URL - Management URL - Developer Portal URL","title":"APIM Components"},{"location":"azure/api_gateway/#kinds-of-apim","text":"On cloud Serverless Self hosted and Federated","title":"Kinds of APIM"},{"location":"azure/api_gateway/#api-gateway","text":"Facade : Accepts API calls and routes them to your backend APIs. AuthT and AuthZ : Verifies API keys, JWT tokens, certificates , and other credentials. API Throttling : Enforces usage quotas and rate limits by Denial of Service (DOS) Caching : Caches backend responses where set up. Logging : Logs for monitoring and reporting. Policy enforcement : Transforms your API on the fly using policy statements","title":"API Gateway"},{"location":"azure/api_gateway/#management-plane","text":"API providers interact with the service through the management plane, which provides full access to the API Management service capabilities. Customers interact with the management plane through Azure tools including the Azure portal, Azure PowerShell, Azure CLI, a Visual Studio Code extension, or client SDKs in several popular programming languages. Use of Management portal? Use the management plane to: Provision and configure API Management service settings Define or import API schemas from a wide range of sources, including OpenAPI specifications, - Azure compute services, or WebSocket or GraphQL backends Package APIs into products Set up policies like quotas or transformations on the APIs Get insights from analytics Manage users","title":"Management plane"},{"location":"azure/api_gateway/#developer-portal","text":"API providers can customize the look and feel of the developer portal by adding custom content, customizing styles, and adding their branding. Extend the developer portal further by self-hosting. Using the developer portal, developers can: Read API documentation Call an API via the interactive console Create an account and subscribe to get API keys Access analytics on their own usage Download API definitions Manage API keys","title":"Developer Portal"},{"location":"azure/api_gateway/#core-concepts","text":"","title":"Core concepts"},{"location":"azure/api_gateway/#policy","text":"With policies, an API publisher can change the behavior of an API through configuration. Policies are a collection of statements that are executed sequentially on the request or response of an API. Popular statements include format conversion from XML to JSON and call-rate limiting to restrict the number of incoming calls from a developer. The policy XML configuration is divided into inbound, backend, outbound, and on-error sections. This series of specified policy statements is executed in order for a request and a response. Policy definaiton <policies> <inbound> <!-- statements to be applied to the request go here --> </inbound> <backend> <!-- statements to be applied before the request is forwarded to the backend service go here --> </backend> <outbound> <!-- statements to be applied to the response go here --> </outbound> <on-error> <!-- statements to be applied if there is an error condition go here --> </on-error> </policies> The policies can be applied at various scopes, which determine the affected APIs or operations and dynamically configured using policy expressions","title":"Policy"},{"location":"azure/api_gateway/#backend-api","text":"A service, most commonly HTTP-based, that implements an API and its operations. Sometimes backend APIs are referred to simply as backends. For more information, see Backends.","title":"Backend API"},{"location":"azure/api_gateway/#frontend-api","text":"API Management serves as mediation layer over the backend APIs. Frontend API is an API that is exposed to API consumers from API Management . You can customize the shape and behavior of a frontend API in API Management without making changes to the backend API(s) that it represents","title":"Frontend API"},{"location":"azure/api_gateway/#products","text":"Products are how APIs are surfaced to developers. Products in API Management have one or more APIs, and can be open or protected. Protected products require a subscription key, while open products can be consumed freely.","title":"Products"},{"location":"azure/api_gateway/#version","text":"A version is a distinct variant of existing frontend API that differs in shape or behavior from the original. Versions give customers a choice of sticking with the original API or upgrading to a new version at the time of their choosing. Versions are a mechanism for releasing breaking changes without impacting API consumers.","title":"Version"},{"location":"azure/api_gateway/#apim-with-aks","text":"APIM can be used with AKS in various use cases Should we connect to pods? In a Kubernetes cluster, containers are deployed in Pods, which are ephemeral and have a lifecycle. When a worker node dies, the Pods running on the node are lost. Therefore, the IP address of a Pod can change anytime. We cannot rely on it to communicate with the pod. To solve this problem, Kubernetes introduced the concept of Services . A Kubernetes Service is an abstraction layer which defines a logic group of Pods and enables external traffic exposure, load balancing and service discovery for those Pods.","title":"APIM with AKS"},{"location":"azure/api_gateway/#direct","text":"Services in an AKS cluster can be exposed publicly using Service types of NodePort, LoadBalancer, or ExternalName. In this case, Services are accessible directly from public internet. After deploying API Management in front of the cluster, we need to ensure all inbound traffic goes through API Management by applying authentication in the microservices.","title":"Direct"},{"location":"azure/api_gateway/#using-ingress","text":"If an API Management instance does not reside in the cluster VNet, Mutual TLS authentication (mTLS) is a robust way of ensuring the traffic is secure and trusted in both directions between an API Management instance and an AKS cluster. Mutual TLS authentication is natively supported by API Management and can be enabled in Kubernetes by installing an Ingress Controller . As a result, authentication will be performed in the Ingress Controller , which simplifies the microservices. Additionally, you can add the IP addresses of API Management to the allowed list by Ingress to make sure only API Management has access to the cluster.","title":"Using Ingress"},{"location":"azure/api_gateway/#revision-and-version","text":"Version : They are for breaking changes Revision : They are for non breaking changes","title":"Revision and Version"},{"location":"azure/app-gw/","text":"App Gateway (L7 LB + WAF) \u00b6 Tldr Azure Load Balancer could load balance any TCP/UDP traffic to the backend servers; however, Azure Application Gateway is designed to distribute the incoming web requests to a web application (using HTTP). Unlike Azure Load Balancer, which operates at layer 4, or the Transport layer, Application Gateway uses layer 7, or the Application layer, routing to route the traffic to the backend web applications. OSI Layer AWS Azure L4 Network LB Azure LB L7 Application LB App Gateway Since Application Gateway is operating at layer 7, the IP addresses of the backend servers are not considered; rather, hostnames and paths are used for routing. Unlike typical load balancers that function at Layer 4 and route traffic based on the source IP address and port, Azure Application Gateway makes routing choices based on additional parameters of an HTTP request, such as URI path or host headers. It is a very helpful and valuable tool for web traffic managers, and it operates similarly to the AWS Application Gateway . Using sticky sessions If you would like to implement session stickiness , Application Gateway supports that as well. Using session stickiness, you can override the default round-robin fashion , and client requests in the same session will be routed to the same backend server. Features \u00b6 Supported protocols : It supports HTTP, HTTPS, HTTP/2, and WebSocket. WAF support : Web Application Firewall can be incorporated with Application Gateway to protect web applications. Encryption : It supports end-to-end request encryption. Autoscaling : You can dynamically scale Application Gateway to handle traffic spikes. Redirection : Traffic can be redirected to another site or from HTTP to HTTPS. Rewrite HTTP headers : It allows passing additional information with the request or response. Components \u00b6 Frontend IP HTTP/HTTPs Listner Routing Rules HTTP Settings Backend Pool Health Probes Frontend IP \u00b6 The IP address connected to an application gateway is known as the \u201cfrontend IP address.\u201d You can set up an application gateway to have a private IP address, a public IP address, or both. An application gateway supports one private or one public IP address. Listner \u00b6 There can be multiple listeners linked to an application gateway, and they can be utilized for the same protocol. When a listener detects incoming client requests, the application gateway directs them to members of the backend pool specified in the rule. They are used for port, protocol and certificate configurations Routing rules \u00b6 When a listener receives a request, the request routing rule either passes it to the backend or redirects it to another location. If the request is sent to the backend, the request routing rule specifies which backend server pool it should be routed to. The request routing rule also specifies whether the headers of the request are to be modified. One listener can be assigned to one rule. Backend Pool \u00b6 A backend pool directs requests to backend servers, who serve the requests. Backend pools can contain VMSS VM App Service Health Probes \u00b6 Application gateway checks the health of all resources in its backend pool and eliminates unhealthy ones automatically. When sick instances become available, it monitors them and adds them back to the healthy backend pool, as well as responding to health probes. Routing methods \u00b6 Path/URL based \u00b6 As shown in the below image, for path-based routing, the Application Gateway inspects the URL paths and then routes the traffic to the different backend pools. For example, you can direct the requests with the path /images/* to the backend pool containing the image documents. Similarly, all URLs containing the path /videos/* can be routed to the backend servers optimized for video streaming. Re-write HTTP headers + URL \u00b6 Application Gateway allows you to rewrite selected content of requests and responses. With this feature, you can translate URLs, query string parameters as well as modify request and response headers. It also allows you to add conditions to ensure that the URL or the specified headers are rewritten only when certain conditions are met. Multi-site hosting \u00b6 Multiple site hosting enables you to configure more than one web application on the same port of application gateways using public-facing listeners. It allows you to configure a more efficient topology for your deployments by adding up to 100+ websites to one application gateway. Each website can be directed to its own backend pool. For example, three domains, contoso.com, fabrikam.com, and adatum.com, point to the IP address of the application gateway. You'd create three multi-site listeners and configure each listener for the respective port and protocol setting.","title":"App Gateway (L7)"},{"location":"azure/app-gw/#app-gateway-l7-lb-waf","text":"Tldr Azure Load Balancer could load balance any TCP/UDP traffic to the backend servers; however, Azure Application Gateway is designed to distribute the incoming web requests to a web application (using HTTP). Unlike Azure Load Balancer, which operates at layer 4, or the Transport layer, Application Gateway uses layer 7, or the Application layer, routing to route the traffic to the backend web applications. OSI Layer AWS Azure L4 Network LB Azure LB L7 Application LB App Gateway Since Application Gateway is operating at layer 7, the IP addresses of the backend servers are not considered; rather, hostnames and paths are used for routing. Unlike typical load balancers that function at Layer 4 and route traffic based on the source IP address and port, Azure Application Gateway makes routing choices based on additional parameters of an HTTP request, such as URI path or host headers. It is a very helpful and valuable tool for web traffic managers, and it operates similarly to the AWS Application Gateway . Using sticky sessions If you would like to implement session stickiness , Application Gateway supports that as well. Using session stickiness, you can override the default round-robin fashion , and client requests in the same session will be routed to the same backend server.","title":"App Gateway (L7 LB + WAF)"},{"location":"azure/app-gw/#features","text":"Supported protocols : It supports HTTP, HTTPS, HTTP/2, and WebSocket. WAF support : Web Application Firewall can be incorporated with Application Gateway to protect web applications. Encryption : It supports end-to-end request encryption. Autoscaling : You can dynamically scale Application Gateway to handle traffic spikes. Redirection : Traffic can be redirected to another site or from HTTP to HTTPS. Rewrite HTTP headers : It allows passing additional information with the request or response.","title":"Features"},{"location":"azure/app-gw/#components","text":"Frontend IP HTTP/HTTPs Listner Routing Rules HTTP Settings Backend Pool Health Probes","title":"Components"},{"location":"azure/app-gw/#frontend-ip","text":"The IP address connected to an application gateway is known as the \u201cfrontend IP address.\u201d You can set up an application gateway to have a private IP address, a public IP address, or both. An application gateway supports one private or one public IP address.","title":"Frontend IP"},{"location":"azure/app-gw/#listner","text":"There can be multiple listeners linked to an application gateway, and they can be utilized for the same protocol. When a listener detects incoming client requests, the application gateway directs them to members of the backend pool specified in the rule. They are used for port, protocol and certificate configurations","title":"Listner"},{"location":"azure/app-gw/#routing-rules","text":"When a listener receives a request, the request routing rule either passes it to the backend or redirects it to another location. If the request is sent to the backend, the request routing rule specifies which backend server pool it should be routed to. The request routing rule also specifies whether the headers of the request are to be modified. One listener can be assigned to one rule.","title":"Routing rules"},{"location":"azure/app-gw/#backend-pool","text":"A backend pool directs requests to backend servers, who serve the requests. Backend pools can contain VMSS VM App Service","title":"Backend Pool"},{"location":"azure/app-gw/#health-probes","text":"Application gateway checks the health of all resources in its backend pool and eliminates unhealthy ones automatically. When sick instances become available, it monitors them and adds them back to the healthy backend pool, as well as responding to health probes.","title":"Health Probes"},{"location":"azure/app-gw/#routing-methods","text":"","title":"Routing methods"},{"location":"azure/app-gw/#pathurl-based","text":"As shown in the below image, for path-based routing, the Application Gateway inspects the URL paths and then routes the traffic to the different backend pools. For example, you can direct the requests with the path /images/* to the backend pool containing the image documents. Similarly, all URLs containing the path /videos/* can be routed to the backend servers optimized for video streaming.","title":"Path/URL based"},{"location":"azure/app-gw/#re-write-http-headers-url","text":"Application Gateway allows you to rewrite selected content of requests and responses. With this feature, you can translate URLs, query string parameters as well as modify request and response headers. It also allows you to add conditions to ensure that the URL or the specified headers are rewritten only when certain conditions are met.","title":"Re-write HTTP headers + URL"},{"location":"azure/app-gw/#multi-site-hosting","text":"Multiple site hosting enables you to configure more than one web application on the same port of application gateways using public-facing listeners. It allows you to configure a more efficient topology for your deployments by adding up to 100+ websites to one application gateway. Each website can be directed to its own backend pool. For example, three domains, contoso.com, fabrikam.com, and adatum.com, point to the IP address of the application gateway. You'd create three multi-site listeners and configure each listener for the respective port and protocol setting.","title":"Multi-site hosting"},{"location":"azure/appservice/","text":"Azure App Service \u00b6 Azure App Service enables you to build and host web apps, mobile back-ends, and RESTful APIs in the programming language of your choice without managing infrastructure. It offers auto-scaling and high availability, supports both Windows and Linux, and enables automated deployments from GitHub, Azure DevOps, or any Git repo. Azure App Service is Microsoft\u2019s PaaS offering. Azure App Service comes in four flavors. Azure Web Apps/WebJobs Azure App Service Web App for Containers Azure Mobile Apps Azure API Apps App Service Plan \u00b6 An App Service plan defines a set of compute resources for a web app to run. An app service always runs in an App Service plan . In addition, Azure Functions also has the option of running in an App Service plan. Contents of Azure app service Operating System (Windows, Linux) Region (West US, East US, etc.) Number of VM instances Size of VM instances (Small, Medium, Large) Pricing tier App Service plans are created at a regional scope. When you create an App Service plan in a certain region (for example, West Europe), a set of compute resources is created for that plan in that region. Whatever apps you put into this App Service plan run on these compute resources as defined by your App Service plan. Moving webapp in App service You can move an app to another App Service plan, as long as the source plan and the target plan are in the same resource group and geographical region. The region in which your app runs is the region of the App Service plan it's in. However, you cannot change an App Service plan's region. Pricing tier \u00b6 The pricing tier of an App Service plan determines what App Service features you get and how much you pay for the plan. Shared \ud83d\udcb0 : It allocate CPU quotas to each app that runs on the shared VM , and the resources cannot scale out. Dedicated \ud83d\udcb0\ud83d\udcb0: Only apps in the same App Service plan share the same compute resources on dedicated VM . The higher the tier, the more VM instances are available to you for scale-out. Isolated \ud83d\udcb0\ud83d\udcb0\ud83d\udcb0\ud83d\udcb0: Your apps run dedicated Azure VMs on dedicated Azure VNets . It provides network isolation on top of compute isolation to your apps. It provides the maximum scale-out capabilities. WebApps \u00b6 Containerization : You can publish using code or docker container Multiple runtimes :We can choose runtime stack as .NET, Java, Node.js, PHP, and Python on Windows/Linux. Backup App service \u00b6 There are two types of backups in App Service. Automatic backups made for your app regularly as long as it's in a supported pricing tier. Custom backups require initial configuration, and can be made on-demand or on a schedule. How backups are stored? After you have made one or more backups for your app, the backups are visible on the Containers page of your storage account , and your app. In the storage account, each backup consists of a .zip file that contains the backup data and an .xml file that contains a manifest of the .zip file contents. You can unzip and browse these files if you want to access your backups without actually performing an app restore. Remember difference in different backups You need a Recovery Service Vault if you want to backup VMs, File Shares, SAP HANA in a VM or SQL Server in a VM. You need a Backup Vault if you want to backup Azure Disks, Azure Blobs or Azure Database for PostgreSQL Server. To backup App Service ,you need a storage account. Choose your backup destination by selecting a Storage Account and Container. The storage account must belong to the same subscription as the app you want to back up. If you wish, you can create a new storage account or a new container in the respective pages. Partial backup \u00b6 Partial backups are supported for custom backups (not for automatic backups ). Sometimes you don't want to back up everything on your app. To exclude folders and files from being stored in your future backups, create a _backup.filter file in the %HOME%\\site\\wwwroot folder of your app. Specify the list of files and folders you want to exclude in this file.","title":"App Service"},{"location":"azure/appservice/#azure-app-service","text":"Azure App Service enables you to build and host web apps, mobile back-ends, and RESTful APIs in the programming language of your choice without managing infrastructure. It offers auto-scaling and high availability, supports both Windows and Linux, and enables automated deployments from GitHub, Azure DevOps, or any Git repo. Azure App Service is Microsoft\u2019s PaaS offering. Azure App Service comes in four flavors. Azure Web Apps/WebJobs Azure App Service Web App for Containers Azure Mobile Apps Azure API Apps","title":"Azure App Service"},{"location":"azure/appservice/#app-service-plan","text":"An App Service plan defines a set of compute resources for a web app to run. An app service always runs in an App Service plan . In addition, Azure Functions also has the option of running in an App Service plan. Contents of Azure app service Operating System (Windows, Linux) Region (West US, East US, etc.) Number of VM instances Size of VM instances (Small, Medium, Large) Pricing tier App Service plans are created at a regional scope. When you create an App Service plan in a certain region (for example, West Europe), a set of compute resources is created for that plan in that region. Whatever apps you put into this App Service plan run on these compute resources as defined by your App Service plan. Moving webapp in App service You can move an app to another App Service plan, as long as the source plan and the target plan are in the same resource group and geographical region. The region in which your app runs is the region of the App Service plan it's in. However, you cannot change an App Service plan's region.","title":"App Service Plan"},{"location":"azure/appservice/#pricing-tier","text":"The pricing tier of an App Service plan determines what App Service features you get and how much you pay for the plan. Shared \ud83d\udcb0 : It allocate CPU quotas to each app that runs on the shared VM , and the resources cannot scale out. Dedicated \ud83d\udcb0\ud83d\udcb0: Only apps in the same App Service plan share the same compute resources on dedicated VM . The higher the tier, the more VM instances are available to you for scale-out. Isolated \ud83d\udcb0\ud83d\udcb0\ud83d\udcb0\ud83d\udcb0: Your apps run dedicated Azure VMs on dedicated Azure VNets . It provides network isolation on top of compute isolation to your apps. It provides the maximum scale-out capabilities.","title":"Pricing tier"},{"location":"azure/appservice/#webapps","text":"Containerization : You can publish using code or docker container Multiple runtimes :We can choose runtime stack as .NET, Java, Node.js, PHP, and Python on Windows/Linux.","title":"WebApps"},{"location":"azure/appservice/#backup-app-service","text":"There are two types of backups in App Service. Automatic backups made for your app regularly as long as it's in a supported pricing tier. Custom backups require initial configuration, and can be made on-demand or on a schedule. How backups are stored? After you have made one or more backups for your app, the backups are visible on the Containers page of your storage account , and your app. In the storage account, each backup consists of a .zip file that contains the backup data and an .xml file that contains a manifest of the .zip file contents. You can unzip and browse these files if you want to access your backups without actually performing an app restore. Remember difference in different backups You need a Recovery Service Vault if you want to backup VMs, File Shares, SAP HANA in a VM or SQL Server in a VM. You need a Backup Vault if you want to backup Azure Disks, Azure Blobs or Azure Database for PostgreSQL Server. To backup App Service ,you need a storage account. Choose your backup destination by selecting a Storage Account and Container. The storage account must belong to the same subscription as the app you want to back up. If you wish, you can create a new storage account or a new container in the respective pages.","title":"Backup App service"},{"location":"azure/appservice/#partial-backup","text":"Partial backups are supported for custom backups (not for automatic backups ). Sometimes you don't want to back up everything on your app. To exclude folders and files from being stored in your future backups, create a _backup.filter file in the %HOME%\\site\\wwwroot folder of your app. Specify the list of files and folders you want to exclude in this file.","title":"Partial backup"},{"location":"azure/arm/","text":"Azure Resource Manager \ud83e\uddbe \u00b6 For if you are creating a virtual machine , you will send our inputs to the Azure Resource Manager , and ARM will forward the request to Microsoft.Compute Compute resource provider after validation. The resource provider will deploy the resource and update you with the status via ARM. Similarly, when you create a networking resource like a virtual network, the resource provisioning is done by the Microsoft.Network resource provider. Note Resource groups is a feature of ARM ARM is not only responsible for the deployment of the resources, but also responsible for the management of resources. Template File \u00b6 In its simplest structure, a template has the following elements: 1 2 3 4 5 6 7 8 9 10 { \"$schema\" : \"https://schema.management.azure.com/schemas/2019-04-01/deploymentTemplate.json#\" , \"contentVersion\" : \"\" , \"apiProfile\" : \"\" , \"parameters\" : { }, \"variables\" : { }, \"functions\" : [ ], \"resources\" : [ ], \"outputs\" : { } } Required values are $Schema contentVersion resources The template has the following sections: Template sections $schema : Location of the JSON schema file that describes the version of the template language. The version number you use depends on the scope of the deployment and your JSON editor. contentVersion : Version of the template (such as 1.0.0.0). You can provide any value for this element. Use this value to document significant changes in your template. When deploying resources using the template, this value can be used to make sure that the right template is being used. apiProfile ; An API version that serves as a collection of API versions for resource types. Use this value to avoid having to specify API versions for each resource in the template. When you specify an API profile version and don't specify an API version for the resource type, Resource Manager uses the API version for that resource type that is defined in the profile. Parameters - Provide values during deployment/runtime that allow the same template to be used with different environments . Variables - Define values that are reused in your templates. They can be constructed from parameter values. User-defined functions - Create customized functions that simplify your template. Resources - Specify the resources to deploy. Outputs - Return values from the deployed resources. Deployment mode \u00b6 Complete mode : In this mode, RM deletes resources that exist in the resource group but aren't specified in the template. Always use the what-if operation before deploying a template in complete mode . What-if shows you which resources will be created, deleted, or modified. Use what-if to avoid unintentionally deleting resources. Incremental mode : In this mode the RM leaves unchanged resources that exist in the resource group but aren't specified in the template. Resources in the template are added to the resource group. Example To illustrate the difference between incremental and complete modes, consider the following scenario. Resource Group contains: - Resource A - Resource B - Resource C Template contains: - Resource A - Resource B - Resource D When deployed in incremental mode, the resource group has: - Resource A - Resource B - Resource C - Resource D When deployed in complete mode, Resource C is deleted. The resource group has: - Resource A - Resource B - Resource D Deployment Scope \u00b6 Scope of deployment can be Management Group Subscription Resource Group Tenant Extension resource: An extension resource is a resource that modifies another resource. For example, you can assign a role to a resource. The role assignment is an extension resource type. You can target your deployment to a resource group, subscription, management group, or tenant. Depending on the scope of the deployment, you use different commands. There is no resource level scope create various resources using powershell # To create an Azure resource, such as a website, Azure SQL Database server, or Azure SQL Database, in a resource group use: New - AzResource # To add a deployment to an existing resource group, use: New - AzResourceGroupDeployment # The New-AzDeployment cmdlet adds a deployment at the current subscription scope. This includes the resources that the deployment requires. New - AzDeployment What-ifs \u00b6 ARM provides the what-if operation to let you see how resources will change if you deploy the template. The what-if operation doesn't make any changes to existing resources. Instead, it predicts the changes if the specified template is deployed. Resource Dependency \u00b6 When deploying resources, you may need to make sure some resources exist before other resources. For example, you need a logical SQL server before deploying a database . You establish this relationship by marking one resource as dependent on the other resource. Use the dependsOn element to define an explicit dependency. Use the reference or list functions to define an implicit dependency. NIC dependency a VNET, NSG, and public IP { \"type\" : \"Microsoft.Network/networkInterfaces\" , \"apiVersion\" : \"2020-06-01\" , \"name\" : \"[variables('networkInterfaceName')]\" , \"location\" : \"[parameters('location')]\" , \"dependsOn\" : [ \"[resourceId('Microsoft.Network/networkSecurityGroups/', parameters('networkSecurityGroupName'))]\" , \"[resourceId('Microsoft.Network/virtualNetworks/', parameters('virtualNetworkName'))]\" , \"[resourceId('Microsoft.Network/publicIpAddresses/', variables('publicIpAddressName'))]\" ], ... }","title":"ARM \ud83e\uddbe"},{"location":"azure/arm/#azure-resource-manager","text":"For if you are creating a virtual machine , you will send our inputs to the Azure Resource Manager , and ARM will forward the request to Microsoft.Compute Compute resource provider after validation. The resource provider will deploy the resource and update you with the status via ARM. Similarly, when you create a networking resource like a virtual network, the resource provisioning is done by the Microsoft.Network resource provider. Note Resource groups is a feature of ARM ARM is not only responsible for the deployment of the resources, but also responsible for the management of resources.","title":"Azure Resource Manager \ud83e\uddbe"},{"location":"azure/arm/#template-file","text":"In its simplest structure, a template has the following elements: 1 2 3 4 5 6 7 8 9 10 { \"$schema\" : \"https://schema.management.azure.com/schemas/2019-04-01/deploymentTemplate.json#\" , \"contentVersion\" : \"\" , \"apiProfile\" : \"\" , \"parameters\" : { }, \"variables\" : { }, \"functions\" : [ ], \"resources\" : [ ], \"outputs\" : { } } Required values are $Schema contentVersion resources The template has the following sections: Template sections $schema : Location of the JSON schema file that describes the version of the template language. The version number you use depends on the scope of the deployment and your JSON editor. contentVersion : Version of the template (such as 1.0.0.0). You can provide any value for this element. Use this value to document significant changes in your template. When deploying resources using the template, this value can be used to make sure that the right template is being used. apiProfile ; An API version that serves as a collection of API versions for resource types. Use this value to avoid having to specify API versions for each resource in the template. When you specify an API profile version and don't specify an API version for the resource type, Resource Manager uses the API version for that resource type that is defined in the profile. Parameters - Provide values during deployment/runtime that allow the same template to be used with different environments . Variables - Define values that are reused in your templates. They can be constructed from parameter values. User-defined functions - Create customized functions that simplify your template. Resources - Specify the resources to deploy. Outputs - Return values from the deployed resources.","title":"Template File"},{"location":"azure/arm/#deployment-mode","text":"Complete mode : In this mode, RM deletes resources that exist in the resource group but aren't specified in the template. Always use the what-if operation before deploying a template in complete mode . What-if shows you which resources will be created, deleted, or modified. Use what-if to avoid unintentionally deleting resources. Incremental mode : In this mode the RM leaves unchanged resources that exist in the resource group but aren't specified in the template. Resources in the template are added to the resource group. Example To illustrate the difference between incremental and complete modes, consider the following scenario. Resource Group contains: - Resource A - Resource B - Resource C Template contains: - Resource A - Resource B - Resource D When deployed in incremental mode, the resource group has: - Resource A - Resource B - Resource C - Resource D When deployed in complete mode, Resource C is deleted. The resource group has: - Resource A - Resource B - Resource D","title":"Deployment mode"},{"location":"azure/arm/#deployment-scope","text":"Scope of deployment can be Management Group Subscription Resource Group Tenant Extension resource: An extension resource is a resource that modifies another resource. For example, you can assign a role to a resource. The role assignment is an extension resource type. You can target your deployment to a resource group, subscription, management group, or tenant. Depending on the scope of the deployment, you use different commands. There is no resource level scope create various resources using powershell # To create an Azure resource, such as a website, Azure SQL Database server, or Azure SQL Database, in a resource group use: New - AzResource # To add a deployment to an existing resource group, use: New - AzResourceGroupDeployment # The New-AzDeployment cmdlet adds a deployment at the current subscription scope. This includes the resources that the deployment requires. New - AzDeployment","title":"Deployment Scope"},{"location":"azure/arm/#what-ifs","text":"ARM provides the what-if operation to let you see how resources will change if you deploy the template. The what-if operation doesn't make any changes to existing resources. Instead, it predicts the changes if the specified template is deployed.","title":"What-ifs"},{"location":"azure/arm/#resource-dependency","text":"When deploying resources, you may need to make sure some resources exist before other resources. For example, you need a logical SQL server before deploying a database . You establish this relationship by marking one resource as dependent on the other resource. Use the dependsOn element to define an explicit dependency. Use the reference or list functions to define an implicit dependency. NIC dependency a VNET, NSG, and public IP { \"type\" : \"Microsoft.Network/networkInterfaces\" , \"apiVersion\" : \"2020-06-01\" , \"name\" : \"[variables('networkInterfaceName')]\" , \"location\" : \"[parameters('location')]\" , \"dependsOn\" : [ \"[resourceId('Microsoft.Network/networkSecurityGroups/', parameters('networkSecurityGroupName'))]\" , \"[resourceId('Microsoft.Network/virtualNetworks/', parameters('virtualNetworkName'))]\" , \"[resourceId('Microsoft.Network/publicIpAddresses/', variables('publicIpAddressName'))]\" ], ... }","title":"Resource Dependency"},{"location":"azure/atm/","text":"Azure Traffic Manager \ud83d\udea6 \u00b6 Azure Traffic Manager is a DNS resolver , which helps in distributing the traffic to different endpoints globally based on the routing method you configure. In other solutions, the request hits the front end and gets routed to the backend based on the rules you configure. However, in the case of Azure Traffic Manager, the requests hit the Azure Traffic Manager, and Azure Traffic Manager returns the endpoint address to the client as a DNS response. As the client is now aware of the endpoint, the client directly reaches out to the endpoint. Tldr The traffic doesn\u2019t transit through the Azure Traffic Manager ; instead, it returns the endpoint address to the client. Resolution process \u00b6 The request from the client reaches the recursive DNS servers, and the request gets forwarded to the Azure Traffic Manager . For example, if you have a website www.amarjitdhillon.com , you will map the www record as a CNAME to the DNS name of the Azure Traffic Manager . A recursive DNS server will forward the request to the Azure Traffic Manager , and based on the routing method you have configured, Traffic Manager determines the best endpoint. Once the endpoint is determined, Traffic Manager will return the IP address of the backend server as a DNS response to the DNS resolver DNS resolver to return endpoint to the client. Since the client knows the IP address to the endpoint, it will directly communicate with the endpoint returned by the Traffic Manager. Routing methods \u00b6 Various routing methods in Traffic Manager are: Priority \u00b6 You can set the priority for each endpoint, and the requests will be served by the endpoint that has the highest priority. If the endpoint with high priority is not healthy, then the endpoint with the second-highest priority serves the requests. Weighted \u00b6 Select Weighted routing when you want to distribute traffic across a set of endpoints based on their weight. Set the weight the same to distribute evenly across all endpoints. Performance \u00b6 Requests are routed to endpoints with lowest network latency . Geographic \u00b6 Requests are routed to endpoints based on the location of the client. This is ideal for serving websites in local language for customers accessing from a country. Multi-value \u00b6 This is used when we can have only IPv4/IPv6 addresses as endpoints. When a query is received for this profile, all healthy endpoints are returned. Subnet \u00b6 This is used to map the end-user IP address ranges to a specific endpoint.","title":"Traffic Manager \ud83d\udea6"},{"location":"azure/atm/#azure-traffic-manager","text":"Azure Traffic Manager is a DNS resolver , which helps in distributing the traffic to different endpoints globally based on the routing method you configure. In other solutions, the request hits the front end and gets routed to the backend based on the rules you configure. However, in the case of Azure Traffic Manager, the requests hit the Azure Traffic Manager, and Azure Traffic Manager returns the endpoint address to the client as a DNS response. As the client is now aware of the endpoint, the client directly reaches out to the endpoint. Tldr The traffic doesn\u2019t transit through the Azure Traffic Manager ; instead, it returns the endpoint address to the client.","title":"Azure Traffic Manager \ud83d\udea6"},{"location":"azure/atm/#resolution-process","text":"The request from the client reaches the recursive DNS servers, and the request gets forwarded to the Azure Traffic Manager . For example, if you have a website www.amarjitdhillon.com , you will map the www record as a CNAME to the DNS name of the Azure Traffic Manager . A recursive DNS server will forward the request to the Azure Traffic Manager , and based on the routing method you have configured, Traffic Manager determines the best endpoint. Once the endpoint is determined, Traffic Manager will return the IP address of the backend server as a DNS response to the DNS resolver DNS resolver to return endpoint to the client. Since the client knows the IP address to the endpoint, it will directly communicate with the endpoint returned by the Traffic Manager.","title":"Resolution process"},{"location":"azure/atm/#routing-methods","text":"Various routing methods in Traffic Manager are:","title":"Routing methods"},{"location":"azure/atm/#priority","text":"You can set the priority for each endpoint, and the requests will be served by the endpoint that has the highest priority. If the endpoint with high priority is not healthy, then the endpoint with the second-highest priority serves the requests.","title":"Priority"},{"location":"azure/atm/#weighted","text":"Select Weighted routing when you want to distribute traffic across a set of endpoints based on their weight. Set the weight the same to distribute evenly across all endpoints.","title":"Weighted"},{"location":"azure/atm/#performance","text":"Requests are routed to endpoints with lowest network latency .","title":"Performance"},{"location":"azure/atm/#geographic","text":"Requests are routed to endpoints based on the location of the client. This is ideal for serving websites in local language for customers accessing from a country.","title":"Geographic"},{"location":"azure/atm/#multi-value","text":"This is used when we can have only IPv4/IPv6 addresses as endpoints. When a query is received for this profile, all healthy endpoints are returned.","title":"Multi-value"},{"location":"azure/atm/#subnet","text":"This is used to map the end-user IP address ranges to a specific endpoint.","title":"Subnet"},{"location":"azure/az-monitor/","text":"Azure Monitor \u00b6 It delivers a comprehensive solution for collecting, analyzing, and acting on telemetry from your cloud and on-premises environments. This information helps you understand how your applications are performing and proactively identify issues that affect them and the resources they depend on. States: - Alert State : set by user such as admin - Monitor state : Set by system Log Analytics Workspace \u00b6 A Log Analytics workspace is a unique environment for log data from Azure Monitor and other Azure services, such as Microsoft Sentinel and Microsoft Defender for Cloud. The connected sources , configuration , and the repository are managed per workspace. Logs \u00b6 Its an event-based data. Example : Syslog in Linux is an example of a log, as the log data is not consistent and the format may vary from source to source. Free form or structured Stored in logs analytics workspace Analysis via Kusto Query Language (KQL) : The first step in writing queries is to understand which table contains the information you need. Some examples include Event, Syslog, Heartbeat, and Alert. View error events from a table named Event using 3 methods search in ( Event ) \"error\" Event | search \"error\" Event | where EventType == \"error\" query perf table Perf | summarize AggregatedValue = count () by CounterPath Count of agent nodes that are sending a heartbeat each day in the last month Heartbeat | where TimeGenerated > startofday ( ago ( 31 d )) | summarize nodes = dcount ( Computer ) by bin ( TimeGenerated , 1 d ) | render timechart Log Analytics \u00b6 A service for aggregating the log data in a single pane so that it can be analyzed, visualized and queried via KQL Agents \u00b6 Azure Monitor Agent (AMA) collects monitoring data from the guest operating system of Azure and hybrid virtual machines and delivers it to Azure Monitor for use Windows agents Linux agents Cost control using agents With the help of the agents configuration , you will be able to declare what logs you want to collect using the agents and what level of logging information you need. In this way, you will have granular control over what is getting ingested into your workspace. Various logs used are: Windows Event Logs : This helps you to select which event log items you want to ingest to the workspace Windows Performance Counters : You can select performance counters of Windows servers and the sample rate. Linux Performance Counters : These are performance counters for Linux servers and their sample rate. Syslog : Control which facilities in Syslog you want to ingest. IIS Logs : This enables collection of W3C format log files from IIS server. DCR \u00b6 Azure Monitor Agent uses data collection rules (DCR) , where you define which data you want each agent to collect. Data collection rules let you manage data collection settings at scale and define unique, scoped configurations for subsets of machines. You can define a rule to send data from multiple machines to multiple destinations across regions and tenants. Data retention \u00b6 Data in each table in a Log Analytics workspace is retained for a specified period of time after which it's either removed or archived with a reduced retention fee. Activity log \u00b6 With the help of an activity log, you can get insights into different operations occurring at the subscription level . Categories \u00b6 As Azure Activity Log is a subscription-wide logging system, Azure has divided the logs ingested into different categories. Administrative : Contains the record of all create, update, delete, and action operations performed through Resource Manager. Examples of Administrative events include create virtual machine and delete network security group. Service Health : Contains the record of any service health incidents that have occurred in Azure. An example of a Service Health event SQL Azure in East US is experiencing downtime. Resource Health : Contains the record of any resource health events that have occurred to your Azure resources. An example of a Resource Health event is Virtual Machine health status changed to unavailable. Alert : Contains the record of activations for Azure alerts. An example of an Alert event is CPU % on myVM has been over 80 for the past 5 minutes. Autoscale : Contains the record of any events related to the operation of the autoscale engine based on any autoscale settings you have defined in your subscription. An example of an Autoscale event is Autoscale scale up action failed. Recommendation : From advisor Security : This includes any security alerts generated by Azure Defender for Servers. Policy : Whenever Azure Policy is evaluated, the effect action will be logged in this category. Metrics \u00b6 Metrics are numerical values that are ingested from Azure resources used to represent the state of the system at a particular point in time. Metric Example For a virtual machine, the metrics available will be CPU Percentage , Network In , Network Out , Memory , etc. On the other hand, if you take a storage account , the available metrics will be Number Of Requests , Number Of Failed Requests , Number Of API Calls , etc. Short time based data. Frequently updated Near real-time data Visualizations via Metrics Explorer Distributed Tracing \u00b6 In monolithic architectures , we've gotten used to debugging with call stacks. Call stacks are brilliant tools for showing the flow of execution ( Method A called Method B , which called Method C ), along with details and parameters about each of those calls. This technique is great for monoliths or services running on a single process. But how do we debug when the call is across a process boundary, not simply a reference on the local stack? That's where distributed tracing comes in. !!! info '' Distributed tracing is the equivalent of call stacks for modern cloud and microservices architectures , with the addition of a simplistic performance profiler thrown in. Diagnostic Settings \u00b6 They are used to define where the logs and metrics will be stored. Action groups \u00b6 What are action groups? An action group is a collection of notification preferences that can be reused in multiple alerts. The notifications and actions that you define inside the action group will be executed when the alert is fired. You can create multiple action groups with different notification preferences, and these can be used across your alerts. They can be found in Monitor \u2192 Alerts \u2192 Action Groups Action groups consist of two parts: notifications and actions Notification types \u00b6 Email/SMS : These will work even if Azure is down while other services needs the Azure to be running. Email Azure Resource Manager ; Role You can send email notifications to Azure RBAC roles like Owner, Contributor, Reader, Monitoring Contributor, and Monitoring Reader that are assigned at the subscription scope. All user principals assigned with any of the aforementioned roles will be notified when the alert is triggered. Azure AD group and service principals are excluded from the email notification. Action types \u00b6 Azure app push notification : Azure Function : Using serverless compute, you can run small chunks of code when the alert is fired. Logic App : run a business process Secure Webhook/WebHook : This is the HTTPS or HTTP endpoint for an external application to communicate. ITSM : You can integrate your ITSM tools like ServiceNow so that whenever an alert is triggered, the corresponding ticket will be created in the ITSM tool. Automation runbook : Event Hub : Ingest event to other systems. Insights \u00b6 This is service specific monitoring feature in Azure VM Insights \u00b6 Used to monitor VM and VMSS. This is also called as Azure monitor for VMs Require Log Analytics Agent to be installed Network Insights \u00b6 No agent installation required for this. Container Insights \u00b6 Used to monitor containers App Insights (for Devs) \u00b6 Application Insights is an extension of Azure Monitor and provides Application Performance Monitoring (also known as \u201cAPM\u201d) features. APM tools are useful to monitor applications from development, through test, and into production . What to use for logging? In addition to collecting Metrics and application Telemetry data , which describe application activities and health, Application Insights can also be used to collect and store application trace logging data . Features \u00b6 Metrics and alerts Application Map Profiler Usage Analytics Instrumentation \u00b6 At a basic level, instrumenting is simply enabling an application to capture telemetry. There are two methods to instrument your application: Manual instrumentation Automatic instrumentation (auto-instrumentation) It can be: Runtime instrumentation Build-time instrumentation Instrumentation key Key of implementing Instrumentation in application and is stored in app insights resource. Auto-instrumentation \u00b6 It's an agent for App insights. Auto-instrumentation enables telemetry collection through configuration without touching the application's code. Although it's more convenient, it tends to be less configurable. It's also not available in all languages The Application Insights agent or SDK pre-processes telemetry and metrics before sending the data to Azure where it's ingested and processed further before being stored in Azure Monitor Logs . Network Watcher \u00b6 It is a regional service which is used to monitor networks. It can monitor IaaS but not PaaS . Azure Network Watcher provides tools to monitor, diagnose, view metrics, and enable or disable logs for resources in an Azure virtual network. Network Watcher is designed to monitor and repair the network health of IaaS (Infrastructure-as-a-Service) products including Virtual Machines (VM), Virtual Networks, Application Gateways, Load balancers, etc. NSG flow logs NSG flow logs is a feature of Azure Network Watcher that allows you to log information about IP traffic flowing through an NSG. These logs will show inbound and outbound flows on a per-rule basis . Monitoring Tools \u00b6 Topology map \u00b6 As resources are added to a virtual network, it can become difficult to understand what resources are in a virtual network and how they relate to each other. The topology capability enables you to generate a visual diagram of the resources in a virtual network and the relationships between the resources Connection Monitor \u00b6 Monitor connectivity between Azure resources on Network. Network Performance Monitor \u00b6 Monitor Network performance, connectivity between Vnets, ExpressRoute etc. Diagnostic Tools \u00b6 Next Hop \u00b6 Next Hop is used to ensure if the traffic is getting routed to the expected destination. Ideally, this will be useful in scenarios where you will be using user-defined routes (UDRs) to verify if the routing rules are working By using Next Hop, you can easily find which route table is used for routing the traffic from a source to destination IP Flow verify \u00b6 IP Flow Verify can be used to quickly troubleshoot connectivity issues from or to a remote IP address from a local IP address. Example When you create a VM, there will be a default NSG that will be assigned to the VM. Let\u2019s assume that even after opening the ports you are not able to connect to the VM remotely via RDP. To understand which rule is blocking the connectivity from the remote IP to the VM, you can use IP Flow Verify Effective security rules \u00b6 As you know, you can apply an NSG at the subnet level and at the NIC level. Sometimes this can get complicated and with the help of effective security rules will be capable of finding the effective rules applied on the traffic. Packet Capture \u00b6 Capture packets from VM for analysis when a condition is met. Connection Troubleshoot \u00b6 Using Connection Troubleshoot, you can check the connectivity from a virtual machine to another VM, FQDN, URI, or IPv4 address. VPN Diagnostic \u00b6 Used to diagnose/troubleshoot VPN related issues. ITSMC \u00b6 IT Service Management Connector (ITSMC) allows you to connect Azure to a supported IT Service Management (ITSM) product or service. Azure services like Azure Log Analytics and Azure Monitor provide tools to detect, analyze, and troubleshoot problems with your Azure and non-Azure resources. But the work items related to an issue typically reside in an ITSM product or service. ITSMC provides a bidirectional connection between Azure and ITSM tools to help you resolve issues faster. supported ITSMC's ITSMC supports connections with the following ITSM tools: - ServiceNow - System Center Service Manager - Provance - Cherwell. Performance Counters \u00b6 Performance counters in Windows and Linux provide insight into the performance of hardware components, OS, and applications. Azure Monitor can collect performance counters from Log Analytics agents such as waagent (Azure Linux Agent) at frequent intervals for Near Real Time (NRT) analysis in addition to aggregating performance data for longer term analysis and reporting. Misc Notes \u00b6 Heartbeat table can tell which systems have not sent heartbeat in last x minutes. Auth event is part of syslog table. The Network Watcher connection troubleshoot provides the capability to check a direct TCP connection from a virtual machine to a virtual machine (VM), fully qualified domain name (FQDN), URI, or IPv4 address. Activity log saves data for 90 days. Performance counters are stored in perf table. Usage Table ; it stores Hourly usage data for each table in the workspace. Log analytics is billed for data ingestion and data retention. 31 days of data retention is free in Log analytics NSG flow logs are stored in JSON format.","title":"Monitor \ud83d\udd2d"},{"location":"azure/az-monitor/#azure-monitor","text":"It delivers a comprehensive solution for collecting, analyzing, and acting on telemetry from your cloud and on-premises environments. This information helps you understand how your applications are performing and proactively identify issues that affect them and the resources they depend on. States: - Alert State : set by user such as admin - Monitor state : Set by system","title":"Azure Monitor"},{"location":"azure/az-monitor/#log-analytics-workspace","text":"A Log Analytics workspace is a unique environment for log data from Azure Monitor and other Azure services, such as Microsoft Sentinel and Microsoft Defender for Cloud. The connected sources , configuration , and the repository are managed per workspace.","title":"Log Analytics Workspace"},{"location":"azure/az-monitor/#logs","text":"Its an event-based data. Example : Syslog in Linux is an example of a log, as the log data is not consistent and the format may vary from source to source. Free form or structured Stored in logs analytics workspace Analysis via Kusto Query Language (KQL) : The first step in writing queries is to understand which table contains the information you need. Some examples include Event, Syslog, Heartbeat, and Alert. View error events from a table named Event using 3 methods search in ( Event ) \"error\" Event | search \"error\" Event | where EventType == \"error\" query perf table Perf | summarize AggregatedValue = count () by CounterPath Count of agent nodes that are sending a heartbeat each day in the last month Heartbeat | where TimeGenerated > startofday ( ago ( 31 d )) | summarize nodes = dcount ( Computer ) by bin ( TimeGenerated , 1 d ) | render timechart","title":"Logs"},{"location":"azure/az-monitor/#log-analytics","text":"A service for aggregating the log data in a single pane so that it can be analyzed, visualized and queried via KQL","title":"Log Analytics"},{"location":"azure/az-monitor/#agents","text":"Azure Monitor Agent (AMA) collects monitoring data from the guest operating system of Azure and hybrid virtual machines and delivers it to Azure Monitor for use Windows agents Linux agents Cost control using agents With the help of the agents configuration , you will be able to declare what logs you want to collect using the agents and what level of logging information you need. In this way, you will have granular control over what is getting ingested into your workspace. Various logs used are: Windows Event Logs : This helps you to select which event log items you want to ingest to the workspace Windows Performance Counters : You can select performance counters of Windows servers and the sample rate. Linux Performance Counters : These are performance counters for Linux servers and their sample rate. Syslog : Control which facilities in Syslog you want to ingest. IIS Logs : This enables collection of W3C format log files from IIS server.","title":"Agents"},{"location":"azure/az-monitor/#dcr","text":"Azure Monitor Agent uses data collection rules (DCR) , where you define which data you want each agent to collect. Data collection rules let you manage data collection settings at scale and define unique, scoped configurations for subsets of machines. You can define a rule to send data from multiple machines to multiple destinations across regions and tenants.","title":"DCR"},{"location":"azure/az-monitor/#data-retention","text":"Data in each table in a Log Analytics workspace is retained for a specified period of time after which it's either removed or archived with a reduced retention fee.","title":"Data retention"},{"location":"azure/az-monitor/#activity-log","text":"With the help of an activity log, you can get insights into different operations occurring at the subscription level .","title":"Activity log"},{"location":"azure/az-monitor/#categories","text":"As Azure Activity Log is a subscription-wide logging system, Azure has divided the logs ingested into different categories. Administrative : Contains the record of all create, update, delete, and action operations performed through Resource Manager. Examples of Administrative events include create virtual machine and delete network security group. Service Health : Contains the record of any service health incidents that have occurred in Azure. An example of a Service Health event SQL Azure in East US is experiencing downtime. Resource Health : Contains the record of any resource health events that have occurred to your Azure resources. An example of a Resource Health event is Virtual Machine health status changed to unavailable. Alert : Contains the record of activations for Azure alerts. An example of an Alert event is CPU % on myVM has been over 80 for the past 5 minutes. Autoscale : Contains the record of any events related to the operation of the autoscale engine based on any autoscale settings you have defined in your subscription. An example of an Autoscale event is Autoscale scale up action failed. Recommendation : From advisor Security : This includes any security alerts generated by Azure Defender for Servers. Policy : Whenever Azure Policy is evaluated, the effect action will be logged in this category.","title":"Categories"},{"location":"azure/az-monitor/#metrics","text":"Metrics are numerical values that are ingested from Azure resources used to represent the state of the system at a particular point in time. Metric Example For a virtual machine, the metrics available will be CPU Percentage , Network In , Network Out , Memory , etc. On the other hand, if you take a storage account , the available metrics will be Number Of Requests , Number Of Failed Requests , Number Of API Calls , etc. Short time based data. Frequently updated Near real-time data Visualizations via Metrics Explorer","title":"Metrics"},{"location":"azure/az-monitor/#distributed-tracing","text":"In monolithic architectures , we've gotten used to debugging with call stacks. Call stacks are brilliant tools for showing the flow of execution ( Method A called Method B , which called Method C ), along with details and parameters about each of those calls. This technique is great for monoliths or services running on a single process. But how do we debug when the call is across a process boundary, not simply a reference on the local stack? That's where distributed tracing comes in. !!! info '' Distributed tracing is the equivalent of call stacks for modern cloud and microservices architectures , with the addition of a simplistic performance profiler thrown in.","title":"Distributed Tracing"},{"location":"azure/az-monitor/#diagnostic-settings","text":"They are used to define where the logs and metrics will be stored.","title":"Diagnostic Settings"},{"location":"azure/az-monitor/#action-groups","text":"What are action groups? An action group is a collection of notification preferences that can be reused in multiple alerts. The notifications and actions that you define inside the action group will be executed when the alert is fired. You can create multiple action groups with different notification preferences, and these can be used across your alerts. They can be found in Monitor \u2192 Alerts \u2192 Action Groups Action groups consist of two parts: notifications and actions","title":"Action groups"},{"location":"azure/az-monitor/#notification-types","text":"Email/SMS : These will work even if Azure is down while other services needs the Azure to be running. Email Azure Resource Manager ; Role You can send email notifications to Azure RBAC roles like Owner, Contributor, Reader, Monitoring Contributor, and Monitoring Reader that are assigned at the subscription scope. All user principals assigned with any of the aforementioned roles will be notified when the alert is triggered. Azure AD group and service principals are excluded from the email notification.","title":"Notification types"},{"location":"azure/az-monitor/#action-types","text":"Azure app push notification : Azure Function : Using serverless compute, you can run small chunks of code when the alert is fired. Logic App : run a business process Secure Webhook/WebHook : This is the HTTPS or HTTP endpoint for an external application to communicate. ITSM : You can integrate your ITSM tools like ServiceNow so that whenever an alert is triggered, the corresponding ticket will be created in the ITSM tool. Automation runbook : Event Hub : Ingest event to other systems.","title":"Action types"},{"location":"azure/az-monitor/#insights","text":"This is service specific monitoring feature in Azure","title":"Insights"},{"location":"azure/az-monitor/#vm-insights","text":"Used to monitor VM and VMSS. This is also called as Azure monitor for VMs Require Log Analytics Agent to be installed","title":"VM Insights"},{"location":"azure/az-monitor/#network-insights","text":"No agent installation required for this.","title":"Network Insights"},{"location":"azure/az-monitor/#container-insights","text":"Used to monitor containers","title":"Container Insights"},{"location":"azure/az-monitor/#app-insights-for-devs","text":"Application Insights is an extension of Azure Monitor and provides Application Performance Monitoring (also known as \u201cAPM\u201d) features. APM tools are useful to monitor applications from development, through test, and into production . What to use for logging? In addition to collecting Metrics and application Telemetry data , which describe application activities and health, Application Insights can also be used to collect and store application trace logging data .","title":"App Insights (for Devs)"},{"location":"azure/az-monitor/#features","text":"Metrics and alerts Application Map Profiler Usage Analytics","title":"Features"},{"location":"azure/az-monitor/#instrumentation","text":"At a basic level, instrumenting is simply enabling an application to capture telemetry. There are two methods to instrument your application: Manual instrumentation Automatic instrumentation (auto-instrumentation) It can be: Runtime instrumentation Build-time instrumentation Instrumentation key Key of implementing Instrumentation in application and is stored in app insights resource.","title":"Instrumentation"},{"location":"azure/az-monitor/#auto-instrumentation","text":"It's an agent for App insights. Auto-instrumentation enables telemetry collection through configuration without touching the application's code. Although it's more convenient, it tends to be less configurable. It's also not available in all languages The Application Insights agent or SDK pre-processes telemetry and metrics before sending the data to Azure where it's ingested and processed further before being stored in Azure Monitor Logs .","title":"Auto-instrumentation"},{"location":"azure/az-monitor/#network-watcher","text":"It is a regional service which is used to monitor networks. It can monitor IaaS but not PaaS . Azure Network Watcher provides tools to monitor, diagnose, view metrics, and enable or disable logs for resources in an Azure virtual network. Network Watcher is designed to monitor and repair the network health of IaaS (Infrastructure-as-a-Service) products including Virtual Machines (VM), Virtual Networks, Application Gateways, Load balancers, etc. NSG flow logs NSG flow logs is a feature of Azure Network Watcher that allows you to log information about IP traffic flowing through an NSG. These logs will show inbound and outbound flows on a per-rule basis .","title":"Network Watcher"},{"location":"azure/az-monitor/#monitoring-tools","text":"","title":"Monitoring Tools"},{"location":"azure/az-monitor/#topology-map","text":"As resources are added to a virtual network, it can become difficult to understand what resources are in a virtual network and how they relate to each other. The topology capability enables you to generate a visual diagram of the resources in a virtual network and the relationships between the resources","title":"Topology map"},{"location":"azure/az-monitor/#connection-monitor","text":"Monitor connectivity between Azure resources on Network.","title":"Connection Monitor"},{"location":"azure/az-monitor/#network-performance-monitor","text":"Monitor Network performance, connectivity between Vnets, ExpressRoute etc.","title":"Network Performance Monitor"},{"location":"azure/az-monitor/#diagnostic-tools","text":"","title":"Diagnostic Tools"},{"location":"azure/az-monitor/#next-hop","text":"Next Hop is used to ensure if the traffic is getting routed to the expected destination. Ideally, this will be useful in scenarios where you will be using user-defined routes (UDRs) to verify if the routing rules are working By using Next Hop, you can easily find which route table is used for routing the traffic from a source to destination","title":"Next Hop"},{"location":"azure/az-monitor/#ip-flow-verify","text":"IP Flow Verify can be used to quickly troubleshoot connectivity issues from or to a remote IP address from a local IP address. Example When you create a VM, there will be a default NSG that will be assigned to the VM. Let\u2019s assume that even after opening the ports you are not able to connect to the VM remotely via RDP. To understand which rule is blocking the connectivity from the remote IP to the VM, you can use IP Flow Verify","title":"IP Flow verify"},{"location":"azure/az-monitor/#effective-security-rules","text":"As you know, you can apply an NSG at the subnet level and at the NIC level. Sometimes this can get complicated and with the help of effective security rules will be capable of finding the effective rules applied on the traffic.","title":"Effective security rules"},{"location":"azure/az-monitor/#packet-capture","text":"Capture packets from VM for analysis when a condition is met.","title":"Packet Capture"},{"location":"azure/az-monitor/#connection-troubleshoot","text":"Using Connection Troubleshoot, you can check the connectivity from a virtual machine to another VM, FQDN, URI, or IPv4 address.","title":"Connection Troubleshoot"},{"location":"azure/az-monitor/#vpn-diagnostic","text":"Used to diagnose/troubleshoot VPN related issues.","title":"VPN Diagnostic"},{"location":"azure/az-monitor/#itsmc","text":"IT Service Management Connector (ITSMC) allows you to connect Azure to a supported IT Service Management (ITSM) product or service. Azure services like Azure Log Analytics and Azure Monitor provide tools to detect, analyze, and troubleshoot problems with your Azure and non-Azure resources. But the work items related to an issue typically reside in an ITSM product or service. ITSMC provides a bidirectional connection between Azure and ITSM tools to help you resolve issues faster. supported ITSMC's ITSMC supports connections with the following ITSM tools: - ServiceNow - System Center Service Manager - Provance - Cherwell.","title":"ITSMC"},{"location":"azure/az-monitor/#performance-counters","text":"Performance counters in Windows and Linux provide insight into the performance of hardware components, OS, and applications. Azure Monitor can collect performance counters from Log Analytics agents such as waagent (Azure Linux Agent) at frequent intervals for Near Real Time (NRT) analysis in addition to aggregating performance data for longer term analysis and reporting.","title":"Performance Counters"},{"location":"azure/az-monitor/#misc-notes","text":"Heartbeat table can tell which systems have not sent heartbeat in last x minutes. Auth event is part of syslog table. The Network Watcher connection troubleshoot provides the capability to check a direct TCP connection from a virtual machine to a virtual machine (VM), fully qualified domain name (FQDN), URI, or IPv4 address. Activity log saves data for 90 days. Performance counters are stored in perf table. Usage Table ; it stores Hourly usage data for each table in the workspace. Log analytics is billed for data ingestion and data retention. 31 days of data retention is free in Log analytics NSG flow logs are stored in JSON format.","title":"Misc Notes"},{"location":"azure/az_commands/","text":"Azure Commands \u00b6 VM realted \u00b6 # Get a list of subscriptions for the logged in account. (autogenerated) az account list az vm list az group list --query \"[?location=='eastus2']\" NSG \u00b6 # get the nsg associated with an RG az network nsg list \\ --resource-group XXX \\ --query '[].name' \\ --output tsv Get NSG rules \u00b6 az network nsg rule list \\ --resource-group XXX \\ --nsg-name my-vmNSG Create a NSG rule \u00b6 Here we are creaating a rule to allow the http access on port 80 az network nsg rule create \\ --resource-group XXX \\ --nsg-name my-vmNSG \\ --name allow-http \\ --protocol tcp \\ --priority 100 \\ --destination-port-range 80 \\ --access Allow Check if rule is added \u00b6 az network nsg rule list \\ --resource-group XXX \\ --nsg-name my-vmNSG \\ --query '[].{Name:name, Priority:priority, Port:destinationPortRange, Access:access}' \\ --output table Name Priority Port Access ----------------- ---------- ------ -------- default-allow-ssh 1000 22 Allow allow-http 100 80 Allow","title":"Az Commands"},{"location":"azure/az_commands/#azure-commands","text":"","title":"Azure Commands"},{"location":"azure/az_commands/#vm-realted","text":"# Get a list of subscriptions for the logged in account. (autogenerated) az account list az vm list az group list --query \"[?location=='eastus2']\"","title":"VM realted"},{"location":"azure/az_commands/#nsg","text":"# get the nsg associated with an RG az network nsg list \\ --resource-group XXX \\ --query '[].name' \\ --output tsv","title":"NSG"},{"location":"azure/az_commands/#get-nsg-rules","text":"az network nsg rule list \\ --resource-group XXX \\ --nsg-name my-vmNSG","title":"Get NSG rules"},{"location":"azure/az_commands/#create-a-nsg-rule","text":"Here we are creaating a rule to allow the http access on port 80 az network nsg rule create \\ --resource-group XXX \\ --nsg-name my-vmNSG \\ --name allow-http \\ --protocol tcp \\ --priority 100 \\ --destination-port-range 80 \\ --access Allow","title":"Create a NSG rule"},{"location":"azure/az_commands/#check-if-rule-is-added","text":"az network nsg rule list \\ --resource-group XXX \\ --nsg-name my-vmNSG \\ --query '[].{Name:name, Priority:priority, Port:destinationPortRange, Access:access}' \\ --output table Name Priority Port Access ----------------- ---------- ------ -------- default-allow-ssh 1000 22 Allow allow-http 100 80 Allow","title":"Check if rule is added"},{"location":"azure/azlb/","text":"Az Load Balancer (L4) \u00b6 In AWS terms, its a Network Load Balancer (NLB) as Azure Load Balancer operates at layer 4 of the Open Systems Interconnection (OSI) model. It's the single point of contact for clients. OSI Layer AWS Azure L4 Network LB Azure LB L7 Application LB App Gateway Load balancer distributes inbound flows that arrive at the load balancer's front end to backend pool instances. These flows are according to configured load-balancing rules and health probes. The backend pool instances can be Azure Virtual Machines (VM) or instances in a Virtual Machine Scale Set (VMSS) . Types of LB \u00b6 The purpose of a load balancer is to distribute the traffic to the servers after verifying the health of the backend server . Depending upon the placement of the load balancer in the architecture, the load balancer can be categorized as: Public/External LB : As the name suggests, a public load balancer will have a public IP address , and it will be Internet facing. In a public load balancer, the public IP address and a port number are mapped to the private IP address and port number of the VMs that are part of the backend pool. Private/Internal LB : There will be scenarios where you want to load balance the requests between resources that are deployed inside a virtual network without exposing any Internet endpoint. For example, this could be a set of database servers that will distribute the database requests coming from the front-end servers . Since the backend database servers cannot be exposed to the Internet, you need to make sure that the load balancer has no public endpoint . Internal load balancers are deployed to distribute the traffic to your backend servers that cannot be exposed to the Internet. The internal load balancer will not have a public IP address and will be using the private IP address for all communication. This private IP address can be reached by the resources within the same virtual network, within peered networks, or from on-premises over VPN. Backend pool \u00b6 The backend pool is a critical component of the load balancer. The backend pool defines the group of resources that will serve traffic for a given load-balancing rule. The backend pool contains the IP address of the network interface cards that are attached to the set of virtual machines or virtual machine scale set In the Standard SKU, you can have up to 1,000 instances, and the Basic SKU can have up to 100 instances in the backend pool. There are two ways of configuring a backend pool : Network Interface Card (NIC) IP address Floating IP \u00b6 Why floating IP is required? If you want to reuse the backend port across multiple rules, you must enable Floating IP in the load balancing rule definition. Floating IP is Azure's terminology for a portion of what is known as Direct Server Return (DSR). DSR consists of two parts: a flow topology and an IP address mapping scheme. At a platform level, Azure Load Balancer always operates in a DSR flow topology regardless of whether Floating IP is enabled or not. This means that the outbound part of a flow is always correctly rewritten to flow directly back to the origin. When Floating IP is enabled, Azure changes the IP address mapping to the Frontend IP address (FIP) of the Load Balancer frontend instead of backend instance's IP (BIP). Without Floating IP, Azure exposes the VM instances' IP. Enabling Floating IP changes the IP address mapping to the Frontend IP of the load Balancer to allow for more flexibility. Health Probes \u00b6 Azure Load Balancer rules require a health probe to detect the endpoint status. The configuration of the health probe and probe responses determines which backend pool instances will receive new connections. Why health probe is required? The purpose of a health probe is to let the load balancer know the status of the application. The health probe will be constantly checking the status of the application using an HTTP or TCP probe. If the application is not responding after a set of consecutive failures, the load balancer will mark the virtual machine as unhealthy. Incoming requests will not be routed to the unhealthy virtual machines. The load balancer will start routing the traffic once the health probe is able to identify that the application is working and is responding to the probe. How health probe works for HTTP? In the HTTP probe, the endpoint will be probed every 15 seconds (default value), and if the response is HTTP 200 , then it means that the application is healthy. If the application is returning a non-2xx response within the timeout period, the virtual machine will be marked unhealthy. Hashing/ LB Rules \u00b6 Without the load balancer rules, the traffic that hits the front end will never reach the backend pool The hash is used to route traffic to healthy backend instances within the backend pool . The algorithm provides stickiness only within a transport session. When the client starts a new session from the same source IP, the source port changes and causes the traffic to go to a different backend instance. key differences 5 tuple (default) : Traffic from the same client IP routed to any healthy instance in the backend pool 3 tuple : Traffic from the same client IP and protocol is routed to the same backend instance. Its also called source IP affinity 2 tuple : Traffic from the same client IP is routed to the same backend instance 5 tuple Hash \u00b6 Azure Load Balancer uses a five tuple hash based distribution mode by default. The five tuple consists of: 5 tuple hash Source IP Source port Destination IP Destination port Protocol type 3 tuple hash \u00b6 Client IP and protocol (3-tuple) - Specifies that successive requests from the same client IP address and protocol combination will be handled by the same backend instance. 3 tuple hash Source IP Destination IP Protocol type 2 tuple hash \u00b6 Client IP (2-tuple) - Specifies that successive requests from the same client IP address will be handled by the same backend instance. 2 tuple hash Source IP Destination IP","title":"Load Balancer (L4) \u2696\ufe0f"},{"location":"azure/azlb/#az-load-balancer-l4","text":"In AWS terms, its a Network Load Balancer (NLB) as Azure Load Balancer operates at layer 4 of the Open Systems Interconnection (OSI) model. It's the single point of contact for clients. OSI Layer AWS Azure L4 Network LB Azure LB L7 Application LB App Gateway Load balancer distributes inbound flows that arrive at the load balancer's front end to backend pool instances. These flows are according to configured load-balancing rules and health probes. The backend pool instances can be Azure Virtual Machines (VM) or instances in a Virtual Machine Scale Set (VMSS) .","title":"Az Load Balancer (L4)"},{"location":"azure/azlb/#types-of-lb","text":"The purpose of a load balancer is to distribute the traffic to the servers after verifying the health of the backend server . Depending upon the placement of the load balancer in the architecture, the load balancer can be categorized as: Public/External LB : As the name suggests, a public load balancer will have a public IP address , and it will be Internet facing. In a public load balancer, the public IP address and a port number are mapped to the private IP address and port number of the VMs that are part of the backend pool. Private/Internal LB : There will be scenarios where you want to load balance the requests between resources that are deployed inside a virtual network without exposing any Internet endpoint. For example, this could be a set of database servers that will distribute the database requests coming from the front-end servers . Since the backend database servers cannot be exposed to the Internet, you need to make sure that the load balancer has no public endpoint . Internal load balancers are deployed to distribute the traffic to your backend servers that cannot be exposed to the Internet. The internal load balancer will not have a public IP address and will be using the private IP address for all communication. This private IP address can be reached by the resources within the same virtual network, within peered networks, or from on-premises over VPN.","title":"Types of LB"},{"location":"azure/azlb/#backend-pool","text":"The backend pool is a critical component of the load balancer. The backend pool defines the group of resources that will serve traffic for a given load-balancing rule. The backend pool contains the IP address of the network interface cards that are attached to the set of virtual machines or virtual machine scale set In the Standard SKU, you can have up to 1,000 instances, and the Basic SKU can have up to 100 instances in the backend pool. There are two ways of configuring a backend pool : Network Interface Card (NIC) IP address","title":"Backend pool"},{"location":"azure/azlb/#floating-ip","text":"Why floating IP is required? If you want to reuse the backend port across multiple rules, you must enable Floating IP in the load balancing rule definition. Floating IP is Azure's terminology for a portion of what is known as Direct Server Return (DSR). DSR consists of two parts: a flow topology and an IP address mapping scheme. At a platform level, Azure Load Balancer always operates in a DSR flow topology regardless of whether Floating IP is enabled or not. This means that the outbound part of a flow is always correctly rewritten to flow directly back to the origin. When Floating IP is enabled, Azure changes the IP address mapping to the Frontend IP address (FIP) of the Load Balancer frontend instead of backend instance's IP (BIP). Without Floating IP, Azure exposes the VM instances' IP. Enabling Floating IP changes the IP address mapping to the Frontend IP of the load Balancer to allow for more flexibility.","title":"Floating IP"},{"location":"azure/azlb/#health-probes","text":"Azure Load Balancer rules require a health probe to detect the endpoint status. The configuration of the health probe and probe responses determines which backend pool instances will receive new connections. Why health probe is required? The purpose of a health probe is to let the load balancer know the status of the application. The health probe will be constantly checking the status of the application using an HTTP or TCP probe. If the application is not responding after a set of consecutive failures, the load balancer will mark the virtual machine as unhealthy. Incoming requests will not be routed to the unhealthy virtual machines. The load balancer will start routing the traffic once the health probe is able to identify that the application is working and is responding to the probe. How health probe works for HTTP? In the HTTP probe, the endpoint will be probed every 15 seconds (default value), and if the response is HTTP 200 , then it means that the application is healthy. If the application is returning a non-2xx response within the timeout period, the virtual machine will be marked unhealthy.","title":"Health Probes"},{"location":"azure/azlb/#hashing-lb-rules","text":"Without the load balancer rules, the traffic that hits the front end will never reach the backend pool The hash is used to route traffic to healthy backend instances within the backend pool . The algorithm provides stickiness only within a transport session. When the client starts a new session from the same source IP, the source port changes and causes the traffic to go to a different backend instance. key differences 5 tuple (default) : Traffic from the same client IP routed to any healthy instance in the backend pool 3 tuple : Traffic from the same client IP and protocol is routed to the same backend instance. Its also called source IP affinity 2 tuple : Traffic from the same client IP is routed to the same backend instance","title":"Hashing/ LB Rules"},{"location":"azure/azlb/#5-tuple-hash","text":"Azure Load Balancer uses a five tuple hash based distribution mode by default. The five tuple consists of: 5 tuple hash Source IP Source port Destination IP Destination port Protocol type","title":"5 tuple Hash"},{"location":"azure/azlb/#3-tuple-hash","text":"Client IP and protocol (3-tuple) - Specifies that successive requests from the same client IP address and protocol combination will be handled by the same backend instance. 3 tuple hash Source IP Destination IP Protocol type","title":"3 tuple hash"},{"location":"azure/azlb/#2-tuple-hash","text":"Client IP (2-tuple) - Specifies that successive requests from the same client IP address will be handled by the same backend instance. 2 tuple hash Source IP Destination IP","title":"2 tuple hash"},{"location":"azure/azure_function/","text":"Azure Function \u00b6 A function contains two important pieces Code : which can be written in a variety of languages Config : the function.json file For compiled languages, this config file is generated automatically from annotations in your code. For scripting languages, you must provide the config file yourself. Function app \u00b6 In Azure Functions , a function app provides the execution context for your individual functions. Function app behaviors apply to all functions hosted by a given function app. All functions in a function app must be of the same language. Scaling of functions in Function app Individual functions in a function app are deployed together and are scaled together. All functions in the same function app share resources, per instance, as the function app scales. Data sharing between App Service \u00b6 Connection strings, environment variables, and other application settings are defined separately for each function app. Any data that must be shared between function apps should be stored externally in a persisted store. Durable Azure Functions \u00b6 Durable Functions is an extension of Azure Functions that lets you write stateful functions in a serverless compute environment. The extension lets you define - Stateful workflows by writing orchestrator functions - Stateful entities by writing entity functions - State-less entities by writing activity functions Note Behind the scenes, the extension manages state, checkpoints, and restarts for you, allowing you to focus on your business logic. App Patterns \u00b6 Example of Durable Azure function \u00b6 To begin with, let\u2019s do a brief explanation about a few key concepts around Azure Durable Functions. Azure Durable Functions basics \u00b6 For those unaware of Azure Durable Functions, it is an extension of Azure Functions that lets you write stateful functions in a serverless compute environment. The extension lets you define stateful workflows by writing orchestrator functions and stateful entities. The 4 main concepts you should know about are: Client functions The client function is responsible for starting, stopping and monitoring the orchestrator functions. Orchestrator functions This function is the heart when building a durable function solution. In this function you write your workflow in code. The workflow can consist of code statements calling other functions like activity functions, other orchestration functions or even wait for other events to occur. An orchestration is started by a client function, a function that on its turn can be triggered by a message in a queue, an HTTP request, or any other trigger mechanism you are familiar with. Every instance of an orchestration function will have an instance identifier, which can be auto-generated or user-generated. Activity functions These activity functions are the basic unit of work in a durable function solution. Each activity function executes one task and can be anything you want. Entity functions Entity functions define operations for reading and updating small pieces of state. We often refer to these stateful entities as durable entities. Like orchestrator functions, entity functions are functions with a special trigger type, entity trigger. They can also be invoked from client functions or from orchestrator functions. Unlike orchestrator functions, entity functions do not have any specific code constraints. Entity functions also manage state explicitly rather than implicitly representing state via control flow. Problem Statement \u00b6 In this section I\u2019ll like to talk a little bit about what I was trying to solve using Durable functions and why Durable functions was a good fit. One of my clients has an HTTP API that uses an Azure Function as its backend, the function takes a few parameters via querystring , runs a query against an Azure Storage Table and returns a result. It\u2019s a really simple setup. Info The problem with this approach lies in the fact that the query takes between 3 and 7 minutes to complete, so most of the time the HTTP function times out because 230 seconds is the maximum amount of time that an HTTP triggered function can take to respond to a request. This is because of the idle timeout of Azure Load Balancer. For longer processing times, I needed to consider using an async pattern or defer the actual work and return an immediate response. Azure Durable Functions provides built-in support for the async http api pattern. Async HTTP API Pattern \u00b6 The async HTTP API pattern addresses the problem of coordinating the state of long-running operations with external clients. A common way to implement this pattern is by having an HTTP endpoint trigger the long-running action. Then, redirect the client to a status endpoint that the client polls to learn when the operation is finished. Scenario \u00b6 The idea behind what we\u2019re going to build is the following one: The customer submits a job by calling an Azure Function that has an HTTP endpoint trigger. This is the Client Function. The submitted job is going to be picked up by the Orchestrator Function and it will call the Activity Function. The Activity Function is going to run the query against the Azure Storage Table and return the result. There is going to be an extra Azure Function that queries the orchestrator function and retrieves the status and the result of a given job. Here\u2019s a diagram: client-function : submits the job that needs to be executed. get-status-function : it is used to retrieve the status and the result of a given job. orchestrator-function : Unwraps the parameters from the submitted job and calls the activity function. query-storage-account-activity-function : Runs a custom query in an Azure Storage Table. Implementation \u00b6 The components of the solution are described in the previous section. Now let\u2019s start building them. client-function \u00b6 This is an Http triggered function. The function receives 2 parameters via QueryString (those parameters are needed to build the query against the Storage Table) and validates both of them. Afterwards, it creates a DurableOrchestrationClient and starts a new orchestration instance using the start_new method. The start_new method takes 3 parameters: Name: The name of the orchestrator function to schedule. -InstanceId: (Optional) The unique ID of the instance. If you don\u2019t specify this parameter, the method uses a random ID. Input: Any JSON-serializable data that should be passed as the input to the orchestrator function. And lastly, it builds the URL from where the client can retrieve the status and the result of the submitted job. import azure.functions as func import azure.durable_functions as df import json import dateutil.parser from urllib.parse import urlparse async def main ( req : func . HttpRequest , starter : str ) -> func . HttpResponse : response = { } headers = { \"Content-Type\" : \"application/json\" } start_date = req . params . get ( 's' ) end_date = req . params . get ( 'e' ) if start_date : try : start_date = dateutil . parser . parse ( start_date , dayfirst = True ) except : response [ 'error' ] = \"Invalid start date format.\" return func . HttpResponse ( json . dumps ( response ) , headers = headers , status_code = 400 ) else : response [ 'error' ] = \"Empty start date.\" return func . HttpResponse ( json . dumps ( response ) , headers = headers , status_code = 400 ) if end_date : try : end_date = dateutil . parser . parse ( end_date , dayfirst = True ) except : response [ 'error' ] = \"Invalid end date format.\" return func . HttpResponse ( json . dumps ( response ) , headers = headers , status_code = 400 ) else : response [ 'error' ] = \"Empty end date.\" return func . HttpResponse ( json . dumps ( response ) , headers = headers , status_code = 400 ) delta = end_date - start_date if delta . days < 1 : response [ 'error' ] = \"Invalid date range.\" return func . HttpResponse ( json . dumps ( response ) , headers = headers , status_code = 400 ) client = df . DurableOrchestrationClient ( starter ) parameters = { \"start\" : start_date . strftime ( \"%Y-%m- %d \" ), \"end\" : end_date . strftime ( \"%Y-%m- %d \" ) } instance_id = await client . start_new ( 'orchestrator-function' , None , parameters ) status_uri = build_api_url ( urlparse ( req . url ) . scheme , req . headers . get ( \"host\" ), instance_id ) response [ \"statusUri\" ] = status_uri return func . HttpResponse ( json . dumps ( response ), headers = headers , status_code = 200 ) def build_api_url(scheme, host, instance_id): return f\"{scheme}://{host}/api/status/{instance_id}\" get-status-function \u00b6 Is this function really needed? Before discussing the implementation of this Azure Function, I think it\u2019s important to talk a bit about if this function is really needed or not. Most of the examples that you are going to find online don\u2019t use a custom function to retrieve the status of the jobs. Instead of that, they use the create_check_status_response method on the Client Function. That means that in the previous section I could have built the client function like this: ... client = df.DurableOrchestrationClient(starter) parameters = { \"start\": start_date.strftime(\"%Y-%m-%d\"), \"end\": end_date.strftime(\"%Y-%m-%d\") } instance_id = await client.start_new('orchestrator-function', None, parameters) return client.create_check_status_response(req, instance_id) And that\u2019s how the function would have responded: { \"id\" : \"b78244c9e19f43e89e7b1578f711940d\" , \"statusQueryGetUri\" : \"http://localhost:7071/runtime/webhooks/durabletask/instances/b78244c9e19f43e89e7b1578f711940d?taskHub=TestHubName&connection=Storage&code=V/xX4rraZaheGNwbMePOhhGtyHxGsgks4Q36WKBdS3WfiOjD5Sb5lw==\" , \"sendEventPostUri\" : \"http://localhost:7071/runtime/webhooks/durabletask/instances/b78244c9e19f43e89e7b1578f711940d/raiseEvent/{eventName}?taskHub=TestHubName&connection=Storage&code=V/xX4rraZaheGNwbMePOhhGtyHxGsgks4Q36WKBdS3WfiOjD5Sb5lw==\" , \"terminatePostUri\" : \"http://localhost:7071/runtime/webhooks/durabletask/instances/b78244c9e19f43e89e7b1578f711940d/terminate?reason={text}&taskHub=TestHubName&connection=Storage&code=V/xX4rraZaheGNwbMePOhhGtyHxGsgks4Q36WKBdS3WfiOjD5Sb5lw==\" , \"rewindPostUri\" : \"http://localhost:7071/runtime/webhooks/durabletask/instances/b78244c9e19f43e89e7b1578f711940d/rewind?reason={text}&taskHub=TestHubName&connection=Storage&code=V/xX4rraZaheGNwbMePOhhGtyHxGsgks4Q36WKBdS3WfiOjD5Sb5lw==\" , \"purgeHistoryDeleteUri\" : \"http://localhost:7071/runtime/webhooks/durabletask/instances/b78244c9e19f43e89e7b1578f711940d?taskHub=TestHubName&connection=Storage&code=V/xX4rraZaheGNwbMePOhhGtyHxGsgks4Q36WKBdS3WfiOjD5Sb5lw==\" , \"restartPostUri\" : \"http://localhost:7071/runtime/webhooks/durabletask/instances/b78244c9e19f43e89e7b1578f711940d/restart?taskHub=TestHubName&connection=Storage&code=V/xX4rraZaheGNwbMePOhhGtyHxGsgks4Q36WKBdS3WfiOjD5Sb5lw==\" } As you can see the create_check_status_response method returns a response that contains links to the various management operations that can be invoked on an orchestration instance. These operations include querying the orchestration status, raising events or terminating the orchestration. So, why I\u2019m building an extra Azure Function to retrieve the status of a job instead of directly using the create_check_status_response method? This is because the general public shouldn\u2019t be able to query the Orchestration status endpoint directly. As you can see the response payload includes the orchestrator management endpoints with their corresponding keys, by providing that, you would allow the clients not only to get the status of a running instance, but also to get the execution history, send external events to the orchestration instance or terminate that instance. Basically if you don\u2019t want your clients messing around with the heart of the function, you need to expose another azure function that returns only the minimum information required. Implementation of get-status-function \u00b6 The get-status-function is an Http triggered function. This function needs the instance_id that the Client function generated when submitting the job to the orchestrator function. To retrieve the status and the result of a submitted job you can use the get_status method. This method queries the orchestrator function to obtain the status of the job. The get_status returns an object with the following properties: Name : The name of the orchestrator function. InstanceId : The instance ID of the orchestration (should be the same as the instanceId input). CreatedTime : The time at which the orchestrator function started running. LastUpdatedTime : The time at which the orchestration last checkpointed. Input : The input of the function as a JSON value. This field isn\u2019t populated if showInput is false. CustomStatus : Custom orchestration status in JSON format. Output : The output of the function as a JSON value (if the function has completed). If the orchestrator function failed, this property includes the failure details. If the orchestrator function was terminated, this property includes the reason for the termination (if any). RuntimeStatus : One of the following values: Pending : The instance has been scheduled but has not yet started running. Running : The instance has started running. Completed : The instance has completed normally. ContinuedAsNew : The instance has restarted itself with a new history. This state is a transient state. Failed : The instance failed with an error. Terminated : The instance was stopped abruptly. History : The execution history of the orchestration. This field is only populated if showHistory is set to true. import azure.functions as func import azure.durable_functions as df import json async def main ( req : func . HttpRequest , starter : str ) -> func . HttpResponse : client = df . DurableOrchestrationClient ( starter ) instance_id = req . route_params [ \"id\" ] response = await client . get_status ( instance_id ) if response . instance_id is None : return func . HttpResponse ( \"Job not found\" , status_code = 404 ) return func . HttpResponse ( json . dumps ({ \"id\" : response . instance_id , \"status\" : response . runtime_status . value , \"result\" : response . output })) orchestrator-function \u00b6 Nothing fancy here. The orchestrator function in this application is really simple because there is no need to orchestrate multiple activies nor build a complex workflow. The function does the following steps: Retrieves the inputs send from the Client Function (These parameters are needed to build the query against the Azure Table Storage) Calls the activity function passing the retrieved inputs. Waits for the activity function to end and returns the result. import azure.durable_functions as df def orchestrator_function ( context : df . DurableOrchestrationContext ): input = context . get_input () result = yield context . call_activity ( 'query-storage-account-activity-function' , { 'start' : input [ 'start' ], 'end' : input [ 'end' ]}) return result main = df . Orchestrator . create ( orchestrator_function ) query-storage-account-activity-function \u00b6 This Activity Function is responsible to run the query against the Azure Table Storage. This function retrieves the Storage Table connection string from an App Configuration with a Key Vault reference ( https://docs.microsoft.com/en-us/azure/azure-app-configuration/overview ), runs the query and returns the result. As you can see this function is a single unit of work and does not contain any reference to the durable-functions library. import json import os from azure.appconfiguration import AzureAppConfigurationClient from azure.identity import DefaultAzureCredential from azure.keyvault.secrets import SecretClient from azure.data.tables import TableClient from pathlib import Path def main ( request : dict ) -> int : start = request [ 'start' ] end = request [ 'end' ] table_conn_str = get_azure_table_connection_string () response = run_query ( start , end , table_conn_str ) return response def run_query ( start , end , table_conn_str ) -> int : items = [] client = TableClient . from_connection_string ( conn_str = table_conn_str , table_name = \"audit\" ) parameters = { \"start\" : start , \"end\" : end } query_filter = \"PartitionKey ge @start and PartitionKey le @end and CertNumber ne ''\" entities = client . query_entities ( query_filter , parameters = parameters , select = 'identificationNumber' ) for entity in entities : items . append ( entity [ 'identificationNumber' ]) return len ( set ( items )) def get_azure_table_connection_string () -> str : defaultAzureCredential = DefaultAzureCredential () app_config_base_url = os . getenv ( 'AppConfigEndpoint' ) app_config_client = AzureAppConfigurationClient ( base_url = app_config_base_url , credential = defaultAzureCredential ) keyvault_value = app_config_client . get_configuration_setting ( key = \"storage-account-connection-string\" , label = \"async-http-api\" ) url_parts = Path ( json . loads ( keyvault_value . value )[ \"uri\" ]) . parts vault_url = \"//\" . join ( url_parts [: 2 ]) kv_secret = url_parts [ - 1 ] kv_client = SecretClient ( vault_url , defaultAzureCredential ) secret_val = kv_client . get_secret ( kv_secret ) . value return secret_val Test \u00b6 Everything is put in place, now let\u2019s test it. If I try to submit a new job with a few invalid parameters, it responds with an error: curl \"https://func-sa-table-durable-dev.azurewebsites.net/api/submit/query\" { \"error\" : \"Empty start date.\" } curl \"https://func-sa-table-durable-dev.azurewebsites.net/api/submit/query?s=10/08/2021&e=abcd\" { \"error\" : \"Invalid end date format.\" } curl \"https://func-sa-table-durable-dev.azurewebsites.net/api/submit/query?s=10/08/2021&e=05/08/2021\" { \"error\" : \"Invalid date range.\" } Copy If I try to submit a new job with valid parameters, the client function responds with the status endpoint. curl \"https://func-sa-table-durable-dev.azurewebsites.net/api/submit/query?s=10/10/2021&e=10/12/2021\" { \"statusUri\" : \"https://func-sa-table-durable-dev.azurewebsites.net/api/status/433ebcfe85ec4012abe94dcda2aa6b00\" } Copy If I query the get-status function right away, it returns that the job is still being executed. curl \"https://func-sa-table-durable-dev.azurewebsites.net/api/status/433ebcfe85ec4012abe94dcda2aa6b00\" { \"id\" : \"433ebcfe85ec4012abe94dcda2aa6b00\" , \"status\" : \"Running\" , \"result\" : null } Copy If I query the get-status function after a few minutes, the job has completed and we can see the result. curl \"https://func-sa-table-durable-dev.azurewebsites.net/api/status/433ebcfe85ec4012abe94dcda2aa6b00\" { \"id\" : \"433ebcfe85ec4012abe94dcda2aa6b00\" , \"status\" : \"Completed\" , \"result\" : 119 } Deployment to Azure \u00b6 I didn\u2019t plan to write about how to deploy these functions to Azure, but it might be useful to someone. Here\u2019s how you can deploy them using: Azure DevOps pipelines - Github Actions - Using Azure Pipelines trigger : none pool : vmImage : 'ubuntu-latest' variables : - name : azureSubscription value : 'cpons-demos-dev' - name : functionAppName value : 'func-staccount-report-query-dev' steps : - task : UsePythonVersion@0 inputs : versionSpec : '3.8' displayName : 'Use Python 3.8' - script : | python -m pip install --upgrade pip pip install --target=\"./.python_packages/lib/site-packages\" -r ./requirements.txt displayName : 'Install dependencies' - task : CopyFiles@2 inputs : SourceFolder : '$(Build.SourcesDirectory)' Contents : '**' TargetFolder : '$(Build.ArtifactStagingDirectory)' - task : ArchiveFiles@2 inputs : rootFolderOrFile : '$(Build.ArtifactStagingDirectory)' includeRootFolder : false archiveType : 'zip' archiveFile : '$(Build.ArtifactStagingDirectory)/$(Build.BuildId).zip' replaceExistingArchive : true - task : AzureFunctionApp@1 inputs : azureSubscription : '$(azureSubscription)' appType : 'functionAppLinux' appName : '$(functionAppName)' package : '$(Build.ArtifactStagingDirectory)/$(Build.BuildId).zip' runtimeStack : 'PYTHON|3.8' Copy Using Github Action name : Deploy Durable Functions to Azure Function App Securing durable functions \u00b6 Access keys - Functions lets you use keys to make it harder to access your HTTP function endpoints during development. Unless the HTTP access level on an HTTP triggered function is set to anonymous, requests must include an API access key in the request. Keys can be host keys function keys System keys : Specific extensions may require a system-managed key to access webhook endpoints. System keys are designed for extension-specific function endpoints that called by internal components. For example, the Event Grid trigger requires that the subscription use a system key when calling the trigger endpoint.","title":"Function \u26a1\ufe0f"},{"location":"azure/azure_function/#azure-function","text":"A function contains two important pieces Code : which can be written in a variety of languages Config : the function.json file For compiled languages, this config file is generated automatically from annotations in your code. For scripting languages, you must provide the config file yourself.","title":"Azure Function"},{"location":"azure/azure_function/#function-app","text":"In Azure Functions , a function app provides the execution context for your individual functions. Function app behaviors apply to all functions hosted by a given function app. All functions in a function app must be of the same language. Scaling of functions in Function app Individual functions in a function app are deployed together and are scaled together. All functions in the same function app share resources, per instance, as the function app scales.","title":"Function app"},{"location":"azure/azure_function/#data-sharing-between-app-service","text":"Connection strings, environment variables, and other application settings are defined separately for each function app. Any data that must be shared between function apps should be stored externally in a persisted store.","title":"Data sharing between App Service"},{"location":"azure/azure_function/#durable-azure-functions","text":"Durable Functions is an extension of Azure Functions that lets you write stateful functions in a serverless compute environment. The extension lets you define - Stateful workflows by writing orchestrator functions - Stateful entities by writing entity functions - State-less entities by writing activity functions Note Behind the scenes, the extension manages state, checkpoints, and restarts for you, allowing you to focus on your business logic.","title":"Durable Azure Functions"},{"location":"azure/azure_function/#app-patterns","text":"","title":"App Patterns"},{"location":"azure/azure_function/#example-of-durable-azure-function","text":"To begin with, let\u2019s do a brief explanation about a few key concepts around Azure Durable Functions.","title":"Example of Durable Azure function"},{"location":"azure/azure_function/#azure-durable-functions-basics","text":"For those unaware of Azure Durable Functions, it is an extension of Azure Functions that lets you write stateful functions in a serverless compute environment. The extension lets you define stateful workflows by writing orchestrator functions and stateful entities. The 4 main concepts you should know about are: Client functions The client function is responsible for starting, stopping and monitoring the orchestrator functions. Orchestrator functions This function is the heart when building a durable function solution. In this function you write your workflow in code. The workflow can consist of code statements calling other functions like activity functions, other orchestration functions or even wait for other events to occur. An orchestration is started by a client function, a function that on its turn can be triggered by a message in a queue, an HTTP request, or any other trigger mechanism you are familiar with. Every instance of an orchestration function will have an instance identifier, which can be auto-generated or user-generated. Activity functions These activity functions are the basic unit of work in a durable function solution. Each activity function executes one task and can be anything you want. Entity functions Entity functions define operations for reading and updating small pieces of state. We often refer to these stateful entities as durable entities. Like orchestrator functions, entity functions are functions with a special trigger type, entity trigger. They can also be invoked from client functions or from orchestrator functions. Unlike orchestrator functions, entity functions do not have any specific code constraints. Entity functions also manage state explicitly rather than implicitly representing state via control flow.","title":"Azure Durable Functions basics"},{"location":"azure/azure_function/#problem-statement","text":"In this section I\u2019ll like to talk a little bit about what I was trying to solve using Durable functions and why Durable functions was a good fit. One of my clients has an HTTP API that uses an Azure Function as its backend, the function takes a few parameters via querystring , runs a query against an Azure Storage Table and returns a result. It\u2019s a really simple setup. Info The problem with this approach lies in the fact that the query takes between 3 and 7 minutes to complete, so most of the time the HTTP function times out because 230 seconds is the maximum amount of time that an HTTP triggered function can take to respond to a request. This is because of the idle timeout of Azure Load Balancer. For longer processing times, I needed to consider using an async pattern or defer the actual work and return an immediate response. Azure Durable Functions provides built-in support for the async http api pattern.","title":"Problem Statement"},{"location":"azure/azure_function/#async-http-api-pattern","text":"The async HTTP API pattern addresses the problem of coordinating the state of long-running operations with external clients. A common way to implement this pattern is by having an HTTP endpoint trigger the long-running action. Then, redirect the client to a status endpoint that the client polls to learn when the operation is finished.","title":"Async HTTP API Pattern"},{"location":"azure/azure_function/#scenario","text":"The idea behind what we\u2019re going to build is the following one: The customer submits a job by calling an Azure Function that has an HTTP endpoint trigger. This is the Client Function. The submitted job is going to be picked up by the Orchestrator Function and it will call the Activity Function. The Activity Function is going to run the query against the Azure Storage Table and return the result. There is going to be an extra Azure Function that queries the orchestrator function and retrieves the status and the result of a given job. Here\u2019s a diagram: client-function : submits the job that needs to be executed. get-status-function : it is used to retrieve the status and the result of a given job. orchestrator-function : Unwraps the parameters from the submitted job and calls the activity function. query-storage-account-activity-function : Runs a custom query in an Azure Storage Table.","title":"Scenario"},{"location":"azure/azure_function/#implementation","text":"The components of the solution are described in the previous section. Now let\u2019s start building them.","title":"Implementation"},{"location":"azure/azure_function/#client-function","text":"This is an Http triggered function. The function receives 2 parameters via QueryString (those parameters are needed to build the query against the Storage Table) and validates both of them. Afterwards, it creates a DurableOrchestrationClient and starts a new orchestration instance using the start_new method. The start_new method takes 3 parameters: Name: The name of the orchestrator function to schedule. -InstanceId: (Optional) The unique ID of the instance. If you don\u2019t specify this parameter, the method uses a random ID. Input: Any JSON-serializable data that should be passed as the input to the orchestrator function. And lastly, it builds the URL from where the client can retrieve the status and the result of the submitted job. import azure.functions as func import azure.durable_functions as df import json import dateutil.parser from urllib.parse import urlparse async def main ( req : func . HttpRequest , starter : str ) -> func . HttpResponse : response = { } headers = { \"Content-Type\" : \"application/json\" } start_date = req . params . get ( 's' ) end_date = req . params . get ( 'e' ) if start_date : try : start_date = dateutil . parser . parse ( start_date , dayfirst = True ) except : response [ 'error' ] = \"Invalid start date format.\" return func . HttpResponse ( json . dumps ( response ) , headers = headers , status_code = 400 ) else : response [ 'error' ] = \"Empty start date.\" return func . HttpResponse ( json . dumps ( response ) , headers = headers , status_code = 400 ) if end_date : try : end_date = dateutil . parser . parse ( end_date , dayfirst = True ) except : response [ 'error' ] = \"Invalid end date format.\" return func . HttpResponse ( json . dumps ( response ) , headers = headers , status_code = 400 ) else : response [ 'error' ] = \"Empty end date.\" return func . HttpResponse ( json . dumps ( response ) , headers = headers , status_code = 400 ) delta = end_date - start_date if delta . days < 1 : response [ 'error' ] = \"Invalid date range.\" return func . HttpResponse ( json . dumps ( response ) , headers = headers , status_code = 400 ) client = df . DurableOrchestrationClient ( starter ) parameters = { \"start\" : start_date . strftime ( \"%Y-%m- %d \" ), \"end\" : end_date . strftime ( \"%Y-%m- %d \" ) } instance_id = await client . start_new ( 'orchestrator-function' , None , parameters ) status_uri = build_api_url ( urlparse ( req . url ) . scheme , req . headers . get ( \"host\" ), instance_id ) response [ \"statusUri\" ] = status_uri return func . HttpResponse ( json . dumps ( response ), headers = headers , status_code = 200 ) def build_api_url(scheme, host, instance_id): return f\"{scheme}://{host}/api/status/{instance_id}\"","title":"client-function"},{"location":"azure/azure_function/#get-status-function","text":"Is this function really needed? Before discussing the implementation of this Azure Function, I think it\u2019s important to talk a bit about if this function is really needed or not. Most of the examples that you are going to find online don\u2019t use a custom function to retrieve the status of the jobs. Instead of that, they use the create_check_status_response method on the Client Function. That means that in the previous section I could have built the client function like this: ... client = df.DurableOrchestrationClient(starter) parameters = { \"start\": start_date.strftime(\"%Y-%m-%d\"), \"end\": end_date.strftime(\"%Y-%m-%d\") } instance_id = await client.start_new('orchestrator-function', None, parameters) return client.create_check_status_response(req, instance_id) And that\u2019s how the function would have responded: { \"id\" : \"b78244c9e19f43e89e7b1578f711940d\" , \"statusQueryGetUri\" : \"http://localhost:7071/runtime/webhooks/durabletask/instances/b78244c9e19f43e89e7b1578f711940d?taskHub=TestHubName&connection=Storage&code=V/xX4rraZaheGNwbMePOhhGtyHxGsgks4Q36WKBdS3WfiOjD5Sb5lw==\" , \"sendEventPostUri\" : \"http://localhost:7071/runtime/webhooks/durabletask/instances/b78244c9e19f43e89e7b1578f711940d/raiseEvent/{eventName}?taskHub=TestHubName&connection=Storage&code=V/xX4rraZaheGNwbMePOhhGtyHxGsgks4Q36WKBdS3WfiOjD5Sb5lw==\" , \"terminatePostUri\" : \"http://localhost:7071/runtime/webhooks/durabletask/instances/b78244c9e19f43e89e7b1578f711940d/terminate?reason={text}&taskHub=TestHubName&connection=Storage&code=V/xX4rraZaheGNwbMePOhhGtyHxGsgks4Q36WKBdS3WfiOjD5Sb5lw==\" , \"rewindPostUri\" : \"http://localhost:7071/runtime/webhooks/durabletask/instances/b78244c9e19f43e89e7b1578f711940d/rewind?reason={text}&taskHub=TestHubName&connection=Storage&code=V/xX4rraZaheGNwbMePOhhGtyHxGsgks4Q36WKBdS3WfiOjD5Sb5lw==\" , \"purgeHistoryDeleteUri\" : \"http://localhost:7071/runtime/webhooks/durabletask/instances/b78244c9e19f43e89e7b1578f711940d?taskHub=TestHubName&connection=Storage&code=V/xX4rraZaheGNwbMePOhhGtyHxGsgks4Q36WKBdS3WfiOjD5Sb5lw==\" , \"restartPostUri\" : \"http://localhost:7071/runtime/webhooks/durabletask/instances/b78244c9e19f43e89e7b1578f711940d/restart?taskHub=TestHubName&connection=Storage&code=V/xX4rraZaheGNwbMePOhhGtyHxGsgks4Q36WKBdS3WfiOjD5Sb5lw==\" } As you can see the create_check_status_response method returns a response that contains links to the various management operations that can be invoked on an orchestration instance. These operations include querying the orchestration status, raising events or terminating the orchestration. So, why I\u2019m building an extra Azure Function to retrieve the status of a job instead of directly using the create_check_status_response method? This is because the general public shouldn\u2019t be able to query the Orchestration status endpoint directly. As you can see the response payload includes the orchestrator management endpoints with their corresponding keys, by providing that, you would allow the clients not only to get the status of a running instance, but also to get the execution history, send external events to the orchestration instance or terminate that instance. Basically if you don\u2019t want your clients messing around with the heart of the function, you need to expose another azure function that returns only the minimum information required.","title":"get-status-function"},{"location":"azure/azure_function/#implementation-of-get-status-function","text":"The get-status-function is an Http triggered function. This function needs the instance_id that the Client function generated when submitting the job to the orchestrator function. To retrieve the status and the result of a submitted job you can use the get_status method. This method queries the orchestrator function to obtain the status of the job. The get_status returns an object with the following properties: Name : The name of the orchestrator function. InstanceId : The instance ID of the orchestration (should be the same as the instanceId input). CreatedTime : The time at which the orchestrator function started running. LastUpdatedTime : The time at which the orchestration last checkpointed. Input : The input of the function as a JSON value. This field isn\u2019t populated if showInput is false. CustomStatus : Custom orchestration status in JSON format. Output : The output of the function as a JSON value (if the function has completed). If the orchestrator function failed, this property includes the failure details. If the orchestrator function was terminated, this property includes the reason for the termination (if any). RuntimeStatus : One of the following values: Pending : The instance has been scheduled but has not yet started running. Running : The instance has started running. Completed : The instance has completed normally. ContinuedAsNew : The instance has restarted itself with a new history. This state is a transient state. Failed : The instance failed with an error. Terminated : The instance was stopped abruptly. History : The execution history of the orchestration. This field is only populated if showHistory is set to true. import azure.functions as func import azure.durable_functions as df import json async def main ( req : func . HttpRequest , starter : str ) -> func . HttpResponse : client = df . DurableOrchestrationClient ( starter ) instance_id = req . route_params [ \"id\" ] response = await client . get_status ( instance_id ) if response . instance_id is None : return func . HttpResponse ( \"Job not found\" , status_code = 404 ) return func . HttpResponse ( json . dumps ({ \"id\" : response . instance_id , \"status\" : response . runtime_status . value , \"result\" : response . output }))","title":"Implementation of get-status-function"},{"location":"azure/azure_function/#orchestrator-function","text":"Nothing fancy here. The orchestrator function in this application is really simple because there is no need to orchestrate multiple activies nor build a complex workflow. The function does the following steps: Retrieves the inputs send from the Client Function (These parameters are needed to build the query against the Azure Table Storage) Calls the activity function passing the retrieved inputs. Waits for the activity function to end and returns the result. import azure.durable_functions as df def orchestrator_function ( context : df . DurableOrchestrationContext ): input = context . get_input () result = yield context . call_activity ( 'query-storage-account-activity-function' , { 'start' : input [ 'start' ], 'end' : input [ 'end' ]}) return result main = df . Orchestrator . create ( orchestrator_function )","title":"orchestrator-function"},{"location":"azure/azure_function/#query-storage-account-activity-function","text":"This Activity Function is responsible to run the query against the Azure Table Storage. This function retrieves the Storage Table connection string from an App Configuration with a Key Vault reference ( https://docs.microsoft.com/en-us/azure/azure-app-configuration/overview ), runs the query and returns the result. As you can see this function is a single unit of work and does not contain any reference to the durable-functions library. import json import os from azure.appconfiguration import AzureAppConfigurationClient from azure.identity import DefaultAzureCredential from azure.keyvault.secrets import SecretClient from azure.data.tables import TableClient from pathlib import Path def main ( request : dict ) -> int : start = request [ 'start' ] end = request [ 'end' ] table_conn_str = get_azure_table_connection_string () response = run_query ( start , end , table_conn_str ) return response def run_query ( start , end , table_conn_str ) -> int : items = [] client = TableClient . from_connection_string ( conn_str = table_conn_str , table_name = \"audit\" ) parameters = { \"start\" : start , \"end\" : end } query_filter = \"PartitionKey ge @start and PartitionKey le @end and CertNumber ne ''\" entities = client . query_entities ( query_filter , parameters = parameters , select = 'identificationNumber' ) for entity in entities : items . append ( entity [ 'identificationNumber' ]) return len ( set ( items )) def get_azure_table_connection_string () -> str : defaultAzureCredential = DefaultAzureCredential () app_config_base_url = os . getenv ( 'AppConfigEndpoint' ) app_config_client = AzureAppConfigurationClient ( base_url = app_config_base_url , credential = defaultAzureCredential ) keyvault_value = app_config_client . get_configuration_setting ( key = \"storage-account-connection-string\" , label = \"async-http-api\" ) url_parts = Path ( json . loads ( keyvault_value . value )[ \"uri\" ]) . parts vault_url = \"//\" . join ( url_parts [: 2 ]) kv_secret = url_parts [ - 1 ] kv_client = SecretClient ( vault_url , defaultAzureCredential ) secret_val = kv_client . get_secret ( kv_secret ) . value return secret_val","title":"query-storage-account-activity-function"},{"location":"azure/azure_function/#test","text":"Everything is put in place, now let\u2019s test it. If I try to submit a new job with a few invalid parameters, it responds with an error: curl \"https://func-sa-table-durable-dev.azurewebsites.net/api/submit/query\" { \"error\" : \"Empty start date.\" } curl \"https://func-sa-table-durable-dev.azurewebsites.net/api/submit/query?s=10/08/2021&e=abcd\" { \"error\" : \"Invalid end date format.\" } curl \"https://func-sa-table-durable-dev.azurewebsites.net/api/submit/query?s=10/08/2021&e=05/08/2021\" { \"error\" : \"Invalid date range.\" } Copy If I try to submit a new job with valid parameters, the client function responds with the status endpoint. curl \"https://func-sa-table-durable-dev.azurewebsites.net/api/submit/query?s=10/10/2021&e=10/12/2021\" { \"statusUri\" : \"https://func-sa-table-durable-dev.azurewebsites.net/api/status/433ebcfe85ec4012abe94dcda2aa6b00\" } Copy If I query the get-status function right away, it returns that the job is still being executed. curl \"https://func-sa-table-durable-dev.azurewebsites.net/api/status/433ebcfe85ec4012abe94dcda2aa6b00\" { \"id\" : \"433ebcfe85ec4012abe94dcda2aa6b00\" , \"status\" : \"Running\" , \"result\" : null } Copy If I query the get-status function after a few minutes, the job has completed and we can see the result. curl \"https://func-sa-table-durable-dev.azurewebsites.net/api/status/433ebcfe85ec4012abe94dcda2aa6b00\" { \"id\" : \"433ebcfe85ec4012abe94dcda2aa6b00\" , \"status\" : \"Completed\" , \"result\" : 119 }","title":"Test"},{"location":"azure/azure_function/#deployment-to-azure","text":"I didn\u2019t plan to write about how to deploy these functions to Azure, but it might be useful to someone. Here\u2019s how you can deploy them using: Azure DevOps pipelines - Github Actions - Using Azure Pipelines trigger : none pool : vmImage : 'ubuntu-latest' variables : - name : azureSubscription value : 'cpons-demos-dev' - name : functionAppName value : 'func-staccount-report-query-dev' steps : - task : UsePythonVersion@0 inputs : versionSpec : '3.8' displayName : 'Use Python 3.8' - script : | python -m pip install --upgrade pip pip install --target=\"./.python_packages/lib/site-packages\" -r ./requirements.txt displayName : 'Install dependencies' - task : CopyFiles@2 inputs : SourceFolder : '$(Build.SourcesDirectory)' Contents : '**' TargetFolder : '$(Build.ArtifactStagingDirectory)' - task : ArchiveFiles@2 inputs : rootFolderOrFile : '$(Build.ArtifactStagingDirectory)' includeRootFolder : false archiveType : 'zip' archiveFile : '$(Build.ArtifactStagingDirectory)/$(Build.BuildId).zip' replaceExistingArchive : true - task : AzureFunctionApp@1 inputs : azureSubscription : '$(azureSubscription)' appType : 'functionAppLinux' appName : '$(functionAppName)' package : '$(Build.ArtifactStagingDirectory)/$(Build.BuildId).zip' runtimeStack : 'PYTHON|3.8' Copy Using Github Action name : Deploy Durable Functions to Azure Function App","title":"Deployment to Azure"},{"location":"azure/azure_function/#securing-durable-functions","text":"Access keys - Functions lets you use keys to make it harder to access your HTTP function endpoints during development. Unless the HTTP access level on an HTTP triggered function is set to anonymous, requests must include an API access key in the request. Keys can be host keys function keys System keys : Specific extensions may require a system-managed key to access webhook endpoints. System keys are designed for extension-specific function endpoints that called by internal components. For example, the Event Grid trigger requires that the subscription use a system key when calling the trigger endpoint.","title":"Securing durable functions"},{"location":"azure/azvm/","text":"AZ VM \u00b6 Disk types \u00b6 VM will use 3 disks; of which 2 comes by default OS disk : The comes with a VM and store OS files. Its C: drive in windows and /dev/sda for Unix-like systems. Temporary disks : They also come by default and store non-persistent data like swap and page files. During a planned maintenance event, redeployment, or host change, the data stored in the temporary disk will be erased. Data will be available during normal reboots, though. Any data on the drive should not be critical to you as it is prone to loss. In Windows, the temporary disk will be labeled as the D: drive , and in Linux, the disk will be labeled as /dev/sdb Data disk : These are additional disks for persistent stores like data files. These disks are VHDs (Virtual Hard Disks). They are page blobs in the blob service. These disks can be labeled with any letter in Windows, and in Linux data disks are labeled from /dev/sdc onward (Un)/managed disks \u00b6 Microsoft has two offerings in the case of storage disks Managed disk : An ARM managed resources with Azure managed Storage Accounts . Since you don\u2019t manage the storage account, the storage account limits don\u2019t apply. Unmanaged disk : In Unmanaged Disk storage, you must create a storage account in resources to hold the disks (VHD files) for your Virtual Machines. Custom data \u00b6 Why do we need it? You might need to inject a script or other metadata into a Microsoft Azure virtual machine (VM) at provisioning time. In other clouds, this concept is often called user data . Microsoft Azure has a similar feature called custom data . Custom data is made available to the VM during the first startup or setup, which is called provisioning. Provisioning is the process where VM creation parameters (for example, hostname, username, password, certificates, custom data, and keys) are made available to the VM. A provisioning agent, such as the Linux Agent or cloud-init, processes those parameters. Custom data is sent to the VM along with the other provisioning configuration information such as the new hostname, username, password, certificates, and keys, etc. This data is passed to the Azure API as base64-encoded data . On Windows, this data ends up in %SYSTEMDRIVE%\\AzureData\\CustomData.bin as a binary file. On Linux, this data is passed to the VM via the ovf-env.xml file, which is copied to the /var/lib/waagent directory during provisioning. How to pass data To use custom data, you must Base64-encode the contents before passing the data to the API--unless you're using a CLI tool that does the conversion for you, such as the Azure CLI. The size can't exceed 64 KB. In the CLI, you can pass your custom data as a file, as the following example shows. The file will be converted to Base64. az vm create \\ --resource-group myResourceGroup \\ --name centos74 \\ --image OpenLogic:CentOS-CI:7-CI:latest \\ --custom-data cloud-init.txt \\ --generate-ssh-keys Proximity Placement groups \u00b6 Placing VMs in a single region reduces the physical distance between the instances. Placing them within a single availability zone will also bring them physically closer together. However, as the Azure footprint grows, a single availability zone may span multiple physical data centers, which may result in a network latency impacting your application. Why to use proximity placement groups? To get VMs as close as possible, achieving the lowest possible latency, you should deploy them within a proximity placement group. A proximity placement group is a logical grouping used to make sure that Azure compute resources are physically located close to each other. Proximity placement groups are useful for workloads where low latency is a requirement. How to use placement group? \u00b6 A proximity placement group is a resource in Azure. You need to create one before using it with other resources. Once created, it could be used with virtual machines , availability sets , or virtual machine scale sets . You specify a proximity placement group when creating compute resources providing the proximity placement group ID . How to move existing resources? You can also move an existing resource into a proximity placement group. When moving a resource into a proximity placement group, you should stop (deallocate) the asset first since it will be redeployed potentially into a different data center in the region to satisfy the co-location constraint . Cloud Init \u00b6 Tldr By default, this agent will process custom data. It accepts multiple formats of custom data, such as cloud-init configuration and scripts. Similar to the Linux Agent, if errors happen during the execution of the configuration processing or scripts when cloud-init is processing the custom data, that's not a fatal provisioning failure. You'll need to create a notification path to alert you of the completion state of the script. Currently, only the Ubuntu images in the Microsoft Azure Gallery have cloud-init preinstalled and configured to act on the custom data sent during provisioning. This means that for Ubuntu you can use custom data to provision a VM using a cloud-init configuration file, or just simply send a script that will be executed by cloud-init during provisioning. HA solutions in VM \u00b6 Various HA solutions are: VMSS Availability Set Availability Zone VMSS (Scale Sets) \u00b6 Azure Virtual Machine Scale Sets (VMSS) provide the management capabilities for applications that run across many VMs, automatic scaling of resources, and load balancing of traffic. With scale sets, you create a virtual machine configuration model, automatically add or remove additional instances based on CPU or memory load, and automatically upgrade to the latest OS version Why to use VMSS? With scale sets , all VM instances are created from the same base OS image and configuration. This approach lets you easily manage hundreds of VMs without extra configuration tasks or network management. Scale sets are used to run multiple instances of your application. If one of these VM instances has a problem, customers continue to access your application through one of the other VM instances with minimal interruption. Scale sets are backed with Azure load balancer for auto-scaling How many VMs can I have in a scale set? A scale set can have 0 to 1,000 virtual machines (VMs) based on platform images or 0 to 600 VMs based on custom images. Availability Set \u00b6 In short, an availability set is a logical grouping of the VMs hosting our application. Since there are multiple instances, you are eliminating the Single Point Of Failure (SPOF) Availability set consists of Fault domains (FD) Update domain (UD) How many FD and UD per set? Every virtual machine that you create will be associated with a UD and FD. You can configure up to 3 FDs and 20 UDs in an availability set. Fault domain (FD) \u00b6 Fault domains represent a set of virtual machines that share a common network switch, power, and air conditioning. FDs can be configured up to a maximum of 3 , and this is the default value while setting up availability sets. They vary by region The number of managed disk fault domains varies by region - either 2 or 3 managed disk fault domains per region. Update domain (UD) \u00b6 UDs represent a group of VMs and the underlying host that can be updated and rebooted at the same time . This will ensure that only one UD is rebooted at a time during planned maintenance events such as patching, firmware updates, etc. Updating VMs in UD The default number of UDs is 5, and if you are creating more than 5 VMs, the 6 th VM will be placed on the 1 st UD, the 7 th will be on the 2 nd , and so 4 th depending upon the number of instances. While a UD is getting rebooted, it\u2019s given 30 minutes to recover before the maintenance task is started on a different domain AS vs AZ \u00b6 Why do we need AS in addition to AZ? An availability set avoids service downtime caused by hardware failure or planned maintenance. However, the protection offered is limited to the datacenter level. What if the entire datacenter is unavailable due to a power outage, natural disaster, or any other reasons? Since the datacenter is down, your workloads will not be available and that is why we need AZ You can use AS and AZ at the same time while creating a VM. Availability Zone (for HA) \u00b6 Within Azure regions, you have unique physical locations named zones. Each zone comprises one or more datacenters. Also, each zone has independent cooling, power, and networking. As these zones are physically isolated and located in different parts of the region, you can deploy our applications to availability zones to protect from datacenter failures. Azure availability zones are connected by a high-performance network with a round-trip latency of less than 2ms. They help your data stay synchronized and accessible when things go wrong. In case of AZ failure AZs are designed so that if one zone is affected, regional services, capacity, and high availability are supported by the remaining two zones. Regions \u00b6 In Azure, every region comprises a set of datacenters that are interconnected with a regional low-latency network Auto-scaling \u00b6 VMSS \u00b6 When you have many VMs that run your application, it\u2019s important to maintain a consistent configuration across your environment. For the reliable performance of your application, the VM size, disk configuration, and application installs should match across all VMs. Virtual Machines Scale Sets provide a logical grouping of platform-managed virtual machines. With scale sets, you create a virtual machine configuration model, automatically add or remove additional instances based on CPU or memory load, and automatically upgrade to the latest OS version. We can set scale-out (increase VM count by 1) and scale-in (decrease VM count by 1 for example) configuration in VMSS. Also, the scale-in policy can be defined as oldest VM or newest VM , or by instance id , etc to make sure which VM gets deleted when the load decreases. Flexible \u00b6 With Flexible orchestration , Azure provides a unified experience across the Azure VM ecosystem. Flexible orchestration offers high availability guarantees (up to 1000 VMs) by spreading VMs across fault domains in a region or within an Availability Zone. This enables you to scale out your application while maintaining fault domain isolation that is essential to run quorum-based or stateful workloads Uniform \u00b6 With uniform orchestration , virtual machine scale sets use a virtual machine profile or template to scale up to desired capacity. While there is some ability to manage or customize individual virtual machine instances, uniform uses identical VM instances. Automate VM deployment \u00b6 This can be done using IaC (Infra as Code) - ARM templates: We can download a template from a running VM - Terraform scripts - Bicep We can use the concept of Golden Image for this where A golden image is a template for a virtual machine, virtual desktop, server, or hard disk drive. Golden images are also known as ghost images, clones, master images, or base images. To create a golden image, an administrator first sets up the computing environment with the exact specifications needed and then saves the disk image as a pattern for future copies. Azure Bastion (Jump box) \u00b6 Hardening the jumpbox VM and keeping it away from vulnerabilities are always tedious tasks for the administrator. Azure offers a life-saver service called Azure Bastion to solve this problem. Tldr With Azure Bastion, you can connect to Azure virtual machines without the need to have public IP addresses. The catch here is Azure Bastion, which is a PaaS solution; this means you don\u2019t have to worry about hardening the infrastructure, as this will be handled by Microsoft Azure The Bastion host is deployed to the virtual network in a separate subnet. The user connects to the Azure portal using any HTML5 browser. The user navigates to the Azure virtual machine to RDP/SSH. Connect Integration - Single-click RDP/SSH session inside the browser No public IP is required on the Azure VM Some useful tips on bastion host It's a fully managed PaaS in Azure that provides RDP/SSH connectivity over TLS/SSL It makes sure that no public IP is exposed. The VMs does not need to have a public IP. The Bastion service will open the RDP/SSH session/connection to your virtual machine over the private IP of your virtual machine, within your virtual network. NSG is not needed because the bastion is hardened internally (they are optional) It is deployed per VNET , earlier it was deployed for each VM and now the process is made more streamlined by Microsoft. Bastion provides secure RDP and SSH connectivity to all the VMs in the virtual network in which it is provisioned It only supports IPV4 addresses There is a separate bastion host subnet to host the jump box. There is a concurrent connection limit with the bastion host (25 for RDP and 50 for SSH) The minimum AzureBastionSubnet size is /26 or larger ( /25, /24 , etc.) Bastion connectivity to Azure Virtual Desktop (WVD) isn't supported. VM Extensions \u00b6 The post-deployment configuration and automation tasks on Azure VMs can be accomplished using small applications that are called vm extensions . Two main extensions are: CSE and DSC CSE \u00b6 Custom Script Extension for Linux (CSE): It is used to automatically invoke scripts and run them on virtual machines post-deployment. DSC \u00b6 Desired State Configuration (DSC) : In the case of CSE, you cannot deal with complex installation procedures such as reboots. Desired State Configuration helps you overcome this crisis and define a state for your virtual machines instead of writing scripts. You can define the state of a machine and enforce the state using the DSC extension handler. You could store these configuration files in Azure Storage, in internal storage, or even in source control. The handler is responsible for pulling the configuration and implementing the desired state on the virtual machine. Even if there are installations that require reboots, DSC will continue the execution of the remaining scripts after reboot Spot instance \u00b6 These are low-priority VMs that are allocated from Azure\u2019s excess capacity in the region. Using spot instances can reduce the cost of instances rather than running them as regular instances. Windows Setup Scripts \u00b6 Setupcomplete.cmd and ErrorHandler.cmd are custom scripts that run during or after the Windows Setup process. They can be used to install applications or run other tasks by using cscript/wscript scripts. %WINDIR%\\Setup\\Scripts\\SetupComplete.cmd : This script runs with local system permissions and starts immediately after the user sees the desktop. %WINDIR%\\Setup\\Scripts\\ErrorHandler.cmd : This script runs automatically when Setup encounters a fatal error. It runs with local system permission. Notes \u00b6 For a Windows VM , it can be up to 15 characters, and Linux VMs can support up to 64 characters. VM name can\u2019t be changed once the VM is deployed, except re-deployment. If you shut down the VM from the operating system \u2014for example, by clicking the Windows button and select Shutdown, then you will be charged for the VM as the hardware allocated is not released. You can use your existing licenses purchased from Software Assurance to cut down this cost. This reduction method is called Azure Hybrid Benefit (AHUB) . AHUB can be used for Windows, Linux (RHEL and SUSE), and SQL virtual machine licenses. You can connect to Windows machines using two options, namely, Remote Desktop Protocol (RDP) for GUI and Windows Remote Management (WinRM) for CLI The public key will end with the extension .pub , and the private key will not have any extension. ssh to vm using ssh -i <path-to-private-key> username@publicIP An availability set is free of cost, and you pay only for the instances that you are deploying. Uploads a virtual hard disk from an on-premises machine to Azure (managed disk or blob) using Add-AzVhd If you make a change to the topology of your network and have Windows VPN clients, the VPN client package for Windows clients must be downloaded and installed again in order for the changes to be applied to the client.] The Set-AzureStaticVNetIP cmdlet sets the static virtual network (VNet) IP address information for a virtual machine object. New-AzureADMSInvitation is used to invite a new external user to your directory.","title":"VM \ud83d\udda5"},{"location":"azure/azvm/#az-vm","text":"","title":"AZ VM"},{"location":"azure/azvm/#disk-types","text":"VM will use 3 disks; of which 2 comes by default OS disk : The comes with a VM and store OS files. Its C: drive in windows and /dev/sda for Unix-like systems. Temporary disks : They also come by default and store non-persistent data like swap and page files. During a planned maintenance event, redeployment, or host change, the data stored in the temporary disk will be erased. Data will be available during normal reboots, though. Any data on the drive should not be critical to you as it is prone to loss. In Windows, the temporary disk will be labeled as the D: drive , and in Linux, the disk will be labeled as /dev/sdb Data disk : These are additional disks for persistent stores like data files. These disks are VHDs (Virtual Hard Disks). They are page blobs in the blob service. These disks can be labeled with any letter in Windows, and in Linux data disks are labeled from /dev/sdc onward","title":"Disk types"},{"location":"azure/azvm/#unmanaged-disks","text":"Microsoft has two offerings in the case of storage disks Managed disk : An ARM managed resources with Azure managed Storage Accounts . Since you don\u2019t manage the storage account, the storage account limits don\u2019t apply. Unmanaged disk : In Unmanaged Disk storage, you must create a storage account in resources to hold the disks (VHD files) for your Virtual Machines.","title":"(Un)/managed disks"},{"location":"azure/azvm/#custom-data","text":"Why do we need it? You might need to inject a script or other metadata into a Microsoft Azure virtual machine (VM) at provisioning time. In other clouds, this concept is often called user data . Microsoft Azure has a similar feature called custom data . Custom data is made available to the VM during the first startup or setup, which is called provisioning. Provisioning is the process where VM creation parameters (for example, hostname, username, password, certificates, custom data, and keys) are made available to the VM. A provisioning agent, such as the Linux Agent or cloud-init, processes those parameters. Custom data is sent to the VM along with the other provisioning configuration information such as the new hostname, username, password, certificates, and keys, etc. This data is passed to the Azure API as base64-encoded data . On Windows, this data ends up in %SYSTEMDRIVE%\\AzureData\\CustomData.bin as a binary file. On Linux, this data is passed to the VM via the ovf-env.xml file, which is copied to the /var/lib/waagent directory during provisioning. How to pass data To use custom data, you must Base64-encode the contents before passing the data to the API--unless you're using a CLI tool that does the conversion for you, such as the Azure CLI. The size can't exceed 64 KB. In the CLI, you can pass your custom data as a file, as the following example shows. The file will be converted to Base64. az vm create \\ --resource-group myResourceGroup \\ --name centos74 \\ --image OpenLogic:CentOS-CI:7-CI:latest \\ --custom-data cloud-init.txt \\ --generate-ssh-keys","title":"Custom data"},{"location":"azure/azvm/#proximity-placement-groups","text":"Placing VMs in a single region reduces the physical distance between the instances. Placing them within a single availability zone will also bring them physically closer together. However, as the Azure footprint grows, a single availability zone may span multiple physical data centers, which may result in a network latency impacting your application. Why to use proximity placement groups? To get VMs as close as possible, achieving the lowest possible latency, you should deploy them within a proximity placement group. A proximity placement group is a logical grouping used to make sure that Azure compute resources are physically located close to each other. Proximity placement groups are useful for workloads where low latency is a requirement.","title":"Proximity Placement groups"},{"location":"azure/azvm/#how-to-use-placement-group","text":"A proximity placement group is a resource in Azure. You need to create one before using it with other resources. Once created, it could be used with virtual machines , availability sets , or virtual machine scale sets . You specify a proximity placement group when creating compute resources providing the proximity placement group ID . How to move existing resources? You can also move an existing resource into a proximity placement group. When moving a resource into a proximity placement group, you should stop (deallocate) the asset first since it will be redeployed potentially into a different data center in the region to satisfy the co-location constraint .","title":"How to use placement group?"},{"location":"azure/azvm/#cloud-init","text":"Tldr By default, this agent will process custom data. It accepts multiple formats of custom data, such as cloud-init configuration and scripts. Similar to the Linux Agent, if errors happen during the execution of the configuration processing or scripts when cloud-init is processing the custom data, that's not a fatal provisioning failure. You'll need to create a notification path to alert you of the completion state of the script. Currently, only the Ubuntu images in the Microsoft Azure Gallery have cloud-init preinstalled and configured to act on the custom data sent during provisioning. This means that for Ubuntu you can use custom data to provision a VM using a cloud-init configuration file, or just simply send a script that will be executed by cloud-init during provisioning.","title":"Cloud Init"},{"location":"azure/azvm/#ha-solutions-in-vm","text":"Various HA solutions are: VMSS Availability Set Availability Zone","title":"HA solutions in VM"},{"location":"azure/azvm/#vmss-scale-sets","text":"Azure Virtual Machine Scale Sets (VMSS) provide the management capabilities for applications that run across many VMs, automatic scaling of resources, and load balancing of traffic. With scale sets, you create a virtual machine configuration model, automatically add or remove additional instances based on CPU or memory load, and automatically upgrade to the latest OS version Why to use VMSS? With scale sets , all VM instances are created from the same base OS image and configuration. This approach lets you easily manage hundreds of VMs without extra configuration tasks or network management. Scale sets are used to run multiple instances of your application. If one of these VM instances has a problem, customers continue to access your application through one of the other VM instances with minimal interruption. Scale sets are backed with Azure load balancer for auto-scaling How many VMs can I have in a scale set? A scale set can have 0 to 1,000 virtual machines (VMs) based on platform images or 0 to 600 VMs based on custom images.","title":"VMSS (Scale Sets)"},{"location":"azure/azvm/#availability-set","text":"In short, an availability set is a logical grouping of the VMs hosting our application. Since there are multiple instances, you are eliminating the Single Point Of Failure (SPOF) Availability set consists of Fault domains (FD) Update domain (UD) How many FD and UD per set? Every virtual machine that you create will be associated with a UD and FD. You can configure up to 3 FDs and 20 UDs in an availability set.","title":"Availability Set"},{"location":"azure/azvm/#fault-domain-fd","text":"Fault domains represent a set of virtual machines that share a common network switch, power, and air conditioning. FDs can be configured up to a maximum of 3 , and this is the default value while setting up availability sets. They vary by region The number of managed disk fault domains varies by region - either 2 or 3 managed disk fault domains per region.","title":"Fault domain (FD)"},{"location":"azure/azvm/#update-domain-ud","text":"UDs represent a group of VMs and the underlying host that can be updated and rebooted at the same time . This will ensure that only one UD is rebooted at a time during planned maintenance events such as patching, firmware updates, etc. Updating VMs in UD The default number of UDs is 5, and if you are creating more than 5 VMs, the 6 th VM will be placed on the 1 st UD, the 7 th will be on the 2 nd , and so 4 th depending upon the number of instances. While a UD is getting rebooted, it\u2019s given 30 minutes to recover before the maintenance task is started on a different domain","title":"Update domain (UD)"},{"location":"azure/azvm/#as-vs-az","text":"Why do we need AS in addition to AZ? An availability set avoids service downtime caused by hardware failure or planned maintenance. However, the protection offered is limited to the datacenter level. What if the entire datacenter is unavailable due to a power outage, natural disaster, or any other reasons? Since the datacenter is down, your workloads will not be available and that is why we need AZ You can use AS and AZ at the same time while creating a VM.","title":"AS vs AZ"},{"location":"azure/azvm/#availability-zone-for-ha","text":"Within Azure regions, you have unique physical locations named zones. Each zone comprises one or more datacenters. Also, each zone has independent cooling, power, and networking. As these zones are physically isolated and located in different parts of the region, you can deploy our applications to availability zones to protect from datacenter failures. Azure availability zones are connected by a high-performance network with a round-trip latency of less than 2ms. They help your data stay synchronized and accessible when things go wrong. In case of AZ failure AZs are designed so that if one zone is affected, regional services, capacity, and high availability are supported by the remaining two zones.","title":"Availability Zone (for HA)"},{"location":"azure/azvm/#regions","text":"In Azure, every region comprises a set of datacenters that are interconnected with a regional low-latency network","title":"Regions"},{"location":"azure/azvm/#auto-scaling","text":"","title":"Auto-scaling"},{"location":"azure/azvm/#vmss","text":"When you have many VMs that run your application, it\u2019s important to maintain a consistent configuration across your environment. For the reliable performance of your application, the VM size, disk configuration, and application installs should match across all VMs. Virtual Machines Scale Sets provide a logical grouping of platform-managed virtual machines. With scale sets, you create a virtual machine configuration model, automatically add or remove additional instances based on CPU or memory load, and automatically upgrade to the latest OS version. We can set scale-out (increase VM count by 1) and scale-in (decrease VM count by 1 for example) configuration in VMSS. Also, the scale-in policy can be defined as oldest VM or newest VM , or by instance id , etc to make sure which VM gets deleted when the load decreases.","title":"VMSS"},{"location":"azure/azvm/#flexible","text":"With Flexible orchestration , Azure provides a unified experience across the Azure VM ecosystem. Flexible orchestration offers high availability guarantees (up to 1000 VMs) by spreading VMs across fault domains in a region or within an Availability Zone. This enables you to scale out your application while maintaining fault domain isolation that is essential to run quorum-based or stateful workloads","title":"Flexible"},{"location":"azure/azvm/#uniform","text":"With uniform orchestration , virtual machine scale sets use a virtual machine profile or template to scale up to desired capacity. While there is some ability to manage or customize individual virtual machine instances, uniform uses identical VM instances.","title":"Uniform"},{"location":"azure/azvm/#automate-vm-deployment","text":"This can be done using IaC (Infra as Code) - ARM templates: We can download a template from a running VM - Terraform scripts - Bicep We can use the concept of Golden Image for this where A golden image is a template for a virtual machine, virtual desktop, server, or hard disk drive. Golden images are also known as ghost images, clones, master images, or base images. To create a golden image, an administrator first sets up the computing environment with the exact specifications needed and then saves the disk image as a pattern for future copies.","title":"Automate VM deployment"},{"location":"azure/azvm/#azure-bastion-jump-box","text":"Hardening the jumpbox VM and keeping it away from vulnerabilities are always tedious tasks for the administrator. Azure offers a life-saver service called Azure Bastion to solve this problem. Tldr With Azure Bastion, you can connect to Azure virtual machines without the need to have public IP addresses. The catch here is Azure Bastion, which is a PaaS solution; this means you don\u2019t have to worry about hardening the infrastructure, as this will be handled by Microsoft Azure The Bastion host is deployed to the virtual network in a separate subnet. The user connects to the Azure portal using any HTML5 browser. The user navigates to the Azure virtual machine to RDP/SSH. Connect Integration - Single-click RDP/SSH session inside the browser No public IP is required on the Azure VM Some useful tips on bastion host It's a fully managed PaaS in Azure that provides RDP/SSH connectivity over TLS/SSL It makes sure that no public IP is exposed. The VMs does not need to have a public IP. The Bastion service will open the RDP/SSH session/connection to your virtual machine over the private IP of your virtual machine, within your virtual network. NSG is not needed because the bastion is hardened internally (they are optional) It is deployed per VNET , earlier it was deployed for each VM and now the process is made more streamlined by Microsoft. Bastion provides secure RDP and SSH connectivity to all the VMs in the virtual network in which it is provisioned It only supports IPV4 addresses There is a separate bastion host subnet to host the jump box. There is a concurrent connection limit with the bastion host (25 for RDP and 50 for SSH) The minimum AzureBastionSubnet size is /26 or larger ( /25, /24 , etc.) Bastion connectivity to Azure Virtual Desktop (WVD) isn't supported.","title":"Azure Bastion (Jump box)"},{"location":"azure/azvm/#vm-extensions","text":"The post-deployment configuration and automation tasks on Azure VMs can be accomplished using small applications that are called vm extensions . Two main extensions are: CSE and DSC","title":"VM Extensions"},{"location":"azure/azvm/#cse","text":"Custom Script Extension for Linux (CSE): It is used to automatically invoke scripts and run them on virtual machines post-deployment.","title":"CSE"},{"location":"azure/azvm/#dsc","text":"Desired State Configuration (DSC) : In the case of CSE, you cannot deal with complex installation procedures such as reboots. Desired State Configuration helps you overcome this crisis and define a state for your virtual machines instead of writing scripts. You can define the state of a machine and enforce the state using the DSC extension handler. You could store these configuration files in Azure Storage, in internal storage, or even in source control. The handler is responsible for pulling the configuration and implementing the desired state on the virtual machine. Even if there are installations that require reboots, DSC will continue the execution of the remaining scripts after reboot","title":"DSC"},{"location":"azure/azvm/#spot-instance","text":"These are low-priority VMs that are allocated from Azure\u2019s excess capacity in the region. Using spot instances can reduce the cost of instances rather than running them as regular instances.","title":"Spot instance"},{"location":"azure/azvm/#windows-setup-scripts","text":"Setupcomplete.cmd and ErrorHandler.cmd are custom scripts that run during or after the Windows Setup process. They can be used to install applications or run other tasks by using cscript/wscript scripts. %WINDIR%\\Setup\\Scripts\\SetupComplete.cmd : This script runs with local system permissions and starts immediately after the user sees the desktop. %WINDIR%\\Setup\\Scripts\\ErrorHandler.cmd : This script runs automatically when Setup encounters a fatal error. It runs with local system permission.","title":"Windows Setup Scripts"},{"location":"azure/azvm/#notes","text":"For a Windows VM , it can be up to 15 characters, and Linux VMs can support up to 64 characters. VM name can\u2019t be changed once the VM is deployed, except re-deployment. If you shut down the VM from the operating system \u2014for example, by clicking the Windows button and select Shutdown, then you will be charged for the VM as the hardware allocated is not released. You can use your existing licenses purchased from Software Assurance to cut down this cost. This reduction method is called Azure Hybrid Benefit (AHUB) . AHUB can be used for Windows, Linux (RHEL and SUSE), and SQL virtual machine licenses. You can connect to Windows machines using two options, namely, Remote Desktop Protocol (RDP) for GUI and Windows Remote Management (WinRM) for CLI The public key will end with the extension .pub , and the private key will not have any extension. ssh to vm using ssh -i <path-to-private-key> username@publicIP An availability set is free of cost, and you pay only for the instances that you are deploying. Uploads a virtual hard disk from an on-premises machine to Azure (managed disk or blob) using Add-AzVhd If you make a change to the topology of your network and have Windows VPN clients, the VPN client package for Windows clients must be downloaded and installed again in order for the changes to be applied to the client.] The Set-AzureStaticVNetIP cmdlet sets the static virtual network (VNet) IP address information for a virtual machine object. New-AzureADMSInvitation is used to invite a new external user to your directory.","title":"Notes"},{"location":"azure/blob/","text":"Blob storage \u00b6 What is Blob? \u00b6 Azure Blob Storage is Microsoft's object storage solution for the cloud. Blob Storage is optimized for storing massive amounts of unstructured data . Unstructured data is data that doesn't adhere to a particular data model or definition, such as text or binary data. How to access Blob? Users or client applications can access objects in Blob Storage via HTTP/HTTPS, from anywhere in the world. 3 kinds of blob storage Block : Variable size blocks, optimized for large blobs Page : fixed size blobs, 512 bytes for example. Optimized for random read/write Append : used for append operations Tip Blob Storage supports Azure Data Lake Storage Gen2, Microsoft's enterprise big data analytics solution for the cloud.","title":"Blob"},{"location":"azure/blob/#blob-storage","text":"","title":"Blob storage"},{"location":"azure/blob/#what-is-blob","text":"Azure Blob Storage is Microsoft's object storage solution for the cloud. Blob Storage is optimized for storing massive amounts of unstructured data . Unstructured data is data that doesn't adhere to a particular data model or definition, such as text or binary data. How to access Blob? Users or client applications can access objects in Blob Storage via HTTP/HTTPS, from anywhere in the world. 3 kinds of blob storage Block : Variable size blocks, optimized for large blobs Page : fixed size blobs, 512 bytes for example. Optimized for random read/write Append : used for append operations Tip Blob Storage supports Azure Data Lake Storage Gen2, Microsoft's enterprise big data analytics solution for the cloud.","title":"What is Blob?"},{"location":"azure/concepts/","text":"Private endpoint \u00b6 A private endpoint is a network interface that uses a private IP address from your virtual network. This network interface connects you privately and securely to a service that's powered by Azure Private Link . By enabling a private endpoint, you're bringing the service into your virtual network. The service could be an Azure service such as: - Azure Storage - Azure Cosmos DB - Azure SQL Database Redundancy in Azure \u00b6 These are the redundancy options in Azure LRS (Locally Redundant Storage) : In LRS, you will have 3 copies of data within a single data center in different fault domains. The copies will always be up-to-date, and all changes are written to the storage account synchronously . ZRS (Zonal Redundant Storage) : It saves one copy of an instance in 3 AZs of a region. GRS (Geo Redundant Storage) : Data is replicated to a secondary region which is selected by Azure based on the regional pairs . This isolation ensures that if the primary is affected due to any calamity, it shouldn\u2019t affect the secondary region. Caution to read data in GRS This method replicates the data to the secondary region ; however, the data in the secondary is not readable . The data can be read only if a failover is initiated to the secondary region. The failover can be initiated by the customer manually or by Microsoft in the case of a regional outage. Read-Access Geo Redundant Storage (RA-GRS) : In RA-GRS, the replication works in the same way as GRS. The key difference here is that you have the provision to read from the secondary region regardless of whether a failover is initiated. The secondary region is always available for read requests. More expensive? It is more expensive compared to GRS. GZRS (Geo Zone Redundant Storage) : In GZRS you replicate the data synchronously in the primary region using ZRS, and then this data is replicated to the secondary region (as 3 copies in the same AZ). SKU \u00b6 SKU is short for Stock-keeping-Unit . Each SKU is catered toward a specific scenario and has differences in scale, features, and pricing. In layman's language, SKU refers to an Item ready for sale. Azure supports both Basic and Standard SKU . For creating an AKS cluster, Azure uses Standard SKU. By utilizing the Standard SKU Load Balancer, Azure offers services like availability zones and larger backend pool sizes. Region Pairs \u00b6 Each Azure region is paired with another region within the same geography (such as US, Europe, or Asia). This approach allows for the replication of resources, such as VM storage, across geography that should reduce the likelihood of natural disasters, etc affecting both regions at once. Authorizing AZ Services \u00b6 SPN Auth \u00b6 The name by which a client uniquely identifies an instance of a service. If you install multiple instances of a service on computers throughout a forest, each instance must have its own SPN. A given service instance can have multiple SPNs if there are multiple names that clients might use for authentication This allows a client application to request that the service authenticate an account even if the client does not have the account name. If you install multiple instances of a service on computers throughout a forest, each instance must have its own SPN. A given service instance can have multiple SPNs if there are multiple names that clients might use for authentication. Did you registered your SPN? Before the Kerberos authentication service can use an SPN to authenticate a service, the SPN must be registered on the account object that the service instance uses to log on. A given SPN can be registered on only one account SAS URI \u00b6 SAS is a secure way to grant limited access to the resources in your storage account to the external world (clients, apps), without compromising your account keys. Shared Access Signature (SAS) is used for controlling access to blob, file, table, and queue storage containers. Never share the access key You would not want to share an access key since it is like a root password to all the containers existing within the Azure Storage account. A shared access signature (SAS) provides secure delegated access to resources in your storage account. It gives you granular control over the type of access you grant to clients, which includes - Interval \u2013 You can specify how long the SAS token should be valid by mentioning the start time and the expiry time. Permission \u2013 You can specify the permission at the granular level, for example, your clients just want to read the blob so grant them only read permission. IP Address \u2013 If you want Azure Storage Account to be accessed from a particular IP or range of IPs then you can specify an optional IP Address or range of IP addresses in your SAS token . Protocol \u2013 If you want the Azure Storage account to be accessed by either HTTPS or HTTP & HTTPS , you can specify the same in the SAS token. Types of shared access signatures \u00b6 Azure Storage supports three types of shared access signatures as shown below: User delegation SAS (Recommended) \u00b6 A user delegation SAS is secured with Azure Active Directory (Azure AD) credentials and also by the permissions specified for the SAS. A user delegation SAS applies to Blob storage only. Recommended approach A user delegation SAS provides superior security to a service SAS or an account SAS. A user delegation SAS is secured with Azure AD credentials so that you do not need to store your account key with your code. Service SAS \u00b6 A service SAS is secured with the storage account key . A service SAS delegates access to a resource in only one of the Azure Storage services: Blob Queue Table Files Account SAS \u00b6 An account SAS is secured with the storage account key . An account SAS delegates access to resources in one or more of the storage services. All the operations available via a service or user delegation SAS are also available via an account SAS. You can also delegate access to the following: Service-level operations (For example, the Get/Set Service Properties and Get Service Stats operations). Read, write, and delete operations that aren't permitted with a service SAS. Do you have access to the Account Key? Both a service SAS and an account SAS are signed with the storage account key. To create a SAS that is signed with the account key, an application must have access to the account key. SAS token \u00b6 What is SAS Token? The SAS token is a string that you generate on the client side, for example by using one of the Azure Storage client libraries. The SAS token is not tracked by Azure Storage in any way. You can create an unlimited number of SAS tokens on the client side. After you create a SAS, you can distribute it to client applications that require access to resources in your storage account. Client applications provide the SAS URI to Azure Storage as part of a request. Then, the service checks the SAS parameters and the signature to verify that it is valid. If the service verifies that the signature is valid, then the request is authorized. Otherwise, the request is declined with error code 403 (Forbidden). ACU \u00b6 The Azure Compute Unit (ACU) is used to help understand the relative compute performance between different Azure series and size VMs. It is based on the A0 (extra small) having a value of 50. A VM with an ACU of 100 has twice the compute of a VM with an ACU of 50. DR \u00b6 Various methods of DR are Backup: Its copy of business critical data Cold site: Little or no infra deployed Hot site: Exact copy of infra Difference between DR, HA and Backup RPO \u00b6 A recovery point objective (RPO) is the maximum length of time permitted that data can be restored from, which may or may not mean data loss. It is the age of the files or data in backup storage required to resume normal operations if a computer system or network failure occurs. Tldr RPO is the time from the last data backup until an incident occurred [that may have caused data loss] and RTO is the time that you set to recover the lost data. RTO \u00b6 The recovery time objective (RTO) is the targeted duration of time between the event of failure and the point where operations resume. SLA \u00b6 It is amount of time a service will be available. We need to use composite SLA as we have multiples services with different SLA's. MTTR \u00b6 It is the mean time a service will take to recover. Always On availability groups \u00b6 The Always On availability groups feature is a high-availability and disaster-recovery solution that provides an enterprise-level alternative to database mirroring. Introduced in SQL Server 2012 (11.x), Always On availability groups maximizes the availability of a set of user databases for an enterprise. An availability group supports a failover environment for a discrete set of user databases, known as availability databases, that fail over together. Number of availability replicas Each set of availability database is hosted by an availability replica. Two types of availability replicas exist: a single primary replica , which hosts the primary databases, and 1 to 8 secondary replicas , each of which hosts a set of secondary databases and serves as a potential failover targets for the availability group. Azure backup \u00b6 It is a managed service for backing up and recovering workloads. Azure File Backup can be used to automate the backup using a backup policy. Azure Snapshots can be used to take manual snapshots of your file share. Azure recovery service vault \u00b6 A traditional backup solution requires infrastructure to be deployed to host backup services; however, with the help of a lightweight agent , you will be able to back up your servers securely to the cloud. Regardless of whether the server is in Azure, on-premises , or in any other cloud provider, you can back up and save the data in Azure Recovery Services Vault . Backup types \u00b6 Various types of backups are: Full Differential Incremental Image credits: Microsoft Learn Full Backup \u00b6 A full backup contains the entire data source. Takes more network bandwidth than differential or incremental backups. It is used for initial backup Differential Backup \u00b6 A differential backup stores the blocks that changed since the initial full backup. Uses a smaller amount of network and storage, and doesn't keep redundant copies of unchanged data. It is not used by Azure Backup as its inefficient. Incremental Backup \u00b6 An incremental backup stores only the blocks of data that changed since the previous backup. High storage and network efficiency. Backup Process \u00b6 Backup for on-prem and Cloud is done as shown below: on-premises machines: \u00b6 You can back up on-premises Windows machines directly to Azure by using the Microsoft Azure Recovery Services (MARS) agent. Linux machines aren't supported. You can back up on-premises machines to a backup server - either System Center Data Protection Manager (DPM) or Microsoft Azure Backup Server (MABS) . You can then back up the backup server to a Recovery Services vault in Azure. Azure VMs: \u00b6 You can back up Azure VMs directly. Azure Backup installs a backup extension to the Azure VM agent that's running on the VM. This extension backs up the entire VM. How to backup specific files in VM You can back up specific files and folders on the Azure VM by running the MARS agent. Default replication used in Microsoft Azure Recovery Services (MARS) is ZRS MARS Agent \u00b6 It can run on individual on-premises Windows Server machines to back up files, folders, and the system state. Runs on Azure VMs to back up files, folders, and the system state. Runs on DPM/MABS servers to back up the DPM/MABS local storage disk to Azure. Azure Site recovery \u00b6 Azure Site Recovery (ASR) It is a disaster recovery solution that can be used to replicate your VMs to a secondary region and fire them up during a regional outage. You can fail over from the Azure portal, and the infrastructure will be created in a secondary region within a few minutes. Snapshots \u00b6 When it comes to development and testing, managed disk snapshots offer a reliable and simple option to back up your VMs. Snapshots are independent read-only copies of the managed disk and can be used to create new managed disks. Image \u00b6 Images will contain the OS disk and all the data disks that were part of the VM. Using this custom image, you can create hundreds of VMs Snapshot and Image difference? Snapshot applies to a single disk ; if you have a VM with multiple disks, then you need to create separate snapshots for each of these disks. In the case of images, all disks are taken into consideration while creating the image.","title":"Az Concepts"},{"location":"azure/concepts/#private-endpoint","text":"A private endpoint is a network interface that uses a private IP address from your virtual network. This network interface connects you privately and securely to a service that's powered by Azure Private Link . By enabling a private endpoint, you're bringing the service into your virtual network. The service could be an Azure service such as: - Azure Storage - Azure Cosmos DB - Azure SQL Database","title":"Private endpoint"},{"location":"azure/concepts/#redundancy-in-azure","text":"These are the redundancy options in Azure LRS (Locally Redundant Storage) : In LRS, you will have 3 copies of data within a single data center in different fault domains. The copies will always be up-to-date, and all changes are written to the storage account synchronously . ZRS (Zonal Redundant Storage) : It saves one copy of an instance in 3 AZs of a region. GRS (Geo Redundant Storage) : Data is replicated to a secondary region which is selected by Azure based on the regional pairs . This isolation ensures that if the primary is affected due to any calamity, it shouldn\u2019t affect the secondary region. Caution to read data in GRS This method replicates the data to the secondary region ; however, the data in the secondary is not readable . The data can be read only if a failover is initiated to the secondary region. The failover can be initiated by the customer manually or by Microsoft in the case of a regional outage. Read-Access Geo Redundant Storage (RA-GRS) : In RA-GRS, the replication works in the same way as GRS. The key difference here is that you have the provision to read from the secondary region regardless of whether a failover is initiated. The secondary region is always available for read requests. More expensive? It is more expensive compared to GRS. GZRS (Geo Zone Redundant Storage) : In GZRS you replicate the data synchronously in the primary region using ZRS, and then this data is replicated to the secondary region (as 3 copies in the same AZ).","title":"Redundancy in Azure"},{"location":"azure/concepts/#sku","text":"SKU is short for Stock-keeping-Unit . Each SKU is catered toward a specific scenario and has differences in scale, features, and pricing. In layman's language, SKU refers to an Item ready for sale. Azure supports both Basic and Standard SKU . For creating an AKS cluster, Azure uses Standard SKU. By utilizing the Standard SKU Load Balancer, Azure offers services like availability zones and larger backend pool sizes.","title":"SKU"},{"location":"azure/concepts/#region-pairs","text":"Each Azure region is paired with another region within the same geography (such as US, Europe, or Asia). This approach allows for the replication of resources, such as VM storage, across geography that should reduce the likelihood of natural disasters, etc affecting both regions at once.","title":"Region Pairs"},{"location":"azure/concepts/#authorizing-az-services","text":"","title":"Authorizing AZ Services"},{"location":"azure/concepts/#spn-auth","text":"The name by which a client uniquely identifies an instance of a service. If you install multiple instances of a service on computers throughout a forest, each instance must have its own SPN. A given service instance can have multiple SPNs if there are multiple names that clients might use for authentication This allows a client application to request that the service authenticate an account even if the client does not have the account name. If you install multiple instances of a service on computers throughout a forest, each instance must have its own SPN. A given service instance can have multiple SPNs if there are multiple names that clients might use for authentication. Did you registered your SPN? Before the Kerberos authentication service can use an SPN to authenticate a service, the SPN must be registered on the account object that the service instance uses to log on. A given SPN can be registered on only one account","title":"SPN Auth"},{"location":"azure/concepts/#sas-uri","text":"SAS is a secure way to grant limited access to the resources in your storage account to the external world (clients, apps), without compromising your account keys. Shared Access Signature (SAS) is used for controlling access to blob, file, table, and queue storage containers. Never share the access key You would not want to share an access key since it is like a root password to all the containers existing within the Azure Storage account. A shared access signature (SAS) provides secure delegated access to resources in your storage account. It gives you granular control over the type of access you grant to clients, which includes - Interval \u2013 You can specify how long the SAS token should be valid by mentioning the start time and the expiry time. Permission \u2013 You can specify the permission at the granular level, for example, your clients just want to read the blob so grant them only read permission. IP Address \u2013 If you want Azure Storage Account to be accessed from a particular IP or range of IPs then you can specify an optional IP Address or range of IP addresses in your SAS token . Protocol \u2013 If you want the Azure Storage account to be accessed by either HTTPS or HTTP & HTTPS , you can specify the same in the SAS token.","title":"SAS URI"},{"location":"azure/concepts/#types-of-shared-access-signatures","text":"Azure Storage supports three types of shared access signatures as shown below:","title":"Types of shared access signatures"},{"location":"azure/concepts/#user-delegation-sas-recommended","text":"A user delegation SAS is secured with Azure Active Directory (Azure AD) credentials and also by the permissions specified for the SAS. A user delegation SAS applies to Blob storage only. Recommended approach A user delegation SAS provides superior security to a service SAS or an account SAS. A user delegation SAS is secured with Azure AD credentials so that you do not need to store your account key with your code.","title":"User delegation SAS (Recommended)"},{"location":"azure/concepts/#service-sas","text":"A service SAS is secured with the storage account key . A service SAS delegates access to a resource in only one of the Azure Storage services: Blob Queue Table Files","title":"Service SAS"},{"location":"azure/concepts/#account-sas","text":"An account SAS is secured with the storage account key . An account SAS delegates access to resources in one or more of the storage services. All the operations available via a service or user delegation SAS are also available via an account SAS. You can also delegate access to the following: Service-level operations (For example, the Get/Set Service Properties and Get Service Stats operations). Read, write, and delete operations that aren't permitted with a service SAS. Do you have access to the Account Key? Both a service SAS and an account SAS are signed with the storage account key. To create a SAS that is signed with the account key, an application must have access to the account key.","title":"Account SAS"},{"location":"azure/concepts/#sas-token","text":"What is SAS Token? The SAS token is a string that you generate on the client side, for example by using one of the Azure Storage client libraries. The SAS token is not tracked by Azure Storage in any way. You can create an unlimited number of SAS tokens on the client side. After you create a SAS, you can distribute it to client applications that require access to resources in your storage account. Client applications provide the SAS URI to Azure Storage as part of a request. Then, the service checks the SAS parameters and the signature to verify that it is valid. If the service verifies that the signature is valid, then the request is authorized. Otherwise, the request is declined with error code 403 (Forbidden).","title":"SAS token"},{"location":"azure/concepts/#acu","text":"The Azure Compute Unit (ACU) is used to help understand the relative compute performance between different Azure series and size VMs. It is based on the A0 (extra small) having a value of 50. A VM with an ACU of 100 has twice the compute of a VM with an ACU of 50.","title":"ACU"},{"location":"azure/concepts/#dr","text":"Various methods of DR are Backup: Its copy of business critical data Cold site: Little or no infra deployed Hot site: Exact copy of infra Difference between DR, HA and Backup","title":"DR"},{"location":"azure/concepts/#rpo","text":"A recovery point objective (RPO) is the maximum length of time permitted that data can be restored from, which may or may not mean data loss. It is the age of the files or data in backup storage required to resume normal operations if a computer system or network failure occurs. Tldr RPO is the time from the last data backup until an incident occurred [that may have caused data loss] and RTO is the time that you set to recover the lost data.","title":"RPO"},{"location":"azure/concepts/#rto","text":"The recovery time objective (RTO) is the targeted duration of time between the event of failure and the point where operations resume.","title":"RTO"},{"location":"azure/concepts/#sla","text":"It is amount of time a service will be available. We need to use composite SLA as we have multiples services with different SLA's.","title":"SLA"},{"location":"azure/concepts/#mttr","text":"It is the mean time a service will take to recover.","title":"MTTR"},{"location":"azure/concepts/#always-on-availability-groups","text":"The Always On availability groups feature is a high-availability and disaster-recovery solution that provides an enterprise-level alternative to database mirroring. Introduced in SQL Server 2012 (11.x), Always On availability groups maximizes the availability of a set of user databases for an enterprise. An availability group supports a failover environment for a discrete set of user databases, known as availability databases, that fail over together. Number of availability replicas Each set of availability database is hosted by an availability replica. Two types of availability replicas exist: a single primary replica , which hosts the primary databases, and 1 to 8 secondary replicas , each of which hosts a set of secondary databases and serves as a potential failover targets for the availability group.","title":"Always On availability groups"},{"location":"azure/concepts/#azure-backup","text":"It is a managed service for backing up and recovering workloads. Azure File Backup can be used to automate the backup using a backup policy. Azure Snapshots can be used to take manual snapshots of your file share.","title":"Azure backup"},{"location":"azure/concepts/#azure-recovery-service-vault","text":"A traditional backup solution requires infrastructure to be deployed to host backup services; however, with the help of a lightweight agent , you will be able to back up your servers securely to the cloud. Regardless of whether the server is in Azure, on-premises , or in any other cloud provider, you can back up and save the data in Azure Recovery Services Vault .","title":"Azure recovery service vault"},{"location":"azure/concepts/#backup-types","text":"Various types of backups are: Full Differential Incremental Image credits: Microsoft Learn","title":"Backup types"},{"location":"azure/concepts/#full-backup","text":"A full backup contains the entire data source. Takes more network bandwidth than differential or incremental backups. It is used for initial backup","title":"Full Backup"},{"location":"azure/concepts/#differential-backup","text":"A differential backup stores the blocks that changed since the initial full backup. Uses a smaller amount of network and storage, and doesn't keep redundant copies of unchanged data. It is not used by Azure Backup as its inefficient.","title":"Differential Backup"},{"location":"azure/concepts/#incremental-backup","text":"An incremental backup stores only the blocks of data that changed since the previous backup. High storage and network efficiency.","title":"Incremental Backup"},{"location":"azure/concepts/#backup-process","text":"Backup for on-prem and Cloud is done as shown below:","title":"Backup Process"},{"location":"azure/concepts/#on-premises-machines","text":"You can back up on-premises Windows machines directly to Azure by using the Microsoft Azure Recovery Services (MARS) agent. Linux machines aren't supported. You can back up on-premises machines to a backup server - either System Center Data Protection Manager (DPM) or Microsoft Azure Backup Server (MABS) . You can then back up the backup server to a Recovery Services vault in Azure.","title":"on-premises machines:"},{"location":"azure/concepts/#azure-vms","text":"You can back up Azure VMs directly. Azure Backup installs a backup extension to the Azure VM agent that's running on the VM. This extension backs up the entire VM. How to backup specific files in VM You can back up specific files and folders on the Azure VM by running the MARS agent. Default replication used in Microsoft Azure Recovery Services (MARS) is ZRS","title":"Azure VMs:"},{"location":"azure/concepts/#mars-agent","text":"It can run on individual on-premises Windows Server machines to back up files, folders, and the system state. Runs on Azure VMs to back up files, folders, and the system state. Runs on DPM/MABS servers to back up the DPM/MABS local storage disk to Azure.","title":"MARS Agent"},{"location":"azure/concepts/#azure-site-recovery","text":"Azure Site Recovery (ASR) It is a disaster recovery solution that can be used to replicate your VMs to a secondary region and fire them up during a regional outage. You can fail over from the Azure portal, and the infrastructure will be created in a secondary region within a few minutes.","title":"Azure Site recovery"},{"location":"azure/concepts/#snapshots","text":"When it comes to development and testing, managed disk snapshots offer a reliable and simple option to back up your VMs. Snapshots are independent read-only copies of the managed disk and can be used to create new managed disks.","title":"Snapshots"},{"location":"azure/concepts/#image","text":"Images will contain the OS disk and all the data disks that were part of the VM. Using this custom image, you can create hundreds of VMs Snapshot and Image difference? Snapshot applies to a single disk ; if you have a VM with multiple disks, then you need to create separate snapshots for each of these disks. In the case of images, all disks are taken into consideration while creating the image.","title":"Image"},{"location":"azure/devops/","text":"Azure Devops \u00b6 Azure Pipelines supports continuous integration (CI) and continuous delivery (CD) to continuously test, build, and deploy your code. You accomplish this by defining a pipeline. What is CI/CD? Continuous integration (CI) automates tests and builds for your project. CI helps to catch bugs or issues early in the development cycle, when they're easier and faster to fix. Items known as artifacts are produced from CI systems. They're used by the continuous delivery release pipelines to drive automatic deployments. Continuous delivery automatically deploys and tests code in multiple stages to help drive quality. Pipelines in Az \u00b6 The Azure cloud provides several types of pipeline, each with a different purpose. The following table lists the different pipelines and what they're used for: AML Pipelines : Used for Model orchestration (Machine learning) by DS ADF Pipelines : Used for Data orchestration (Data prep) by DE Azure Pipelines : Used for Code & app orchestration by develper Concepts \u00b6 Trigger \u00b6 A trigger tells a Pipeline to run. Examples: build triggers and release triggers Pipeline \u00b6 A pipeline is made up of one or more stages. A pipeline can deploy to one or more environments. Stage \u00b6 A stage is a way of organizing jobs in a pipeline and each stage can have one or more jobs. Tldr A stage is a logical boundary in the pipeline. It can be used to mark separation of concerns (for example, Build, QA, and production). Each stage contains one or more jobs. When you define multiple stages in a pipeline, by default, they run one after the other. You can specify the conditions for when a stage runs. Job \u00b6 Each job runs on one agent. A job can also be agentless. Tip Each job runs on an agent. A job represents an execution boundary of a set of steps. All of the steps run together on the same agent. Jobs are most useful when you want to run a series of steps in different environments Agent \u00b6 Each agent runs a job that contains one or more steps. An agent is computing infrastructure with installed agent software that runs one job at a time. For example, your job could run on a Microsoft-hosted Ubuntu agent. Remember The agent communicates with Azure Pipelines or Azure DevOps Server to determine which job it needs to run, and to report the logs and job status. This communication is always initiated by the agent. There are 2 types of Agents as explained below: Microsoft Hosted agents \u00b6 If your pipelines are in Azure Pipelines, then you've got a convenient option to run your jobs using a Microsoft-hosted agent. Managed solution With Microsoft-hosted agents, maintenance and upgrades are taken care of for you. Each time you run a pipeline, you get a fresh virtual machine for each job in the pipeline. The virtual machine is discarded after one job (which means any change that a job makes to the virtual machine file system, such as checking out code, will be unavailable to the next job). Microsoft-hosted agents can run jobs directly on the VM or in a container. Self hosted agent \u00b6 An agent that you set up and manage on your own to run jobs is a self-hosted agent. Self-hosted agents give you more control to install dependent software needed for your builds and deployments Also, machine-level caches and configuration persist from run to run, which can boost speed. When to use self hosted agents? Below are the scenarios in which you should look at configuring Self-Hosted Agents If 10GB free space in the Virtual Machine (Agent) is not sufficient for your build needs. When you want a Virtual Machine , whose capacity is greater than of Standard DS2V2 When you would like to use a Software that is not available in the Microsoft hosted Build Agents Agent modes \u00b6 You can run your self-hosted agent as either Service Interactive process. After you've configured the agent, we recommend you first try it in interactive mode to make sure it works. Step \u00b6 A step can be a task or script and is the smallest building block of a pipeline . Task \u00b6 A task is a pre-packaged script that performs an action, such as invoking a REST API or publishing a build - artifact. Artifact \u00b6 An artifact is a collection of files or packages published by a run. Run \u00b6 A run represents one execution of a pipeline. It collects the logs associated with running the steps and the results of running tests. During a run, Azure Pipelines will first process the pipeline and then send the run to one or more agents. Each agent will run jobs. Agent pools \u00b6 An agent pool is a collection of agents. Instead of managing each agent individually, you organize agents into agent pools. When you configure an agent, it is registered with a single pool, and when you create a pipeline, you specify the pool in which the pipeline runs. When you run the pipeline, it runs on an agent from that pool that meets the demands of the pipeline. PAT (Personal Access Token) \u00b6 Generate and use a PAT to connect an agent with Azure Pipelines. PAT is the only scheme that works with Azure Pipelines. The PAT must have Agent Pools (read, manage) scope (for a deployment group agent, the PAT must have Deployment group (read, manage) scope), and while a single PAT can be used for registering multiple agents, the PAT is used only at the time of registering the agent, and not for subsequent communication.","title":"Azure Devops"},{"location":"azure/devops/#azure-devops","text":"Azure Pipelines supports continuous integration (CI) and continuous delivery (CD) to continuously test, build, and deploy your code. You accomplish this by defining a pipeline. What is CI/CD? Continuous integration (CI) automates tests and builds for your project. CI helps to catch bugs or issues early in the development cycle, when they're easier and faster to fix. Items known as artifacts are produced from CI systems. They're used by the continuous delivery release pipelines to drive automatic deployments. Continuous delivery automatically deploys and tests code in multiple stages to help drive quality.","title":"Azure Devops"},{"location":"azure/devops/#pipelines-in-az","text":"The Azure cloud provides several types of pipeline, each with a different purpose. The following table lists the different pipelines and what they're used for: AML Pipelines : Used for Model orchestration (Machine learning) by DS ADF Pipelines : Used for Data orchestration (Data prep) by DE Azure Pipelines : Used for Code & app orchestration by develper","title":"Pipelines in Az"},{"location":"azure/devops/#concepts","text":"","title":"Concepts"},{"location":"azure/devops/#trigger","text":"A trigger tells a Pipeline to run. Examples: build triggers and release triggers","title":"Trigger"},{"location":"azure/devops/#pipeline","text":"A pipeline is made up of one or more stages. A pipeline can deploy to one or more environments.","title":"Pipeline"},{"location":"azure/devops/#stage","text":"A stage is a way of organizing jobs in a pipeline and each stage can have one or more jobs. Tldr A stage is a logical boundary in the pipeline. It can be used to mark separation of concerns (for example, Build, QA, and production). Each stage contains one or more jobs. When you define multiple stages in a pipeline, by default, they run one after the other. You can specify the conditions for when a stage runs.","title":"Stage"},{"location":"azure/devops/#job","text":"Each job runs on one agent. A job can also be agentless. Tip Each job runs on an agent. A job represents an execution boundary of a set of steps. All of the steps run together on the same agent. Jobs are most useful when you want to run a series of steps in different environments","title":"Job"},{"location":"azure/devops/#agent","text":"Each agent runs a job that contains one or more steps. An agent is computing infrastructure with installed agent software that runs one job at a time. For example, your job could run on a Microsoft-hosted Ubuntu agent. Remember The agent communicates with Azure Pipelines or Azure DevOps Server to determine which job it needs to run, and to report the logs and job status. This communication is always initiated by the agent. There are 2 types of Agents as explained below:","title":"Agent"},{"location":"azure/devops/#microsoft-hosted-agents","text":"If your pipelines are in Azure Pipelines, then you've got a convenient option to run your jobs using a Microsoft-hosted agent. Managed solution With Microsoft-hosted agents, maintenance and upgrades are taken care of for you. Each time you run a pipeline, you get a fresh virtual machine for each job in the pipeline. The virtual machine is discarded after one job (which means any change that a job makes to the virtual machine file system, such as checking out code, will be unavailable to the next job). Microsoft-hosted agents can run jobs directly on the VM or in a container.","title":"Microsoft Hosted agents"},{"location":"azure/devops/#self-hosted-agent","text":"An agent that you set up and manage on your own to run jobs is a self-hosted agent. Self-hosted agents give you more control to install dependent software needed for your builds and deployments Also, machine-level caches and configuration persist from run to run, which can boost speed. When to use self hosted agents? Below are the scenarios in which you should look at configuring Self-Hosted Agents If 10GB free space in the Virtual Machine (Agent) is not sufficient for your build needs. When you want a Virtual Machine , whose capacity is greater than of Standard DS2V2 When you would like to use a Software that is not available in the Microsoft hosted Build Agents","title":"Self hosted agent"},{"location":"azure/devops/#agent-modes","text":"You can run your self-hosted agent as either Service Interactive process. After you've configured the agent, we recommend you first try it in interactive mode to make sure it works.","title":"Agent modes"},{"location":"azure/devops/#step","text":"A step can be a task or script and is the smallest building block of a pipeline .","title":"Step"},{"location":"azure/devops/#task","text":"A task is a pre-packaged script that performs an action, such as invoking a REST API or publishing a build - artifact.","title":"Task"},{"location":"azure/devops/#artifact","text":"An artifact is a collection of files or packages published by a run.","title":"Artifact"},{"location":"azure/devops/#run","text":"A run represents one execution of a pipeline. It collects the logs associated with running the steps and the results of running tests. During a run, Azure Pipelines will first process the pipeline and then send the run to one or more agents. Each agent will run jobs.","title":"Run"},{"location":"azure/devops/#agent-pools","text":"An agent pool is a collection of agents. Instead of managing each agent individually, you organize agents into agent pools. When you configure an agent, it is registered with a single pool, and when you create a pipeline, you specify the pool in which the pipeline runs. When you run the pipeline, it runs on an agent from that pool that meets the demands of the pipeline.","title":"Agent pools"},{"location":"azure/devops/#pat-personal-access-token","text":"Generate and use a PAT to connect an agent with Azure Pipelines. PAT is the only scheme that works with Azure Pipelines. The PAT must have Agent Pools (read, manage) scope (for a deployment group agent, the PAT must have Deployment group (read, manage) scope), and while a single PAT can be used for registering multiple agents, the PAT is used only at the time of registering the agent, and not for subsequent communication.","title":"PAT (Personal Access Token)"},{"location":"azure/dns/","text":"DNS \u260e\ufe0f \u00b6 In Azure, Azure DNS is used to host DNS zones for providing name resolution . By using Azure DNS, we will be able to manage zone and records in the same way we used to do in on-premises; however, the only difference is that everything is managed from the Azure portal How it ensures relability? DNS domains in Azure DNS are hosted on Azure's global network of DNS name servers, providing resiliency and HA. Azure DNS uses anycast networking , so each DNS query is answered by the closest available DNS server. Another most asked question is: Can I use my own domain name? Azure DNS also supports private DNS domains so that you can use your own custom domain names rather than being stuck with the Azure-provided names . DNS query resolve \u00b6 This is done in 4 steps as shown below DNS recursor/resolver - The DNS recursor is a server designed to receive queries from client machines through applications such as web browsers. Typically the recursor is then responsible for making additional requests in order to satisfy the client\u2019s DNS query. After receiving a DNS query from a web client, a recursive resolver will either respond with cached data, OR send a request to a root nameserver, followed by another request to a TLD nameserver, and then one last request to an authoritative nameserver. Root nameserver - A root server accepts a recursive resolver's query which includes a domain name, and the root nameserver responds by directing the recursive resolver to a TLD nameserver, based on the extension of that domain ( .com, .net, .org, etc .). TLD nameserver - The top level domain server hosts the last portion of a hostname (In example.com, the TLD server is \u201ccom\u201d). Authoritative nameserver - The authoritative nameserver is the last stop in the nameserver query. If the authoritative name server has access to the requested record, it will return the IP address for the requested hostname back to the DNS Recursor/Resolver that made the initial request. Various steps taken to resovle the DNS quey are shown below Basics \u00b6 DNS zones \u00b6 A DNS zone is used to host the DNS records for a particular domain. To start hosting your domain in Azure DNS, you need to create a DNS zone for that domain name. Each DNS record for your domain is then created inside this DNS zone. How zone is managed? These zones differentiate between distinctly managed areas in the DNS namespace. A DNS zone is a portion of the DNS namespace that is managed by a specific organization or administrator . Zone files must always start with a Start of Authority (SOA) record , which contains important information including contact information for the zone administrator. These are of 2 types: public and private Public Zone \u00b6 DNS resolver can be accessed from public internet. Private Zone \u00b6 The records contained in a private DNS zone aren't resolvable from the Internet. DNS resolution against a private DNS zone works only from virtual networks that are linked to it using private network link Auto registration in private DNS When you link a virtual network with a private DNS zone with auto registration setting enabled, a DNS record gets created for each virtual machine deployed in the virtual network. TTL \u00b6 The time to live, or TTL, specifies how long each record is cached by clients before being requeried. In the below example, the TTL is 3600 seconds or 1 hour . record set example www . amarjitdhillon . com 3600 IN A 134.170.185.46 Record set \u00b6 Azure DNS manages all DNS records using record sets. A record set (also known as a resource record set) is the collection of DNS records in a zone that have the same name and are of the same type. Most record sets contain a single record. record set example www . amarjitdhillon . com 3600 IN A 134.170.185.46 www . amarjitdhillon . com 3600 IN A 134.170.188.221 Exception in record set The SOA and CNAME record types are exceptions. The DNS standards don't permit multiple records with the same name for these types, therefore these record sets can only contain a single record. DNS Records \u00b6 Azure DNS supports all common DNS record types including A, AAAA, MX, CAA, CNAME, PTR, SOA, SRV, and TXT records CNAME \u00b6 What is CNAME? \u00b6 When the \"www\" subdomain is set to be an alias for the root domain name, a subdomain like www.samplesite.webname.com will have a CNAME record that points to the root domain webname.com. CNAME Example \u00b6 Suppose blog.adhillon.com has a CNAME record with a value of adhillon.com (without the \u2018blog\u2019). This means when a DNS server hits the DNS records for blog.adhillon.com , it actually triggers another DNS lookup to adhillon.com , returning adhillon.com\u2019s IP address via its A record. In this case we would say that adhillon.com is the canonical name (or true name) of blog.adhillon.com IP update scenario Oftentimes, when sites have subdomains such as blog.adhillon.com or test.adhillon.com , those subdomains will have CNAME records that point to a root domain ( adhillon.com ). This way if the IP address of the host changes, only the DNS A record for the root domain needs to be updated and all the CNAME records will follow along with whatever changes are made to the root. Here are some of the cases in which CNAME has to be used: Uses of CNAME Records To send visitors from several websites owned by the same person or group to the main website To give each network service, such as File Transfer Protocol (FTP) or email, its own hostname and point it to the root domain To give each customer a subdomain on the domain of a single service provider and use CNAME to point the subdomain to the customer's root domain To register the same domain in more than one country and point the versions for each country to the main domain. CAA records \u00b6 CAA records allow domain owners to specify which Certificate Authorities (CAs) are authorized to issue certificates for their domain. This record allows CAs to avoid mis-issuing certificates in some circumstances CAA record for AWS 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 $ dig CAA aws.amazon.com ; <<>> DiG 9 .10.6 <<>> CAA aws.amazon.com ;; global options: +cmd ;; Got answer: ;; ->>HEADER <<- opco de: QUERY, status: NOERROR, id: 33510 ;; flags: qr rd ra ; QUERY: 1 , ANSWER: 2 , AUTHORITY: 1 , ADDITIONAL: 1 ;; OPT PSEUDOSECTION: ; EDNS: version: 0 , flags: ; udp: 512 ;; QUESTION SECTION: ; aws.amazon.com. IN CAA ;; ANSWER SECTION: aws.amazon.com. 31 IN CNAME tp.8e49140c2-frontier.amazon.com. tp.8e49140c2-frontier.amazon.com. 60 IN CNAME dr49lng3n1n2s.cloudfront.net. ;; AUTHORITY SECTION: dr49lng3n1n2s.cloudfront.net. 60 IN SOA ns-905.awsdns-49.net. awsdns-hostmaster.amazon.com. 1 7200 900 1209600 86400 ;; Query time: 46 msec ;; SERVER: 8 .8.8.8#53 ( 8 .8.8.8 ) ;; WHEN: Thu Dec 29 00 :39:57 EST 2022 ;; MSG SIZE rcvd: 192 MX record \u00b6 A DNS mail exchange (MX) record directs email to a mail server. The MX record indicates how email messages should be routed in accordance with the Simple Mail Transfer Protocol (SMTP, the standard protocol for all email). check MX record for Google $ dig MX google.com ; <<>> DiG 9 .10.6 <<>> MX google.com ;; global options: +cmd ;; Got answer: ;; ->>HEADER <<- opco de: QUERY, status: NOERROR, id: 1277 ;; flags: qr rd ra ; QUERY: 1 , ANSWER: 1 , AUTHORITY: 0 , ADDITIONAL: 1 ;; OPT PSEUDOSECTION: ; EDNS: version: 0 , flags: ; udp: 512 ;; QUESTION SECTION: ; google.com. IN MX ;; ANSWER SECTION: google.com. 222 IN MX 10 smtp.google.com. ;; Query time: 40 msec ;; SERVER: 8 .8.8.8#53 ( 8 .8.8.8 ) ;; WHEN: Thu Dec 29 00 :37:31 EST 2022 ;; MSG SIZE rcvd: 60 NS record \u00b6 NS stands for \u2018nameserver,\u2019 and the nameserver record indicates which DNS server is authoritative for that domain (i.e. which server contains the actual DNS records). Basically, NS records tell the Internet where to go to find out a domain's IP address. A domain often has multiple NS records which can indicate primary and secondary nameservers for that domain. Do you need NS record for your website? Without properly configured NS records, users will be unable to load a website or application. A record \u00b6 The \"A\" stands for \"address\" and this is the most fundamental type of DNS record: it indicates the IP address of a given domain. It allows you to use memonic names, such as www.amarjitdhillon.com , in place of IP addresses like 162.0.232.222 A records only hold IPv4 addresses . If a website has an IPv6 address, it will instead use an AAAA record . Use dig command to find A record like shown below using dig to find dns records $ dig amarjitdhillon.com ; <<>> DiG 9 .10.6 <<>> amarjitdhillon.com ;; global options: +cmd ;; Got answer: ;; ->>HEADER <<- opco de: QUERY, status: NOERROR, id: 25476 ;; flags: qr rd ra ; QUERY: 1 , ANSWER: 1 , AUTHORITY: 0 , ADDITIONAL: 1 ;; OPT PSEUDOSECTION: ; EDNS: version: 0 , flags: ; udp: 512 ;; QUESTION SECTION: ; amarjitdhillon.com. IN A ;; ANSWER SECTION: amarjitdhillon.com. 1200 IN A 162 .0.232.222 ;; Query time: 48 msec ;; SERVER: 8 .8.8.8#53 ( 8 .8.8.8 ) ;; WHEN: Thu Dec 29 00 :33:13 EST 2022 ;; MSG SIZE rcvd: 63 We can also get the IP using the nslookup as shown below $ nslookup amarjitdhillon.com Server: 8 .8.8.8 Address: 8 .8.8.8#53 Non-authoritative answer: Name: amarjitdhillon.com Address: 162 .0.232.222 AAAA record \u00b6 DNS AAAA records match a domain name to an IPv6 address. DNS AAAA records are exactly like DNS A records , except that they store a domain's IPv6 address instead of its IPv4 address. TXT record \u00b6 This record is used to associate text with a domain. SOA (Start of authority) \u00b6 The DNS \u2018start of authority\u2019 (SOA) record stores important information about a domain or zone such as the email address of the administrator, when the domain was last updated, and how long the server should wait between refreshes. $ nslookup -type = SOA contoso.com Server: 8 .8.8.8 Address: 8 .8.8.8#53 Non-authoritative answer: contoso.com origin = ns1-205.azure-dns.com mail addr = azuredns-hostmaster.microsoft.com serial = 1 refresh = 3600 retry = 300 expire = 2419200 minimum = 300 CERT (Certificate record) \u00b6 It stores public key certificates. SRV (Service Location record) \u00b6 It is used to specify a port for specific services.","title":"DNS \u260e\ufe0f"},{"location":"azure/dns/#dns","text":"In Azure, Azure DNS is used to host DNS zones for providing name resolution . By using Azure DNS, we will be able to manage zone and records in the same way we used to do in on-premises; however, the only difference is that everything is managed from the Azure portal How it ensures relability? DNS domains in Azure DNS are hosted on Azure's global network of DNS name servers, providing resiliency and HA. Azure DNS uses anycast networking , so each DNS query is answered by the closest available DNS server. Another most asked question is: Can I use my own domain name? Azure DNS also supports private DNS domains so that you can use your own custom domain names rather than being stuck with the Azure-provided names .","title":"DNS \u260e\ufe0f"},{"location":"azure/dns/#dns-query-resolve","text":"This is done in 4 steps as shown below DNS recursor/resolver - The DNS recursor is a server designed to receive queries from client machines through applications such as web browsers. Typically the recursor is then responsible for making additional requests in order to satisfy the client\u2019s DNS query. After receiving a DNS query from a web client, a recursive resolver will either respond with cached data, OR send a request to a root nameserver, followed by another request to a TLD nameserver, and then one last request to an authoritative nameserver. Root nameserver - A root server accepts a recursive resolver's query which includes a domain name, and the root nameserver responds by directing the recursive resolver to a TLD nameserver, based on the extension of that domain ( .com, .net, .org, etc .). TLD nameserver - The top level domain server hosts the last portion of a hostname (In example.com, the TLD server is \u201ccom\u201d). Authoritative nameserver - The authoritative nameserver is the last stop in the nameserver query. If the authoritative name server has access to the requested record, it will return the IP address for the requested hostname back to the DNS Recursor/Resolver that made the initial request. Various steps taken to resovle the DNS quey are shown below","title":"DNS query resolve"},{"location":"azure/dns/#basics","text":"","title":"Basics"},{"location":"azure/dns/#dns-zones","text":"A DNS zone is used to host the DNS records for a particular domain. To start hosting your domain in Azure DNS, you need to create a DNS zone for that domain name. Each DNS record for your domain is then created inside this DNS zone. How zone is managed? These zones differentiate between distinctly managed areas in the DNS namespace. A DNS zone is a portion of the DNS namespace that is managed by a specific organization or administrator . Zone files must always start with a Start of Authority (SOA) record , which contains important information including contact information for the zone administrator. These are of 2 types: public and private","title":"DNS zones"},{"location":"azure/dns/#public-zone","text":"DNS resolver can be accessed from public internet.","title":"Public Zone"},{"location":"azure/dns/#private-zone","text":"The records contained in a private DNS zone aren't resolvable from the Internet. DNS resolution against a private DNS zone works only from virtual networks that are linked to it using private network link Auto registration in private DNS When you link a virtual network with a private DNS zone with auto registration setting enabled, a DNS record gets created for each virtual machine deployed in the virtual network.","title":"Private Zone"},{"location":"azure/dns/#ttl","text":"The time to live, or TTL, specifies how long each record is cached by clients before being requeried. In the below example, the TTL is 3600 seconds or 1 hour . record set example www . amarjitdhillon . com 3600 IN A 134.170.185.46","title":"TTL"},{"location":"azure/dns/#record-set","text":"Azure DNS manages all DNS records using record sets. A record set (also known as a resource record set) is the collection of DNS records in a zone that have the same name and are of the same type. Most record sets contain a single record. record set example www . amarjitdhillon . com 3600 IN A 134.170.185.46 www . amarjitdhillon . com 3600 IN A 134.170.188.221 Exception in record set The SOA and CNAME record types are exceptions. The DNS standards don't permit multiple records with the same name for these types, therefore these record sets can only contain a single record.","title":"Record set"},{"location":"azure/dns/#dns-records","text":"Azure DNS supports all common DNS record types including A, AAAA, MX, CAA, CNAME, PTR, SOA, SRV, and TXT records","title":"DNS Records"},{"location":"azure/dns/#cname","text":"","title":"CNAME"},{"location":"azure/dns/#what-is-cname","text":"When the \"www\" subdomain is set to be an alias for the root domain name, a subdomain like www.samplesite.webname.com will have a CNAME record that points to the root domain webname.com.","title":"What is CNAME?"},{"location":"azure/dns/#cname-example","text":"Suppose blog.adhillon.com has a CNAME record with a value of adhillon.com (without the \u2018blog\u2019). This means when a DNS server hits the DNS records for blog.adhillon.com , it actually triggers another DNS lookup to adhillon.com , returning adhillon.com\u2019s IP address via its A record. In this case we would say that adhillon.com is the canonical name (or true name) of blog.adhillon.com IP update scenario Oftentimes, when sites have subdomains such as blog.adhillon.com or test.adhillon.com , those subdomains will have CNAME records that point to a root domain ( adhillon.com ). This way if the IP address of the host changes, only the DNS A record for the root domain needs to be updated and all the CNAME records will follow along with whatever changes are made to the root. Here are some of the cases in which CNAME has to be used: Uses of CNAME Records To send visitors from several websites owned by the same person or group to the main website To give each network service, such as File Transfer Protocol (FTP) or email, its own hostname and point it to the root domain To give each customer a subdomain on the domain of a single service provider and use CNAME to point the subdomain to the customer's root domain To register the same domain in more than one country and point the versions for each country to the main domain.","title":"CNAME Example"},{"location":"azure/dns/#caa-records","text":"CAA records allow domain owners to specify which Certificate Authorities (CAs) are authorized to issue certificates for their domain. This record allows CAs to avoid mis-issuing certificates in some circumstances CAA record for AWS 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 $ dig CAA aws.amazon.com ; <<>> DiG 9 .10.6 <<>> CAA aws.amazon.com ;; global options: +cmd ;; Got answer: ;; ->>HEADER <<- opco de: QUERY, status: NOERROR, id: 33510 ;; flags: qr rd ra ; QUERY: 1 , ANSWER: 2 , AUTHORITY: 1 , ADDITIONAL: 1 ;; OPT PSEUDOSECTION: ; EDNS: version: 0 , flags: ; udp: 512 ;; QUESTION SECTION: ; aws.amazon.com. IN CAA ;; ANSWER SECTION: aws.amazon.com. 31 IN CNAME tp.8e49140c2-frontier.amazon.com. tp.8e49140c2-frontier.amazon.com. 60 IN CNAME dr49lng3n1n2s.cloudfront.net. ;; AUTHORITY SECTION: dr49lng3n1n2s.cloudfront.net. 60 IN SOA ns-905.awsdns-49.net. awsdns-hostmaster.amazon.com. 1 7200 900 1209600 86400 ;; Query time: 46 msec ;; SERVER: 8 .8.8.8#53 ( 8 .8.8.8 ) ;; WHEN: Thu Dec 29 00 :39:57 EST 2022 ;; MSG SIZE rcvd: 192","title":"CAA records"},{"location":"azure/dns/#mx-record","text":"A DNS mail exchange (MX) record directs email to a mail server. The MX record indicates how email messages should be routed in accordance with the Simple Mail Transfer Protocol (SMTP, the standard protocol for all email). check MX record for Google $ dig MX google.com ; <<>> DiG 9 .10.6 <<>> MX google.com ;; global options: +cmd ;; Got answer: ;; ->>HEADER <<- opco de: QUERY, status: NOERROR, id: 1277 ;; flags: qr rd ra ; QUERY: 1 , ANSWER: 1 , AUTHORITY: 0 , ADDITIONAL: 1 ;; OPT PSEUDOSECTION: ; EDNS: version: 0 , flags: ; udp: 512 ;; QUESTION SECTION: ; google.com. IN MX ;; ANSWER SECTION: google.com. 222 IN MX 10 smtp.google.com. ;; Query time: 40 msec ;; SERVER: 8 .8.8.8#53 ( 8 .8.8.8 ) ;; WHEN: Thu Dec 29 00 :37:31 EST 2022 ;; MSG SIZE rcvd: 60","title":"MX record"},{"location":"azure/dns/#ns-record","text":"NS stands for \u2018nameserver,\u2019 and the nameserver record indicates which DNS server is authoritative for that domain (i.e. which server contains the actual DNS records). Basically, NS records tell the Internet where to go to find out a domain's IP address. A domain often has multiple NS records which can indicate primary and secondary nameservers for that domain. Do you need NS record for your website? Without properly configured NS records, users will be unable to load a website or application.","title":"NS record"},{"location":"azure/dns/#a-record","text":"The \"A\" stands for \"address\" and this is the most fundamental type of DNS record: it indicates the IP address of a given domain. It allows you to use memonic names, such as www.amarjitdhillon.com , in place of IP addresses like 162.0.232.222 A records only hold IPv4 addresses . If a website has an IPv6 address, it will instead use an AAAA record . Use dig command to find A record like shown below using dig to find dns records $ dig amarjitdhillon.com ; <<>> DiG 9 .10.6 <<>> amarjitdhillon.com ;; global options: +cmd ;; Got answer: ;; ->>HEADER <<- opco de: QUERY, status: NOERROR, id: 25476 ;; flags: qr rd ra ; QUERY: 1 , ANSWER: 1 , AUTHORITY: 0 , ADDITIONAL: 1 ;; OPT PSEUDOSECTION: ; EDNS: version: 0 , flags: ; udp: 512 ;; QUESTION SECTION: ; amarjitdhillon.com. IN A ;; ANSWER SECTION: amarjitdhillon.com. 1200 IN A 162 .0.232.222 ;; Query time: 48 msec ;; SERVER: 8 .8.8.8#53 ( 8 .8.8.8 ) ;; WHEN: Thu Dec 29 00 :33:13 EST 2022 ;; MSG SIZE rcvd: 63 We can also get the IP using the nslookup as shown below $ nslookup amarjitdhillon.com Server: 8 .8.8.8 Address: 8 .8.8.8#53 Non-authoritative answer: Name: amarjitdhillon.com Address: 162 .0.232.222","title":"A record"},{"location":"azure/dns/#aaaa-record","text":"DNS AAAA records match a domain name to an IPv6 address. DNS AAAA records are exactly like DNS A records , except that they store a domain's IPv6 address instead of its IPv4 address.","title":"AAAA record"},{"location":"azure/dns/#txt-record","text":"This record is used to associate text with a domain.","title":"TXT record"},{"location":"azure/dns/#soa-start-of-authority","text":"The DNS \u2018start of authority\u2019 (SOA) record stores important information about a domain or zone such as the email address of the administrator, when the domain was last updated, and how long the server should wait between refreshes. $ nslookup -type = SOA contoso.com Server: 8 .8.8.8 Address: 8 .8.8.8#53 Non-authoritative answer: contoso.com origin = ns1-205.azure-dns.com mail addr = azuredns-hostmaster.microsoft.com serial = 1 refresh = 3600 retry = 300 expire = 2419200 minimum = 300","title":"SOA (Start of authority)"},{"location":"azure/dns/#cert-certificate-record","text":"It stores public key certificates.","title":"CERT (Certificate record)"},{"location":"azure/dns/#srv-service-location-record","text":"It is used to specify a port for specific services.","title":"SRV (Service Location record)"},{"location":"azure/entra/","text":"Entra \u00b6 Access reviews \u00b6 Access reviews in Azure Active Directory (Azure AD), part of Microsoft Entra, enable organizations to efficiently manage group memberships, access to enterprise applications, and role assignments. User's access can be reviewed regularly to make sure only the right people have continued access. Why are access reviews important? Azure AD enables you to collaborate with users from inside your organization and with external users. Users can join groups, invite guests, connect to cloud apps, and work remotely from their work or personal devices. The convenience of using self-service has led to a need for better access management capabilities. As new employees join, how do you ensure they have the access they need to be productive? As people move teams or leave the company, how do you make sure that their old access is removed? Excessive access rights can lead to compromises. Excessive access right may also lead audit findings as they indicate a lack of control over access. You have to proactively engage with resource owners to ensure they regularly review who has access to their resources.","title":"Entra"},{"location":"azure/entra/#entra","text":"","title":"Entra"},{"location":"azure/entra/#access-reviews","text":"Access reviews in Azure Active Directory (Azure AD), part of Microsoft Entra, enable organizations to efficiently manage group memberships, access to enterprise applications, and role assignments. User's access can be reviewed regularly to make sure only the right people have continued access. Why are access reviews important? Azure AD enables you to collaborate with users from inside your organization and with external users. Users can join groups, invite guests, connect to cloud apps, and work remotely from their work or personal devices. The convenience of using self-service has led to a need for better access management capabilities. As new employees join, how do you ensure they have the access they need to be productive? As people move teams or leave the company, how do you make sure that their old access is removed? Excessive access rights can lead to compromises. Excessive access right may also lead audit findings as they indicate a lack of control over access. You have to proactively engage with resource owners to ensure they regularly review who has access to their resources.","title":"Access reviews"},{"location":"azure/event_grid/","text":"Event Grid \u00b6 Why event grid is required? The world we live in is reactive . Events take place, and we respond to those events. The reactive nature of Azure services is no exception to this paradigm. Example: A newly updated blob to Storage service needs to be processed to import data into CosmosDB. A provisioned resource needs to be tagged. Ability to notify subscribers scaled-out between multiple regions with non-ubiquitous receiving mechanisms (webhooks, queues, streams, etc). Reacting to the events taking place on a cloud-scale is not what the messaging services covered earlier were built for. That\u2019s what Event Grid was created for. Event Grid is to build reactive architecture. Integrations \u00b6 Custom WebHooks : Use webhooks for customizable endpoints that respond to events. Azure Functions : Use Azure Functions for serverless response to events. Logic Apps : Use Logic Apps to automate business processes for responding to events. Storage Queues : Use Queue storage to receive events that need to be pulled. By sending events to Queue storage, the app can pull and process events on its own schedule. Event Hubs : Use Event Hubs when your solution gets events faster than it can process the events. Azure Automation : Use Azure Automation to process events with automated runbooks. Hybrid Connections : Use Azure Relay Hybrid Connections to send events to applications that are within an enterprise network and don\u2019t have a publicly accessible endpoint","title":"Event Grid \u26ed"},{"location":"azure/event_grid/#event-grid","text":"Why event grid is required? The world we live in is reactive . Events take place, and we respond to those events. The reactive nature of Azure services is no exception to this paradigm. Example: A newly updated blob to Storage service needs to be processed to import data into CosmosDB. A provisioned resource needs to be tagged. Ability to notify subscribers scaled-out between multiple regions with non-ubiquitous receiving mechanisms (webhooks, queues, streams, etc). Reacting to the events taking place on a cloud-scale is not what the messaging services covered earlier were built for. That\u2019s what Event Grid was created for. Event Grid is to build reactive architecture.","title":"Event Grid"},{"location":"azure/event_grid/#integrations","text":"Custom WebHooks : Use webhooks for customizable endpoints that respond to events. Azure Functions : Use Azure Functions for serverless response to events. Logic Apps : Use Logic Apps to automate business processes for responding to events. Storage Queues : Use Queue storage to receive events that need to be pulled. By sending events to Queue storage, the app can pull and process events on its own schedule. Event Hubs : Use Event Hubs when your solution gets events faster than it can process the events. Azure Automation : Use Azure Automation to process events with automated runbooks. Hybrid Connections : Use Azure Relay Hybrid Connections to send events to applications that are within an enterprise network and don\u2019t have a publicly accessible endpoint","title":"Integrations"},{"location":"azure/event_hub/","text":"Event Hub (Pull based ) \u00b6 Event Grid Vs Event Hub \u00b6 A key difference between Event Grid and Event Hubs is in the way event data is made available to the subscribers. Event Grid pushes the ingested data to the subscribers whereas Event Hub makes the data available in a pull model . What is Azure Event Hubs? Platform-as-a-Service Event Stream Broker Use the Apache Kafka\u00ae API, but with far lower cost and better performance. Fully managed: You use the features, Azure deals with everything else AMQP 1.0 standards compliant, Apache Kafka\u00ae wire-compatible Polyglot Azure SDK and cross-platform client support Event Hub Vs Kafka \u00b6 What Event hub is not? \u00b6 Event-hub Architecture \u00b6 Concepts \u00b6 Azure Event Hubs is part of a fleet of services, which also includes Azure Service Bus , and Azure Event Grid . Namescpaces \u00b6 Event hub capture \u00b6 Usign the AVRO schemas as shown below Stream Processing \u00b6 When it comes to stream processing, there are generally two approaches to working through the infinite stream of input data (or tuples): Tuple at a time :You can process one tuple at a time with downstream processing applications, Micro batching : You can create small batches (consisting of a few hundred or a few thousand tuples) and process these micro-batches with your downstream applications. Kafka and Event hub mapping \u00b6 Kafka Event Hub Cluster Namespace Topic Event hub Partition Partition Consumer Group Consumer Group Offset Offset Pubsub \u00b6 The producer (known as the publisher in this context) has no expectations that the events will result in any action. Interested consumer(s) , can subscribe, listen for events, and take actions depending on their consumption scenario. Events can have multiple subscribers or no subscribers at all. Two different subscribers can react to an event with different actions and not be aware of one another. The producer and consumer are loosely coupled and managed independently. The consumer isn't expected to acknowledge the event back to the producer. A consumer that is no longer interested in the events, can unsubscribe. The consumer is removed from the pipeline without affecting the producer or the overall functionality of the system. How loose coupling is ensured? A message broker provides temporal decoupling. The producer and consumer don't have to run concurrently. A producer can send a message to the message broker regardless of the availability of the consumer. Conversely, the consumer isn't restricted by the producer's availability. Event publishers \u00b6 Any entity that sends data to an event hub is an event publisher (synonymously used with event producer ). Event publishers can publish events using HTTPS or AMQP 1.0 or the Kafka protocol . Event publishers use AAD based authorization with OAuth2-issued JWT tokens or an Event Hub-specific Shared Access Signature (SAS) token to gain publishing access. HTTPS or AMQP AMQP requires the establishment of a persistent bidirectional socket in + transport level security (TLS). AMQP has higher network costs when initializing the session, however HTTPS requires extra TLS overhead for every request. AMQP has higher performance for frequent publishers and can achieve much lower latencies when used with asynchronous publishing code. Consumer Groups \u00b6 The logical group of consumers that receive messages from each Event Hub partition is called a consumer group . The intention of a consumer group is to represent a single downstream processing application, where that application consists of multiple parallel processes, each consuming and processing messages from a partition. All consumers must belong to a consumer group. The consumer group also acts to limit concurrent access to a given partition by multiple consumers, which is desired for most applications, because two consumers could mean data is being redundantly processed by downstream components and could have unintended consequences. A diagam of non-competing consumers is shown below: Whos responsibility to state management? In competing consumers, the queue system itself keeps track of the delivery state of every message. In Event Hubs, no such state is tracked, so managing the state of progress through the queue becomes the responsibility of the individual consumer. Event retention \u00b6 Published events are removed from an event hub based on a configurable, timed-based retention policy. Here are a few important points: The default value and shortest possible retention period is 1 day (24 hours). For Event Hubs Standard, the maximum retention period is 7 days. For Event Hubs Premium and Dedicated, the maximum retention period is 90 days. If you change the retention period, it applies to all events including events that are already in the event hub I need to hold my events If you need to archive events beyond the allowed retention period, you can have them automatically stored in Azure Storage or Azure Data Lake by turning on the Event Hubs Capture feature . Publisher policy \u00b6 Event Hubs enables granular control over event publishers through publisher policies. Publisher policies are run-time features designed to facilitate large numbers of independent event publishers Event Hubs Capture \u00b6 Event Hubs Capture enables you to automatically capture the streaming data in Event Hubs and save it to your choice of either a Blob storage account, or an Azure Data Lake Storage account. You can enable capture from the Azure portal, and specify a minimum size and time window to perform the capture. Using Event Hubs Capture, you specify your own Azure Blob Storage account and container, or Azure Data Lake Storage account, one of which is used to store the captured data. Captured data is written in the Apache Avro format . Partitions \u00b6 Event Hubs organizes sequences of events sent to an event hub into one or more partitions. As newer events arrive, they're added to the end of this sequence. You can't change the partition count for an event hub after its creation except for the event hub in a dedicated cluster and premium tier A partition can be thought of as a \"commit log\". Partitions hold event data that contains body of the event, a user-defined property bag describing the event, metadata such as its offset in the partition, its number in the stream sequence, and service-side timestamp at which it was accepted. Mapping of events to partitions \u00b6 You can use a partition key to map incoming event data into specific partitions for the purpose of data organization. The partition key is a sender-supplied value passed into an event hub. It is processed through a static hashing function , which creates the partition assignment. If you don't specify a partition key when publishing an event, a round-robin assignment is used. The event publisher is only aware of its partition key, not the partition to which the events are published. This decoupling of key and partition insulates the sender from needing to know too much about the downstream processing. Throughput units \u00b6 Tldr The throughput capacity of Event Hubs is controlled by throughput units. Throughput units are pre-purchased units of capacity. A single throughput unit lets you: Ingress : Up to 1 MB per second or 1000 events per second (whichever comes first). Egress : Up to 2 MB per second or 4096 events per second. Beyond the capacity of the purchased throughput units, ingress is throttled and a ServerBusyException is returned Dead-letter queue (DLQ) \u00b6 A Service Bus queue has a default subqueue, called the dead-letter queue (DLQ) to hold messages that couldn't be delivered or processed. Service Bus or the message processing logic in the consumer can add messages to the DLQ. The DLQ keeps the messages until they are retrieved from the queue. QOS \u00b6 At least once At most once Exactly once","title":"Event Hub \u2604\ufe0f"},{"location":"azure/event_hub/#event-hub-pull-based","text":"","title":"Event Hub (Pull based )"},{"location":"azure/event_hub/#event-grid-vs-event-hub","text":"A key difference between Event Grid and Event Hubs is in the way event data is made available to the subscribers. Event Grid pushes the ingested data to the subscribers whereas Event Hub makes the data available in a pull model . What is Azure Event Hubs? Platform-as-a-Service Event Stream Broker Use the Apache Kafka\u00ae API, but with far lower cost and better performance. Fully managed: You use the features, Azure deals with everything else AMQP 1.0 standards compliant, Apache Kafka\u00ae wire-compatible Polyglot Azure SDK and cross-platform client support","title":"Event Grid Vs Event Hub"},{"location":"azure/event_hub/#event-hub-vs-kafka","text":"","title":"Event Hub Vs Kafka"},{"location":"azure/event_hub/#what-event-hub-is-not","text":"","title":"What Event hub is not?"},{"location":"azure/event_hub/#event-hub-architecture","text":"","title":"Event-hub Architecture"},{"location":"azure/event_hub/#concepts","text":"Azure Event Hubs is part of a fleet of services, which also includes Azure Service Bus , and Azure Event Grid .","title":"Concepts"},{"location":"azure/event_hub/#namescpaces","text":"","title":"Namescpaces"},{"location":"azure/event_hub/#event-hub-capture","text":"Usign the AVRO schemas as shown below","title":"Event hub capture"},{"location":"azure/event_hub/#stream-processing","text":"When it comes to stream processing, there are generally two approaches to working through the infinite stream of input data (or tuples): Tuple at a time :You can process one tuple at a time with downstream processing applications, Micro batching : You can create small batches (consisting of a few hundred or a few thousand tuples) and process these micro-batches with your downstream applications.","title":"Stream Processing"},{"location":"azure/event_hub/#kafka-and-event-hub-mapping","text":"Kafka Event Hub Cluster Namespace Topic Event hub Partition Partition Consumer Group Consumer Group Offset Offset","title":"Kafka and Event hub mapping"},{"location":"azure/event_hub/#pubsub","text":"The producer (known as the publisher in this context) has no expectations that the events will result in any action. Interested consumer(s) , can subscribe, listen for events, and take actions depending on their consumption scenario. Events can have multiple subscribers or no subscribers at all. Two different subscribers can react to an event with different actions and not be aware of one another. The producer and consumer are loosely coupled and managed independently. The consumer isn't expected to acknowledge the event back to the producer. A consumer that is no longer interested in the events, can unsubscribe. The consumer is removed from the pipeline without affecting the producer or the overall functionality of the system. How loose coupling is ensured? A message broker provides temporal decoupling. The producer and consumer don't have to run concurrently. A producer can send a message to the message broker regardless of the availability of the consumer. Conversely, the consumer isn't restricted by the producer's availability.","title":"Pubsub"},{"location":"azure/event_hub/#event-publishers","text":"Any entity that sends data to an event hub is an event publisher (synonymously used with event producer ). Event publishers can publish events using HTTPS or AMQP 1.0 or the Kafka protocol . Event publishers use AAD based authorization with OAuth2-issued JWT tokens or an Event Hub-specific Shared Access Signature (SAS) token to gain publishing access. HTTPS or AMQP AMQP requires the establishment of a persistent bidirectional socket in + transport level security (TLS). AMQP has higher network costs when initializing the session, however HTTPS requires extra TLS overhead for every request. AMQP has higher performance for frequent publishers and can achieve much lower latencies when used with asynchronous publishing code.","title":"Event publishers"},{"location":"azure/event_hub/#consumer-groups","text":"The logical group of consumers that receive messages from each Event Hub partition is called a consumer group . The intention of a consumer group is to represent a single downstream processing application, where that application consists of multiple parallel processes, each consuming and processing messages from a partition. All consumers must belong to a consumer group. The consumer group also acts to limit concurrent access to a given partition by multiple consumers, which is desired for most applications, because two consumers could mean data is being redundantly processed by downstream components and could have unintended consequences. A diagam of non-competing consumers is shown below: Whos responsibility to state management? In competing consumers, the queue system itself keeps track of the delivery state of every message. In Event Hubs, no such state is tracked, so managing the state of progress through the queue becomes the responsibility of the individual consumer.","title":"Consumer Groups"},{"location":"azure/event_hub/#event-retention","text":"Published events are removed from an event hub based on a configurable, timed-based retention policy. Here are a few important points: The default value and shortest possible retention period is 1 day (24 hours). For Event Hubs Standard, the maximum retention period is 7 days. For Event Hubs Premium and Dedicated, the maximum retention period is 90 days. If you change the retention period, it applies to all events including events that are already in the event hub I need to hold my events If you need to archive events beyond the allowed retention period, you can have them automatically stored in Azure Storage or Azure Data Lake by turning on the Event Hubs Capture feature .","title":"Event retention"},{"location":"azure/event_hub/#publisher-policy","text":"Event Hubs enables granular control over event publishers through publisher policies. Publisher policies are run-time features designed to facilitate large numbers of independent event publishers","title":"Publisher policy"},{"location":"azure/event_hub/#event-hubs-capture","text":"Event Hubs Capture enables you to automatically capture the streaming data in Event Hubs and save it to your choice of either a Blob storage account, or an Azure Data Lake Storage account. You can enable capture from the Azure portal, and specify a minimum size and time window to perform the capture. Using Event Hubs Capture, you specify your own Azure Blob Storage account and container, or Azure Data Lake Storage account, one of which is used to store the captured data. Captured data is written in the Apache Avro format .","title":"Event Hubs Capture"},{"location":"azure/event_hub/#partitions","text":"Event Hubs organizes sequences of events sent to an event hub into one or more partitions. As newer events arrive, they're added to the end of this sequence. You can't change the partition count for an event hub after its creation except for the event hub in a dedicated cluster and premium tier A partition can be thought of as a \"commit log\". Partitions hold event data that contains body of the event, a user-defined property bag describing the event, metadata such as its offset in the partition, its number in the stream sequence, and service-side timestamp at which it was accepted.","title":"Partitions"},{"location":"azure/event_hub/#mapping-of-events-to-partitions","text":"You can use a partition key to map incoming event data into specific partitions for the purpose of data organization. The partition key is a sender-supplied value passed into an event hub. It is processed through a static hashing function , which creates the partition assignment. If you don't specify a partition key when publishing an event, a round-robin assignment is used. The event publisher is only aware of its partition key, not the partition to which the events are published. This decoupling of key and partition insulates the sender from needing to know too much about the downstream processing.","title":"Mapping of events to partitions"},{"location":"azure/event_hub/#throughput-units","text":"Tldr The throughput capacity of Event Hubs is controlled by throughput units. Throughput units are pre-purchased units of capacity. A single throughput unit lets you: Ingress : Up to 1 MB per second or 1000 events per second (whichever comes first). Egress : Up to 2 MB per second or 4096 events per second. Beyond the capacity of the purchased throughput units, ingress is throttled and a ServerBusyException is returned","title":"Throughput units"},{"location":"azure/event_hub/#dead-letter-queue-dlq","text":"A Service Bus queue has a default subqueue, called the dead-letter queue (DLQ) to hold messages that couldn't be delivered or processed. Service Bus or the message processing logic in the consumer can add messages to the DLQ. The DLQ keeps the messages until they are retrieved from the queue.","title":"Dead-letter queue (DLQ)"},{"location":"azure/event_hub/#qos","text":"At least once At most once Exactly once","title":"QOS"},{"location":"azure/front-door/","text":"Front Door \ud83d\udeaa \u00b6 While both Front Door and Application Gateway are layer 7 (HTTP/HTTPS) load balancers, the primary difference is that Front Door is a non-regional service whereas Application Gateway is a regional service . The following are the features offered by Azure Front Door: - Enhanced performance : Users will be connected to the nearest point of presence (POP) using the split TCP anycast protocol. - Heath probes : Using smart health probes in Azure Front Door you can monitor the latency and availability of the web application. Also, instance failover to another region can be triggered if the backend is found unhealthy. - URL based routing : You can route based on the path in the URL. - Multiple-site routing: This helps in hosting multiple sites behind the same Front Door. - Session affinity : You can route the users to the same backend targets using the cookie-based session affinity feature. - TLS termination : It supports TLS termination so that you can reduce the load on the backend servers.","title":"Front Door \ud83d\udeaa"},{"location":"azure/front-door/#front-door","text":"While both Front Door and Application Gateway are layer 7 (HTTP/HTTPS) load balancers, the primary difference is that Front Door is a non-regional service whereas Application Gateway is a regional service . The following are the features offered by Azure Front Door: - Enhanced performance : Users will be connected to the nearest point of presence (POP) using the split TCP anycast protocol. - Heath probes : Using smart health probes in Azure Front Door you can monitor the latency and availability of the web application. Also, instance failover to another region can be triggered if the backend is found unhealthy. - URL based routing : You can route based on the path in the URL. - Multiple-site routing: This helps in hosting multiple sites behind the same Front Door. - Session affinity : You can route the users to the same backend targets using the cookie-based session affinity feature. - TLS termination : It supports TLS termination so that you can reduce the load on the backend servers.","title":"Front Door \ud83d\udeaa"},{"location":"azure/grid-hub-bus/","text":"Comparing Event Grid/Hub and Service Bus \u00b6 This blog is to compare 3 Async messaging systems in Azure as there are lot of overlapping features among these 3 Queueing patterns \u00b6 Let's first see what is the difference in terms of push/pull model queue/topics non partitioned/partitioned topics The below list shows the services in various cloud providers such as AWS, GCP and Azure Specific patterns on Azure are shown below Its worth noting that Event Grid is push based while other 2 are pull based Event Hub is for high thoughput scenario as it uses multiple partitions. Event Hubs also has the unique ability to ingest massive volume of data (1 million messages per second) in an unmatchable speed. Service Bus connects any devices, application or services running in the cloud to any other applications or services and transfers data between them. The data being transferred can be in XML, JSON, or text format. Azure Event Hubs focuses more on event streaming whereas Azure Service Bus is more focused on high-value enterprise messaging, which means the later is focused on messages rather than events. When to use what? \u00b6 If you are implementing a pattern which uses a Request/Reply message, then use Azure Service Bus . Azure Event Hubs is more likely to be used if you\u2019re implementing patterns with Event Messages. If you want a reliable event ingestor which can deal massive scale of event messages, then preferably you should go for Event Hubs. Use Event Grid is generally used for building reactive architecture. Event Grids VS Event Hubs \u00b6 Event Grids doesn\u2019t guarantee the order of the events, but Event Hubs use partitions which are ordered sequences, so it can maintain the order of the events in the same partition. Event Hubs are accepting only endpoints for the ingestion of data and they don\u2019t provide a mechanism for sending data back to publishers. On the other hand, Event Grids sends HTTP requests to notify events that happen in publishers. Event Grid can trigger an Azure Function. In the case of Event Hubs, the Azure Function needs to pull and process an event. Event Grids is a distribution system, not a queueing mechanism. If an event is pushed in, it gets pushed out immediately and if it doesn\u2019t get handled, it\u2019s gone forever. Unless we send the undelivered events to a storage account. This process is known as dead-lettering. In Event Hubs the data can be kept for up to seven days and then replayed. This gives us the ability to resume from a certain point or to restart from an older point in time and reprocess events when we need it. Event Hubs vs Service Bus \u00b6 To the external publisher or the receiver Service Bus and Event Hubs can look very similar and this is what makes it difficult to understand the differences between the two and when to use what. Event Hubs focuses on event streaming where Service Bus is more of a traditional messaging broker . Service Bus is used as the backbone to connects applications running in the cloud to other applications or services and transfers data between them whereas Event Hubs is more concerned about receiving massive volume of data with high throughout and low latency . Event Hubs decouples multiple event-producers from event-receivers whereas Service Bus aims to decouple applications. Service Bus messaging supports a message property \u2018Time to Live\u2019 whereas Event Hubs has a default retention period of 7 days. Service Bus has the concept of message session. It allows relating messages based on their session-id property whereas Event Hubs does not. Service Bus the messages are pulled out by the receiver & cannot be processed again whereas Event Hubs message can be ingested by multiple receivers . Service Bus uses the terminology of queues and topics whereas Event Hubs partitions terminology is used. Patterns \u00b6 Event Grid \u00b6 Event-hub \u00b6 Service bus \u00b6","title":"Service Bus vs Event Grid/Hub\ufe0f"},{"location":"azure/grid-hub-bus/#comparing-event-gridhub-and-service-bus","text":"This blog is to compare 3 Async messaging systems in Azure as there are lot of overlapping features among these 3","title":"Comparing Event Grid/Hub and Service Bus"},{"location":"azure/grid-hub-bus/#queueing-patterns","text":"Let's first see what is the difference in terms of push/pull model queue/topics non partitioned/partitioned topics The below list shows the services in various cloud providers such as AWS, GCP and Azure Specific patterns on Azure are shown below Its worth noting that Event Grid is push based while other 2 are pull based Event Hub is for high thoughput scenario as it uses multiple partitions. Event Hubs also has the unique ability to ingest massive volume of data (1 million messages per second) in an unmatchable speed. Service Bus connects any devices, application or services running in the cloud to any other applications or services and transfers data between them. The data being transferred can be in XML, JSON, or text format. Azure Event Hubs focuses more on event streaming whereas Azure Service Bus is more focused on high-value enterprise messaging, which means the later is focused on messages rather than events.","title":"Queueing patterns"},{"location":"azure/grid-hub-bus/#when-to-use-what","text":"If you are implementing a pattern which uses a Request/Reply message, then use Azure Service Bus . Azure Event Hubs is more likely to be used if you\u2019re implementing patterns with Event Messages. If you want a reliable event ingestor which can deal massive scale of event messages, then preferably you should go for Event Hubs. Use Event Grid is generally used for building reactive architecture.","title":"When to use what?"},{"location":"azure/grid-hub-bus/#event-grids-vs-event-hubs","text":"Event Grids doesn\u2019t guarantee the order of the events, but Event Hubs use partitions which are ordered sequences, so it can maintain the order of the events in the same partition. Event Hubs are accepting only endpoints for the ingestion of data and they don\u2019t provide a mechanism for sending data back to publishers. On the other hand, Event Grids sends HTTP requests to notify events that happen in publishers. Event Grid can trigger an Azure Function. In the case of Event Hubs, the Azure Function needs to pull and process an event. Event Grids is a distribution system, not a queueing mechanism. If an event is pushed in, it gets pushed out immediately and if it doesn\u2019t get handled, it\u2019s gone forever. Unless we send the undelivered events to a storage account. This process is known as dead-lettering. In Event Hubs the data can be kept for up to seven days and then replayed. This gives us the ability to resume from a certain point or to restart from an older point in time and reprocess events when we need it.","title":"Event Grids VS Event Hubs"},{"location":"azure/grid-hub-bus/#event-hubs-vs-service-bus","text":"To the external publisher or the receiver Service Bus and Event Hubs can look very similar and this is what makes it difficult to understand the differences between the two and when to use what. Event Hubs focuses on event streaming where Service Bus is more of a traditional messaging broker . Service Bus is used as the backbone to connects applications running in the cloud to other applications or services and transfers data between them whereas Event Hubs is more concerned about receiving massive volume of data with high throughout and low latency . Event Hubs decouples multiple event-producers from event-receivers whereas Service Bus aims to decouple applications. Service Bus messaging supports a message property \u2018Time to Live\u2019 whereas Event Hubs has a default retention period of 7 days. Service Bus has the concept of message session. It allows relating messages based on their session-id property whereas Event Hubs does not. Service Bus the messages are pulled out by the receiver & cannot be processed again whereas Event Hubs message can be ingested by multiple receivers . Service Bus uses the terminology of queues and topics whereas Event Hubs partitions terminology is used.","title":"Event Hubs vs Service Bus"},{"location":"azure/grid-hub-bus/#patterns","text":"","title":"Patterns"},{"location":"azure/grid-hub-bus/#event-grid","text":"","title":"Event Grid"},{"location":"azure/grid-hub-bus/#event-hub","text":"","title":"Event-hub"},{"location":"azure/grid-hub-bus/#service-bus","text":"","title":"Service bus"},{"location":"azure/keyVault/","text":"Azure Key Vault \u00b6 Azure Key Vault is one of several key management solutions in Azure, and helps solve the following problems: Secrets Management - Azure Key Vault can be used to Securely store and tightly control access to tokens, passwords, certificates, API keys, and other secrets Key Management - Azure Key Vault can be used as a Key Management solution. Azure Key Vault makes it easy to create and control the encryption keys used to encrypt your data. Certificate Management - Azure Key Vault lets you easily provision, manage, and deploy public and private TLS/SSL certificates for use with Azure and your internal connected resources. To access any item stored in a Key Vault, you must first be authenticated. Azure Key Vault supports the following authentication capabilities, which have been previously discussed: 1. Service principals 2. Managed identities","title":"Key Vault \ud83d\udd11"},{"location":"azure/keyVault/#azure-key-vault","text":"Azure Key Vault is one of several key management solutions in Azure, and helps solve the following problems: Secrets Management - Azure Key Vault can be used to Securely store and tightly control access to tokens, passwords, certificates, API keys, and other secrets Key Management - Azure Key Vault can be used as a Key Management solution. Azure Key Vault makes it easy to create and control the encryption keys used to encrypt your data. Certificate Management - Azure Key Vault lets you easily provision, manage, and deploy public and private TLS/SSL certificates for use with Azure and your internal connected resources. To access any item stored in a Key Vault, you must first be authenticated. Azure Key Vault supports the following authentication capabilities, which have been previously discussed: 1. Service principals 2. Managed identities","title":"Azure Key Vault"},{"location":"azure/logicApp/","text":"What is Logic App? \u00b6 This service defines the workflow which can consuming a range of APIs exposed as connectors. These Logic App connectors will perform the sequence of actions defined in the workflow whenever the trigger gets fired. Roles \u00b6 Logic App Operator - Lets you read, enable, and disable logic apps, but not edit or update them. Logic App Contributor - Lets you create, manage logic apps, but not access to them.","title":"Logic App \ud83e\ude84"},{"location":"azure/logicApp/#what-is-logic-app","text":"This service defines the workflow which can consuming a range of APIs exposed as connectors. These Logic App connectors will perform the sequence of actions defined in the workflow whenever the trigger gets fired.","title":"What is Logic App?"},{"location":"azure/logicApp/#roles","text":"Logic App Operator - Lets you read, enable, and disable logic apps, but not edit or update them. Logic App Contributor - Lets you create, manage logic apps, but not access to them.","title":"Roles"},{"location":"azure/networkingInfra/","text":"Locking Infra \u00b6 As an administrator , you can lock an Azure subscription, resource group, or resource to protect them from accidental user deletions and modifications. The lock overrides any user permissions. Lock Inheritance When you apply a lock at a parent scope, all resources within that scope inherit the same lock. Even resources you add later inherit the same parent lock. The most restrictive lock in the inheritance takes precedence. Tags \u00b6 You can apply tags to your Azure resources, resource groups, and subscriptions. Tags are metadata elements that you apply to your Azure resources. They're key-value pairs that help you identify resources based on settings that are relevant to your organization Are tags inherited? Resources don't inherit the tags you apply to a resource group or a subscription Region \u00b6 Each Azure region features datacenters deployed within a latency-defined perimeter. They're connected through a dedicated regional low-latency network. Region pairs \u00b6 Most Azure regions are paired with another region within the same geography (such as US, Europe, or Asia) at least 300 miles away. For example, if a region in a pair was affected by a natural disaster, services would automatically fail over to the other region in its region pair. Sovereign Regions \u00b6 In addition to regular regions , Azure also has sovereign regions . Sovereign regions are instances of Azure that are isolated from the main instance of Azure. You may need to use a sovereign region for compliance or legal purposes. AZ \u00b6 Azure az's are physically separate locations within each Azure region that are tolerant to local failures. To ensure resiliency, a minimum of three separate availability zones are present in all AZ-enabled regions . AZ's are connected by a high-performance network with a round-trip latency of less than 2ms . AZ's are designed so that if one zone is affected, then services are supported by the remaining 2 zones. Management groups \u00b6 Why do we need it? \u00b6 If you have many subscriptions, you might need a way to efficiently manage access, policies, and compliance for those subscriptions. Access to MG's No one is given default access to the root management group. Azure AD Global Administrators are the only users that can elevate themselves to gain access. Once they have access to the root management group, the global administrators can assign any Azure role to other users to manage it. Note All subscriptions and management groups fold up to the one root management group within the directory. The root management group can't be moved or deleted, unlike other management groups. All Azure customers can see the root management group, but not all customers have access to manage that root management group. By default, the root management group's display name is Tenant root group and operates itself as a management group. The ID is the same value as the Azure Active Directory (Azure AD) tenant ID. Solution \u00b6 Azure management groups provide a level of scope above subscriptions. You organize subscriptions into containers called management groups and apply governance conditions to the management groups. All subscriptions within a management group automatically inherit the conditions applied to the management group, the same way that resource groups inherit settings from subscriptions and resources inherit from resource groups. Azure Subscription \u00b6 In Azure, subscriptions are a unit of management, billing, and scale. Similar to how resource groups are a way to logically organize resources, subscriptions allow you to logically organize your resource groups and facilitate billing. An account can have multiple subscriptions, but it\u2019s only required to have one. In a multi-subscription account, you can use the subscriptions to configure different billing models and apply different access-management policies. Resource groups \u00b6 Resource groups are simply groupings of resources. When you create a resource, you\u2019re required to place it into a resource group . While a resource group can contain many resources, a single resource can only be in one resource group at a time. Note Some resources may be moved between resource groups, but when you move a resource to a new group, it will no longer be associated with the former group. Additionally, resource groups can't be nested, meaning you can\u2019t put resource group B inside of resource group A. Benefit of using RG \u00b6 When you apply an action to a resource group, that action will apply to all the resources within the resource group. If you delete a resource group , all the resources will be deleted. If you grant or deny access to a resource group, you\u2019ve granted or denied access to all the resources within the resource group. Virtual machine scale sets \u00b6 Virtual machine scale sets let you create and manage a group of identical, load-balanced VMs. Scale sets allow you to centrally manage, configure, and update a large number of VMs in minutes. The number of VM instances can automatically increase or decrease in response to demand, or you can set it to scale based on a defined schedule. Tip Virtual machine scale sets also automatically deploy a load balancer to make sure that your resources are being used efficiently Azure Container Instances \u00b6 Azure Container Instances offer the fastest and simplest way to run a container in Azure; without having to manage any virtual machines or adopt any additional services. Azure Container Instances are a platform as a service (PaaS) offering. Azure Container Instances allow you to upload your containers and then the service will run the containers for you. Azure Functions \u00b6 Azure Functions is an event-driven, serverless compute option that doesn\u2019t require maintaining virtual machines or containers. If you build an app using VMs or containers, those resources have to be \u201crunning\u201d in order for your app to function. With Azure Functions, an event wakes the function, alleviating the need to keep resources provisioned when there are no events. Azure Functions runs your code when it's triggered and automatically deallocates resources when the function is finished. In this model, you're only charged for the CPU time used while your function runs. Remember Functions can be either stateless or stateful. When they're stateless (the default), they behave as if they're restarted every time they respond to an event. When they're stateful (called Durable Functions), a context is passed through the function to track prior activity. Connecting with On-Premise \u00b6 ExpressRoute \u00b6 An ExpressRoute instance is the only connection between an Azure VNet and an on-premise network that does not cross into the internet. The connection is private and managed directly by an ExpressRoute partner . Azure ExpressRoute lets you extend your on-premises networks into the Microsoft cloud over a private connection, with the help of a connectivity provider . This connection is called an ExpressRoute Circuit . With ExpressRoute, you can establish connections to Microsoft cloud services, such as Microsoft Azure and Microsoft 365. This allows you to connect offices, datacenters, or other facilities to the Microsoft cloud. Each location would have its own ExpressRoute circuit. How does ExpressRoute work on high level? ExpressRoute uses the BGP . BGP is used to exchange routes between on-premises networks and resources running in Azure. This protocol enables dynamic routing between your on-premises network and services running in the Microsoft cloud. Point-to-Site Connection (VPN) \u00b6 A point-to-site (P2S) connection, on the other hand, is an inexpensive solution for connecting any supported Azure resource to one existing in another network. Site-to-Site Connection (VPN) \u00b6 Unlike P2S, traffic passing through a site-to-site (S2S) connection is sent within an encrypted tunnel across the internet. The primary difference is that the connectivity happens between Azure VPN Gateway and an on-premise VPN device. NSG and ASG \u00b6 NSG's (Network Security Group) & ASG's (Application Security Group) are the main Azure Resources that are used to administrate and control network traffic within a virtual network (vNET). Network Security Group (NSG) \u00b6 NSG's control access by permitting or denying network traffic in a number of ways, whether it be:- Communication between different workloads on a vNET Network connectivity from on-site environment into Azure Direct internet connection Theoretically speaking, it is just a group of Access Control List rules that either allow or deny network traffic to a specific destination located on your vNET. Remember NSG's can be applied either on a virtual machine or subnet (one NSG can be applied to multiple subnets or virtual machines Application Security Group \u00b6 An ASG provides the ability to group a set of Azure VMs and define NSGs specifically for that group. An application security group is a logical collection of virtual machines (NICs). You join virtual machines to the application security group, and then use the application security group as a source or destination in NSG rules. Remember A Virtual Machine can be attached to more than one Application Security Group. This helps in cases of multi-application servers. As per the rules defined in NSG, the WebServer and AppServer ASG are able to communicate with one another while the WebServer and DbServer ASG are not. Tldr Application Security Groups helps to manage the security of Virtual Machines by grouping them according the applications that runs on them. It is a feature that allows the application-centric use of Network Security Groups. ASG allow us to define fine-grained network security policies based on applications instead of explicit IP addresses. Using alias/monikers We can define ASG by providing a alias/moniker that fits our needs. NSGs and ASGs are not omnipresent When NSGs and ASGs are discussed by IT professionals working with Azure, those conversations generally occur within the context of Azure resources such as Azure VNets and Azure VMs. This is because you cannot currently configure NSGs or ASGs for Azure resources that exist outside a VNet. There are no Azure-wide capabilities to configure NSGs or ASGs for Azure App Service, Azure Storage, or Azure Event Hub Networking \u00b6 Home address \u00b6 What is special about 127.0.0.1 ? The 127.0.0.1 IPv4 address is commonly referred to as a loopback address . The IP address is typically mapped to the host name localhost and can be referred to as home in networking. This is useful when accessing a website hosted on the machine that you are currently logged into. For example, http://localhost accesses a website hosted on the currently logged into machine. The IPv6 address for localhost is ::1 NAT \u00b6 AT simplifies outbound Internet connectivity for virtual networks. When configured on a subnet, all outbound connectivity uses the Virtual Network NAT's static public IP addresses. Example As shown in the below diagram the address for VMs in the VNET are 10.0.0.* . As we know that this address are not accessible on the internet (they are private). A NAT translates the private IP address into a public IP address . In other words, the NAT device has both a private IP address as well as a public IP How does the router know the response from browser that has the destination address that is now 155.35.25.26 needs to be directed to 10.0.0.4 ? The answer is found by digging into a socket. A socket is the combination of an IP address:port number . More specifically, it has a source IP address, source port, destination IP address, and destination port Network Peering \u00b6 Peering allows two virtual networks to connect directly to each other. Network traffic between peered networks is private, and travels on the Microsoft backbone network, never entering the public internet. Peering enables resources in each virtual network to communicate with each other. These virtual networks can be in separate regions, which allows you to create a global interconnected network through Azure. UDR \u00b6 User-defined routes (UDR) allow you to control the routing tables between subnets within a virtual network or between virtual networks. This allows for greater control over network traffic flow.","title":"Networking Infrastructure"},{"location":"azure/networkingInfra/#locking-infra","text":"As an administrator , you can lock an Azure subscription, resource group, or resource to protect them from accidental user deletions and modifications. The lock overrides any user permissions. Lock Inheritance When you apply a lock at a parent scope, all resources within that scope inherit the same lock. Even resources you add later inherit the same parent lock. The most restrictive lock in the inheritance takes precedence.","title":"Locking Infra"},{"location":"azure/networkingInfra/#tags","text":"You can apply tags to your Azure resources, resource groups, and subscriptions. Tags are metadata elements that you apply to your Azure resources. They're key-value pairs that help you identify resources based on settings that are relevant to your organization Are tags inherited? Resources don't inherit the tags you apply to a resource group or a subscription","title":"Tags"},{"location":"azure/networkingInfra/#region","text":"Each Azure region features datacenters deployed within a latency-defined perimeter. They're connected through a dedicated regional low-latency network.","title":"Region"},{"location":"azure/networkingInfra/#region-pairs","text":"Most Azure regions are paired with another region within the same geography (such as US, Europe, or Asia) at least 300 miles away. For example, if a region in a pair was affected by a natural disaster, services would automatically fail over to the other region in its region pair.","title":"Region pairs"},{"location":"azure/networkingInfra/#sovereign-regions","text":"In addition to regular regions , Azure also has sovereign regions . Sovereign regions are instances of Azure that are isolated from the main instance of Azure. You may need to use a sovereign region for compliance or legal purposes.","title":"Sovereign Regions"},{"location":"azure/networkingInfra/#az","text":"Azure az's are physically separate locations within each Azure region that are tolerant to local failures. To ensure resiliency, a minimum of three separate availability zones are present in all AZ-enabled regions . AZ's are connected by a high-performance network with a round-trip latency of less than 2ms . AZ's are designed so that if one zone is affected, then services are supported by the remaining 2 zones.","title":"AZ"},{"location":"azure/networkingInfra/#management-groups","text":"","title":"Management groups"},{"location":"azure/networkingInfra/#why-do-we-need-it","text":"If you have many subscriptions, you might need a way to efficiently manage access, policies, and compliance for those subscriptions. Access to MG's No one is given default access to the root management group. Azure AD Global Administrators are the only users that can elevate themselves to gain access. Once they have access to the root management group, the global administrators can assign any Azure role to other users to manage it. Note All subscriptions and management groups fold up to the one root management group within the directory. The root management group can't be moved or deleted, unlike other management groups. All Azure customers can see the root management group, but not all customers have access to manage that root management group. By default, the root management group's display name is Tenant root group and operates itself as a management group. The ID is the same value as the Azure Active Directory (Azure AD) tenant ID.","title":"Why do we need it?"},{"location":"azure/networkingInfra/#solution","text":"Azure management groups provide a level of scope above subscriptions. You organize subscriptions into containers called management groups and apply governance conditions to the management groups. All subscriptions within a management group automatically inherit the conditions applied to the management group, the same way that resource groups inherit settings from subscriptions and resources inherit from resource groups.","title":"Solution"},{"location":"azure/networkingInfra/#azure-subscription","text":"In Azure, subscriptions are a unit of management, billing, and scale. Similar to how resource groups are a way to logically organize resources, subscriptions allow you to logically organize your resource groups and facilitate billing. An account can have multiple subscriptions, but it\u2019s only required to have one. In a multi-subscription account, you can use the subscriptions to configure different billing models and apply different access-management policies.","title":"Azure Subscription"},{"location":"azure/networkingInfra/#resource-groups","text":"Resource groups are simply groupings of resources. When you create a resource, you\u2019re required to place it into a resource group . While a resource group can contain many resources, a single resource can only be in one resource group at a time. Note Some resources may be moved between resource groups, but when you move a resource to a new group, it will no longer be associated with the former group. Additionally, resource groups can't be nested, meaning you can\u2019t put resource group B inside of resource group A.","title":"Resource groups"},{"location":"azure/networkingInfra/#benefit-of-using-rg","text":"When you apply an action to a resource group, that action will apply to all the resources within the resource group. If you delete a resource group , all the resources will be deleted. If you grant or deny access to a resource group, you\u2019ve granted or denied access to all the resources within the resource group.","title":"Benefit of using RG"},{"location":"azure/networkingInfra/#virtual-machine-scale-sets","text":"Virtual machine scale sets let you create and manage a group of identical, load-balanced VMs. Scale sets allow you to centrally manage, configure, and update a large number of VMs in minutes. The number of VM instances can automatically increase or decrease in response to demand, or you can set it to scale based on a defined schedule. Tip Virtual machine scale sets also automatically deploy a load balancer to make sure that your resources are being used efficiently","title":"Virtual machine scale sets"},{"location":"azure/networkingInfra/#azure-container-instances","text":"Azure Container Instances offer the fastest and simplest way to run a container in Azure; without having to manage any virtual machines or adopt any additional services. Azure Container Instances are a platform as a service (PaaS) offering. Azure Container Instances allow you to upload your containers and then the service will run the containers for you.","title":"Azure Container Instances"},{"location":"azure/networkingInfra/#azure-functions","text":"Azure Functions is an event-driven, serverless compute option that doesn\u2019t require maintaining virtual machines or containers. If you build an app using VMs or containers, those resources have to be \u201crunning\u201d in order for your app to function. With Azure Functions, an event wakes the function, alleviating the need to keep resources provisioned when there are no events. Azure Functions runs your code when it's triggered and automatically deallocates resources when the function is finished. In this model, you're only charged for the CPU time used while your function runs. Remember Functions can be either stateless or stateful. When they're stateless (the default), they behave as if they're restarted every time they respond to an event. When they're stateful (called Durable Functions), a context is passed through the function to track prior activity.","title":"Azure Functions"},{"location":"azure/networkingInfra/#connecting-with-on-premise","text":"","title":"Connecting with On-Premise"},{"location":"azure/networkingInfra/#expressroute","text":"An ExpressRoute instance is the only connection between an Azure VNet and an on-premise network that does not cross into the internet. The connection is private and managed directly by an ExpressRoute partner . Azure ExpressRoute lets you extend your on-premises networks into the Microsoft cloud over a private connection, with the help of a connectivity provider . This connection is called an ExpressRoute Circuit . With ExpressRoute, you can establish connections to Microsoft cloud services, such as Microsoft Azure and Microsoft 365. This allows you to connect offices, datacenters, or other facilities to the Microsoft cloud. Each location would have its own ExpressRoute circuit. How does ExpressRoute work on high level? ExpressRoute uses the BGP . BGP is used to exchange routes between on-premises networks and resources running in Azure. This protocol enables dynamic routing between your on-premises network and services running in the Microsoft cloud.","title":"ExpressRoute"},{"location":"azure/networkingInfra/#point-to-site-connection-vpn","text":"A point-to-site (P2S) connection, on the other hand, is an inexpensive solution for connecting any supported Azure resource to one existing in another network.","title":"Point-to-Site Connection (VPN)"},{"location":"azure/networkingInfra/#site-to-site-connection-vpn","text":"Unlike P2S, traffic passing through a site-to-site (S2S) connection is sent within an encrypted tunnel across the internet. The primary difference is that the connectivity happens between Azure VPN Gateway and an on-premise VPN device.","title":"Site-to-Site Connection (VPN)"},{"location":"azure/networkingInfra/#nsg-and-asg","text":"NSG's (Network Security Group) & ASG's (Application Security Group) are the main Azure Resources that are used to administrate and control network traffic within a virtual network (vNET).","title":"NSG and ASG"},{"location":"azure/networkingInfra/#network-security-group-nsg","text":"NSG's control access by permitting or denying network traffic in a number of ways, whether it be:- Communication between different workloads on a vNET Network connectivity from on-site environment into Azure Direct internet connection Theoretically speaking, it is just a group of Access Control List rules that either allow or deny network traffic to a specific destination located on your vNET. Remember NSG's can be applied either on a virtual machine or subnet (one NSG can be applied to multiple subnets or virtual machines","title":"Network Security Group (NSG)"},{"location":"azure/networkingInfra/#application-security-group","text":"An ASG provides the ability to group a set of Azure VMs and define NSGs specifically for that group. An application security group is a logical collection of virtual machines (NICs). You join virtual machines to the application security group, and then use the application security group as a source or destination in NSG rules. Remember A Virtual Machine can be attached to more than one Application Security Group. This helps in cases of multi-application servers. As per the rules defined in NSG, the WebServer and AppServer ASG are able to communicate with one another while the WebServer and DbServer ASG are not. Tldr Application Security Groups helps to manage the security of Virtual Machines by grouping them according the applications that runs on them. It is a feature that allows the application-centric use of Network Security Groups. ASG allow us to define fine-grained network security policies based on applications instead of explicit IP addresses. Using alias/monikers We can define ASG by providing a alias/moniker that fits our needs. NSGs and ASGs are not omnipresent When NSGs and ASGs are discussed by IT professionals working with Azure, those conversations generally occur within the context of Azure resources such as Azure VNets and Azure VMs. This is because you cannot currently configure NSGs or ASGs for Azure resources that exist outside a VNet. There are no Azure-wide capabilities to configure NSGs or ASGs for Azure App Service, Azure Storage, or Azure Event Hub","title":"Application Security Group"},{"location":"azure/networkingInfra/#networking","text":"","title":"Networking"},{"location":"azure/networkingInfra/#home-address","text":"What is special about 127.0.0.1 ? The 127.0.0.1 IPv4 address is commonly referred to as a loopback address . The IP address is typically mapped to the host name localhost and can be referred to as home in networking. This is useful when accessing a website hosted on the machine that you are currently logged into. For example, http://localhost accesses a website hosted on the currently logged into machine. The IPv6 address for localhost is ::1","title":"Home address"},{"location":"azure/networkingInfra/#nat","text":"AT simplifies outbound Internet connectivity for virtual networks. When configured on a subnet, all outbound connectivity uses the Virtual Network NAT's static public IP addresses. Example As shown in the below diagram the address for VMs in the VNET are 10.0.0.* . As we know that this address are not accessible on the internet (they are private). A NAT translates the private IP address into a public IP address . In other words, the NAT device has both a private IP address as well as a public IP How does the router know the response from browser that has the destination address that is now 155.35.25.26 needs to be directed to 10.0.0.4 ? The answer is found by digging into a socket. A socket is the combination of an IP address:port number . More specifically, it has a source IP address, source port, destination IP address, and destination port","title":"NAT"},{"location":"azure/networkingInfra/#network-peering","text":"Peering allows two virtual networks to connect directly to each other. Network traffic between peered networks is private, and travels on the Microsoft backbone network, never entering the public internet. Peering enables resources in each virtual network to communicate with each other. These virtual networks can be in separate regions, which allows you to create a global interconnected network through Azure.","title":"Network Peering"},{"location":"azure/networkingInfra/#udr","text":"User-defined routes (UDR) allow you to control the routing tables between subnets within a virtual network or between virtual networks. This allows for greater control over network traffic flow.","title":"UDR"},{"location":"azure/office365/","text":"Office 365 \ud83d\udcdd \u00b6 Microsoft 365 is a multi-tenant, cloud-based, Software-as-a-Service (SaaS) subscription offering from Microsoft. Various components of office 365 are: Outlook Teams Sharepoint Onedrive Word,PowerPoint and Excel Microsoft 365 Groups \u00b6 Microsoft 365 Groups is the foundational membership service that drives all teamwork across Microsoft 365. With Microsoft 365 Groups, you can give a group of people access to a collection of shared resources. These resources include: A shared Outlook inbox A shared calendar A SharePoint document library A Planner A OneNote notebook Power BI Expiration Policy With the increase in usage of Microsoft 365 groups and Microsoft Teams , administrators and users need a way to clean up unused groups and teams. A Microsoft 365 groups expiration policy can help remove inactive groups from the system and make things cleaner.","title":"Office 365 \ud83d\udcdd"},{"location":"azure/office365/#office-365","text":"Microsoft 365 is a multi-tenant, cloud-based, Software-as-a-Service (SaaS) subscription offering from Microsoft. Various components of office 365 are: Outlook Teams Sharepoint Onedrive Word,PowerPoint and Excel","title":"Office 365 \ud83d\udcdd"},{"location":"azure/office365/#microsoft-365-groups","text":"Microsoft 365 Groups is the foundational membership service that drives all teamwork across Microsoft 365. With Microsoft 365 Groups, you can give a group of people access to a collection of shared resources. These resources include: A shared Outlook inbox A shared calendar A SharePoint document library A Planner A OneNote notebook Power BI Expiration Policy With the increase in usage of Microsoft 365 groups and Microsoft Teams , administrators and users need a way to clean up unused groups and teams. A Microsoft 365 groups expiration policy can help remove inactive groups from the system and make things cleaner.","title":"Microsoft 365 Groups"},{"location":"azure/serviceFabric/","text":"Service Fabric is Microsoft's container orchestrator for deploying and managing microservices across a cluster of machines. Service Fabric provides a sophisticated, lightweight runtime that supports stateless and stateful microservices","title":"Service Fabric"},{"location":"azure/service_bus/","text":"Service Bus \ud83d\ude8c \u00b6 Tldr Azure Service Bus is a fully-managed enterprise message broker with message queues and publish-subscribe topics (in a namespace). Azure Service Bus' primary protocol is AMQP 1.0 and it can be used from any AMQP 1.0 compliant protocol client. JMS 2.0 compliant Polyglot Azure SDK and cross-platform client support Demo's \u00b6 Adding message to the queue \u00b6 Architecture \u00b6 Authorization \u00b6 Concepts \u00b6 Messages \u00b6 Data is transferred between different applications and services using messages. Messages can be in JSON , XML , Apache Avro , Plain Text format. Queues \u00b6 Messages are sent to and received from queues. Queues store messages until the receiving application is available to receive and process them. Once the broker accepts the message, the message is always held durably in triple-redundant storage, spread across availability zones if the namespace is zone-enabled. Messages are delivered in pull mode , only delivering messages when requested. Dead letter queue and rejected queues are shown below Topics \u00b6 While a queue is often used for point-to-point communication , topics are useful in publish/subscribe scenarios . Subscriptions \u00b6 Subscriptions are named entities . Subscriptions are durable by default, but can be configured to expire and then be automatically deleted. Define Filters and rules You can define rules on a subscription. A subscription rule has a filter to define a condition for the message to be copied into the subscription and an optional action that can modify message metadata . Subscribers can define which messages they want to receive from a topic. These messages are specified in the form of one or more named subscription rules. Each rule consists of a filter condition that selects particular messages, and optionally contains an action that annotates the selected message. Namespaces \u00b6 A Service Bus namespace is your own capacity slice of a large cluster made up of dozens of all-active virtual machines. Multiple queues and topics can be in a single namespace, and namespaces often serve as application containers. Partitioned \u00b6 Service Bus partitions enable queues and topics, or messaging entities, to be partitioned across multiple message brokers and messaging stores. Partitioning means that the overall throughput of a partitioned entity is no longer limited by the performance of a single message broker or messaging store. In addition, a temporary outage of a messaging store does not render a partitioned queue or topic unavailable Message sessions \u00b6 To realize a first-in, first-out (FIFO) guarantee in processing messages in Service Bus queue or subscriptions, use sessions. Sessions can also be used in implementing request-response patterns. The request-response pattern enables the sender application to send a request and provides a way for the receiver to correctly send a response back to the sender application. Auto-forwarding \u00b6 The Auto-forwarding feature enables you to chain a queue or subscription to another queue or topic that is part of the same namespace. When auto-forwarding is enabled, Service Bus automatically removes messages that are placed in the first queue or subscription (source) and puts them in the second queue or topic (destination). Dead-lettering \u00b6 Service Bus queues and topic subscriptions provide a secondary subqueue, called a dead-letter queue (DLQ) . The dead letter queue holds messages that can't be delivered to any receiver, or messages that can't be processed. You can then remove messages from the DLQ and inspect them. Auto-delete on idle \u00b6 Auto-delete on idle enables you to specify an idle interval after which the queue is automatically deleted. The interval is reset when there's traffic on the queue. The minimum duration is 5 minutes. Scheduled delivery \u00b6 You can submit messages to a queue or topic for delayed processing; for example, to schedule a job to become available for processing by a system at a certain time","title":"Service Bus \ud83d\ude8c\ufe0f"},{"location":"azure/service_bus/#service-bus","text":"Tldr Azure Service Bus is a fully-managed enterprise message broker with message queues and publish-subscribe topics (in a namespace). Azure Service Bus' primary protocol is AMQP 1.0 and it can be used from any AMQP 1.0 compliant protocol client. JMS 2.0 compliant Polyglot Azure SDK and cross-platform client support","title":"Service Bus \ud83d\ude8c"},{"location":"azure/service_bus/#demos","text":"","title":"Demo's"},{"location":"azure/service_bus/#adding-message-to-the-queue","text":"","title":"Adding message to the queue"},{"location":"azure/service_bus/#architecture","text":"","title":"Architecture"},{"location":"azure/service_bus/#authorization","text":"","title":"Authorization"},{"location":"azure/service_bus/#concepts","text":"","title":"Concepts"},{"location":"azure/service_bus/#messages","text":"Data is transferred between different applications and services using messages. Messages can be in JSON , XML , Apache Avro , Plain Text format.","title":"Messages"},{"location":"azure/service_bus/#queues","text":"Messages are sent to and received from queues. Queues store messages until the receiving application is available to receive and process them. Once the broker accepts the message, the message is always held durably in triple-redundant storage, spread across availability zones if the namespace is zone-enabled. Messages are delivered in pull mode , only delivering messages when requested. Dead letter queue and rejected queues are shown below","title":"Queues"},{"location":"azure/service_bus/#topics","text":"While a queue is often used for point-to-point communication , topics are useful in publish/subscribe scenarios .","title":"Topics"},{"location":"azure/service_bus/#subscriptions","text":"Subscriptions are named entities . Subscriptions are durable by default, but can be configured to expire and then be automatically deleted. Define Filters and rules You can define rules on a subscription. A subscription rule has a filter to define a condition for the message to be copied into the subscription and an optional action that can modify message metadata . Subscribers can define which messages they want to receive from a topic. These messages are specified in the form of one or more named subscription rules. Each rule consists of a filter condition that selects particular messages, and optionally contains an action that annotates the selected message.","title":"Subscriptions"},{"location":"azure/service_bus/#namespaces","text":"A Service Bus namespace is your own capacity slice of a large cluster made up of dozens of all-active virtual machines. Multiple queues and topics can be in a single namespace, and namespaces often serve as application containers.","title":"Namespaces"},{"location":"azure/service_bus/#partitioned","text":"Service Bus partitions enable queues and topics, or messaging entities, to be partitioned across multiple message brokers and messaging stores. Partitioning means that the overall throughput of a partitioned entity is no longer limited by the performance of a single message broker or messaging store. In addition, a temporary outage of a messaging store does not render a partitioned queue or topic unavailable","title":"Partitioned"},{"location":"azure/service_bus/#message-sessions","text":"To realize a first-in, first-out (FIFO) guarantee in processing messages in Service Bus queue or subscriptions, use sessions. Sessions can also be used in implementing request-response patterns. The request-response pattern enables the sender application to send a request and provides a way for the receiver to correctly send a response back to the sender application.","title":"Message sessions"},{"location":"azure/service_bus/#auto-forwarding","text":"The Auto-forwarding feature enables you to chain a queue or subscription to another queue or topic that is part of the same namespace. When auto-forwarding is enabled, Service Bus automatically removes messages that are placed in the first queue or subscription (source) and puts them in the second queue or topic (destination).","title":"Auto-forwarding"},{"location":"azure/service_bus/#dead-lettering","text":"Service Bus queues and topic subscriptions provide a secondary subqueue, called a dead-letter queue (DLQ) . The dead letter queue holds messages that can't be delivered to any receiver, or messages that can't be processed. You can then remove messages from the DLQ and inspect them.","title":"Dead-lettering"},{"location":"azure/service_bus/#auto-delete-on-idle","text":"Auto-delete on idle enables you to specify an idle interval after which the queue is automatically deleted. The interval is reset when there's traffic on the queue. The minimum duration is 5 minutes.","title":"Auto-delete on idle"},{"location":"azure/service_bus/#scheduled-delivery","text":"You can submit messages to a queue or topic for delayed processing; for example, to schedule a job to become available for processing by a system at a certain time","title":"Scheduled delivery"},{"location":"azure/snowflake/","text":"Features \u00b6 Compression \u00b6 Another fundamental engineering work from Snowflake was the proprietary and sizeable compression. Faster compression and less data transfer led to less Input Output (IO) costs and an overall faster architecture. It is not uncommon to see 5x compression when migrating data to Snowflake.","title":"Snowflake \u2744\ufe0f"},{"location":"azure/snowflake/#features","text":"","title":"Features"},{"location":"azure/snowflake/#compression","text":"Another fundamental engineering work from Snowflake was the proprietary and sizeable compression. Faster compression and less data transfer led to less Input Output (IO) costs and an overall faster architecture. It is not uncommon to see 5x compression when migrating data to Snowflake.","title":"Compression"},{"location":"azure/storage/","text":"Storage Accounts \ud83d\uddc4 \u00b6 Data objects in Azure Storage are accessible from anywhere in the world over HTTP or HTTPS via a REST API.The Azure Storage platform includes the following data services: Azure Blobs : A massively scalable object store . Also includes support for big data analytics through Data Lake Storage Gen2. Azure Files : Cloud based file sharing service.This file share can be mounted to multiple VMs or on-premises machines, which is ideal for sharing files across machines. Azure Queues : Allows for asynchronous message queueing between application components.Messages can be stored and retrieved using queues. These stored messages can be up to 64 KB in size and can be accessed from anywhere in the world over HTTP or HTTPS . Azure Tables : A NoSQL store for schemaless storage of semi-structured data. Tables is a NoSQL datastore that is now part of Azure Cosmos DB . Besides the Table Storage, Cosmos DB offers a new Table API with additional features such as turnkey failover, global distribution, automatic secondary indexes, and throughput optimized tables. Azure Disk : Azure Disks provides persistent storage to Azure Vm, Azure VMSS etc. Naming in Storage accounts The name for the storage account is unique across Azure. Each object stored in the storage account is represented using a unique URL . During the creation of the storage account, you need to pass the name of the storage account to the Azure Resource Manager. Using this storage account name, endpoints are created. For example, if the name of your storage account is amar_blog_storage , then the default endpoints will be as follows: Blobs https://amar_blog_storage.blob.core.windows.net Tables https://amar_blog_storage.table.core.windows.net Files https://amar_blog_storage.file.core.windows.net Queues https://amar_blog_storage.queue.core.windows.net Things to consider when architecting? These are things to consider for Azure blob store when doing architecture - Account type: v1 or v2 - Performance tier: standard or performance - Replication: LRS, ZRS, ZLRS, GRS - Access Tier: Hold, cold or archived. Blob/Disc/File diff \u00b6 Azure File Storage may be mounted as an SMB volume (so that all instances of your app can work with it). Note: This is not something readily supported with Web Apps currently - you'd only be able to write to the file share via API, not via attached disk. Azure File Storage volumes support up to 5TB each, and throughput is max. 60MB/sec across the share. It's backed by Azure blob storage (so, just as durable as blobs). Azure Disks are again blob-backed (page blobs), up to 1TB each. Each disk is mountable to a single VM. Throughput is higher than File Storage. Cannot be shared across VMs without your own solution to sync data. Once mounted and formatted, accessible just like any other local file (e.g. no modifications to your app) Azure blobs : they can be accessed via REST API/SDK and are not mountable as a disk/drive. Without modifying your app, you'd need to make sure blob contents were copied to local disk to perform operations on the content (you can't just open a blob as a file and modify it). Storage Account types \u00b6 General-Purpose v2 : This is recommended for most cases. This storage account type provides the blob, file, queue and table service. General-purpose v2 accounts deliver the lowest per-gigabyte capacity prices for Azure Storage, as well as industry-competitive transaction prices. General-purpose v1 : this also provides the blob, file, queue and table service but is older version of this account type. General Purpose v1 (GPv1) accounts do not support tiering. BlockBlobStorage : this is specifically when you want premium performance for storing block or appending blobs FileStorage : This is specifically when you want premium performance for file-ONLY storage BlobStorage : This is legacy storage account. Use General-purpose v2 account as much as possible. SA access types \u00b6 Access keys : These are not preferred as you give whole access. They provide unlimited access They are auto generated Identity based : Using on-prem AD or Azure AD. They use RBAC instead of keys. SAS tokens : We can provide more granular level details such as access level, dates and IP values. Azure blob \u00b6 As Azure Blob Storage is for unstructured data, you can store any type of text or binary data. Blob storage is also referred to as object storage Tip $logs is the system container that you should not mess with Blob structure is shown below Blob Storage comprises three resources. Storage account : Used to provide complete access. They are not recommended and should be stored in Key Vault . Container in the storage account Blobs/objects stored in the container Default access When you create a container, you need to provide the public access level , which specifies whether you want to expose the data stored in the container publicly. By default, the contents are private Types of blobs \u00b6 Block \u00b6 For audio and video files Append blobs \u00b6 For log files Page blob \u00b6 Used as VM disks Container/blob visibility \u00b6 We have the following options to control the visibility of container/blob: Private : This is the default option; no anonymous access is allowed to containers and blobs . Blob (public) : This will grant anonymous public read access to the blobs alone . Container (public) : This will grant anonymous public read and list access to the container and all the blobs stored inside the container . Storage Lifecycle \u00b6 Blob Access Tiers \u00b6 The access tiers are defined based on the usage pattern or frequency of access. When you create a file, it will be accessed frequently; gradually the frequency of access will reduce, and eventually you will not be accessing the file. However, you would still like to keep the file because of the data retention policies and for auditing purposes . Choosing the right access tier will help you optimize the cost of data storage and data access. Hot (Online) \u00b6 Optimized for frequent access of objects. From a cost perspective, accessing data in the Hot tier is the least expensive compared to the other tiers; however, the data storage costs are higher. When you create a new storage account, this is the default tier. Cool (Online) \u00b6 Optimized for storing data that is not accessed very frequently and is stored for at least 30 days. Storing data in the Cool tier is cheaper than the Hot tier; however, accessing data in the Cool tier is more expensive than Hot tier. Archive (Offline) \u00b6 Optimized for storing data that can tolerate hours of retrieval latency and will remain in the Archive tier for at least 180 days. When it comes to storing data, the Archive tier is the most cost-effective tier. How to access data from Archive tier? While a blob is in the archive tier, it can't be read or modified. To read or download a blob in the archive tier, you must first rehydrate it to an online tier, either hot or cool. Data in the archive tier can take up to 15 hours to rehydrate, depending on the priority you specify for the rehydration operation . Only storage accounts that are configured for LRS, GRS, or RA-GRS support moving blobs to the archive tier. The archive tier isn't supported for ZRS, GZRS, or RA-GZRS accounts. Be careful while choosing access tier If you are setting up the access tier from the storage account level, you will have only two choices: Hot and Cool . This access tier will be inherited by all objects stored in the storage account. The Archive tier can be set at the individual object level only. Tiering Policy \u00b6 Azure Storage lifecycle management offers a rule-based policy that you can use to transition blob data to the appropriate access tiers or to expire data at the end of the data lifecycle. More than 1 policy on same blob If you define more than one action on the same blob, lifecycle management applies the least expensive action to the blob. For example, action delete is cheaper than action tierToArchive . Action tierToArchive is cheaper than action tierToCool . Object replication \u00b6 Object replication asynchronously copies block blobs in a container according to rules that you configure. The contents of the blob, any versions associated with the blob, and the blob's metadata and properties are all copied from the source container to the destination container. Requirements for replication Object replication requires that the following Azure Storage features to be enabled: Change feed : Must be enabled on the source account. Blob versioning : Must be enabled on both the source and destination accounts. The replicaiton process between 2 SA's is shown below Immutable blob (WORM) \u00b6 Immutable storage for Azure Blob Storage enables users to store business-critical data in a WORM (Write Once, Read Many) state. While in a WORM state, data cannot be modified or deleted for a user-specified interval. By configuring immutability policies for blob data, you can protect your data from overwrites and deletes. Immutable storage for Azure Blob Storage supports 2 types of immutability policies: Time-based retention policies : With a time-based retention policy, users can set policies to store data for a specified interval. When a time-based retention policy is set, objects can be created and read, but not modified or deleted . After the retention period has expired, objects can be deleted but not overwritten. Legal hold policies : A legal hold stores immutable data until the legal hold is explicitly cleared. When a legal hold is set, objects can be created and read, but not modified or deleted. Encryption scopes \u00b6 Encryption scopes enable you to manage encryption with a key that is scoped to a container or an individual blob. You can use encryption scopes to create secure boundaries between data that resides in the same storage account but belongs to different customers. How encryption scopes work? By default, a storage account is encrypted with a key that is scoped to the entire storage account . When you define an encryption scope, you specify a key that may be scoped to a container or an individual blob . When the encryption scope is applied to a blob, the blob is encrypted with that key. When the encryption scope is applied to a container, it serves as the default scope for blobs in that container, so that all blobs that are uploaded to that container may be encrypted with the same key. Azure File \u00b6 It has hierarchical file structure Generic file share structure ## Generic structure < storageAccountName >. file . core . windows . net \\ < fileShareName > # if storageAccountName == amarTestStorage and fileShareName == dataFile01 , then URI is amarTestStorage . file . core . windows . net \\ dataFile01 Difference between Azure File and Azure Blob? Azure Blobs uses a flat namespace that includes containers and objects. Azure Files uses directory objects as you have seen with our traditional file shares. Azure Blobs is accessed via containers , and Azure Files is accessed through file shares . Azure Blobs is accessed via an HTTP/HTTPS connection, and Azure Files is accessed via the SMB/NFS protocol when mounted to a virtual machine. Azure Blobs doesn\u2019t need to be mounted and can be accessed directly from any client that supports HTTP calls. Azure Files needs to be mounted to virtual machines before working with the data . On a side note, you can still manage the files in Azure Files via tools like the Azure portal and Azure Storage Explorer without the need to mount it. Azure File Share \u00b6 File shares are not something new; you still have on-premises file shares that serve your enterprise needs and requirements. Azure File Share is a cloud-based file share that enables you to access the file share from any computer anywhere in the world. Azure File Sync \u00b6 With Azure File Sync , you will be using Azure Files as a centralized location for storing files. In simple words, you can synchronize the files you have on-premises with Azure File Share . An Azure File Sync deployment has these components: Azure file share : An Azure file share is a serverless cloud file share, which provides the cloud endpoint of an Azure File Sync relationship . Server endpoint : The path on the Windows Server that is being synced to an Azure file share. This can be a specific folder on a volume or the root of the volume. Multiple server endpoints can exist on the same volume if their namespaces do not overlap (as shown in the figure) Sync group : The object that defines the sync relationship between a cloud endpoint, or Azure file share, and a server endpoint . Endpoints within a sync group are kept in sync with each other. If for example, you have two distinct sets of files that you want to manage with Azure File Sync, you would create two sync groups and add different endpoints to each sync group. File Share steps \u00b6 Steps to synchronize the files in the file share named data to an on-premises server named Server1 Step 1: Install the Azure File Sync agent on Server1 - The Azure File Sync agent is a downloadable package that enables Windows Server to be synced with an Azure file share. Step 2: Register Server1 - Register Windows Server with Storage Sync Service. Registering your Windows Server with a Storage Sync Service establishes a trust relationship between your server and the Storage Sync Service. Step 3: Create a sync group and a cloud endpoint - A sync group defines the sync topology for a set of files. Endpoints within a sync group are kept in sync with each other. A sync group must contain one cloud, which represents an Azure file share and one or more server endpoints. A server endpoint represents a path on registered server. Storage Security \u00b6 Remember You can control fine-grained access to data objects using SAS token instead of using access keys as you can define time-bound access . Authorization options \u00b6 The following are the authorization options available to Azure Storage: Azure AD : Using Azure AD: you can authorize access to Azure Storage via role-based access control (RBAC). With RBAC, you can assign fine-grained access to users, groups, or applications. Shared Key : Every storage account has two keys: primary and secondary. The access keys of the storage account will be used in the Authorization header of the API calls. Shared Access Signatures (SAS), you can limit access to services with specified permissions and over a specified timeframe. Anonymous Access to Containers and Blobs : If you set the access level to blob or container, you can have public access to the objects without the need to pass keys or grant authorization. AzCopy \u00b6 AzCopy is a next-generation command-line tool for copying data from or to Azure Blob and Azure Files. Behind the scenes, Azure Storage Explorer uses AzCopy to accomplish all the data transfer operations. With AzCopy you can copy data in the following scenarios: Copy data from a local machine to Azure Blobs or Azure Files Copy data from Azure Blobs or Azure Files to a local machine Copy data between storage accounts Custom Domain Configuration \u00b6 Accessing the service using the default domain may be hectic, and customers would like to use their own custom domains to represent their storage space.","title":"Storage Account \ud83d\uddc4"},{"location":"azure/storage/#storage-accounts","text":"Data objects in Azure Storage are accessible from anywhere in the world over HTTP or HTTPS via a REST API.The Azure Storage platform includes the following data services: Azure Blobs : A massively scalable object store . Also includes support for big data analytics through Data Lake Storage Gen2. Azure Files : Cloud based file sharing service.This file share can be mounted to multiple VMs or on-premises machines, which is ideal for sharing files across machines. Azure Queues : Allows for asynchronous message queueing between application components.Messages can be stored and retrieved using queues. These stored messages can be up to 64 KB in size and can be accessed from anywhere in the world over HTTP or HTTPS . Azure Tables : A NoSQL store for schemaless storage of semi-structured data. Tables is a NoSQL datastore that is now part of Azure Cosmos DB . Besides the Table Storage, Cosmos DB offers a new Table API with additional features such as turnkey failover, global distribution, automatic secondary indexes, and throughput optimized tables. Azure Disk : Azure Disks provides persistent storage to Azure Vm, Azure VMSS etc. Naming in Storage accounts The name for the storage account is unique across Azure. Each object stored in the storage account is represented using a unique URL . During the creation of the storage account, you need to pass the name of the storage account to the Azure Resource Manager. Using this storage account name, endpoints are created. For example, if the name of your storage account is amar_blog_storage , then the default endpoints will be as follows: Blobs https://amar_blog_storage.blob.core.windows.net Tables https://amar_blog_storage.table.core.windows.net Files https://amar_blog_storage.file.core.windows.net Queues https://amar_blog_storage.queue.core.windows.net Things to consider when architecting? These are things to consider for Azure blob store when doing architecture - Account type: v1 or v2 - Performance tier: standard or performance - Replication: LRS, ZRS, ZLRS, GRS - Access Tier: Hold, cold or archived.","title":"Storage Accounts \ud83d\uddc4"},{"location":"azure/storage/#blobdiscfile-diff","text":"Azure File Storage may be mounted as an SMB volume (so that all instances of your app can work with it). Note: This is not something readily supported with Web Apps currently - you'd only be able to write to the file share via API, not via attached disk. Azure File Storage volumes support up to 5TB each, and throughput is max. 60MB/sec across the share. It's backed by Azure blob storage (so, just as durable as blobs). Azure Disks are again blob-backed (page blobs), up to 1TB each. Each disk is mountable to a single VM. Throughput is higher than File Storage. Cannot be shared across VMs without your own solution to sync data. Once mounted and formatted, accessible just like any other local file (e.g. no modifications to your app) Azure blobs : they can be accessed via REST API/SDK and are not mountable as a disk/drive. Without modifying your app, you'd need to make sure blob contents were copied to local disk to perform operations on the content (you can't just open a blob as a file and modify it).","title":"Blob/Disc/File diff"},{"location":"azure/storage/#storage-account-types","text":"General-Purpose v2 : This is recommended for most cases. This storage account type provides the blob, file, queue and table service. General-purpose v2 accounts deliver the lowest per-gigabyte capacity prices for Azure Storage, as well as industry-competitive transaction prices. General-purpose v1 : this also provides the blob, file, queue and table service but is older version of this account type. General Purpose v1 (GPv1) accounts do not support tiering. BlockBlobStorage : this is specifically when you want premium performance for storing block or appending blobs FileStorage : This is specifically when you want premium performance for file-ONLY storage BlobStorage : This is legacy storage account. Use General-purpose v2 account as much as possible.","title":"Storage Account types"},{"location":"azure/storage/#sa-access-types","text":"Access keys : These are not preferred as you give whole access. They provide unlimited access They are auto generated Identity based : Using on-prem AD or Azure AD. They use RBAC instead of keys. SAS tokens : We can provide more granular level details such as access level, dates and IP values.","title":"SA access types"},{"location":"azure/storage/#azure-blob","text":"As Azure Blob Storage is for unstructured data, you can store any type of text or binary data. Blob storage is also referred to as object storage Tip $logs is the system container that you should not mess with Blob structure is shown below Blob Storage comprises three resources. Storage account : Used to provide complete access. They are not recommended and should be stored in Key Vault . Container in the storage account Blobs/objects stored in the container Default access When you create a container, you need to provide the public access level , which specifies whether you want to expose the data stored in the container publicly. By default, the contents are private","title":"Azure blob"},{"location":"azure/storage/#types-of-blobs","text":"","title":"Types of blobs"},{"location":"azure/storage/#block","text":"For audio and video files","title":"Block"},{"location":"azure/storage/#append-blobs","text":"For log files","title":"Append blobs"},{"location":"azure/storage/#page-blob","text":"Used as VM disks","title":"Page blob"},{"location":"azure/storage/#containerblob-visibility","text":"We have the following options to control the visibility of container/blob: Private : This is the default option; no anonymous access is allowed to containers and blobs . Blob (public) : This will grant anonymous public read access to the blobs alone . Container (public) : This will grant anonymous public read and list access to the container and all the blobs stored inside the container .","title":"Container/blob visibility"},{"location":"azure/storage/#storage-lifecycle","text":"","title":"Storage Lifecycle"},{"location":"azure/storage/#blob-access-tiers","text":"The access tiers are defined based on the usage pattern or frequency of access. When you create a file, it will be accessed frequently; gradually the frequency of access will reduce, and eventually you will not be accessing the file. However, you would still like to keep the file because of the data retention policies and for auditing purposes . Choosing the right access tier will help you optimize the cost of data storage and data access.","title":"Blob Access Tiers"},{"location":"azure/storage/#hot-online","text":"Optimized for frequent access of objects. From a cost perspective, accessing data in the Hot tier is the least expensive compared to the other tiers; however, the data storage costs are higher. When you create a new storage account, this is the default tier.","title":"Hot (Online)"},{"location":"azure/storage/#cool-online","text":"Optimized for storing data that is not accessed very frequently and is stored for at least 30 days. Storing data in the Cool tier is cheaper than the Hot tier; however, accessing data in the Cool tier is more expensive than Hot tier.","title":"Cool (Online)"},{"location":"azure/storage/#archive-offline","text":"Optimized for storing data that can tolerate hours of retrieval latency and will remain in the Archive tier for at least 180 days. When it comes to storing data, the Archive tier is the most cost-effective tier. How to access data from Archive tier? While a blob is in the archive tier, it can't be read or modified. To read or download a blob in the archive tier, you must first rehydrate it to an online tier, either hot or cool. Data in the archive tier can take up to 15 hours to rehydrate, depending on the priority you specify for the rehydration operation . Only storage accounts that are configured for LRS, GRS, or RA-GRS support moving blobs to the archive tier. The archive tier isn't supported for ZRS, GZRS, or RA-GZRS accounts. Be careful while choosing access tier If you are setting up the access tier from the storage account level, you will have only two choices: Hot and Cool . This access tier will be inherited by all objects stored in the storage account. The Archive tier can be set at the individual object level only.","title":"Archive (Offline)"},{"location":"azure/storage/#tiering-policy","text":"Azure Storage lifecycle management offers a rule-based policy that you can use to transition blob data to the appropriate access tiers or to expire data at the end of the data lifecycle. More than 1 policy on same blob If you define more than one action on the same blob, lifecycle management applies the least expensive action to the blob. For example, action delete is cheaper than action tierToArchive . Action tierToArchive is cheaper than action tierToCool .","title":"Tiering Policy"},{"location":"azure/storage/#object-replication","text":"Object replication asynchronously copies block blobs in a container according to rules that you configure. The contents of the blob, any versions associated with the blob, and the blob's metadata and properties are all copied from the source container to the destination container. Requirements for replication Object replication requires that the following Azure Storage features to be enabled: Change feed : Must be enabled on the source account. Blob versioning : Must be enabled on both the source and destination accounts. The replicaiton process between 2 SA's is shown below","title":"Object replication"},{"location":"azure/storage/#immutable-blob-worm","text":"Immutable storage for Azure Blob Storage enables users to store business-critical data in a WORM (Write Once, Read Many) state. While in a WORM state, data cannot be modified or deleted for a user-specified interval. By configuring immutability policies for blob data, you can protect your data from overwrites and deletes. Immutable storage for Azure Blob Storage supports 2 types of immutability policies: Time-based retention policies : With a time-based retention policy, users can set policies to store data for a specified interval. When a time-based retention policy is set, objects can be created and read, but not modified or deleted . After the retention period has expired, objects can be deleted but not overwritten. Legal hold policies : A legal hold stores immutable data until the legal hold is explicitly cleared. When a legal hold is set, objects can be created and read, but not modified or deleted.","title":"Immutable blob (WORM)"},{"location":"azure/storage/#encryption-scopes","text":"Encryption scopes enable you to manage encryption with a key that is scoped to a container or an individual blob. You can use encryption scopes to create secure boundaries between data that resides in the same storage account but belongs to different customers. How encryption scopes work? By default, a storage account is encrypted with a key that is scoped to the entire storage account . When you define an encryption scope, you specify a key that may be scoped to a container or an individual blob . When the encryption scope is applied to a blob, the blob is encrypted with that key. When the encryption scope is applied to a container, it serves as the default scope for blobs in that container, so that all blobs that are uploaded to that container may be encrypted with the same key.","title":"Encryption scopes"},{"location":"azure/storage/#azure-file","text":"It has hierarchical file structure Generic file share structure ## Generic structure < storageAccountName >. file . core . windows . net \\ < fileShareName > # if storageAccountName == amarTestStorage and fileShareName == dataFile01 , then URI is amarTestStorage . file . core . windows . net \\ dataFile01 Difference between Azure File and Azure Blob? Azure Blobs uses a flat namespace that includes containers and objects. Azure Files uses directory objects as you have seen with our traditional file shares. Azure Blobs is accessed via containers , and Azure Files is accessed through file shares . Azure Blobs is accessed via an HTTP/HTTPS connection, and Azure Files is accessed via the SMB/NFS protocol when mounted to a virtual machine. Azure Blobs doesn\u2019t need to be mounted and can be accessed directly from any client that supports HTTP calls. Azure Files needs to be mounted to virtual machines before working with the data . On a side note, you can still manage the files in Azure Files via tools like the Azure portal and Azure Storage Explorer without the need to mount it.","title":"Azure File"},{"location":"azure/storage/#azure-file-share","text":"File shares are not something new; you still have on-premises file shares that serve your enterprise needs and requirements. Azure File Share is a cloud-based file share that enables you to access the file share from any computer anywhere in the world.","title":"Azure File Share"},{"location":"azure/storage/#azure-file-sync","text":"With Azure File Sync , you will be using Azure Files as a centralized location for storing files. In simple words, you can synchronize the files you have on-premises with Azure File Share . An Azure File Sync deployment has these components: Azure file share : An Azure file share is a serverless cloud file share, which provides the cloud endpoint of an Azure File Sync relationship . Server endpoint : The path on the Windows Server that is being synced to an Azure file share. This can be a specific folder on a volume or the root of the volume. Multiple server endpoints can exist on the same volume if their namespaces do not overlap (as shown in the figure) Sync group : The object that defines the sync relationship between a cloud endpoint, or Azure file share, and a server endpoint . Endpoints within a sync group are kept in sync with each other. If for example, you have two distinct sets of files that you want to manage with Azure File Sync, you would create two sync groups and add different endpoints to each sync group.","title":"Azure File Sync"},{"location":"azure/storage/#file-share-steps","text":"Steps to synchronize the files in the file share named data to an on-premises server named Server1 Step 1: Install the Azure File Sync agent on Server1 - The Azure File Sync agent is a downloadable package that enables Windows Server to be synced with an Azure file share. Step 2: Register Server1 - Register Windows Server with Storage Sync Service. Registering your Windows Server with a Storage Sync Service establishes a trust relationship between your server and the Storage Sync Service. Step 3: Create a sync group and a cloud endpoint - A sync group defines the sync topology for a set of files. Endpoints within a sync group are kept in sync with each other. A sync group must contain one cloud, which represents an Azure file share and one or more server endpoints. A server endpoint represents a path on registered server.","title":"File Share steps"},{"location":"azure/storage/#storage-security","text":"Remember You can control fine-grained access to data objects using SAS token instead of using access keys as you can define time-bound access .","title":"Storage Security"},{"location":"azure/storage/#authorization-options","text":"The following are the authorization options available to Azure Storage: Azure AD : Using Azure AD: you can authorize access to Azure Storage via role-based access control (RBAC). With RBAC, you can assign fine-grained access to users, groups, or applications. Shared Key : Every storage account has two keys: primary and secondary. The access keys of the storage account will be used in the Authorization header of the API calls. Shared Access Signatures (SAS), you can limit access to services with specified permissions and over a specified timeframe. Anonymous Access to Containers and Blobs : If you set the access level to blob or container, you can have public access to the objects without the need to pass keys or grant authorization.","title":"Authorization options"},{"location":"azure/storage/#azcopy","text":"AzCopy is a next-generation command-line tool for copying data from or to Azure Blob and Azure Files. Behind the scenes, Azure Storage Explorer uses AzCopy to accomplish all the data transfer operations. With AzCopy you can copy data in the following scenarios: Copy data from a local machine to Azure Blobs or Azure Files Copy data from Azure Blobs or Azure Files to a local machine Copy data between storage accounts","title":"AzCopy"},{"location":"azure/storage/#custom-domain-configuration","text":"Accessing the service using the default domain may be hectic, and customers would like to use their own custom domains to represent their storage space.","title":"Custom Domain Configuration"},{"location":"azure/vnet/","text":"Virtual Network \ud83d\udd78 \u00b6 VNet enables many types of Azure resources, such as Azure Virtual Machines (VM), to securely communicate with each other. Virtual networks and subnets span all availability zones in a region. You don't need to divide them by availability zones to accommodate zonal resources Default communication to Internet All resources in a VNet can communicate outbound to the internet, by default. You can communicate inbound to a resource by assigning a public IP address or a public Load Balancer. VNet components \u00b6 Address space \u00b6 When creating a VNet, you must specify a custom private IP address space using public and private addresses. Azure assigns resources in a virtual network a private IP address from the address space that you assign. For example, if you deploy a VM in a VNet with address space, 10.0.0.0/16 , the VM will be assigned a private IP like 10.0.0.4 . Address range Largest is x.x.x.x/8 and smallest is x.x.x.x/29 Subnets: \u00b6 Subnets enable you to segment the virtual network into one or more subnetworks and allocate a portion of the virtual network\u2019s address space to each subnet. Reserved IP addresses are x.x.x.0-3 and x.x.x.255 . Some tips for VNets \u00b6 Private IP addresses are supported using in-built DHCP server . Vnet supportes IPv4 and IPV6 public IPs VNET Subnet: They are used as Gateway subnets We can have a default subnet Vnet has Azure provided DNS , we can also use the custom DNS too. At the subnet level, you can configure Network Security Groups (NSGs) to secure your workloads A DMZ subnet use case \u00b6 In this typical use case, our goal is to make sure that all traffic coming from the public subnet should be routed to the DMZ before it gets routed to the private subnet. The reason for this is that a public subnet contains workloads that are Internet facing, and a private subnet contains workloads that are not exposed to the Internet, such as databases, application logic, backend servers, etc . By default, the communication from the public subnet is allowed to the private subnet; however, we need to make sure that the traffic is filtered before it reaches the private subnet. This will help us to protect the private workloads, as all traffic is matched against the rules that you have set up in the NVA (WAF in this case) . If there is any attack or malicious traffic, NVA will take care of it. Now, the question is, how can we implement this? As you can see in above diagram, we have used the routing table with User Defined Route (UDR) to accomplish this task. By default, the traffic from the public subnet to the private subnet is allowed, but using UDR, you are forcing the packets to go through the DMZ subnet . Regions \u00b6 VNet is scoped to a single region/location ; however, multiple virtual networks from different regions can be connected together using peering. Choose region with care When you create a virtual network in Azure, you will get an option to choose the region. Depending on the region you choose, the virtual network will be deployed to the respective region, and the virtual machines deployed to the virtual network will also fall under the same region. Subscription \u00b6 VNet is scoped to a subscription. You can implement multiple virtual networks within each Azure subscription and Azure region. VNET peering \u00b6 Virtual network peering enables you to seamlessly connect two or more Virtual Networks in Azure. The virtual networks appear as one for connectivity purposes. The traffic between virtual machines in peered virtual networks uses the Microsoft backbone infrastructure. Like traffic between virtual machines in the same network, traffic is routed through Microsoft's private network only. VNET non-reciprocal and non-trasitive connection If we setup a connection between 2 VNets, that connection is non-reciprocal as shown in the diagram below. That is why we have to setup 2 connections in this case. The connction between different VNETS is non-trasitive as shown below. Active-passive and active-active peering are possible. Lets take an example of on-site and cloud peering with active-passive mode as shown below Azure supports the following types of peering: Virtual network peering : Connecting virtual networks within the same Azure region. Global virtual network peering : Connecting virtual networks across Azure regions. Express Route \u00b6 ExpressRoute lets you extend your on-premises networks into the Microsoft cloud over a private connection with the help of a connectivity provider. With ExpressRoute , you can establish connections to Microsoft cloud services, such as Microsoft Azure and Microsoft 365 . Remember ExpressRoute connections don't go over the public Internet. This allows ExpressRoute connections to offer more reliability, faster speeds We have 2 options in Express routes ExpressRoute Direct : It does not require connectivity provider (service provider), it directly connects to Microsoft Edge Routers What is FastPath? FastPath is an additional feature that can be used with ExpressRoute Direc. When enabled, FastPath sends network traffic directly to virtual machines in the virtual network, bypassing the gateway ( ExpressRoute gateway is still needed though) ExpresRoute Circuit : It uses a service provider . Points to remember Allocate two /30 subnets that are not used anywhere else in your network topology. One subnet will be used for the primary connction ; the other will be used for the secondary connction as shown in the diagram above. From each of these subnets, you will assign the first usable IP address to your router because Microsoft uses the second usable IP for its router. A valid VLAN ID is required to establish this peering on. Ensure that no other peering in the circuit uses the same VLAN ID. For both primary and secondary links you must use the same VLAN ID. This information is provided from your provider. Circuits and Peering \u00b6 Circuit \u00b6 ExpressRoute circuit is a logical connection between on-premises infrastructure and Microsoft cloud services through a connectivity provider ExpressRoute circuits do not map to any physical entities A circuit is uniquely identified by a standard GUID called as a service key (s-key) There is a 1:1 mapping between an ExpressRoute circuit and the s-key The service key is the only piece of information exchanged between Microsoft, the connectivity provider, and you (s-key is not a secret for security purposes) A circuit consists of 2 connections: Primary connection : main Secondary connection : redundent for HA Peering \u00b6 A peering is the interconnection between on-premise network and Microsoft cloud services (Azure, Microsoft 365). ExpressRoute circuits can include two independent peerings: - Private peering : Azure VNet - Microsoft peering : Microsoft 365, Dynamics 365 etc. Basics \u00b6 NIC \u00b6 Virtual machines have one or more network interface cards (NICs) that exist in the same region as the virtual network. You assign an IP address (either statically or dynamically) from a subnet in the virtual network. This action allows the virtual machine to communicate with other resources in the virtual network (and any peered network). Public IPs \u00b6 Public IP addresses are associated with a virtual machine NIC, public load balancer, VPN gateways, application gateways, and any other resource that can be accessed from the Internet. Here also we can choose the allocation method to be static or dynamic . However, the availability of allocation methods depends on which SKU of public IP address we are using. The SKU is more like a pricing tier, where you will find different prices based on which SKU you are selecting Basic SKU : Static or Dynamic IP that is accessible by default and needs NSG to configure Standard SKU : Static PIP, that is not accessible by defult and needs NSG to allow traffic Private Endpoint \u00b6 Before defining private endpoint, lets clear some basics terms: Some basics: revised Azure Private Link enables you to access Azure PaaS Services (for example, Azure Storage and SQL Database) and Azure hosted customer-owned/partner services over a Private Endpoint in your virtual network. Traffic between your virtual network and the service traverses over the Microsoft backbone network, eliminating exposure from the public Internet. Azure Private Link service is the reference to your own service that is powered by Azure Private Link. A private-link resource is the destination target of a specified private endpoint. A private endpoint is a network interface that uses a private IP address from your virtual network. This network interface connects you privately and securely to a service that's powered by Azure Private Link. By enabling a private endpoint, you're bringing the service into your virtual network. Private endpoint has NIC attached to it. The value of the private IP address remains unchanged for the entire lifecycle of the private endpoint. The private endpoint must be deployed in the same region and subscription as the virtual network. The private-link resource can be deployed in a different region than the one for the virtual network and private endpoint. Private Endpoint Example Imgine that there is a virtual network using the address range 10.10.0.0/16. Inside this virtual network are two subnets, SubnetA ( 10.10.5.0/24 ) and SubnetB ( 10.10.6.0/24 ). One or more virtual machines reside in SubnetA . A storage account has a private endpoint with an IP address of 10.10.6.5 inside SubnetB . The virtual machines inside SubnetA access the storage account through the storage account\u2019s private endpoint IP address located in SubnetB . Resources in other networks can access the storage account through the private endpoint IP address. Using private endpoints we can drill down to a sub-resource as shown below. This can't be done using Service Endpoint as it will allow access to the resource level instead of sub-resource level. When to use Private endpoint? Private endpoints enable connectivity between the customers from the same: Virtual network Regionally peered virtual networks Globally peered virtual networks On-premises environments that use VPN or Express Route Services that are powered by Private Link Private Link Service \u00b6 Azure Private Link service is the reference to your own service that is powered by Azure Private Link . Your service that is running behind Azure Standard Load Balancer can be enabled for Private Link access so that consumers to your service can access it privately from their own VNets. Your customers can create a private endpoint inside their virtual network and map it to this service. Service Endpoint \u00b6 Service endpoints direct VNet traffic off the public Internet and to the Azure backbone network . They are enabled per subnet using policies. This service does not provide private IPs to the services. Service Endpoints enables private IP addresses in the their VNet to reach the endpoint of an Azure service without needing a public IP address on the their VNet. These VM's will connect to public IP of the service though. Service endpoints apply to all instances of the Azure resource, not just the ones you create. If you want to limit virtual network traffic to specific instances or regions of a resource, you need a service endpoint policy. Example of service endpoint Review the image below representing service endpoints in action. In this example, you have enabled a service endpoint for the Microsoft.SQL service on SubnetA . A virtual machine in SubnetA uses its private IP address to access a SQL server hosting several databases. The virtual machine connects using the public IP addresses associated with the Microsoft.SQL service Notice that Subnet B does not have service endpoints enabled for its subnet for the Microsoft.SQL service . Compute resources in this subnet use the public IP address for the virtual network to connect to the public IP address of the Azure service. Service Endpoint Policies \u00b6 Virtual Network (VNet) service endpoint policies allow you to filter egress virtual network traffic to Azure Storage accounts over service endpoint , and allow data exfiltration to only specific Azure Storage accounts. Endpoint policies provide granular access control for virtual network traffic to Azure Storage when connecting over service endpoint. Private Vs Service Endpoints \u00b6 Public and Private IPs in service/private endpoints With service endpoints , you're still connecting to the target resource's public endpoint. This effectively extends the identity of the VNet to the target resource. With private endpoints , you're assigning the target resource a private IP address from the VNet, essentially bringing it into the network. The target resource's public IP address doesn't go away, but you can lock it down so all traffic from the internet is denied. Here are some tips on when to use which endpint When to use which endpoint? If you want to be able to block all internet traffic to a target resource, use a private endpoint. If you're dealing with traffic from on-premises, use a private endpoint. If you want to secure a specific sub-resource to your VNet resources, use a private endpoint. If you want to secure a specific storage account to your VNet resources, you can use a private endpoint, or a service endpoint with a service endpoint policy . If you don't need a private IP address at the destination, service endpoints are considerably easier to create and maintain, and they don't require special DNS configuration. if cost is a concern, note that service endpoints are free. Network Routing types \u00b6 Network routes or route tables have existed in traditional networks for an exceptionally long time. The routes that are part of the route table decide how to direct a packet to the destination. System routes \u00b6 They are built in routes which cannot be modified VM's default communicaiton to internet Whenever we create a VM in Azure, the VM will be able to communicate with the Internet without setting up any routes . In AWS, we need to create different gateways like NAT Gateway or Internet Gateway to facilitate the connection from a VM to the Internet. However, in Azure this is enabled by default with the help of system routes Custom/ User defined routes (UDR) \u00b6 User defined routes (UDR) or BGP routes which can override the system routes . Precenence order of the routes Custom routes > BGP routes > System routes Block access to public internet We can block access to public internet by adding a custom route from 0.0.0.0/0 to None as shown below NSG (L3 and L4) \u00b6 Tldr They are used to control the flow of traffic inside the VNET and are similar to Firewalls An NSG is a collection of security rules that can be used to allow or deny inbound or outbound traffic. Is NSG associted with subnet? As shown in the diagram, NSG can be associated with a subnet , or a NIC based on your requirement. NSGs are reusable, which means you can have multiple NICs or subnet associations to a single NSG. NSG as no effect unless its associated with either NIC or a Subnet. Priorities are used in NSG to evaluate; lower the number, higher the priority. NSG Effective rules \u00b6 When you apply NSG to both subnet and NIC, then effective rules come into the picture . If you are using NSGs at both the subnet and NIC level, the rules will be evaluated at both levels. Rules evaluation order Inbound Traffic : Each security rule will be evaluated independently when the NSG is applied to both the subnet and NIC levels. Incoming traffic will be first evaluated against the rules applied on the subnet level; if there is an allow rule, then the packet will be sent to the NIC. The rules set at the NIC level will be evaluated now, and only if there is an allow rule then the packet is allowed; otherwise, it will be dropped. In short, first at Subnet then NIC Outbound Traffic : all outgoing traffic will be first evaluated against the NIC rules; if there is an allow rule, then the traffic is evaluated at the subnet level. In short, first at NIC then Subnet NVA \u00b6 Network Virtual Appliance NVA s are VMs that can optimize our networks with routing and firewall capabilities. Azure Firewall (L7 and L4) \u00b6 Using Azure Firewall, we can create, enforce, and manage network policies across virtual networks and subscriptions. Azure Firewall provides more features then NSG. Also, Azure firewall is different than WAF Setting for Azure firewall is shown below Do we need WAF in addition to NSG? Yes, we need both. The main difference is that NSG operates at layers 3 and 4 of the OSI layer; on the other hand, Azure Firewall works at layers 7 and 4. It comes in 3 flavours Standard Premium Basic FQDN Tags \u00b6 We can use FQDN tag in the Azure firewall rules. An FQDN tag represents a group of fully qualified domain names (FQDNs) associated with well known Microsoft services. Service tags \u00b6 A service tag represents a group of IP address prefixes to help minimize complexity for security rule creation. You can\u2019t create your own service tag, nor specify which IP addresses are included within a tag. Microsoft manages the address prefixes encompassed by the service tag, and automatically updates the service tag as addresses change. How to use service tags? You can use service tags to define network access controls on network security groups , Azure Firewall , and user-defined routes . Use service tags in place of specific IP addresses when you create security rules and routes. By specifying the service tag name, such as Storage , in the appropriate source or destination field of a security rule, you can allow or deny the traffic for the corresponding service. 2 tags are used in the below diagram in the destination fields Firewall Policy/Rules \u00b6 Rule collection group : It is used to group rule collections. They're the first unit to be processed by the Azure Firewall and they follow a priority order based on values Rule collection : it belongs to a rule collection group, and it contains one or multiple rules. They're the second unit processed by the firewall and they follow a priority order based on values. Rule collections must have a defined action (allow or deny) and a priority value Rules used in Firewall are of 3 types NAT Network (L4) Application (L7) Priority of Firewall rules NAT rule > Network rule > Application rule NAT rules \u00b6 Using a NAT rule collection, you will be able to translate a public IP and port to a private IP and port NAT example using VM Assume you have a Ubuntu server with private IP address, say, 10.0.0.2 , and the SSH server is available on port 22. Instead of exposing this port and IP to the Internet, you can create a NAT rule on the firewall. Using the public IP of the firewall and a random port, you can set up a NAT rule. Let\u2019s assume that the public IP address of the firewall is 66.142.43.02 ; using NAT rule you can redirect all traffic hitting port 5000 of the firewall to be translated to 10.0.0.2 and port 22. In short, translate 66.142.43.02:5000 to 10.0.0.2:22 . In this way, you can translate the public IP address and port to a private IP address and port. Inbound DNAT \u00b6 Inbound Internet network traffic to your firewall public IP address is translated ( Destination Network Address Translation ) and filtered to the private IP addresses on your virtual networks. Outbound SNAT \u00b6 All outbound virtual network traffic IP addresses are translated to the Azure Firewall public IP ( Source Network Address Translation ). You can identify and allow traffic originating from your virtual network to remote Internet destinations. Network rules \u00b6 A network rule should be in place for any non-HTTP/S traffic to be allowed through the firewall. For a source to communicate with a destination deployed behind a firewall, you need to have a network rule configured from the source to the destination. If there is no rule that specifically calls out this incoming traffic, then it will be dropped. Tldr You can use a network rule when you want to filter traffic based on IP addresses, any ports, and any protocols. Application rules \u00b6 Application rules allow or deny outbound and east-west traffic based on the application layer (L7). You can use an application rule when you want to filter traffic based on fully qualified domain names (FQDNs), URLs, and HTTP/HTTPS protocols. if you configure network rules and application rules, then network rules are applied in priority order before application rules. The rules are terminating. So, if a match is found in a network rule, no other rules are processed. ASG \u00b6 Application security groups enable you to configure network security as a natural extension of an application's structure, allowing you to group virtual machines and define network security policies based on those groups. You can reuse your security policy at scale without manual maintenance of explicit IP addresses.","title":"VNet \ud83d\udd78"},{"location":"azure/vnet/#virtual-network","text":"VNet enables many types of Azure resources, such as Azure Virtual Machines (VM), to securely communicate with each other. Virtual networks and subnets span all availability zones in a region. You don't need to divide them by availability zones to accommodate zonal resources Default communication to Internet All resources in a VNet can communicate outbound to the internet, by default. You can communicate inbound to a resource by assigning a public IP address or a public Load Balancer.","title":"Virtual Network \ud83d\udd78"},{"location":"azure/vnet/#vnet-components","text":"","title":"VNet components"},{"location":"azure/vnet/#address-space","text":"When creating a VNet, you must specify a custom private IP address space using public and private addresses. Azure assigns resources in a virtual network a private IP address from the address space that you assign. For example, if you deploy a VM in a VNet with address space, 10.0.0.0/16 , the VM will be assigned a private IP like 10.0.0.4 . Address range Largest is x.x.x.x/8 and smallest is x.x.x.x/29","title":"Address space"},{"location":"azure/vnet/#subnets","text":"Subnets enable you to segment the virtual network into one or more subnetworks and allocate a portion of the virtual network\u2019s address space to each subnet. Reserved IP addresses are x.x.x.0-3 and x.x.x.255 .","title":"Subnets:"},{"location":"azure/vnet/#some-tips-for-vnets","text":"Private IP addresses are supported using in-built DHCP server . Vnet supportes IPv4 and IPV6 public IPs VNET Subnet: They are used as Gateway subnets We can have a default subnet Vnet has Azure provided DNS , we can also use the custom DNS too. At the subnet level, you can configure Network Security Groups (NSGs) to secure your workloads","title":"Some tips for VNets"},{"location":"azure/vnet/#a-dmz-subnet-use-case","text":"In this typical use case, our goal is to make sure that all traffic coming from the public subnet should be routed to the DMZ before it gets routed to the private subnet. The reason for this is that a public subnet contains workloads that are Internet facing, and a private subnet contains workloads that are not exposed to the Internet, such as databases, application logic, backend servers, etc . By default, the communication from the public subnet is allowed to the private subnet; however, we need to make sure that the traffic is filtered before it reaches the private subnet. This will help us to protect the private workloads, as all traffic is matched against the rules that you have set up in the NVA (WAF in this case) . If there is any attack or malicious traffic, NVA will take care of it. Now, the question is, how can we implement this? As you can see in above diagram, we have used the routing table with User Defined Route (UDR) to accomplish this task. By default, the traffic from the public subnet to the private subnet is allowed, but using UDR, you are forcing the packets to go through the DMZ subnet .","title":"A DMZ subnet use case"},{"location":"azure/vnet/#regions","text":"VNet is scoped to a single region/location ; however, multiple virtual networks from different regions can be connected together using peering. Choose region with care When you create a virtual network in Azure, you will get an option to choose the region. Depending on the region you choose, the virtual network will be deployed to the respective region, and the virtual machines deployed to the virtual network will also fall under the same region.","title":"Regions"},{"location":"azure/vnet/#subscription","text":"VNet is scoped to a subscription. You can implement multiple virtual networks within each Azure subscription and Azure region.","title":"Subscription"},{"location":"azure/vnet/#vnet-peering","text":"Virtual network peering enables you to seamlessly connect two or more Virtual Networks in Azure. The virtual networks appear as one for connectivity purposes. The traffic between virtual machines in peered virtual networks uses the Microsoft backbone infrastructure. Like traffic between virtual machines in the same network, traffic is routed through Microsoft's private network only. VNET non-reciprocal and non-trasitive connection If we setup a connection between 2 VNets, that connection is non-reciprocal as shown in the diagram below. That is why we have to setup 2 connections in this case. The connction between different VNETS is non-trasitive as shown below. Active-passive and active-active peering are possible. Lets take an example of on-site and cloud peering with active-passive mode as shown below Azure supports the following types of peering: Virtual network peering : Connecting virtual networks within the same Azure region. Global virtual network peering : Connecting virtual networks across Azure regions.","title":"VNET peering"},{"location":"azure/vnet/#express-route","text":"ExpressRoute lets you extend your on-premises networks into the Microsoft cloud over a private connection with the help of a connectivity provider. With ExpressRoute , you can establish connections to Microsoft cloud services, such as Microsoft Azure and Microsoft 365 . Remember ExpressRoute connections don't go over the public Internet. This allows ExpressRoute connections to offer more reliability, faster speeds We have 2 options in Express routes ExpressRoute Direct : It does not require connectivity provider (service provider), it directly connects to Microsoft Edge Routers What is FastPath? FastPath is an additional feature that can be used with ExpressRoute Direc. When enabled, FastPath sends network traffic directly to virtual machines in the virtual network, bypassing the gateway ( ExpressRoute gateway is still needed though) ExpresRoute Circuit : It uses a service provider . Points to remember Allocate two /30 subnets that are not used anywhere else in your network topology. One subnet will be used for the primary connction ; the other will be used for the secondary connction as shown in the diagram above. From each of these subnets, you will assign the first usable IP address to your router because Microsoft uses the second usable IP for its router. A valid VLAN ID is required to establish this peering on. Ensure that no other peering in the circuit uses the same VLAN ID. For both primary and secondary links you must use the same VLAN ID. This information is provided from your provider.","title":"Express Route"},{"location":"azure/vnet/#circuits-and-peering","text":"","title":"Circuits and Peering"},{"location":"azure/vnet/#circuit","text":"ExpressRoute circuit is a logical connection between on-premises infrastructure and Microsoft cloud services through a connectivity provider ExpressRoute circuits do not map to any physical entities A circuit is uniquely identified by a standard GUID called as a service key (s-key) There is a 1:1 mapping between an ExpressRoute circuit and the s-key The service key is the only piece of information exchanged between Microsoft, the connectivity provider, and you (s-key is not a secret for security purposes) A circuit consists of 2 connections: Primary connection : main Secondary connection : redundent for HA","title":"Circuit"},{"location":"azure/vnet/#peering","text":"A peering is the interconnection between on-premise network and Microsoft cloud services (Azure, Microsoft 365). ExpressRoute circuits can include two independent peerings: - Private peering : Azure VNet - Microsoft peering : Microsoft 365, Dynamics 365 etc.","title":"Peering"},{"location":"azure/vnet/#basics","text":"","title":"Basics"},{"location":"azure/vnet/#nic","text":"Virtual machines have one or more network interface cards (NICs) that exist in the same region as the virtual network. You assign an IP address (either statically or dynamically) from a subnet in the virtual network. This action allows the virtual machine to communicate with other resources in the virtual network (and any peered network).","title":"NIC"},{"location":"azure/vnet/#public-ips","text":"Public IP addresses are associated with a virtual machine NIC, public load balancer, VPN gateways, application gateways, and any other resource that can be accessed from the Internet. Here also we can choose the allocation method to be static or dynamic . However, the availability of allocation methods depends on which SKU of public IP address we are using. The SKU is more like a pricing tier, where you will find different prices based on which SKU you are selecting Basic SKU : Static or Dynamic IP that is accessible by default and needs NSG to configure Standard SKU : Static PIP, that is not accessible by defult and needs NSG to allow traffic","title":"Public IPs"},{"location":"azure/vnet/#private-endpoint","text":"Before defining private endpoint, lets clear some basics terms: Some basics: revised Azure Private Link enables you to access Azure PaaS Services (for example, Azure Storage and SQL Database) and Azure hosted customer-owned/partner services over a Private Endpoint in your virtual network. Traffic between your virtual network and the service traverses over the Microsoft backbone network, eliminating exposure from the public Internet. Azure Private Link service is the reference to your own service that is powered by Azure Private Link. A private-link resource is the destination target of a specified private endpoint. A private endpoint is a network interface that uses a private IP address from your virtual network. This network interface connects you privately and securely to a service that's powered by Azure Private Link. By enabling a private endpoint, you're bringing the service into your virtual network. Private endpoint has NIC attached to it. The value of the private IP address remains unchanged for the entire lifecycle of the private endpoint. The private endpoint must be deployed in the same region and subscription as the virtual network. The private-link resource can be deployed in a different region than the one for the virtual network and private endpoint. Private Endpoint Example Imgine that there is a virtual network using the address range 10.10.0.0/16. Inside this virtual network are two subnets, SubnetA ( 10.10.5.0/24 ) and SubnetB ( 10.10.6.0/24 ). One or more virtual machines reside in SubnetA . A storage account has a private endpoint with an IP address of 10.10.6.5 inside SubnetB . The virtual machines inside SubnetA access the storage account through the storage account\u2019s private endpoint IP address located in SubnetB . Resources in other networks can access the storage account through the private endpoint IP address. Using private endpoints we can drill down to a sub-resource as shown below. This can't be done using Service Endpoint as it will allow access to the resource level instead of sub-resource level. When to use Private endpoint? Private endpoints enable connectivity between the customers from the same: Virtual network Regionally peered virtual networks Globally peered virtual networks On-premises environments that use VPN or Express Route Services that are powered by Private Link","title":"Private Endpoint"},{"location":"azure/vnet/#private-link-service","text":"Azure Private Link service is the reference to your own service that is powered by Azure Private Link . Your service that is running behind Azure Standard Load Balancer can be enabled for Private Link access so that consumers to your service can access it privately from their own VNets. Your customers can create a private endpoint inside their virtual network and map it to this service.","title":"Private Link Service"},{"location":"azure/vnet/#service-endpoint","text":"Service endpoints direct VNet traffic off the public Internet and to the Azure backbone network . They are enabled per subnet using policies. This service does not provide private IPs to the services. Service Endpoints enables private IP addresses in the their VNet to reach the endpoint of an Azure service without needing a public IP address on the their VNet. These VM's will connect to public IP of the service though. Service endpoints apply to all instances of the Azure resource, not just the ones you create. If you want to limit virtual network traffic to specific instances or regions of a resource, you need a service endpoint policy. Example of service endpoint Review the image below representing service endpoints in action. In this example, you have enabled a service endpoint for the Microsoft.SQL service on SubnetA . A virtual machine in SubnetA uses its private IP address to access a SQL server hosting several databases. The virtual machine connects using the public IP addresses associated with the Microsoft.SQL service Notice that Subnet B does not have service endpoints enabled for its subnet for the Microsoft.SQL service . Compute resources in this subnet use the public IP address for the virtual network to connect to the public IP address of the Azure service.","title":"Service Endpoint"},{"location":"azure/vnet/#service-endpoint-policies","text":"Virtual Network (VNet) service endpoint policies allow you to filter egress virtual network traffic to Azure Storage accounts over service endpoint , and allow data exfiltration to only specific Azure Storage accounts. Endpoint policies provide granular access control for virtual network traffic to Azure Storage when connecting over service endpoint.","title":"Service Endpoint Policies"},{"location":"azure/vnet/#private-vs-service-endpoints","text":"Public and Private IPs in service/private endpoints With service endpoints , you're still connecting to the target resource's public endpoint. This effectively extends the identity of the VNet to the target resource. With private endpoints , you're assigning the target resource a private IP address from the VNet, essentially bringing it into the network. The target resource's public IP address doesn't go away, but you can lock it down so all traffic from the internet is denied. Here are some tips on when to use which endpint When to use which endpoint? If you want to be able to block all internet traffic to a target resource, use a private endpoint. If you're dealing with traffic from on-premises, use a private endpoint. If you want to secure a specific sub-resource to your VNet resources, use a private endpoint. If you want to secure a specific storage account to your VNet resources, you can use a private endpoint, or a service endpoint with a service endpoint policy . If you don't need a private IP address at the destination, service endpoints are considerably easier to create and maintain, and they don't require special DNS configuration. if cost is a concern, note that service endpoints are free.","title":"Private Vs Service Endpoints"},{"location":"azure/vnet/#network-routing-types","text":"Network routes or route tables have existed in traditional networks for an exceptionally long time. The routes that are part of the route table decide how to direct a packet to the destination.","title":"Network Routing types"},{"location":"azure/vnet/#system-routes","text":"They are built in routes which cannot be modified VM's default communicaiton to internet Whenever we create a VM in Azure, the VM will be able to communicate with the Internet without setting up any routes . In AWS, we need to create different gateways like NAT Gateway or Internet Gateway to facilitate the connection from a VM to the Internet. However, in Azure this is enabled by default with the help of system routes","title":"System routes"},{"location":"azure/vnet/#custom-user-defined-routes-udr","text":"User defined routes (UDR) or BGP routes which can override the system routes . Precenence order of the routes Custom routes > BGP routes > System routes Block access to public internet We can block access to public internet by adding a custom route from 0.0.0.0/0 to None as shown below","title":"Custom/ User defined routes (UDR)"},{"location":"azure/vnet/#nsg-l3-and-l4","text":"Tldr They are used to control the flow of traffic inside the VNET and are similar to Firewalls An NSG is a collection of security rules that can be used to allow or deny inbound or outbound traffic. Is NSG associted with subnet? As shown in the diagram, NSG can be associated with a subnet , or a NIC based on your requirement. NSGs are reusable, which means you can have multiple NICs or subnet associations to a single NSG. NSG as no effect unless its associated with either NIC or a Subnet. Priorities are used in NSG to evaluate; lower the number, higher the priority.","title":"NSG (L3 and L4)"},{"location":"azure/vnet/#nsg-effective-rules","text":"When you apply NSG to both subnet and NIC, then effective rules come into the picture . If you are using NSGs at both the subnet and NIC level, the rules will be evaluated at both levels. Rules evaluation order Inbound Traffic : Each security rule will be evaluated independently when the NSG is applied to both the subnet and NIC levels. Incoming traffic will be first evaluated against the rules applied on the subnet level; if there is an allow rule, then the packet will be sent to the NIC. The rules set at the NIC level will be evaluated now, and only if there is an allow rule then the packet is allowed; otherwise, it will be dropped. In short, first at Subnet then NIC Outbound Traffic : all outgoing traffic will be first evaluated against the NIC rules; if there is an allow rule, then the traffic is evaluated at the subnet level. In short, first at NIC then Subnet","title":"NSG Effective rules"},{"location":"azure/vnet/#nva","text":"Network Virtual Appliance NVA s are VMs that can optimize our networks with routing and firewall capabilities.","title":"NVA"},{"location":"azure/vnet/#azure-firewall-l7-and-l4","text":"Using Azure Firewall, we can create, enforce, and manage network policies across virtual networks and subscriptions. Azure Firewall provides more features then NSG. Also, Azure firewall is different than WAF Setting for Azure firewall is shown below Do we need WAF in addition to NSG? Yes, we need both. The main difference is that NSG operates at layers 3 and 4 of the OSI layer; on the other hand, Azure Firewall works at layers 7 and 4. It comes in 3 flavours Standard Premium Basic","title":"Azure Firewall (L7 and L4)"},{"location":"azure/vnet/#fqdn-tags","text":"We can use FQDN tag in the Azure firewall rules. An FQDN tag represents a group of fully qualified domain names (FQDNs) associated with well known Microsoft services.","title":"FQDN Tags"},{"location":"azure/vnet/#service-tags","text":"A service tag represents a group of IP address prefixes to help minimize complexity for security rule creation. You can\u2019t create your own service tag, nor specify which IP addresses are included within a tag. Microsoft manages the address prefixes encompassed by the service tag, and automatically updates the service tag as addresses change. How to use service tags? You can use service tags to define network access controls on network security groups , Azure Firewall , and user-defined routes . Use service tags in place of specific IP addresses when you create security rules and routes. By specifying the service tag name, such as Storage , in the appropriate source or destination field of a security rule, you can allow or deny the traffic for the corresponding service. 2 tags are used in the below diagram in the destination fields","title":"Service tags"},{"location":"azure/vnet/#firewall-policyrules","text":"Rule collection group : It is used to group rule collections. They're the first unit to be processed by the Azure Firewall and they follow a priority order based on values Rule collection : it belongs to a rule collection group, and it contains one or multiple rules. They're the second unit processed by the firewall and they follow a priority order based on values. Rule collections must have a defined action (allow or deny) and a priority value Rules used in Firewall are of 3 types NAT Network (L4) Application (L7) Priority of Firewall rules NAT rule > Network rule > Application rule","title":"Firewall Policy/Rules"},{"location":"azure/vnet/#nat-rules","text":"Using a NAT rule collection, you will be able to translate a public IP and port to a private IP and port NAT example using VM Assume you have a Ubuntu server with private IP address, say, 10.0.0.2 , and the SSH server is available on port 22. Instead of exposing this port and IP to the Internet, you can create a NAT rule on the firewall. Using the public IP of the firewall and a random port, you can set up a NAT rule. Let\u2019s assume that the public IP address of the firewall is 66.142.43.02 ; using NAT rule you can redirect all traffic hitting port 5000 of the firewall to be translated to 10.0.0.2 and port 22. In short, translate 66.142.43.02:5000 to 10.0.0.2:22 . In this way, you can translate the public IP address and port to a private IP address and port.","title":"NAT rules"},{"location":"azure/vnet/#inbound-dnat","text":"Inbound Internet network traffic to your firewall public IP address is translated ( Destination Network Address Translation ) and filtered to the private IP addresses on your virtual networks.","title":"Inbound DNAT"},{"location":"azure/vnet/#outbound-snat","text":"All outbound virtual network traffic IP addresses are translated to the Azure Firewall public IP ( Source Network Address Translation ). You can identify and allow traffic originating from your virtual network to remote Internet destinations.","title":"Outbound SNAT"},{"location":"azure/vnet/#network-rules","text":"A network rule should be in place for any non-HTTP/S traffic to be allowed through the firewall. For a source to communicate with a destination deployed behind a firewall, you need to have a network rule configured from the source to the destination. If there is no rule that specifically calls out this incoming traffic, then it will be dropped. Tldr You can use a network rule when you want to filter traffic based on IP addresses, any ports, and any protocols.","title":"Network rules"},{"location":"azure/vnet/#application-rules","text":"Application rules allow or deny outbound and east-west traffic based on the application layer (L7). You can use an application rule when you want to filter traffic based on fully qualified domain names (FQDNs), URLs, and HTTP/HTTPS protocols. if you configure network rules and application rules, then network rules are applied in priority order before application rules. The rules are terminating. So, if a match is found in a network rule, no other rules are processed.","title":"Application rules"},{"location":"azure/vnet/#asg","text":"Application security groups enable you to configure network security as a natural extension of an application's structure, allowing you to group virtual machines and define network security policies based on those groups. You can reuse your security policy at scale without manual maintenance of explicit IP addresses.","title":"ASG"},{"location":"azure/vpn_gw/","text":"VPN gateways \u00b6 A VPN gateway is a type of virtual network gateway. Azure VPN Gateway instances are deployed in a dedicated subnet of the virtual network and enable the following connectivity: Connect on-premises datacenters to virtual networks through a site-to-site connection. Connect individual devices to virtual networks through a point-to-site connection. Connect virtual networks to other virtual networks through a network-to-network connection. How many VPN's can be deployed? You can deploy only one VPN gateway in each virtual network . However, you can use one gateway to connect to multiple locations, which includes other virtual networks or on-premises datacenters. Gateway Subnet \u00b6 Before you deploy the VPN gateway, you need to add a dedicated subnet to the vnet you want to connect. This subnet is for hosting the VPN gateway instances, and a CIDR block of /27 or /28 would suffice. The name of the subnet should be GatewaySubnet . HA modes \u00b6 Active-Active \u00b6 Active-StandBy \u00b6 Types of VPN GW \u00b6 There are 2 types of VPN gateways: Policy-based VPN gateways : They specify statically the IP address of packets that should be encrypted through each tunnel. This type of device evaluates every data packet against those sets of IP addresses to choose the tunnel where that packet is going to be sent through. Route-based gateways : IPSec tunnels are modeled as a network interface or virtual tunnel interface. IP routing (either static routes or dynamic routing protocols) decides which one of these tunnel interfaces to use when sending each packet. Route-based VPNs are the preferred connection method for on-premises devices. They're more resilient to topology changes such as the creation of new subnets. Azure to On-Premises \u00b6 There are 2 ways to connect on-premises to Azure 1. VPN gateway 2. ExpressRoute VPN Gateway connection \u00b6 The local network gateway refers to the on-premises location. You create a reference resource called a local network gateway in Azure to specify your on-premises site . While creating the local network gateway, you will specify the address prefixes that are there in the on-premises network. You can use either an IP address or an FQDN to specify your on-premises VPN device. Azure VPN gateway will be establishing connectivity to this device. Other than the IP address and address prefix, you will be asked to choose the subscription, resource group, and location for the resource. Using a P2S connection, you can create a secure connection from an individual computer to an Azure virtual network over VPN. The P2S connections are always initiated from the client machine. All clients need to download the VPN profile and install it on their device to establish the P2S connection. Before Azure accepts P2S requests from the clients, authentication should be done first.","title":"VPN Gateway"},{"location":"azure/vpn_gw/#vpn-gateways","text":"A VPN gateway is a type of virtual network gateway. Azure VPN Gateway instances are deployed in a dedicated subnet of the virtual network and enable the following connectivity: Connect on-premises datacenters to virtual networks through a site-to-site connection. Connect individual devices to virtual networks through a point-to-site connection. Connect virtual networks to other virtual networks through a network-to-network connection. How many VPN's can be deployed? You can deploy only one VPN gateway in each virtual network . However, you can use one gateway to connect to multiple locations, which includes other virtual networks or on-premises datacenters.","title":"VPN gateways"},{"location":"azure/vpn_gw/#gateway-subnet","text":"Before you deploy the VPN gateway, you need to add a dedicated subnet to the vnet you want to connect. This subnet is for hosting the VPN gateway instances, and a CIDR block of /27 or /28 would suffice. The name of the subnet should be GatewaySubnet .","title":"Gateway Subnet"},{"location":"azure/vpn_gw/#ha-modes","text":"","title":"HA modes"},{"location":"azure/vpn_gw/#active-active","text":"","title":"Active-Active"},{"location":"azure/vpn_gw/#active-standby","text":"","title":"Active-StandBy"},{"location":"azure/vpn_gw/#types-of-vpn-gw","text":"There are 2 types of VPN gateways: Policy-based VPN gateways : They specify statically the IP address of packets that should be encrypted through each tunnel. This type of device evaluates every data packet against those sets of IP addresses to choose the tunnel where that packet is going to be sent through. Route-based gateways : IPSec tunnels are modeled as a network interface or virtual tunnel interface. IP routing (either static routes or dynamic routing protocols) decides which one of these tunnel interfaces to use when sending each packet. Route-based VPNs are the preferred connection method for on-premises devices. They're more resilient to topology changes such as the creation of new subnets.","title":"Types of VPN GW"},{"location":"azure/vpn_gw/#azure-to-on-premises","text":"There are 2 ways to connect on-premises to Azure 1. VPN gateway 2. ExpressRoute","title":"Azure to On-Premises"},{"location":"azure/vpn_gw/#vpn-gateway-connection","text":"The local network gateway refers to the on-premises location. You create a reference resource called a local network gateway in Azure to specify your on-premises site . While creating the local network gateway, you will specify the address prefixes that are there in the on-premises network. You can use either an IP address or an FQDN to specify your on-premises VPN device. Azure VPN gateway will be establishing connectivity to this device. Other than the IP address and address prefix, you will be asked to choose the subscription, resource group, and location for the resource. Using a P2S connection, you can create a secure connection from an individual computer to an Azure virtual network over VPN. The P2S connections are always initiated from the client machine. All clients need to download the VPN profile and install it on their device to establish the P2S connection. Before Azure accepts P2S requests from the clients, authentication should be done first.","title":"VPN Gateway connection"},{"location":"azure/waf/","text":"WAF \u00b6 Azure Web Application Firewall is a cloud-native service that protects web apps from common web-hacking techniques such as SQL injection and security vulnerabilities such as cross-site scripting WAF VS AppGateway Application gateways provide more capabilities than a web application firewall (WAF) WAF can be deployed with the below services Azure Application Gateway Azure Front Door Azure Content Delivery Network (CDN) WAF on Application Gateway is based on the Core Rule Set (CRS) from the Open Web Application Security Project (OWASP). using pre-configured rules Protect your web applications in just a few minutes with the latest managed and preconfigured rule sets . The Azure Web Application Firewall detection engine combined with updated rule sets increases security, reduces false positives, and improves performance.","title":"WAF"},{"location":"azure/waf/#waf","text":"Azure Web Application Firewall is a cloud-native service that protects web apps from common web-hacking techniques such as SQL injection and security vulnerabilities such as cross-site scripting WAF VS AppGateway Application gateways provide more capabilities than a web application firewall (WAF) WAF can be deployed with the below services Azure Application Gateway Azure Front Door Azure Content Delivery Network (CDN) WAF on Application Gateway is based on the Core Rule Set (CRS) from the Open Web Application Security Project (OWASP). using pre-configured rules Protect your web applications in just a few minutes with the latest managed and preconfigured rule sets . The Azure Web Application Firewall detection engine combined with updated rule sets increases security, reduces false positives, and improves performance.","title":"WAF"},{"location":"k8s/commands/","text":"Non-Admin Commands \u00b6 How k apply command works \u00b6 It compares the local file with last applied config local file with kubernetes live object Where is the last applied config stored? It is stored in the with kubernetes live object in an annotation as shown below Commands for CKA/CKAD exam \u00b6 -o yaml : This will output the resource definition in YAML format on screen. --dry-run : By default as soon as the command is run, the resource will be created. If you simply want to test your command , use the --dry-run=client option. This will not create the resource, instead, tell you whether the resource can be created and if your command is right. # replace the file with temp created yaml file for forbidden updates k replace --force -f <file-name.yaml> # create an nginx pod k run nginx --image = nginx # Generate a pod manifest file using the dry-run which does not create a resource. Also save the file to local k run nginx --image = nginx --dry-run = client -o yaml > pod.yaml # Generate a deployment manifest file using the dry-run which does not create a resource k create deployment --image = nginx dep_name --dry-run = client -o yaml > dep.yaml # Generate a deployment manifest file with 4 replicas using the dry-run which does not create a resource k create deployment --image = nginx dep_name --replicas = 4 --dry-run = client -o yaml > dep # Scale deployment kubectl scale deployment nginx --replicas = 4 # Create a Service named redis-service of type ClusterIP to expose pod redis on port 6379 kubectl expose pod redis --port = 6379 --name redis-service --dry-run = client -o yaml #Create a Service named nginx of type NodePort to expose pod nginx's port 80 on port 30080 on the nodes # This will automatically use the pod's labels as selectors, but you cannot specify the node port. kubectl expose pod nginx --type = NodePort --port = 80 --name = nginx-service --dry-run = client -o yaml # create a pod with labels k run redis --image = redis:alpine --labels = tier = db # select the resources with node name label k get po/deploy/ep -l kubernetes.io/hostname = <node-name> Namespace \u00b6 # create a namescpace kubectl create -f custom-namespace.yaml # using declarative command kubectl create namespace custom-namespace # create a resource in namespace kubectl create -f kubia-manual.yaml -n custom-namespace # delete namespace kubectl delete ns custom-namespace Get the detailed request logs \u00b6 k get po -v = 6 Pod \u00b6 POD Creation POD Logs SSH Get/Delete Logs # create a pod using dry-run kubectl run busybox --image = busybox --restart = Never --dry-run -o yaml > testPod.yaml # create a pod with come commands using dry-run kubectl run busyboxWithCommands --image = busybox --restart = Never --dry-run -o yaml -- bin/sh -c \"sleep 3600; ls\" > testPod.yaml # settting image kubectl set image pod podname nginx = nginx:1.15-alpine # it's better to edit pod sometime and make quick changes kubectl edit pod podName # Create the pod named amardev with version 1.17.4 and expose it on port 80 kubectl run amardev --image = nginx:1.17.4 --restart = Never --port = 80 # add the command in the pod by editing it command: [ '/bin/bash' , '-c' , 'sleep 5000' ] # adding args in the pod args: [ '--color' , 'green' ] # get logs for nginx pod kubectl logs nginx # get previous logs of the pod kubectl logs nginx -p # just open the terminal for the pod with one container kubectl exec -it nginx -- /bin/sh # echo hello world in the container kubectl exec -it nginx -c containerName -- /bin/sh -c 'echo hello world' # ssh to multi-container pod kubectl exec -it multi-cont-pod -c main-container -- sh cat /var/log/main.txt # get all pods kubectl get pods # get info for particular pod kubectl get po kubia-liveness # show labels while showing pods kubectl get pods --show-labels # select pods with multiple labels kubectl get all --selector env = prod,bu = finance,tier = frontend # Warn: delete all pods kubectl delete po --all # Warning: delete pods in all namespaces kubectl delete all --all # delete pod kubectl delete po podName # Important: delete pods using labels kubectl delete po -l creation_method = manual # get the metircs about the nodes kubectl top node/pod # See the pod logs kubectl logs podName | less # Tail the pod logs using `-f` kubectl logs -f podName Labels and annotations \u00b6 # show labels kubectl get pods --show-labels # apply label kubectl run nginx-dev1 --image = nginx --restart = Never --labels = env = dev #Get the pods with label env=dev kebectl get pods -l env = dev --show-labels # show labels which env in dev and prod kubectl get pods -l 'env in (dev,prod)' # update the label with overwrite kubectl label po podone area = monteal --overwrite # remove label named env kubectl label pods podone env- # show the labels for the nodes kubectl get nodes --show-labels # Annotate the pods with name=webapp kubectl annnotate po podone name = webapp Configmap \u00b6 # create a configmap kubectl create configmap <cm-name> --from-literal = special.how = very --from-literal = special.type = charm # Example k create cm webapp-config-map --from-literal APP_COLOR = darkblue --from-literal APP_OTHER = disregard Env variables \u00b6 # set an env variable while creating a pod kubectl run nginx --image = nginx --restart = Never --env = var1 = val1 # get all of the env variables for a pod kubectl exec -it nginx -- env Logs \u00b6 # check logs for a container 1 and 2 in the pod busybox kubectl logs busybox -c busybox1 kubectl logs busybox -c busybox2 # Check the previous logs of the second container busybox2 if any kubectl logs busybox -c busybox2 --previous # Run command ls in the third container busybox3 of the above pod kubectl exec busybox -c busybox3 -- ls # Show metrics of the above pod containers and puts them into the file.log and verify kubectl top pod busybox --containers > file.log Deployment \u00b6 # create a deployment with a name and replicas of 3 kubectl create deployment webapp --image = nginx --replicas = 3 # scale deployment to have 20 replicas kubectl scale deploy webapp --replicas = 20 # get the rollout status kubectl rollout status deploy webapp # get the rollout history kubectl rollout history deploy webapp # delete the deployment and watch it getting deleted kubectl delete deployment webapp --watch # update the image in the deployment. Note that here nginx in the container name # set image will cause the rollut to happen, so you can check the status using the `kubectl rollout status` command kubectl set image deployment depName nginx = nginx:1.17.4 # undo the deployment to revision 1 kubectl rollout undo deployment webapp --to-revision = 1 # Check the history of the specific revision of that deployment kubectl rollout history deployment webapp --revision = 3 # pause the rollout kubectl rollout pause deployment webapp # resume the rollout kubectl rollout resume deployment webapp # scale a deployment kubectl autoscale deployment webapp --min = 5 --max = 10 --cpu-percent = 80 Jobs \u00b6 # jobs will create a pod and then finish when work is done # then we can see the output of that job using the logs command for the pod created kubectl create job jobName --image = nginx -- node -v # create a job which will save the TIME information kubectl create job jonname --image = nginx -- /bin/sh -c \"date; echo 'time container'\" # delete all the jobs kubectl delete jobs --all Cron Jobs \u00b6 # get all the cronjobs kubectl get cj # create a cronjob which will show the time everyminute kubectl create cronjob timecj --image = nginx --schedule = \"*/1 * * * *\" -- /bin/sh -c \"date\" Secret \u00b6 # create a secret and add some values kubectl create secret generic db-secret --from-literal = DB_Host = sql01 Base64 encode/decode \u00b6 Encoding De-coding echo -n 'Lets learn K8S' | base64 echo -n 'TGV0cyBsZWFybiBLOFM=' | base64 -- decode Data secret \u00b6 In this secret, the values are base64 encoded as shown below apiVersion : v1 kind : Secret metadata : name : mysecret type : Opaque data : username : YWRtaW4= password : MWYyZDFlMmU2N2Rm Data + stringData secret \u00b6 apiVersion : v1 kind : Secret metadata : name : mysecret type : Opaque data : username : YWRtaW4= stringData : name : Amarjit See Secret contents \u00b6 Encoded de-coded kubectl get secret sampleSecret -o jsonpath = '{.data}' kubectl get secret sampleSecret -o jsonpath = '{.data}' | base64 --decode We can see the contents of secret that was created using HPA \u00b6 # horizontal pod autoscaling kubectl get hpa Taint and Toleration \u00b6 # creating taints kubectl taint node nodename key = value:NoSchedule ###Example k taint node node01 spray = mortein:NoSchedule # see the taint using describe kubectl describe deployment depname | grep -i taint # remove the Taints in the node using, add - at end to remove the taint kubectl taint node nodename node-role.kubernetes.io/master:NoSchedule- # Add toleration to pod Role \u00b6 Create a role kubectl create role developer --namespace = default --verb = list,create,delete --resource = pods Create a role binding kubectl create rolebinding dev-user-binding --namespace = default --role = developer --user = dev-user Security Context \u00b6 # to add the user id we can defice security context in the container and pod level using securityContext : runAsUser : 1000 runAsOwner : 1010= # capabilities are added at the container level not at the pod level containers : securityContext : capabilities : add : [ 'SYS_TIME' ] runAsUser : 1000 runAsOwner : 1010 Session Affinity \u00b6 # use the below affinity in the pod spec affinity : nodeAffinity : requiredDuringSchedulingIgnoredDuringExecution : nodeSelectorTerms : - matchExpressions : - key : labelKey operator : In values : - labelValue Notes \u00b6 NodeSelector is used in the pod if we want it to get allocated to a particular node A Job creates one or more Pods and ensures that a specified number of them successfully terminate. As pods successfully complete, the Job tracks the successful completions. When a specified number of successful completions is reached, the task (ie, Job) is complete. Deleting a Job will clean up the Pods it created. A hostPath volume mounts a file or directory from the host node\u2019s filesystem into your Pod. pod talks to API-server using the service account Use nodeaffinity to place the pods in the right nodes. Use the label to select the node. First apply a label on the node and then use that label on the node affinity Startup probe, the application will have a maximum of 5 minutes (30 * 10 = 300s) to finish its startup. Once the startup probe has succeeded once, the liveness probe takes over to provide a fast response to container deadlocks. If the startup probe never succeeds, the container is killed after 300s and subject to the pod's restartPolicy See special chars in VIM use :set list to show the special chars and :set nolist to go back to normal Admin Commands \u00b6 Get current context \u00b6 # show all conexts, this is detailed k config get-contexts # just the context name k config current-context See the access level \u00b6 Check access level for you # Check if you can create pod k auth can-i create pod # Can I delete node? k auth can-i delete node # check all you can access k auth can-i --list = true Check access for someone else Only Admin can check this # Can dev user delete the node k auth can-i delete node --as dev-user See list of resources \u00b6 Namespaced Resorces k api-resources --namespaced = true Non-Namespaced Resorces k api-resources --namespaced = false See resources without headers \u00b6 # get the pods k get po --no-headers # count pods k get pods --no-headers | wc -l","title":"Commands"},{"location":"k8s/commands/#non-admin-commands","text":"","title":"Non-Admin Commands"},{"location":"k8s/commands/#how-k-apply-command-works","text":"It compares the local file with last applied config local file with kubernetes live object Where is the last applied config stored? It is stored in the with kubernetes live object in an annotation as shown below","title":"How k apply command works"},{"location":"k8s/commands/#commands-for-ckackad-exam","text":"-o yaml : This will output the resource definition in YAML format on screen. --dry-run : By default as soon as the command is run, the resource will be created. If you simply want to test your command , use the --dry-run=client option. This will not create the resource, instead, tell you whether the resource can be created and if your command is right. # replace the file with temp created yaml file for forbidden updates k replace --force -f <file-name.yaml> # create an nginx pod k run nginx --image = nginx # Generate a pod manifest file using the dry-run which does not create a resource. Also save the file to local k run nginx --image = nginx --dry-run = client -o yaml > pod.yaml # Generate a deployment manifest file using the dry-run which does not create a resource k create deployment --image = nginx dep_name --dry-run = client -o yaml > dep.yaml # Generate a deployment manifest file with 4 replicas using the dry-run which does not create a resource k create deployment --image = nginx dep_name --replicas = 4 --dry-run = client -o yaml > dep # Scale deployment kubectl scale deployment nginx --replicas = 4 # Create a Service named redis-service of type ClusterIP to expose pod redis on port 6379 kubectl expose pod redis --port = 6379 --name redis-service --dry-run = client -o yaml #Create a Service named nginx of type NodePort to expose pod nginx's port 80 on port 30080 on the nodes # This will automatically use the pod's labels as selectors, but you cannot specify the node port. kubectl expose pod nginx --type = NodePort --port = 80 --name = nginx-service --dry-run = client -o yaml # create a pod with labels k run redis --image = redis:alpine --labels = tier = db # select the resources with node name label k get po/deploy/ep -l kubernetes.io/hostname = <node-name>","title":"Commands for CKA/CKAD exam"},{"location":"k8s/commands/#namespace","text":"# create a namescpace kubectl create -f custom-namespace.yaml # using declarative command kubectl create namespace custom-namespace # create a resource in namespace kubectl create -f kubia-manual.yaml -n custom-namespace # delete namespace kubectl delete ns custom-namespace","title":"Namespace"},{"location":"k8s/commands/#get-the-detailed-request-logs","text":"k get po -v = 6","title":"Get the detailed request logs"},{"location":"k8s/commands/#pod","text":"POD Creation POD Logs SSH Get/Delete Logs # create a pod using dry-run kubectl run busybox --image = busybox --restart = Never --dry-run -o yaml > testPod.yaml # create a pod with come commands using dry-run kubectl run busyboxWithCommands --image = busybox --restart = Never --dry-run -o yaml -- bin/sh -c \"sleep 3600; ls\" > testPod.yaml # settting image kubectl set image pod podname nginx = nginx:1.15-alpine # it's better to edit pod sometime and make quick changes kubectl edit pod podName # Create the pod named amardev with version 1.17.4 and expose it on port 80 kubectl run amardev --image = nginx:1.17.4 --restart = Never --port = 80 # add the command in the pod by editing it command: [ '/bin/bash' , '-c' , 'sleep 5000' ] # adding args in the pod args: [ '--color' , 'green' ] # get logs for nginx pod kubectl logs nginx # get previous logs of the pod kubectl logs nginx -p # just open the terminal for the pod with one container kubectl exec -it nginx -- /bin/sh # echo hello world in the container kubectl exec -it nginx -c containerName -- /bin/sh -c 'echo hello world' # ssh to multi-container pod kubectl exec -it multi-cont-pod -c main-container -- sh cat /var/log/main.txt # get all pods kubectl get pods # get info for particular pod kubectl get po kubia-liveness # show labels while showing pods kubectl get pods --show-labels # select pods with multiple labels kubectl get all --selector env = prod,bu = finance,tier = frontend # Warn: delete all pods kubectl delete po --all # Warning: delete pods in all namespaces kubectl delete all --all # delete pod kubectl delete po podName # Important: delete pods using labels kubectl delete po -l creation_method = manual # get the metircs about the nodes kubectl top node/pod # See the pod logs kubectl logs podName | less # Tail the pod logs using `-f` kubectl logs -f podName","title":"Pod"},{"location":"k8s/commands/#labels-and-annotations","text":"# show labels kubectl get pods --show-labels # apply label kubectl run nginx-dev1 --image = nginx --restart = Never --labels = env = dev #Get the pods with label env=dev kebectl get pods -l env = dev --show-labels # show labels which env in dev and prod kubectl get pods -l 'env in (dev,prod)' # update the label with overwrite kubectl label po podone area = monteal --overwrite # remove label named env kubectl label pods podone env- # show the labels for the nodes kubectl get nodes --show-labels # Annotate the pods with name=webapp kubectl annnotate po podone name = webapp","title":"Labels and annotations"},{"location":"k8s/commands/#configmap","text":"# create a configmap kubectl create configmap <cm-name> --from-literal = special.how = very --from-literal = special.type = charm # Example k create cm webapp-config-map --from-literal APP_COLOR = darkblue --from-literal APP_OTHER = disregard","title":"Configmap"},{"location":"k8s/commands/#env-variables","text":"# set an env variable while creating a pod kubectl run nginx --image = nginx --restart = Never --env = var1 = val1 # get all of the env variables for a pod kubectl exec -it nginx -- env","title":"Env variables"},{"location":"k8s/commands/#logs","text":"# check logs for a container 1 and 2 in the pod busybox kubectl logs busybox -c busybox1 kubectl logs busybox -c busybox2 # Check the previous logs of the second container busybox2 if any kubectl logs busybox -c busybox2 --previous # Run command ls in the third container busybox3 of the above pod kubectl exec busybox -c busybox3 -- ls # Show metrics of the above pod containers and puts them into the file.log and verify kubectl top pod busybox --containers > file.log","title":"Logs"},{"location":"k8s/commands/#deployment","text":"# create a deployment with a name and replicas of 3 kubectl create deployment webapp --image = nginx --replicas = 3 # scale deployment to have 20 replicas kubectl scale deploy webapp --replicas = 20 # get the rollout status kubectl rollout status deploy webapp # get the rollout history kubectl rollout history deploy webapp # delete the deployment and watch it getting deleted kubectl delete deployment webapp --watch # update the image in the deployment. Note that here nginx in the container name # set image will cause the rollut to happen, so you can check the status using the `kubectl rollout status` command kubectl set image deployment depName nginx = nginx:1.17.4 # undo the deployment to revision 1 kubectl rollout undo deployment webapp --to-revision = 1 # Check the history of the specific revision of that deployment kubectl rollout history deployment webapp --revision = 3 # pause the rollout kubectl rollout pause deployment webapp # resume the rollout kubectl rollout resume deployment webapp # scale a deployment kubectl autoscale deployment webapp --min = 5 --max = 10 --cpu-percent = 80","title":"Deployment"},{"location":"k8s/commands/#jobs","text":"# jobs will create a pod and then finish when work is done # then we can see the output of that job using the logs command for the pod created kubectl create job jobName --image = nginx -- node -v # create a job which will save the TIME information kubectl create job jonname --image = nginx -- /bin/sh -c \"date; echo 'time container'\" # delete all the jobs kubectl delete jobs --all","title":"Jobs"},{"location":"k8s/commands/#cron-jobs","text":"# get all the cronjobs kubectl get cj # create a cronjob which will show the time everyminute kubectl create cronjob timecj --image = nginx --schedule = \"*/1 * * * *\" -- /bin/sh -c \"date\"","title":"Cron Jobs"},{"location":"k8s/commands/#secret","text":"# create a secret and add some values kubectl create secret generic db-secret --from-literal = DB_Host = sql01","title":"Secret"},{"location":"k8s/commands/#base64-encodedecode","text":"Encoding De-coding echo -n 'Lets learn K8S' | base64 echo -n 'TGV0cyBsZWFybiBLOFM=' | base64 -- decode","title":"Base64 encode/decode"},{"location":"k8s/commands/#data-secret","text":"In this secret, the values are base64 encoded as shown below apiVersion : v1 kind : Secret metadata : name : mysecret type : Opaque data : username : YWRtaW4= password : MWYyZDFlMmU2N2Rm","title":"Data secret"},{"location":"k8s/commands/#data-stringdata-secret","text":"apiVersion : v1 kind : Secret metadata : name : mysecret type : Opaque data : username : YWRtaW4= stringData : name : Amarjit","title":"Data + stringData secret"},{"location":"k8s/commands/#see-secret-contents","text":"Encoded de-coded kubectl get secret sampleSecret -o jsonpath = '{.data}' kubectl get secret sampleSecret -o jsonpath = '{.data}' | base64 --decode We can see the contents of secret that was created using","title":"See Secret contents"},{"location":"k8s/commands/#hpa","text":"# horizontal pod autoscaling kubectl get hpa","title":"HPA"},{"location":"k8s/commands/#taint-and-toleration","text":"# creating taints kubectl taint node nodename key = value:NoSchedule ###Example k taint node node01 spray = mortein:NoSchedule # see the taint using describe kubectl describe deployment depname | grep -i taint # remove the Taints in the node using, add - at end to remove the taint kubectl taint node nodename node-role.kubernetes.io/master:NoSchedule- # Add toleration to pod","title":"Taint and Toleration"},{"location":"k8s/commands/#role","text":"Create a role kubectl create role developer --namespace = default --verb = list,create,delete --resource = pods Create a role binding kubectl create rolebinding dev-user-binding --namespace = default --role = developer --user = dev-user","title":"Role"},{"location":"k8s/commands/#security-context","text":"# to add the user id we can defice security context in the container and pod level using securityContext : runAsUser : 1000 runAsOwner : 1010= # capabilities are added at the container level not at the pod level containers : securityContext : capabilities : add : [ 'SYS_TIME' ] runAsUser : 1000 runAsOwner : 1010","title":"Security Context"},{"location":"k8s/commands/#session-affinity","text":"# use the below affinity in the pod spec affinity : nodeAffinity : requiredDuringSchedulingIgnoredDuringExecution : nodeSelectorTerms : - matchExpressions : - key : labelKey operator : In values : - labelValue","title":"Session Affinity"},{"location":"k8s/commands/#notes","text":"NodeSelector is used in the pod if we want it to get allocated to a particular node A Job creates one or more Pods and ensures that a specified number of them successfully terminate. As pods successfully complete, the Job tracks the successful completions. When a specified number of successful completions is reached, the task (ie, Job) is complete. Deleting a Job will clean up the Pods it created. A hostPath volume mounts a file or directory from the host node\u2019s filesystem into your Pod. pod talks to API-server using the service account Use nodeaffinity to place the pods in the right nodes. Use the label to select the node. First apply a label on the node and then use that label on the node affinity Startup probe, the application will have a maximum of 5 minutes (30 * 10 = 300s) to finish its startup. Once the startup probe has succeeded once, the liveness probe takes over to provide a fast response to container deadlocks. If the startup probe never succeeds, the container is killed after 300s and subject to the pod's restartPolicy See special chars in VIM use :set list to show the special chars and :set nolist to go back to normal","title":"Notes"},{"location":"k8s/commands/#admin-commands","text":"","title":"Admin Commands"},{"location":"k8s/commands/#get-current-context","text":"# show all conexts, this is detailed k config get-contexts # just the context name k config current-context","title":"Get current context"},{"location":"k8s/commands/#see-the-access-level","text":"Check access level for you # Check if you can create pod k auth can-i create pod # Can I delete node? k auth can-i delete node # check all you can access k auth can-i --list = true Check access for someone else Only Admin can check this # Can dev user delete the node k auth can-i delete node --as dev-user","title":"See the access level"},{"location":"k8s/commands/#see-list-of-resources","text":"Namespaced Resorces k api-resources --namespaced = true Non-Namespaced Resorces k api-resources --namespaced = false","title":"See list of resources"},{"location":"k8s/commands/#see-resources-without-headers","text":"# get the pods k get po --no-headers # count pods k get pods --no-headers | wc -l","title":"See resources without headers"},{"location":"k8s/deployment_python/","text":"Deployment \u00b6 A Deployment provides declarative updates for Pods and ReplicaSets. You describe a desired state in a Deployment, and the Deployment Controller changes the actual state to the desired state at a controlled rate A deployment can create a RS Do not manage ReplicaSets owned by a Deployment. Rollout and rollbacks \u00b6 A rollout is created in deployment a status of which can be shown as below # check rollout status using below kubectl rollout status deployment <nginx-deployment> #To check the revisions and history of rollout, you can use following command k rollout history deployment <myapp-deployment> Update a deployment using rolling update \u00b6 2 strategies are used mainly: 1. Re-create 2. Rolling update. Remember If you don\u2019t specify a strategy while creating the deployment it will assume it to be rolling update. In other words rolling update is the default deployment strategy. # here nginx indicates the Container the update will take place and nginx:1.16.1 indicates the new image and its tag. k set image deployment <nginx-deployment> podName = nginx:1.16.1 # Example k set image deployment frontend simple-webapp = kodekloud/webapp-color:v2 Rollback \u00b6 Say for instance, once you upgrade your application you realize something is inferior right. Something is wrong with the new version of the build when you used to upgrade. So you would like to roll back your update. Kubernetes deployments allow you to roll back to a previous revision. To undo a change, run the following command. $ kubectl rollout undo deployment/myapp-deployment Create a deployment \u00b6 Use the below template apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment # This name will become the basis for the ReplicaSets and Pods which are created later. labels: app: nginx spec: replicas: 3 # The Deployment creates a ReplicaSet that creates three replicated Pods, indicated by the .spec.replicas field. selector: matchLabels: app: nginx # The .spec.selector field defines how the created ReplicaSet finds which Pods to manage. template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.14.2 ports: - containerPort: 80 Create K8S deployment via Python \u00b6 Install the K8S client using $ pip install kubernetes Create 2 files create-deployment.py k8s-deployment.yaml Python file code \u00b6 create-deployment.py from os import path import yaml from kubernetes import client , config def main (): # Configs can be set in Configuration class directly or using helper # utility. If no argument provided, the config will be loaded from # default location. config . load_kube_config () with open ( path . join ( path . dirname ( __file__ ), \"k8s-deployment.yaml\" )) as f : dep = yaml . safe_load ( f ) k8s_apps_v1 = client . AppsV1Api () resp = k8s_apps_v1 . create_namespaced_deployment ( body = dep , namespace = \"default\" ) print ( \"Deployment created. status=' %s '\" % resp . metadata . name ) if __name__ == '__main__' : main () Deployment file \u00b6 k8s-deployment.yaml apiVersion : apps/v1 kind : Deployment metadata : name : k8s-deployment labels : app : nginx spec : replicas : 3 selector : matchLabels : app : nginx template : metadata : labels : app : nginx spec : containers : - name : nginx image : nginx:1.15.4 ports : - containerPort : 80 Creating deployment \u00b6 python3 create - deployment . py","title":"Deployment"},{"location":"k8s/deployment_python/#deployment","text":"A Deployment provides declarative updates for Pods and ReplicaSets. You describe a desired state in a Deployment, and the Deployment Controller changes the actual state to the desired state at a controlled rate A deployment can create a RS Do not manage ReplicaSets owned by a Deployment.","title":"Deployment"},{"location":"k8s/deployment_python/#rollout-and-rollbacks","text":"A rollout is created in deployment a status of which can be shown as below # check rollout status using below kubectl rollout status deployment <nginx-deployment> #To check the revisions and history of rollout, you can use following command k rollout history deployment <myapp-deployment>","title":"Rollout and rollbacks"},{"location":"k8s/deployment_python/#update-a-deployment-using-rolling-update","text":"2 strategies are used mainly: 1. Re-create 2. Rolling update. Remember If you don\u2019t specify a strategy while creating the deployment it will assume it to be rolling update. In other words rolling update is the default deployment strategy. # here nginx indicates the Container the update will take place and nginx:1.16.1 indicates the new image and its tag. k set image deployment <nginx-deployment> podName = nginx:1.16.1 # Example k set image deployment frontend simple-webapp = kodekloud/webapp-color:v2","title":"Update a deployment using rolling update"},{"location":"k8s/deployment_python/#rollback","text":"Say for instance, once you upgrade your application you realize something is inferior right. Something is wrong with the new version of the build when you used to upgrade. So you would like to roll back your update. Kubernetes deployments allow you to roll back to a previous revision. To undo a change, run the following command. $ kubectl rollout undo deployment/myapp-deployment","title":"Rollback"},{"location":"k8s/deployment_python/#create-a-deployment","text":"Use the below template apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment # This name will become the basis for the ReplicaSets and Pods which are created later. labels: app: nginx spec: replicas: 3 # The Deployment creates a ReplicaSet that creates three replicated Pods, indicated by the .spec.replicas field. selector: matchLabels: app: nginx # The .spec.selector field defines how the created ReplicaSet finds which Pods to manage. template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.14.2 ports: - containerPort: 80","title":"Create a deployment"},{"location":"k8s/deployment_python/#create-k8s-deployment-via-python","text":"Install the K8S client using $ pip install kubernetes Create 2 files create-deployment.py k8s-deployment.yaml","title":"Create K8S deployment via Python"},{"location":"k8s/deployment_python/#python-file-code","text":"create-deployment.py from os import path import yaml from kubernetes import client , config def main (): # Configs can be set in Configuration class directly or using helper # utility. If no argument provided, the config will be loaded from # default location. config . load_kube_config () with open ( path . join ( path . dirname ( __file__ ), \"k8s-deployment.yaml\" )) as f : dep = yaml . safe_load ( f ) k8s_apps_v1 = client . AppsV1Api () resp = k8s_apps_v1 . create_namespaced_deployment ( body = dep , namespace = \"default\" ) print ( \"Deployment created. status=' %s '\" % resp . metadata . name ) if __name__ == '__main__' : main ()","title":"Python file code"},{"location":"k8s/deployment_python/#deployment-file","text":"k8s-deployment.yaml apiVersion : apps/v1 kind : Deployment metadata : name : k8s-deployment labels : app : nginx spec : replicas : 3 selector : matchLabels : app : nginx template : metadata : labels : app : nginx spec : containers : - name : nginx image : nginx:1.15.4 ports : - containerPort : 80","title":"Deployment file"},{"location":"k8s/deployment_python/#creating-deployment","text":"python3 create - deployment . py","title":"Creating deployment"},{"location":"k8s/ds/","text":"DeamonSet \u00b6 A DaemonSet ensures that all (or some) Nodes run a copy of a Pod. As nodes are added to the cluster, Pods are added to them. As nodes are removed from the cluster, those Pods are garbage collected. Deleting a DaemonSet will clean up the Pods it created. Typical uses of a DaemonSet Running a cluster storage daemon on every node Log collection : Running a logs collection daemon on every node Monitoring : Running a node monitoring daemon on every node Commands \u00b6 k get ds We can not create ds using the kubectl, so we can The fastest way to create is to create a deployment kubectl create deploy nginx --image = nginx --dry-run -o yaml > nginx-ds.yaml Now replace the line kind: Deployment with kind: DaemonSet in nginx-ds.yaml and remove the line replicas: 1 , strategy {} and status {} as well. Otherwise, it shows error for some required fields like this error: error validating \"nginx-ds.yaml\" : error validating data: [ ValidationError ( DaemonSet.spec ) : unknown field \"strategy\" in io.k8s.api.apps.v1.DaemonSetSpec, ValidationError ( DaemonSet.status ) : missing required field \"currentNumberScheduled\" in io.k8s.api.apps.v1.DaemonSetStatus,ValidationError ( DaemonSet.status ) : missing required field \"numberMisscheduled\" in io.k8s.api.apps.v1.DaemonSetStatus, ValidationError ( DaemonSet.status ) : missing required field \"desiredNumberScheduled\" in io.k8s.api.apps.v1.DaemonSetStatus, ValidationError ( DaemonSet.status ) : missing required field \"numberReady\" in io.k8s.api.apps.v1.DaemonSetStatus ] ; if you choose to ignore these errors, turn validation off with --validate = false","title":"DeamonSet"},{"location":"k8s/ds/#deamonset","text":"A DaemonSet ensures that all (or some) Nodes run a copy of a Pod. As nodes are added to the cluster, Pods are added to them. As nodes are removed from the cluster, those Pods are garbage collected. Deleting a DaemonSet will clean up the Pods it created. Typical uses of a DaemonSet Running a cluster storage daemon on every node Log collection : Running a logs collection daemon on every node Monitoring : Running a node monitoring daemon on every node","title":"DeamonSet"},{"location":"k8s/ds/#commands","text":"k get ds We can not create ds using the kubectl, so we can The fastest way to create is to create a deployment kubectl create deploy nginx --image = nginx --dry-run -o yaml > nginx-ds.yaml Now replace the line kind: Deployment with kind: DaemonSet in nginx-ds.yaml and remove the line replicas: 1 , strategy {} and status {} as well. Otherwise, it shows error for some required fields like this error: error validating \"nginx-ds.yaml\" : error validating data: [ ValidationError ( DaemonSet.spec ) : unknown field \"strategy\" in io.k8s.api.apps.v1.DaemonSetSpec, ValidationError ( DaemonSet.status ) : missing required field \"currentNumberScheduled\" in io.k8s.api.apps.v1.DaemonSetStatus,ValidationError ( DaemonSet.status ) : missing required field \"numberMisscheduled\" in io.k8s.api.apps.v1.DaemonSetStatus, ValidationError ( DaemonSet.status ) : missing required field \"desiredNumberScheduled\" in io.k8s.api.apps.v1.DaemonSetStatus, ValidationError ( DaemonSet.status ) : missing required field \"numberReady\" in io.k8s.api.apps.v1.DaemonSetStatus ] ; if you choose to ignore these errors, turn validation off with --validate = false","title":"Commands"},{"location":"k8s/ingress/","text":"Ingress \u00b6 Ingress Fanout \u00b6 Fanout Ingress Example apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : simple-fanout-example spec : rules : - host : ammarjitdhillon.com http : paths : - path : /sa pathType : Prefix backend : service : name : serviceA port : number : 4200 - path : /sb pathType : Prefix backend : service : name : serviceB port : number : 8080","title":"Ingress"},{"location":"k8s/ingress/#ingress","text":"","title":"Ingress"},{"location":"k8s/ingress/#ingress-fanout","text":"Fanout Ingress Example apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : simple-fanout-example spec : rules : - host : ammarjitdhillon.com http : paths : - path : /sa pathType : Prefix backend : service : name : serviceA port : number : 4200 - path : /sb pathType : Prefix backend : service : name : serviceB port : number : 8080","title":"Ingress Fanout"},{"location":"k8s/node-affinity/","text":"Node affinity \u00b6 How to place a pod to a node? They are 3 ways to place a POD on a specific node: - Node Selectors: With the Node Selectors we cannot provide advanced expressions like OR or NOT with node selectors. nodeSelector is the simplest recommended form of node selection constraint. You can add the nodeSelector field to your Pod specification and specify the node labels you want the target node to have. Kubernetes only schedules the Pod onto nodes that have each of the labels you specify. - Taints and Tolerations: - Node Affinity Node affinity allows you to tell Kubernetes to schedule pods only to specific subsets of nodes. Some benefits of affinity and anti-affinity over the nodeSelectors are The affinity/anti-affinity language is more expressive. nodeSelector only selects nodes with all the specified labels. Affinity/anti-affinity gives you more control over the selection logic. You can indicate that a rule is soft or preferred , so that the scheduler still schedules the Pod even if it can't find a matching node. You can constrain a Pod using labels on other Pods running on the node (or other topological domain), instead of just node labels, which allows you to define rules for which Pods can be co-located on a node. pod with node affinity and anti-affinity apiVersion: v1 kind: Pod metadata: name: with-node-affinity spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: topology.kubernetes.io/zone operator: In values: - antarctica-east1 - antarctica-west1 preferredDuringSchedulingIgnoredDuringExecution: - weight: 1 preference: matchExpressions: - key: another-node-label-key operator: In values: - another-node-label-value containers: - name: with-node-affinity image: registry.k8s.io/pause:2.0","title":"Node Affinity"},{"location":"k8s/node-affinity/#node-affinity","text":"How to place a pod to a node? They are 3 ways to place a POD on a specific node: - Node Selectors: With the Node Selectors we cannot provide advanced expressions like OR or NOT with node selectors. nodeSelector is the simplest recommended form of node selection constraint. You can add the nodeSelector field to your Pod specification and specify the node labels you want the target node to have. Kubernetes only schedules the Pod onto nodes that have each of the labels you specify. - Taints and Tolerations: - Node Affinity Node affinity allows you to tell Kubernetes to schedule pods only to specific subsets of nodes. Some benefits of affinity and anti-affinity over the nodeSelectors are The affinity/anti-affinity language is more expressive. nodeSelector only selects nodes with all the specified labels. Affinity/anti-affinity gives you more control over the selection logic. You can indicate that a rule is soft or preferred , so that the scheduler still schedules the Pod even if it can't find a matching node. You can constrain a Pod using labels on other Pods running on the node (or other topological domain), instead of just node labels, which allows you to define rules for which Pods can be co-located on a node. pod with node affinity and anti-affinity apiVersion: v1 kind: Pod metadata: name: with-node-affinity spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: topology.kubernetes.io/zone operator: In values: - antarctica-east1 - antarctica-west1 preferredDuringSchedulingIgnoredDuringExecution: - weight: 1 preference: matchExpressions: - key: another-node-label-key operator: In values: - another-node-label-value containers: - name: with-node-affinity image: registry.k8s.io/pause:2.0","title":"Node affinity"},{"location":"k8s/ns/","text":"Namespace \u00b6 Basics \u00b6 In Kubernetes, namespaces provides a mechanism for isolating groups of resources within a single cluster. Names of resources need to be unique within a namespace, but not across namespaces. Namespace-based scoping is applicable only for namespaced objects (e.g. Deployments, Services, etc) and not for cluster-wide objects (e.g. StorageClass, Nodes, PersistentVolumes, etc). Remember Namespaces cannot be nested inside one another and each Kubernetes resource can only be in one namespace. An example of namespace is shown below: Initial namespaces \u00b6 Kubernetes starts with 4 initial namespaces: default \u00b6 Kubernetes includes this namespace so that you can start using your new cluster without first creating a namespace. kube-node-lease \u00b6 This namespace holds Lease objects associated with each node. Node leases allow the kubelet to send heartbeats so that the control plane can detect node failure. kube-public \u00b6 This namespace is readable by all clients (including those not authenticated). This namespace is mostly reserved for cluster usage, in case that some resources should be visible and readable publicly throughout the whole cluster. The public aspect of this namespace is only a convention, not a requirement. kube-system \u00b6 The namespace for objects created by the Kubernetes system. Namespaces and DNS \u00b6 When you create a Service, it creates a corresponding DNS entry. This entry is of the form <service-name>.<namespace-name>.svc.cluster.local , which means that if a container only uses <service-name> , it will resolve to the service which is local to a namespace. This is useful for using the same configuration across multiple namespaces such as Development, Staging and Production. If you want to reach across namespaces, you need to use the fully qualified domain name (FQDN) . Commands \u00b6 Set the default namespace \u00b6 kubectl config set-context --current --namespace = <insert-namespace-name-here> # Validate it kubectl config view --minify | grep namespace: See namespaced and non-namespaced resources \u00b6 Most Kubernetes resources (e.g. pods, services, replication controllers, and others) are in some namespaces. However, namespace resources are not themselves in a namespace. And low-level resources, such as nodes and persistentVolumes , are not in any namespace. # In a namespace kubectl api-resources --namespaced = true # Not in a namespace kubectl api-resources --namespaced = false","title":"Namespace"},{"location":"k8s/ns/#namespace","text":"","title":"Namespace"},{"location":"k8s/ns/#basics","text":"In Kubernetes, namespaces provides a mechanism for isolating groups of resources within a single cluster. Names of resources need to be unique within a namespace, but not across namespaces. Namespace-based scoping is applicable only for namespaced objects (e.g. Deployments, Services, etc) and not for cluster-wide objects (e.g. StorageClass, Nodes, PersistentVolumes, etc). Remember Namespaces cannot be nested inside one another and each Kubernetes resource can only be in one namespace. An example of namespace is shown below:","title":"Basics"},{"location":"k8s/ns/#initial-namespaces","text":"Kubernetes starts with 4 initial namespaces:","title":"Initial namespaces"},{"location":"k8s/ns/#default","text":"Kubernetes includes this namespace so that you can start using your new cluster without first creating a namespace.","title":"default"},{"location":"k8s/ns/#kube-node-lease","text":"This namespace holds Lease objects associated with each node. Node leases allow the kubelet to send heartbeats so that the control plane can detect node failure.","title":"kube-node-lease"},{"location":"k8s/ns/#kube-public","text":"This namespace is readable by all clients (including those not authenticated). This namespace is mostly reserved for cluster usage, in case that some resources should be visible and readable publicly throughout the whole cluster. The public aspect of this namespace is only a convention, not a requirement.","title":"kube-public"},{"location":"k8s/ns/#kube-system","text":"The namespace for objects created by the Kubernetes system.","title":"kube-system"},{"location":"k8s/ns/#namespaces-and-dns","text":"When you create a Service, it creates a corresponding DNS entry. This entry is of the form <service-name>.<namespace-name>.svc.cluster.local , which means that if a container only uses <service-name> , it will resolve to the service which is local to a namespace. This is useful for using the same configuration across multiple namespaces such as Development, Staging and Production. If you want to reach across namespaces, you need to use the fully qualified domain name (FQDN) .","title":"Namespaces and DNS"},{"location":"k8s/ns/#commands","text":"","title":"Commands"},{"location":"k8s/ns/#set-the-default-namespace","text":"kubectl config set-context --current --namespace = <insert-namespace-name-here> # Validate it kubectl config view --minify | grep namespace:","title":"Set the default namespace"},{"location":"k8s/ns/#see-namespaced-and-non-namespaced-resources","text":"Most Kubernetes resources (e.g. pods, services, replication controllers, and others) are in some namespaces. However, namespace resources are not themselves in a namespace. And low-level resources, such as nodes and persistentVolumes , are not in any namespace. # In a namespace kubectl api-resources --namespaced = true # Not in a namespace kubectl api-resources --namespaced = false","title":"See namespaced and non-namespaced resources"},{"location":"k8s/poc_minikube/","text":"Demo Python app using Minikube \u00b6 Simple echo container \ud83d\uddf3 \u00b6 Create a Dockerfile Dockerfile FROM alpine CMD [ \"echo\" , \"Welcome to my blog\" ] Build image in minikube and run the pod \ud83d\ude80 Build image and run it minikube image build -t test . # build image to minikube directly kubectl run test-pod --image = test --image-pull-policy = Never --restart = Never k logs test-pod #check pod logs # below is the output Welcome to my blog Docker setup \ud83d\udea2 \u00b6 Build an image \ud83d\udc77\ud83c\udffb\u200d\u2642\ufe0f \u00b6 docker build -f ../pathToDockerFile/Dockerfile -t demo:latest . Run image \ud83c\udfc3\u200d\u2642\ufe0f \u00b6 # run image docker run -p 5000 :5000 demo # (1) # get pid on a port num lsof -i:port_num kill -9 pid # single liner to kill process on a certain port number kill -9 $( lsof -t -i:5000 ) # Remove all containers to unbind the port (Not recommended) docker rm -fv $( docker ps -aq ) Here we are mapping the 5000 port of container to the 5000 port of host Remove image from registry \u274c \u00b6 Before removing the image from docker registry, we need to stop and delete the container docker ps -all # see list of containers docker rm CONTAINER_ID # remove container that is running image we want to remove. docker image ls # list of all images docker rmi -f IMAGE_ID # remove the image Generic Docker commands \ud83d\udda5 \u00b6 Some important Docker commands \ud83d\udef3 # Build an image from a Dockerfile docker image build # Show the history of an image docker image history # Import the contents from a tarball to create a filesystem image docker image import # Display detailed information on one or more images docker image inspect # Load an image from a tar archive or STDIN docker image load # List images docker image ls # Remove unused images docker image prune # Pull an image or a repository from a registry docker image pull # Push an image or a repository to a registry docker image push # Remove one or more images docker image rm # Remove one or more images docker image save #Save one or more images to a tar archive (streamed to STDOUT by default) # Create a tag TARGET_IMAGE that refers to SOURCE_IMAGE docker image tag ** Minikube setup ** \u2693\ufe0f \u00b6 What is Hyperkit? HyperKit is an open-source hypervisor for macOS hypervisor, optimized for lightweight virtual machines and container deployment. This is the default hypervisor for Minikube in Mac. Start minikube \u00b6 # using docker as hypervisor minikube start --driver = docker # enable kubelet service, if needed systemctl enable kubelet.service Check status \ud83d\udcc8 \u00b6 minikube status will result in something similar as shown below Output of minikube status is shown below minikube type : Control Plane host : Running kubelet : Running apiserver : Running kubeconfig : Configured Check the nodes of minikube using kubectl get nodes # (1) This will show control-plane in case of minikube as whole setup is on local Lauch dashboard \ud83d\udda5 \u00b6 This is helpful to see various resources in one place. minikube dashboard Registry options: \u00b6 Get current registry docker info | grep Regis There are 3 options Use existing docker resigtry Setup a private registry Directly build images on Minikube 1. Docker container registry \u00b6 minikube config set driver docker # set the driver minikube config get driver # confirm the driver status (1) Should get docker as output Setup registry addon \ud83d\udce2 \u00b6 Steps to setup registry addon $ minikube addons configure registry-creds # (1) Do you want to enable AWS Elastic Container Registry? [ y/n ] : n Do you want to enable Google Container Registry? [ y/n ] : n Do you want to enable Docker Registry? [ y/n ] : y # (2) -- Enter docker registry server url: https://index.docker.io/v1/ -- Enter docker registry username: amar -- Enter docker registry password: Do you want to enable Azure Container Registry? [ y/n ] : n \u2705 registry-creds was successfully configured GCR/ECR/ACR/Docker : minikube has an addon, registry-creds which maps credentials into minikube to support pulling from Google Container Registry (GCR), Amazon\u2019s EC2 Container Registry (ECR), Azure Container Registry (ACR), and Private Docker registries. You will need to run minikube addons configure registry-creds and minikube addons enable registry-creds to get up and running. Setup your own username and password Cofigure to use Docker registry \u00b6 If you want to create the registry on minikube's Docker then run eval $( minikube docker-env ) What does this command do? The command minikube docker-env returns a set of bash environment variable exports to configure your local environment to re-use the Docker daemon inside the Minikube instance. Passing this output through eval causes bash to evaluate these exports and put them into effect. You can review the specific commands which will be executed in your shell by omitting the evaluation step and running minikube docker-env directly. However, this will not perform the configuration \u2013 the output needs to be evaluated for that. Enable registry addon \ud83d\udccc \u00b6 $ minikube addons enable registry-creds \u2757 registry-creds is a 3rd party addon and not maintained or verified by minikube maintainers, enable at your own risk. \u25aa Using image upmcenterprises/registry-creds:1.10 \ud83c\udf1f The 'registry-creds' addon is enabled check more info on https://minikube.sigs.k8s.io/docs/handbook/registry/ 2. Setup private registry \ud83d\uddf3 \u00b6 To make docker available on the host machine's terminal Otherwise enter in the virtual machine via minikube ssh , and then proceed with the following steps Depending on your operating system, minikube will automatically mount your homepath onto the VM. you'll need to add the local registry as insecure in order to use http (may not apply when using localhost but does apply if using the local hostname) Don't use http in production, make the effort for securing things up. Use a local registry: Create a local resigtry docker run -d -p 5000 :5000 \\ --restart = always \\ --name local-registry registry:2 Now tag your image properly: Tag an image docker tag ubuntu localhost:5000/ubuntu Note The localhost should be changed to dns name of the machine running registry container. Now push your image to local registry: Push Image docker push localhost:5000/ubuntu You should be able to pull it back: Pull Image docker pull localhost:5000/ubuntu Now, change your yaml file to use the local registry. 3.Building on Minikube \u00b6 minikube image build -t test_app . Confirm it using minikube image ls Create K8S manifests: Generic \u00b6 Crate a pod \u00b6 You can create a pod if needed as shown below create a pod manifest using dry run kubectl run mypod --image = docker.io/library/demo:latest \\ --labels app = demo-dp \\ --dry-run = client -o yaml > test_pod.yaml In this example we are directly creating a deployment, so creating a pod is not required. Create a deployment \u00b6 Create depyloyment using declarative commands kubectl create deployment demo-dp \\ --image = docker.io/library/demo:latest \\ --replicas = 1 -o yaml > dep_demo.yaml Create a service \ud83c\udf7b \u00b6 The service yaml config can be apiVersion : v1 kind : Service metadata : labels : app : backend name : backend spec : ports : - name : api protocol : TCP port : 10000 selector : app : backend Confirm the resources \u00b6 Check if service and deployment is created. You can check the endpoint using k get ep # (1) Check if endpoint is created ssh into pod using k exec -it POD_ID -- /bin/sh ``` Now check the servcie status using ``` bash $ k exec -it POD_ID -- /bin/sh printenv | grep SERVICE # run this command after doing `exec -it` Sample output of printenv \ud83d\udcdd KUBERNETES_SERVICE_PORT=443 HELLO_PYTHON_SERVICE_PORT_6000_TCP_ADDR=10.109.182.67 HELLO_PYTHON_SERVICE_PORT_6000_TCP_PORT=6000 HELLO_PYTHON_SERVICE_PORT_6000_TCP_PROTO=tcp DEMO_SERVICE_PORT_6000_TCP_ADDR=10.103.151.245 DEMO_SERVICE_PORT_6000_TCP_PORT=6000 DEMO_SERVICE_PORT_6000_TCP_PROTO=tcp HELLO_PYTHON_SERVICE_SERVICE_HOST=10.109.182.67 HELLO_PYTHON_SERVICE_PORT_6000_TCP=tcp://10.109.182.67:6000 DEMO_SERVICE_SERVICE_HOST=10.103.151.245 HELLO_PYTHON_SERVICE_PORT=tcp://10.109.182.67:6000 HELLO_PYTHON_SERVICE_SERVICE_PORT=6000 DEMO_SERVICE_PORT_6000_TCP=tcp://10.103.151.245:6000 KUBERNETES_SERVICE_PORT_HTTPS=443 DEMO_SERVICE_PORT=tcp://10.103.151.245:6000 DEMO_SERVICE_SERVICE_PORT=6000 KUBERNETES_SERVICE_HOST=10.96.0.1 POC example using Fast API \u00b6 Below are the steps to create a sample POC Install FastAPI on local \u00b6 Install fastapi using $ python -m pip install fastapi uvicorn [ standard ] Run server on local using $ uvicorn demo_main:app --reload Get access to swagger docs using http://127.0.0.1:8000/docs Create the FastAPI Code Create an app directory and enter it. Create an empty file init .py. Create a main.py file with below code FastAPI sample code \ud83d\udc68\ud83c\udffb\u200d\ud83d\udcbb from typing import Union from fastapi import FastAPI app = FastAPI () @app . get ( \"/\" ) def read_root (): return { \"Hello\" : \"World\" } @app . get ( \"/items/ {item_id} \" ) def read_item ( item_id : int , q : Union [ str , None ] = None ): return { \"item_id\" : item_id , \"q\" : q } Why we need init .py? Files named __init__.py are used to mark directories on disk as Python package directories. If you have the files mydir / spam / __init__ . py mydir / spam / module . py and mydir is on your path, you can import the code in module.py as import spam.module or from spam import module If you remove the __init__.py file, Python will no longer look for submodules inside that directory, so attempts to import the module will fail. The __init__.py file is usually empty, but can be used to export selected portions of the package under more convenient name, hold convenience functions, etc. Given the example above, the contents of the init module can be accessed as import spam Requirements file \u00b6 An requirements.txt is shown below fastapi pydantic uvicorn Create a DockerFile \u00b6 Now in the same project directory create a file Dockerfile with: Dockerfile for FastAPI \ud83d\ude80 FROM python:3.9 WORKDIR /code COPY ./requirements.txt /code/requirements.txt RUN pip install --no-cache-dir --upgrade -r /code/requirements.txt COPY ./app /code/app CMD [ \"uvicorn\" , \"app.main:app\" , \"--host\" , \"0.0.0.0\" , \"--port\" , \"80\" ] You should now have a directory structure like: . \u251c\u2500\u2500 app \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u2514\u2500\u2500 main.py \u251c\u2500\u2500 Dockerfile \u2514\u2500\u2500 requirements.txt Create image on Minikube \u00b6 Create a image using the sample code minikube image build -t test_app . Create k8s manifests for POC \u00b6 Using Kubectl Using Python Using Kubectl \u00b6 Given the fact that we have 2 files create_deployment.yaml create_servcie.yaml we can create a deployment using the following kubectl apply -f create_service kubectl apply -f create_deployment Using Python \u00b6 Install the required package pip install kubernetes Confirm the installation using $ pip list | grep kuber kubernetes 24 .2.0 Imagine that we have below service and deployment code create_service.yaml apiVersion : v1 kind : Service metadata : name : demo-service-api spec : selector : app : demo-api ports : - protocol : \"TCP\" port : 80 targetPort : 80 type : LoadBalancer A sample deployment manifest create_deployment.yaml apiVersion : apps/v1 kind : Deployment metadata : name : demo-deploy-api spec : replicas : 1 selector : matchLabels : app : demo-api template : metadata : labels : app : demo-api spec : containers : - name : demo-api image : fast imagePullPolicy : Never ports : - containerPort : 80 code to create manifests using Python K8S API createApplication.py from os import path import yaml from kubernetes import client , config from kubernetes.client import V1DeleteOptions def main (): # Configs can be set in Configuration class directly or using helper # utility. If no argument provided, the config will be loaded from # default location. config . load_kube_config () k8s_apps_v1 = client . AppsV1Api () core_v1_api = client . CoreV1Api () create_service ( core_v1_api ) create_deployment ( k8s_apps_v1 ) del_opts = V1DeleteOptions () api_client = get_custom_objects_api_client () def create_deployment ( k8s_apps_v1 ): with open ( path . join ( path . dirname ( __file__ ), \"demo_deployment.yaml\" )) as f : dep = yaml . safe_load ( f ) resp = k8s_apps_v1 . create_namespaced_deployment ( body = dep , namespace = \"default\" ) print ( \"Deployment created. status=' %s '\" % resp . metadata . name ) def create_service ( core_v1_api ): with open ( path . join ( path . dirname ( __file__ ), \"demo_service.yaml\" )) as f : service = yaml . safe_load ( f ) resp = core_v1_api . create_namespaced_service ( body = service , namespace = \"default\" ) resp = core_v1_api . delete print ( \"Service status=' %s '\" % resp . metadata . name ) if __name__ == '__main__' : main () Create the resources using python createApplication . py","title":"Demo app in Minikube"},{"location":"k8s/poc_minikube/#demo-python-app-using-minikube","text":"","title":"Demo Python app using Minikube"},{"location":"k8s/poc_minikube/#simple-echo-container","text":"Create a Dockerfile Dockerfile FROM alpine CMD [ \"echo\" , \"Welcome to my blog\" ] Build image in minikube and run the pod \ud83d\ude80 Build image and run it minikube image build -t test . # build image to minikube directly kubectl run test-pod --image = test --image-pull-policy = Never --restart = Never k logs test-pod #check pod logs # below is the output Welcome to my blog","title":"Simple echo container \ud83d\uddf3"},{"location":"k8s/poc_minikube/#docker-setup","text":"","title":"Docker setup \ud83d\udea2"},{"location":"k8s/poc_minikube/#build-an-image","text":"docker build -f ../pathToDockerFile/Dockerfile -t demo:latest .","title":"Build an image \ud83d\udc77\ud83c\udffb\u200d\u2642\ufe0f"},{"location":"k8s/poc_minikube/#run-image","text":"# run image docker run -p 5000 :5000 demo # (1) # get pid on a port num lsof -i:port_num kill -9 pid # single liner to kill process on a certain port number kill -9 $( lsof -t -i:5000 ) # Remove all containers to unbind the port (Not recommended) docker rm -fv $( docker ps -aq ) Here we are mapping the 5000 port of container to the 5000 port of host","title":"Run image \ud83c\udfc3\u200d\u2642\ufe0f"},{"location":"k8s/poc_minikube/#remove-image-from-registry","text":"Before removing the image from docker registry, we need to stop and delete the container docker ps -all # see list of containers docker rm CONTAINER_ID # remove container that is running image we want to remove. docker image ls # list of all images docker rmi -f IMAGE_ID # remove the image","title":"Remove image from registry \u274c"},{"location":"k8s/poc_minikube/#generic-docker-commands","text":"Some important Docker commands \ud83d\udef3 # Build an image from a Dockerfile docker image build # Show the history of an image docker image history # Import the contents from a tarball to create a filesystem image docker image import # Display detailed information on one or more images docker image inspect # Load an image from a tar archive or STDIN docker image load # List images docker image ls # Remove unused images docker image prune # Pull an image or a repository from a registry docker image pull # Push an image or a repository to a registry docker image push # Remove one or more images docker image rm # Remove one or more images docker image save #Save one or more images to a tar archive (streamed to STDOUT by default) # Create a tag TARGET_IMAGE that refers to SOURCE_IMAGE docker image tag","title":"Generic Docker commands \ud83d\udda5"},{"location":"k8s/poc_minikube/#minikube-setup","text":"What is Hyperkit? HyperKit is an open-source hypervisor for macOS hypervisor, optimized for lightweight virtual machines and container deployment. This is the default hypervisor for Minikube in Mac.","title":"** Minikube setup ** \u2693\ufe0f"},{"location":"k8s/poc_minikube/#start-minikube","text":"# using docker as hypervisor minikube start --driver = docker # enable kubelet service, if needed systemctl enable kubelet.service","title":"Start minikube"},{"location":"k8s/poc_minikube/#check-status","text":"minikube status will result in something similar as shown below Output of minikube status is shown below minikube type : Control Plane host : Running kubelet : Running apiserver : Running kubeconfig : Configured Check the nodes of minikube using kubectl get nodes # (1) This will show control-plane in case of minikube as whole setup is on local","title":"Check status \ud83d\udcc8"},{"location":"k8s/poc_minikube/#lauch-dashboard","text":"This is helpful to see various resources in one place. minikube dashboard","title":"Lauch dashboard \ud83d\udda5"},{"location":"k8s/poc_minikube/#registry-options","text":"Get current registry docker info | grep Regis There are 3 options Use existing docker resigtry Setup a private registry Directly build images on Minikube","title":"Registry options:"},{"location":"k8s/poc_minikube/#1-docker-container-registry","text":"minikube config set driver docker # set the driver minikube config get driver # confirm the driver status (1) Should get docker as output","title":"1. Docker container registry"},{"location":"k8s/poc_minikube/#setup-registry-addon","text":"Steps to setup registry addon $ minikube addons configure registry-creds # (1) Do you want to enable AWS Elastic Container Registry? [ y/n ] : n Do you want to enable Google Container Registry? [ y/n ] : n Do you want to enable Docker Registry? [ y/n ] : y # (2) -- Enter docker registry server url: https://index.docker.io/v1/ -- Enter docker registry username: amar -- Enter docker registry password: Do you want to enable Azure Container Registry? [ y/n ] : n \u2705 registry-creds was successfully configured GCR/ECR/ACR/Docker : minikube has an addon, registry-creds which maps credentials into minikube to support pulling from Google Container Registry (GCR), Amazon\u2019s EC2 Container Registry (ECR), Azure Container Registry (ACR), and Private Docker registries. You will need to run minikube addons configure registry-creds and minikube addons enable registry-creds to get up and running. Setup your own username and password","title":"Setup registry addon \ud83d\udce2"},{"location":"k8s/poc_minikube/#cofigure-to-use-docker-registry","text":"If you want to create the registry on minikube's Docker then run eval $( minikube docker-env ) What does this command do? The command minikube docker-env returns a set of bash environment variable exports to configure your local environment to re-use the Docker daemon inside the Minikube instance. Passing this output through eval causes bash to evaluate these exports and put them into effect. You can review the specific commands which will be executed in your shell by omitting the evaluation step and running minikube docker-env directly. However, this will not perform the configuration \u2013 the output needs to be evaluated for that.","title":"Cofigure to use Docker registry"},{"location":"k8s/poc_minikube/#enable-registry-addon","text":"$ minikube addons enable registry-creds \u2757 registry-creds is a 3rd party addon and not maintained or verified by minikube maintainers, enable at your own risk. \u25aa Using image upmcenterprises/registry-creds:1.10 \ud83c\udf1f The 'registry-creds' addon is enabled check more info on https://minikube.sigs.k8s.io/docs/handbook/registry/","title":"Enable registry addon \ud83d\udccc"},{"location":"k8s/poc_minikube/#2-setup-private-registry","text":"To make docker available on the host machine's terminal Otherwise enter in the virtual machine via minikube ssh , and then proceed with the following steps Depending on your operating system, minikube will automatically mount your homepath onto the VM. you'll need to add the local registry as insecure in order to use http (may not apply when using localhost but does apply if using the local hostname) Don't use http in production, make the effort for securing things up. Use a local registry: Create a local resigtry docker run -d -p 5000 :5000 \\ --restart = always \\ --name local-registry registry:2 Now tag your image properly: Tag an image docker tag ubuntu localhost:5000/ubuntu Note The localhost should be changed to dns name of the machine running registry container. Now push your image to local registry: Push Image docker push localhost:5000/ubuntu You should be able to pull it back: Pull Image docker pull localhost:5000/ubuntu Now, change your yaml file to use the local registry.","title":"2. Setup private registry \ud83d\uddf3"},{"location":"k8s/poc_minikube/#3building-on-minikube","text":"minikube image build -t test_app . Confirm it using minikube image ls","title":"3.Building on Minikube"},{"location":"k8s/poc_minikube/#create-k8s-manifests-generic","text":"","title":"Create K8S manifests: Generic"},{"location":"k8s/poc_minikube/#crate-a-pod","text":"You can create a pod if needed as shown below create a pod manifest using dry run kubectl run mypod --image = docker.io/library/demo:latest \\ --labels app = demo-dp \\ --dry-run = client -o yaml > test_pod.yaml In this example we are directly creating a deployment, so creating a pod is not required.","title":"Crate a pod"},{"location":"k8s/poc_minikube/#create-a-deployment","text":"Create depyloyment using declarative commands kubectl create deployment demo-dp \\ --image = docker.io/library/demo:latest \\ --replicas = 1 -o yaml > dep_demo.yaml","title":"Create a deployment"},{"location":"k8s/poc_minikube/#create-a-service","text":"The service yaml config can be apiVersion : v1 kind : Service metadata : labels : app : backend name : backend spec : ports : - name : api protocol : TCP port : 10000 selector : app : backend","title":"Create a service \ud83c\udf7b"},{"location":"k8s/poc_minikube/#confirm-the-resources","text":"Check if service and deployment is created. You can check the endpoint using k get ep # (1) Check if endpoint is created ssh into pod using k exec -it POD_ID -- /bin/sh ``` Now check the servcie status using ``` bash $ k exec -it POD_ID -- /bin/sh printenv | grep SERVICE # run this command after doing `exec -it` Sample output of printenv \ud83d\udcdd KUBERNETES_SERVICE_PORT=443 HELLO_PYTHON_SERVICE_PORT_6000_TCP_ADDR=10.109.182.67 HELLO_PYTHON_SERVICE_PORT_6000_TCP_PORT=6000 HELLO_PYTHON_SERVICE_PORT_6000_TCP_PROTO=tcp DEMO_SERVICE_PORT_6000_TCP_ADDR=10.103.151.245 DEMO_SERVICE_PORT_6000_TCP_PORT=6000 DEMO_SERVICE_PORT_6000_TCP_PROTO=tcp HELLO_PYTHON_SERVICE_SERVICE_HOST=10.109.182.67 HELLO_PYTHON_SERVICE_PORT_6000_TCP=tcp://10.109.182.67:6000 DEMO_SERVICE_SERVICE_HOST=10.103.151.245 HELLO_PYTHON_SERVICE_PORT=tcp://10.109.182.67:6000 HELLO_PYTHON_SERVICE_SERVICE_PORT=6000 DEMO_SERVICE_PORT_6000_TCP=tcp://10.103.151.245:6000 KUBERNETES_SERVICE_PORT_HTTPS=443 DEMO_SERVICE_PORT=tcp://10.103.151.245:6000 DEMO_SERVICE_SERVICE_PORT=6000 KUBERNETES_SERVICE_HOST=10.96.0.1","title":"Confirm the resources"},{"location":"k8s/poc_minikube/#poc-example-using-fast-api","text":"Below are the steps to create a sample POC","title":"POC example using Fast API"},{"location":"k8s/poc_minikube/#install-fastapi-on-local","text":"Install fastapi using $ python -m pip install fastapi uvicorn [ standard ] Run server on local using $ uvicorn demo_main:app --reload Get access to swagger docs using http://127.0.0.1:8000/docs Create the FastAPI Code Create an app directory and enter it. Create an empty file init .py. Create a main.py file with below code FastAPI sample code \ud83d\udc68\ud83c\udffb\u200d\ud83d\udcbb from typing import Union from fastapi import FastAPI app = FastAPI () @app . get ( \"/\" ) def read_root (): return { \"Hello\" : \"World\" } @app . get ( \"/items/ {item_id} \" ) def read_item ( item_id : int , q : Union [ str , None ] = None ): return { \"item_id\" : item_id , \"q\" : q } Why we need init .py? Files named __init__.py are used to mark directories on disk as Python package directories. If you have the files mydir / spam / __init__ . py mydir / spam / module . py and mydir is on your path, you can import the code in module.py as import spam.module or from spam import module If you remove the __init__.py file, Python will no longer look for submodules inside that directory, so attempts to import the module will fail. The __init__.py file is usually empty, but can be used to export selected portions of the package under more convenient name, hold convenience functions, etc. Given the example above, the contents of the init module can be accessed as import spam","title":"Install FastAPI on local"},{"location":"k8s/poc_minikube/#requirements-file","text":"An requirements.txt is shown below fastapi pydantic uvicorn","title":"Requirements file"},{"location":"k8s/poc_minikube/#create-a-dockerfile","text":"Now in the same project directory create a file Dockerfile with: Dockerfile for FastAPI \ud83d\ude80 FROM python:3.9 WORKDIR /code COPY ./requirements.txt /code/requirements.txt RUN pip install --no-cache-dir --upgrade -r /code/requirements.txt COPY ./app /code/app CMD [ \"uvicorn\" , \"app.main:app\" , \"--host\" , \"0.0.0.0\" , \"--port\" , \"80\" ] You should now have a directory structure like: . \u251c\u2500\u2500 app \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u2514\u2500\u2500 main.py \u251c\u2500\u2500 Dockerfile \u2514\u2500\u2500 requirements.txt","title":"Create a DockerFile"},{"location":"k8s/poc_minikube/#create-image-on-minikube","text":"Create a image using the sample code minikube image build -t test_app .","title":"Create image on Minikube"},{"location":"k8s/poc_minikube/#create-k8s-manifests-for-poc","text":"Using Kubectl Using Python","title":"Create k8s manifests for POC"},{"location":"k8s/poc_minikube/#using-kubectl","text":"Given the fact that we have 2 files create_deployment.yaml create_servcie.yaml we can create a deployment using the following kubectl apply -f create_service kubectl apply -f create_deployment","title":"Using Kubectl"},{"location":"k8s/poc_minikube/#using-python","text":"Install the required package pip install kubernetes Confirm the installation using $ pip list | grep kuber kubernetes 24 .2.0 Imagine that we have below service and deployment code create_service.yaml apiVersion : v1 kind : Service metadata : name : demo-service-api spec : selector : app : demo-api ports : - protocol : \"TCP\" port : 80 targetPort : 80 type : LoadBalancer A sample deployment manifest create_deployment.yaml apiVersion : apps/v1 kind : Deployment metadata : name : demo-deploy-api spec : replicas : 1 selector : matchLabels : app : demo-api template : metadata : labels : app : demo-api spec : containers : - name : demo-api image : fast imagePullPolicy : Never ports : - containerPort : 80 code to create manifests using Python K8S API createApplication.py from os import path import yaml from kubernetes import client , config from kubernetes.client import V1DeleteOptions def main (): # Configs can be set in Configuration class directly or using helper # utility. If no argument provided, the config will be loaded from # default location. config . load_kube_config () k8s_apps_v1 = client . AppsV1Api () core_v1_api = client . CoreV1Api () create_service ( core_v1_api ) create_deployment ( k8s_apps_v1 ) del_opts = V1DeleteOptions () api_client = get_custom_objects_api_client () def create_deployment ( k8s_apps_v1 ): with open ( path . join ( path . dirname ( __file__ ), \"demo_deployment.yaml\" )) as f : dep = yaml . safe_load ( f ) resp = k8s_apps_v1 . create_namespaced_deployment ( body = dep , namespace = \"default\" ) print ( \"Deployment created. status=' %s '\" % resp . metadata . name ) def create_service ( core_v1_api ): with open ( path . join ( path . dirname ( __file__ ), \"demo_service.yaml\" )) as f : service = yaml . safe_load ( f ) resp = core_v1_api . create_namespaced_service ( body = service , namespace = \"default\" ) resp = core_v1_api . delete print ( \"Service status=' %s '\" % resp . metadata . name ) if __name__ == '__main__' : main () Create the resources using python createApplication . py","title":"Using Python"},{"location":"k8s/pod/","text":"Pod \u00b6 Static Pods \u00b6 It is possible to create Pods by writing a file to a certain directory watched by Kubelet. These are called static pods. Unlike DaemonSet, static Pods cannot be managed with kubectl or other Kubernetes API clients. Static Pods do not depend on the apiserver, making them useful in cluster bootstrapping cases. The kubelet agent is responsible to watch each static Pod and restart it if it crashes. Also, static Pods may be deprecated in the future. Note The static Pods running on a node are visible on the API server but cannot be controlled by the API Server. Edit an existing POD \u00b6 Warning Remember, you CANNOT edit specifications of an existing POD other than the below. spec.containers[*].image spec.initContainers[*].image spec.activeDeadlineSeconds spec.tolerations For example, you cannot edit the environment variables, service accounts, resource limits (all of which we will discuss later) of a running pod. But if you really want to, you have 2 options: Option A \u00b6 For the 1 st option, run the kubectl edit pod <pod name> command. This will open the pod specification in an editor ( vi editor ). Then edit the required properties. When you try to save it, you will be denied. This is because you are attempting to edit a field on the pod that is not editable. A copy of the file with your changes is saved in a temporary location as shown above. You can then delete the existing pod by running the command: kubectl delete pod webapp Then create a new pod with your changes using the temporary file kubectl create -f /tmp/kubectl-edit-ccvrq.yaml Option B \u00b6 The 2 nd option is to extract the pod definition in YAML format to a file using the command kubectl get pod webapp -o yaml > my-new-pod.yaml Then make the changes to the exported file using an editor (vi editor). Save the changes vi my-new-pod.yaml Then delete the existing pod kubectl delete pod webapp Then create a new pod with the edited file kubectl create -f my-new-pod.yaml Multi-container pods \u00b6 In a multi-container pod , each container is expected to run a process that stays alive as long as the POD's lifecycle. For example in the multi-container pod that has a web application and logging agent , both the containers are expected to stay alive at all times. The process running in the log agent container is expected to stay alive as long as the web application is running. If any of them fails, the POD restarts. But at times you may want to run a process that runs to completion in a container. For example a process that pulls a code or binary from a repository that will be used by the main web application. That is a task that will be run only one time when the pod is first created. Or a process that waits for an external service or database to be up before the actual application starts. That's where initContainers comes in. Init Container \u00b6 An initContainer is configured in a pod like all other containers, except that it is specified inside a initContainers section, like this: apiVersion : v1 kind : Pod metadata : name : amar-app-pod labels : app : amar-app spec : containers : - name : myapp-container image : busybox:1.28 command : [ 'sh' , '-c' , 'echo The app is running! && sleep 3600' ] initContainers : - name : init-myservice image : busybox command : [ 'sh' , '-c' , 'git clone <some-repository-that-will-be-used-by-application> ; done;' ] Init container must run to completion When a POD is first created the initContainer is run, and the process in the initContainer must run to a completion before the real container hosting the application starts. Multiple init containers \u00b6 You can configure multiple such initContainers as well, like how we did for multi-containers pod. In that case each init container is run one at a time in sequential order. If any of the initContainers fail to complete, Kubernetes restarts the Pod repeatedly until the Init Container succeeds. apiVersion : v1 kind : Pod metadata : name : amar-app-pod labels : app : amar-app spec : containers : - name : myapp-container image : busybox:1.28 command : [ 'sh' , '-c' , 'echo The app is running! && sleep 3600' ] initContainers : - name : init-myservice image : busybox:1.28 command : [ 'sh' , '-c' , 'until nslookup myservice; do echo waiting for myservice; sleep 2; done;' ] - name : init-mydb image : busybox:1.28 command : [ 'sh' , '-c' , 'until nslookup mydb; do echo waiting for mydb; sleep 2; done;' ]","title":"Pod"},{"location":"k8s/pod/#pod","text":"","title":"Pod"},{"location":"k8s/pod/#static-pods","text":"It is possible to create Pods by writing a file to a certain directory watched by Kubelet. These are called static pods. Unlike DaemonSet, static Pods cannot be managed with kubectl or other Kubernetes API clients. Static Pods do not depend on the apiserver, making them useful in cluster bootstrapping cases. The kubelet agent is responsible to watch each static Pod and restart it if it crashes. Also, static Pods may be deprecated in the future. Note The static Pods running on a node are visible on the API server but cannot be controlled by the API Server.","title":"Static Pods"},{"location":"k8s/pod/#edit-an-existing-pod","text":"Warning Remember, you CANNOT edit specifications of an existing POD other than the below. spec.containers[*].image spec.initContainers[*].image spec.activeDeadlineSeconds spec.tolerations For example, you cannot edit the environment variables, service accounts, resource limits (all of which we will discuss later) of a running pod. But if you really want to, you have 2 options:","title":"Edit an existing POD"},{"location":"k8s/pod/#option-a","text":"For the 1 st option, run the kubectl edit pod <pod name> command. This will open the pod specification in an editor ( vi editor ). Then edit the required properties. When you try to save it, you will be denied. This is because you are attempting to edit a field on the pod that is not editable. A copy of the file with your changes is saved in a temporary location as shown above. You can then delete the existing pod by running the command: kubectl delete pod webapp Then create a new pod with your changes using the temporary file kubectl create -f /tmp/kubectl-edit-ccvrq.yaml","title":"Option A"},{"location":"k8s/pod/#option-b","text":"The 2 nd option is to extract the pod definition in YAML format to a file using the command kubectl get pod webapp -o yaml > my-new-pod.yaml Then make the changes to the exported file using an editor (vi editor). Save the changes vi my-new-pod.yaml Then delete the existing pod kubectl delete pod webapp Then create a new pod with the edited file kubectl create -f my-new-pod.yaml","title":"Option B"},{"location":"k8s/pod/#multi-container-pods","text":"In a multi-container pod , each container is expected to run a process that stays alive as long as the POD's lifecycle. For example in the multi-container pod that has a web application and logging agent , both the containers are expected to stay alive at all times. The process running in the log agent container is expected to stay alive as long as the web application is running. If any of them fails, the POD restarts. But at times you may want to run a process that runs to completion in a container. For example a process that pulls a code or binary from a repository that will be used by the main web application. That is a task that will be run only one time when the pod is first created. Or a process that waits for an external service or database to be up before the actual application starts. That's where initContainers comes in.","title":"Multi-container pods"},{"location":"k8s/pod/#init-container","text":"An initContainer is configured in a pod like all other containers, except that it is specified inside a initContainers section, like this: apiVersion : v1 kind : Pod metadata : name : amar-app-pod labels : app : amar-app spec : containers : - name : myapp-container image : busybox:1.28 command : [ 'sh' , '-c' , 'echo The app is running! && sleep 3600' ] initContainers : - name : init-myservice image : busybox command : [ 'sh' , '-c' , 'git clone <some-repository-that-will-be-used-by-application> ; done;' ] Init container must run to completion When a POD is first created the initContainer is run, and the process in the initContainer must run to a completion before the real container hosting the application starts.","title":"Init Container"},{"location":"k8s/pod/#multiple-init-containers","text":"You can configure multiple such initContainers as well, like how we did for multi-containers pod. In that case each init container is run one at a time in sequential order. If any of the initContainers fail to complete, Kubernetes restarts the Pod repeatedly until the Init Container succeeds. apiVersion : v1 kind : Pod metadata : name : amar-app-pod labels : app : amar-app spec : containers : - name : myapp-container image : busybox:1.28 command : [ 'sh' , '-c' , 'echo The app is running! && sleep 3600' ] initContainers : - name : init-myservice image : busybox:1.28 command : [ 'sh' , '-c' , 'until nslookup myservice; do echo waiting for myservice; sleep 2; done;' ] - name : init-mydb image : busybox:1.28 command : [ 'sh' , '-c' , 'until nslookup mydb; do echo waiting for mydb; sleep 2; done;' ]","title":"Multiple init containers"},{"location":"k8s/rc/","text":"Replication Controller \u00b6 How RC works? If there are too many pods, the ReplicationController terminates the extra pods. If there are too few, the ReplicationController starts more pods. Unlike manually created pods, the pods maintained by a ReplicationController are automatically replaced if they fail, are deleted, or are terminated. A ReplicationController ensures that a specified number of pod replicas are running at any one time. In other words, a ReplicationController makes sure that a pod or a homogeneous set of pods is always up and available. Replication controller can span multiple nodes. Replication controller (RC) is the old tech while Replica Set (RS) is the new one. Exmaple RC apiVersion: v1 kind: ReplicationController metadata: # this metadata is for RC name: nginx labels: env: dev # label for RC spec: # spec for RC replicas: 3 # total replicas for RC. Defaults to 1 if not specified selector: app: nginx # A ReplicationController manages all the pods with labels that match this selector template: # This template is for pod, if new one has to be created. This is the required component metadata: # this metadata is for POD name: nginx labels: app: nginx # If specified, the .spec.template.metadata.labels must be equal to the .spec.selector, or it will be rejected by the API spec: # spec for pods containers: - name: nginx-by-amar image: nginx ports: - containerPort: 80 Deleting only a ReplicationController \u00b6 You can delete a ReplicationController without affecting any of its pods. Using kubectl, specify the --cascade=orphan option to kubectl delete.","title":"Replication Controller"},{"location":"k8s/rc/#replication-controller","text":"How RC works? If there are too many pods, the ReplicationController terminates the extra pods. If there are too few, the ReplicationController starts more pods. Unlike manually created pods, the pods maintained by a ReplicationController are automatically replaced if they fail, are deleted, or are terminated. A ReplicationController ensures that a specified number of pod replicas are running at any one time. In other words, a ReplicationController makes sure that a pod or a homogeneous set of pods is always up and available. Replication controller can span multiple nodes. Replication controller (RC) is the old tech while Replica Set (RS) is the new one. Exmaple RC apiVersion: v1 kind: ReplicationController metadata: # this metadata is for RC name: nginx labels: env: dev # label for RC spec: # spec for RC replicas: 3 # total replicas for RC. Defaults to 1 if not specified selector: app: nginx # A ReplicationController manages all the pods with labels that match this selector template: # This template is for pod, if new one has to be created. This is the required component metadata: # this metadata is for POD name: nginx labels: app: nginx # If specified, the .spec.template.metadata.labels must be equal to the .spec.selector, or it will be rejected by the API spec: # spec for pods containers: - name: nginx-by-amar image: nginx ports: - containerPort: 80","title":"Replication Controller"},{"location":"k8s/rc/#deleting-only-a-replicationcontroller","text":"You can delete a ReplicationController without affecting any of its pods. Using kubectl, specify the --cascade=orphan option to kubectl delete.","title":"Deleting only a ReplicationController"},{"location":"k8s/rs/","text":"Replica Set \u00b6 Replication controller (RC) is the old tech while Replica Set (RS) is the new one. A ReplicaSet's purpose is to maintain a stable set of replica Pods running at any given time. As such, it is often used to guarantee the availability of a specified number of identical Pods . A ReplicaSet then fulfills its purpose by creating and deleting Pods as needed to reach the desired number. When a ReplicaSet needs to create new Pods, it uses its Pod template. Using Deployment and RS together ReplicaSet ensures that a specified number of pod replicas are running at any given time. However, a Deployment is a higher-level concept that manages ReplicaSets and provides declarative updates to Pods along with a lot of other useful features. Imperative create \u00b6 k create -f RS-FILE.YAML # or use this one k apply -f RS-FILE.YAML A sample file is given below apiVersion: apps/v1 kind: ReplicaSet metadata: name: frontend labels: app: guestbook tier: frontend spec: # modify replicas according to your case replicas: 3 selector: matchLabels: tier: frontend template: metadata: labels: tier: frontend spec: containers: - name: php-redis image: gcr.io/google_samples/gb-frontend:v3 Scaling an RS \u00b6 # Get the values from the file k replace -f RS-FILE.YAML # the file will be used while replicas will be replaced by the number you specify k scale --replicas = 6 -f RS-FILE.YAML # this one is recommended if already existing k scale --replicas = 6 rs RS-NAME","title":"Replica Set"},{"location":"k8s/rs/#replica-set","text":"Replication controller (RC) is the old tech while Replica Set (RS) is the new one. A ReplicaSet's purpose is to maintain a stable set of replica Pods running at any given time. As such, it is often used to guarantee the availability of a specified number of identical Pods . A ReplicaSet then fulfills its purpose by creating and deleting Pods as needed to reach the desired number. When a ReplicaSet needs to create new Pods, it uses its Pod template. Using Deployment and RS together ReplicaSet ensures that a specified number of pod replicas are running at any given time. However, a Deployment is a higher-level concept that manages ReplicaSets and provides declarative updates to Pods along with a lot of other useful features.","title":"Replica Set"},{"location":"k8s/rs/#imperative-create","text":"k create -f RS-FILE.YAML # or use this one k apply -f RS-FILE.YAML A sample file is given below apiVersion: apps/v1 kind: ReplicaSet metadata: name: frontend labels: app: guestbook tier: frontend spec: # modify replicas according to your case replicas: 3 selector: matchLabels: tier: frontend template: metadata: labels: tier: frontend spec: containers: - name: php-redis image: gcr.io/google_samples/gb-frontend:v3","title":"Imperative create"},{"location":"k8s/rs/#scaling-an-rs","text":"# Get the values from the file k replace -f RS-FILE.YAML # the file will be used while replicas will be replaced by the number you specify k scale --replicas = 6 -f RS-FILE.YAML # this one is recommended if already existing k scale --replicas = 6 rs RS-NAME","title":"Scaling an RS"},{"location":"k8s/svc/","text":"Networking \u00b6 Service \u00b6 A Kubernetes Service is a resource you create to make a single, constant point of entry to a group of pods providing the same service. Each service has an IP address and port that never change while the service exists. Clients can open connections to that IP and port, and those connections are then routed to one of the pods backing that service. This way, clients of service don\u2019t need to know the location of individual pods providing the service, allowing those pods to be moved around the cluster at any time. If we do an exec to service IP, then we will be able to exec to one of the pods backed by it. Info List the pods with the kubectl get pods command and choose one as your target for the exec command (in the following example, I've chosen the kubia-7nog1 pod as the target). You'll also need to obtain the cluster IP of your service (using kubectl get svc ). When running the following commands yourself, be sure to replace the pod name and the service IP with your own: kubectl exec kubia-7nog1 -- curl -s http://10.111.249.153 Various service types are: NodePort ClusterIP Load Balancer ExternalName NodePort \u00b6 By creating a NodePort service, you make Kubernetes reserve a port on all its nodes (the same port number is used across all of them) and forward incoming connections to the pods that are part of the service. If you set the type field to NodePort, the Kubernetes control plane allocates a port from a range specified by --service-node-port-range flag (default: 30000-32767). Each node proxies that port (the same port number on every Node) into your Service. Every node in the cluster configures itself to listen on that assigned port and to forward traffic to one of the ready endpoints associated with that Service. You'll be able to contact the type: NodePort Service, from outside the cluster, by connecting to any node using the appropriate protocol (for example: TCP), and the appropriate port (as assigned to that Service). As shown above, how the same port is allocated to all the nodes in order to make the service accessible. Remember NodePort service can be accessed through the service's internal cluster IP and through any node's IP and the reserved node port Specifying the port isn\u2019t mandatory; Kubernetes will choose a random port if you omit it. example of NodePort apiVersion : v1 kind : Service metadata : name : my-service spec : type : NodePort selector : app.kubernetes.io/name : MyApp ports : # By default and for convenience, the `targetPort` is set to the same value as the `port` field. - port : 80 targetPort : 80 # Optional field # By default and for convenience, the Kubernetes control plane will allocate a port from a range (default: 30000-32767) nodePort : 30007 ClusterIP \u00b6 Tldr Creates internal IP addresses for communication within the AKS cluster . This is ideal for internal communication between components. This is the default service type To make the pod accessible from the outside, we need to expose it through a Service object. You\u2019ll create a special service of type LoadBalancer , because if you create a regular service (a ClusterIP service ), like the pod, it would also only be accessible from inside the cluster. By creating a LoadBalancer-type service, an external load balancer will be created and you can connect to the pod through the load balancer\u2019s public IP. The ClusterIP provides a load-balanced IP address. One or more pods that match a label selector can forward traffic to the IP address. The ClusterIP service must define one or more ports to listen on with target ports to forward TCP/UDP traffic to containers. The IP address that is used for the ClusterIP is not routable outside the cluster, like the pod IP address is LoadBalancer \u00b6 Kubernetes clusters running on cloud providers usually support the automatic provision of a load balancer from the cloud infrastructure. All you need to do is set the service\u2019s type to LoadBalancer instead of NodePort. The load balancer will have its own unique, publicly accessible IP address and will redirect all connections to your service. You can thus access your service through the load balancer\u2019s IP address A more detailed LB example is shown below. This works out of box on various cloud platfroms such as GCP and Azure etc. apiVersion : v1 kind : Service metadata : name : my-service spec : selector : app.kubernetes.io/name : MyApp ports : - protocol : TCP port : 80 targetPort : 9376 clusterIP : 10.0.171.239 type : LoadBalancer status : loadBalancer : ingress : - ip : 192.0.2.127 Warning Some cloud providers allow you to specify the loadBalancerIP. In those cases, the load-balancer is created with the user-specified loadBalancerIP. If the loadBalancerIP field is not specified, the loadBalancer is set up with an ephemeral IP address. If you specify a loadBalancerIP but your cloud provider does not support the feature, the load balancer IP field that you set is ignored. ExternalName \u00b6 Creates a specific DNS entry for easier application access. Network Plugins \u00b6 Kubenet \u00b6 The kubenet networking option is the default configuration for AKS cluster creation. With kubenet: Nodes receive an IP address from the Azure VNet subnet. Pods receive an IP address from a logically different address space than the nodes' Azure virtual network subnet. NAT is then configured so that the pods can reach resources on the Azure virtual network. The source IP address of the traffic is translated to the node's primary IP address. Why to use Kubenet Only the nodes receive a routable IP address . The pods use NAT to communicate with other resources outside the AKS cluster. This approach reduces the number of IP addresses you need to reserve in your network space for pods to use. CNI \u00b6 With Azure CNI, every pod gets an IP address from the subnet and can be accessed directly. These IP addresses must be planned in advance and unique across your network space. IP exhaustation Each node has a configuration parameter for the maximum number of pods it supports. The equivalent number of IP addresses per node are then reserved up front. This approach can lead to IP address exhaustion or the need to rebuild clusters in a larger subnet as your application demands grow, so it's important to plan properly.","title":"Service"},{"location":"k8s/svc/#networking","text":"","title":"Networking"},{"location":"k8s/svc/#service","text":"A Kubernetes Service is a resource you create to make a single, constant point of entry to a group of pods providing the same service. Each service has an IP address and port that never change while the service exists. Clients can open connections to that IP and port, and those connections are then routed to one of the pods backing that service. This way, clients of service don\u2019t need to know the location of individual pods providing the service, allowing those pods to be moved around the cluster at any time. If we do an exec to service IP, then we will be able to exec to one of the pods backed by it. Info List the pods with the kubectl get pods command and choose one as your target for the exec command (in the following example, I've chosen the kubia-7nog1 pod as the target). You'll also need to obtain the cluster IP of your service (using kubectl get svc ). When running the following commands yourself, be sure to replace the pod name and the service IP with your own: kubectl exec kubia-7nog1 -- curl -s http://10.111.249.153 Various service types are: NodePort ClusterIP Load Balancer ExternalName","title":"Service"},{"location":"k8s/svc/#nodeport","text":"By creating a NodePort service, you make Kubernetes reserve a port on all its nodes (the same port number is used across all of them) and forward incoming connections to the pods that are part of the service. If you set the type field to NodePort, the Kubernetes control plane allocates a port from a range specified by --service-node-port-range flag (default: 30000-32767). Each node proxies that port (the same port number on every Node) into your Service. Every node in the cluster configures itself to listen on that assigned port and to forward traffic to one of the ready endpoints associated with that Service. You'll be able to contact the type: NodePort Service, from outside the cluster, by connecting to any node using the appropriate protocol (for example: TCP), and the appropriate port (as assigned to that Service). As shown above, how the same port is allocated to all the nodes in order to make the service accessible. Remember NodePort service can be accessed through the service's internal cluster IP and through any node's IP and the reserved node port Specifying the port isn\u2019t mandatory; Kubernetes will choose a random port if you omit it. example of NodePort apiVersion : v1 kind : Service metadata : name : my-service spec : type : NodePort selector : app.kubernetes.io/name : MyApp ports : # By default and for convenience, the `targetPort` is set to the same value as the `port` field. - port : 80 targetPort : 80 # Optional field # By default and for convenience, the Kubernetes control plane will allocate a port from a range (default: 30000-32767) nodePort : 30007","title":"NodePort"},{"location":"k8s/svc/#clusterip","text":"Tldr Creates internal IP addresses for communication within the AKS cluster . This is ideal for internal communication between components. This is the default service type To make the pod accessible from the outside, we need to expose it through a Service object. You\u2019ll create a special service of type LoadBalancer , because if you create a regular service (a ClusterIP service ), like the pod, it would also only be accessible from inside the cluster. By creating a LoadBalancer-type service, an external load balancer will be created and you can connect to the pod through the load balancer\u2019s public IP. The ClusterIP provides a load-balanced IP address. One or more pods that match a label selector can forward traffic to the IP address. The ClusterIP service must define one or more ports to listen on with target ports to forward TCP/UDP traffic to containers. The IP address that is used for the ClusterIP is not routable outside the cluster, like the pod IP address is","title":"ClusterIP"},{"location":"k8s/svc/#loadbalancer","text":"Kubernetes clusters running on cloud providers usually support the automatic provision of a load balancer from the cloud infrastructure. All you need to do is set the service\u2019s type to LoadBalancer instead of NodePort. The load balancer will have its own unique, publicly accessible IP address and will redirect all connections to your service. You can thus access your service through the load balancer\u2019s IP address A more detailed LB example is shown below. This works out of box on various cloud platfroms such as GCP and Azure etc. apiVersion : v1 kind : Service metadata : name : my-service spec : selector : app.kubernetes.io/name : MyApp ports : - protocol : TCP port : 80 targetPort : 9376 clusterIP : 10.0.171.239 type : LoadBalancer status : loadBalancer : ingress : - ip : 192.0.2.127 Warning Some cloud providers allow you to specify the loadBalancerIP. In those cases, the load-balancer is created with the user-specified loadBalancerIP. If the loadBalancerIP field is not specified, the loadBalancer is set up with an ephemeral IP address. If you specify a loadBalancerIP but your cloud provider does not support the feature, the load balancer IP field that you set is ignored.","title":"LoadBalancer"},{"location":"k8s/svc/#externalname","text":"Creates a specific DNS entry for easier application access.","title":"ExternalName"},{"location":"k8s/svc/#network-plugins","text":"","title":"Network Plugins"},{"location":"k8s/svc/#kubenet","text":"The kubenet networking option is the default configuration for AKS cluster creation. With kubenet: Nodes receive an IP address from the Azure VNet subnet. Pods receive an IP address from a logically different address space than the nodes' Azure virtual network subnet. NAT is then configured so that the pods can reach resources on the Azure virtual network. The source IP address of the traffic is translated to the node's primary IP address. Why to use Kubenet Only the nodes receive a routable IP address . The pods use NAT to communicate with other resources outside the AKS cluster. This approach reduces the number of IP addresses you need to reserve in your network space for pods to use.","title":"Kubenet"},{"location":"k8s/svc/#cni","text":"With Azure CNI, every pod gets an IP address from the subnet and can be accessed directly. These IP addresses must be planned in advance and unique across your network space. IP exhaustation Each node has a configuration parameter for the maximum number of pods it supports. The equivalent number of IP addresses per node are then reserved up front. This approach can lead to IP address exhaustion or the need to rebuild clusters in a larger subnet as your application demands grow, so it's important to plan properly.","title":"CNI"},{"location":"k8s/admin/monitoring/","text":"Monitoring \u00b6 There are a number of open-source solutions available today, such as Metrics server Prometheus Elastic stack Proprietary solutions are: DataDog Dynatrace Heapster vs Metrics Server \u00b6 Heapster is the one of the original projects that enabled monitoring and analysis feature for Kubernetes. Remember Heapster is now deprecated and a slimmed down version was formed known as the Metric Server. You can have one metrics server per Kubernetes cluster. The metric server retrieves metrics from each of the Kubernetes nodes and PODs, aggregates them and stores them in memory. You will see a lot of reference online when you look for reference architecture on monitoring Kubernetes. Can I use metrics server for historical data? Note that the metric server is only an in-memory monitoring solution and doesn\u2019t store the metrics on the disk and as a result you cannot see historical performance data . Logging \u00b6 # see logs for a pod k logs <podname> # see logs for a container kubectl logs <podname> -c <container_name> # tail the logs k logs -f <podname> Check out How log rotation is handled?","title":"Monitoring"},{"location":"k8s/admin/monitoring/#monitoring","text":"There are a number of open-source solutions available today, such as Metrics server Prometheus Elastic stack Proprietary solutions are: DataDog Dynatrace","title":"Monitoring"},{"location":"k8s/admin/monitoring/#heapster-vs-metrics-server","text":"Heapster is the one of the original projects that enabled monitoring and analysis feature for Kubernetes. Remember Heapster is now deprecated and a slimmed down version was formed known as the Metric Server. You can have one metrics server per Kubernetes cluster. The metric server retrieves metrics from each of the Kubernetes nodes and PODs, aggregates them and stores them in memory. You will see a lot of reference online when you look for reference architecture on monitoring Kubernetes. Can I use metrics server for historical data? Note that the metric server is only an in-memory monitoring solution and doesn\u2019t store the metrics on the disk and as a result you cannot see historical performance data .","title":"Heapster vs Metrics Server"},{"location":"k8s/admin/monitoring/#logging","text":"# see logs for a pod k logs <podname> # see logs for a container kubectl logs <podname> -c <container_name> # tail the logs k logs -f <podname> Check out How log rotation is handled?","title":"Logging"},{"location":"k8s/admin/scheduler/","text":"Scheduler \u00b6 If no scheduler is defined, then we can manually specify the node mame as shown below","title":"Scheduler"},{"location":"k8s/admin/scheduler/#scheduler","text":"If no scheduler is defined, then we can manually specify the node mame as shown below","title":"Scheduler"},{"location":"k8s/admin/security/","text":"K8S Security \u00b6 AuthT \u00b6 we define who can access. For machines, we can create Service Accounts K8s does not create/manage user accounts but it does for service accounts AuthN \u00b6 Using RBAC Various Authorization methods are AlwaysAllow \u00b6 Allows all calls AlwaysDeny \u00b6 Deny all calls ABAC \u00b6 Attribute-based access control (ABAC) defines an access control paradigm whereby access rights are granted to users through the use of policies which combine attributes together. RBAC \u00b6 Role-based access control (RBAC) is a method of regulating access to computer or network resources based on the roles of individual users within your organization. The RBAC API declares four kinds of Kubernetes object: Role ClusterRole RoleBinding ClusterRoleBinding Role is Namespaced but not ClusterRole A Role always sets permissions within a particular namespace; when you create a Role, you have to specify the namespace it belongs in. ClusterRole, by contrast, is a non-namespaced resource. The resources have different names (Role and ClusterRole) because a Kubernetes object always has to be either namespaced or not namespaced; it can't be both. Role \u00b6 Role in the default namespace that can be used to grant read access to pods apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : namespace : default name : pod-reader rules : - apiGroups : [ \"\" ] # \"\" indicates the core API group resources : [ \"pods\" ] verbs : [ \"get\" , \"watch\" , \"list\" ] Rolebinding \u00b6 allows 'amar' to read pods in the default namespace. apiVersion : rbac.authorization.k8s.io/v1 # This role binding allows \"amar\" to read pods in the \"default\" namespace. # You need to already have a Role named \"pod-reader\" in that namespace. kind : RoleBinding metadata : name : read-pods namespace : default subjects : # You can specify more than one \"subject\" - kind : User name : amar # \"name\" is case-sensitive apiGroup : rbac.authorization.k8s.io roleRef : # \"roleRef\" specifies the binding to a Role / ClusterRole kind : Role #this must be Role or ClusterRole name : pod-reader # this must match the name of the Role or ClusterRole you wish to bind to apiGroup : rbac.authorization.k8s.io ClusterRole \u00b6 ClusterRolebinding \u00b6 Node Authorizer \u00b6 The requests from kubelet are handled by Node Authorizer. Node authorization is a special-purpose authorization mode that specifically authorizes API requests made by kubelets. Note They are used for access within the cluster Webhook \u00b6 Used to manage it externally using Open Policy Agent for example TLS certificates \u00b6 An SSL/TLS certificate is a digital object that allows systems to verify the identity & subsequently establish an encrypted network connection to another system using the Secure Sockets Layer/Transport Layer Security (SSL/TLS) protocol. Certificates are used within a cryptographic system known as a Public Key Infrastructure (PKI) . PKI provides a way for one party to establish the identity of another party using certificates if they both trust a third-party - known as a certificate authority. The format for public and private keys are shown below: Here are the various kinds of certs involved - client certificate - Server certificate - Root certificate (CA) Various keys used by server in K8s are Put it all together View/Approve CSR's \u00b6 # View CSR's using k get csr # See more details for CSR k get csr <amar-dhillon> -o yaml # Approve the CSR using k certificate approve <certificate-name> # Deny CSR using kubectl certificate deny <certificate-name> # Delete CSR using kubectl delete csr <certificate-name> Delete CSR controlplane ~ \u279c k delete csr agent-smith certificatesigningrequest.certificates.k8s.io \"agent-smith\" deleted View Certificates in K8s \u00b6 run the below command cat /etc/kubernetes/manifests/kube-apiserver.yaml # look for the below `--tls-cert-file` line Below is the example to see cert file used by Kube API Server controlplane ~ \u279c cat /etc/kubernetes/manifests/kube-apiserver.yaml | grep -i tls - --tls-cert-file = /etc/kubernetes/pki/apiserver.crt - --tls-private-key-file = /etc/kubernetes/pki/apiserver.key Identify the Certificate file used to authenticate kube-apiserver as a client to ETCD Server. controlplane ~ \u279c cat /etc/kubernetes/manifests/kube-apiserver.yaml | grep -i etcd - --etcd-cafile = /etc/kubernetes/pki/etcd/ca.crt - --etcd-certfile = /etc/kubernetes/pki/apiserver-etcd-client.crt - --etcd-keyfile = /etc/kubernetes/pki/apiserver-etcd-client.key - --etcd-servers = https://127.0.0.1:2379 Identify the key used to authenticate kubeapi-server to the kubelet server. # Look for the certs in the api-server file controlplane ~ \u279c cat /etc/kubernetes/manifests/kube-apiserver.yaml | grep -i kubelet - --kubelet-client-certificate = /etc/kubernetes/pki/apiserver-kubelet-client.crt - --kubelet-client-key = /etc/kubernetes/pki/apiserver-kubelet-client.key - --kubelet-preferred-address-types = InternalIP,ExternalIP,Hostname Identify the ETCD Server Certificate used to host ETCD server. # Look for the certs in the etcd `cert-file` controlplane ~ \u279c cat /etc/kubernetes/manifests/etcd.yaml | grep cert - --cert-file = /etc/kubernetes/pki/etcd/server.crt - --client-cert-auth = true - --peer-cert-file = /etc/kubernetes/pki/etcd/peer.crt - --peer-client-cert-auth = true name: etcd-certs name: etcd-certs Warning ETCD can have its own CA. So this may be a different CA certificate than the one used by kube-api server. Identify the ETCD Server CA Root Certificate used to serve ETCD Server. # Cert file is shown below as `peer-trusted-ca-file` property controlplane ~ \u279c cat /etc/kubernetes/manifests/etcd.yaml | grep ca - --peer-trusted-ca-file = /etc/kubernetes/pki/etcd/ca.crt - --trusted-ca-file = /etc/kubernetes/pki/etcd/ca.crt priorityClassName: system-node-critical check for issuer of certificate # check for issuer in this command openssl x509 -in /etc/kubernetes/pki/apiserver.crt -text Which of the below alternate names is not configured on the Kube API Server Certificate? # Look at Alternative Names. openssl x509 -in /etc/kubernetes/pki/apiserver.crt -text What is the Common Name (CN) configured on the ETCD Server certificate? # check for Subject CN here openssl x509 -in /etc/kubernetes/pki/etcd/server.crt --text How long, from the issued date, is the Kube-API Server Certificate valid for? # Check expiry date in this file openssl x509 -in /etc/kubernetes/pki/apiserver.crt --text Check Kube API Server logs \u00b6 Run crictl ps -a command to identify the kube-api server container. Run crictl logs container-id command to view the logs. Kubeconfig \u00b6 A Kubeconfig is a YAML file with all the Kubernetes cluster details, certificates, and secret token to authenticate the cluster. You might get this config file directly from the cluster administrator or from a cloud platform if you are using managed Kubernetes cluster. Default location for kubeconfig is ~/.kube/config See current context kubectl config --kubeconfig = /root/my-kube-config current-context update context \u00b6 kubectl config --kubeconfig = /root/my-kube-config use-context research # if using the default context file, then kubectl config use-context <amar-dev-cluster> Using the Kubeconfig File With Kubectl \u00b6 You can pass the Kubeconfig file with the Kubectl command to override the current context and KUBECONFIG env variable. kubectl get nodes --kubeconfig = $HOME /.kube/dev_cluster_config","title":"Security"},{"location":"k8s/admin/security/#k8s-security","text":"","title":"K8S Security"},{"location":"k8s/admin/security/#autht","text":"we define who can access. For machines, we can create Service Accounts K8s does not create/manage user accounts but it does for service accounts","title":"AuthT"},{"location":"k8s/admin/security/#authn","text":"Using RBAC Various Authorization methods are","title":"AuthN"},{"location":"k8s/admin/security/#alwaysallow","text":"Allows all calls","title":"AlwaysAllow"},{"location":"k8s/admin/security/#alwaysdeny","text":"Deny all calls","title":"AlwaysDeny"},{"location":"k8s/admin/security/#abac","text":"Attribute-based access control (ABAC) defines an access control paradigm whereby access rights are granted to users through the use of policies which combine attributes together.","title":"ABAC"},{"location":"k8s/admin/security/#rbac","text":"Role-based access control (RBAC) is a method of regulating access to computer or network resources based on the roles of individual users within your organization. The RBAC API declares four kinds of Kubernetes object: Role ClusterRole RoleBinding ClusterRoleBinding Role is Namespaced but not ClusterRole A Role always sets permissions within a particular namespace; when you create a Role, you have to specify the namespace it belongs in. ClusterRole, by contrast, is a non-namespaced resource. The resources have different names (Role and ClusterRole) because a Kubernetes object always has to be either namespaced or not namespaced; it can't be both.","title":"RBAC"},{"location":"k8s/admin/security/#role","text":"Role in the default namespace that can be used to grant read access to pods apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : namespace : default name : pod-reader rules : - apiGroups : [ \"\" ] # \"\" indicates the core API group resources : [ \"pods\" ] verbs : [ \"get\" , \"watch\" , \"list\" ]","title":"Role"},{"location":"k8s/admin/security/#rolebinding","text":"allows 'amar' to read pods in the default namespace. apiVersion : rbac.authorization.k8s.io/v1 # This role binding allows \"amar\" to read pods in the \"default\" namespace. # You need to already have a Role named \"pod-reader\" in that namespace. kind : RoleBinding metadata : name : read-pods namespace : default subjects : # You can specify more than one \"subject\" - kind : User name : amar # \"name\" is case-sensitive apiGroup : rbac.authorization.k8s.io roleRef : # \"roleRef\" specifies the binding to a Role / ClusterRole kind : Role #this must be Role or ClusterRole name : pod-reader # this must match the name of the Role or ClusterRole you wish to bind to apiGroup : rbac.authorization.k8s.io","title":"Rolebinding"},{"location":"k8s/admin/security/#clusterrole","text":"","title":"ClusterRole"},{"location":"k8s/admin/security/#clusterrolebinding","text":"","title":"ClusterRolebinding"},{"location":"k8s/admin/security/#node-authorizer","text":"The requests from kubelet are handled by Node Authorizer. Node authorization is a special-purpose authorization mode that specifically authorizes API requests made by kubelets. Note They are used for access within the cluster","title":"Node Authorizer"},{"location":"k8s/admin/security/#webhook","text":"Used to manage it externally using Open Policy Agent for example","title":"Webhook"},{"location":"k8s/admin/security/#tls-certificates","text":"An SSL/TLS certificate is a digital object that allows systems to verify the identity & subsequently establish an encrypted network connection to another system using the Secure Sockets Layer/Transport Layer Security (SSL/TLS) protocol. Certificates are used within a cryptographic system known as a Public Key Infrastructure (PKI) . PKI provides a way for one party to establish the identity of another party using certificates if they both trust a third-party - known as a certificate authority. The format for public and private keys are shown below: Here are the various kinds of certs involved - client certificate - Server certificate - Root certificate (CA) Various keys used by server in K8s are Put it all together","title":"TLS certificates"},{"location":"k8s/admin/security/#viewapprove-csrs","text":"# View CSR's using k get csr # See more details for CSR k get csr <amar-dhillon> -o yaml # Approve the CSR using k certificate approve <certificate-name> # Deny CSR using kubectl certificate deny <certificate-name> # Delete CSR using kubectl delete csr <certificate-name> Delete CSR controlplane ~ \u279c k delete csr agent-smith certificatesigningrequest.certificates.k8s.io \"agent-smith\" deleted","title":"View/Approve CSR's"},{"location":"k8s/admin/security/#view-certificates-in-k8s","text":"run the below command cat /etc/kubernetes/manifests/kube-apiserver.yaml # look for the below `--tls-cert-file` line Below is the example to see cert file used by Kube API Server controlplane ~ \u279c cat /etc/kubernetes/manifests/kube-apiserver.yaml | grep -i tls - --tls-cert-file = /etc/kubernetes/pki/apiserver.crt - --tls-private-key-file = /etc/kubernetes/pki/apiserver.key Identify the Certificate file used to authenticate kube-apiserver as a client to ETCD Server. controlplane ~ \u279c cat /etc/kubernetes/manifests/kube-apiserver.yaml | grep -i etcd - --etcd-cafile = /etc/kubernetes/pki/etcd/ca.crt - --etcd-certfile = /etc/kubernetes/pki/apiserver-etcd-client.crt - --etcd-keyfile = /etc/kubernetes/pki/apiserver-etcd-client.key - --etcd-servers = https://127.0.0.1:2379 Identify the key used to authenticate kubeapi-server to the kubelet server. # Look for the certs in the api-server file controlplane ~ \u279c cat /etc/kubernetes/manifests/kube-apiserver.yaml | grep -i kubelet - --kubelet-client-certificate = /etc/kubernetes/pki/apiserver-kubelet-client.crt - --kubelet-client-key = /etc/kubernetes/pki/apiserver-kubelet-client.key - --kubelet-preferred-address-types = InternalIP,ExternalIP,Hostname Identify the ETCD Server Certificate used to host ETCD server. # Look for the certs in the etcd `cert-file` controlplane ~ \u279c cat /etc/kubernetes/manifests/etcd.yaml | grep cert - --cert-file = /etc/kubernetes/pki/etcd/server.crt - --client-cert-auth = true - --peer-cert-file = /etc/kubernetes/pki/etcd/peer.crt - --peer-client-cert-auth = true name: etcd-certs name: etcd-certs Warning ETCD can have its own CA. So this may be a different CA certificate than the one used by kube-api server. Identify the ETCD Server CA Root Certificate used to serve ETCD Server. # Cert file is shown below as `peer-trusted-ca-file` property controlplane ~ \u279c cat /etc/kubernetes/manifests/etcd.yaml | grep ca - --peer-trusted-ca-file = /etc/kubernetes/pki/etcd/ca.crt - --trusted-ca-file = /etc/kubernetes/pki/etcd/ca.crt priorityClassName: system-node-critical check for issuer of certificate # check for issuer in this command openssl x509 -in /etc/kubernetes/pki/apiserver.crt -text Which of the below alternate names is not configured on the Kube API Server Certificate? # Look at Alternative Names. openssl x509 -in /etc/kubernetes/pki/apiserver.crt -text What is the Common Name (CN) configured on the ETCD Server certificate? # check for Subject CN here openssl x509 -in /etc/kubernetes/pki/etcd/server.crt --text How long, from the issued date, is the Kube-API Server Certificate valid for? # Check expiry date in this file openssl x509 -in /etc/kubernetes/pki/apiserver.crt --text","title":"View Certificates in K8s"},{"location":"k8s/admin/security/#check-kube-api-server-logs","text":"Run crictl ps -a command to identify the kube-api server container. Run crictl logs container-id command to view the logs.","title":"Check Kube API Server logs"},{"location":"k8s/admin/security/#kubeconfig","text":"A Kubeconfig is a YAML file with all the Kubernetes cluster details, certificates, and secret token to authenticate the cluster. You might get this config file directly from the cluster administrator or from a cloud platform if you are using managed Kubernetes cluster. Default location for kubeconfig is ~/.kube/config See current context kubectl config --kubeconfig = /root/my-kube-config current-context","title":"Kubeconfig"},{"location":"k8s/admin/security/#update-context","text":"kubectl config --kubeconfig = /root/my-kube-config use-context research # if using the default context file, then kubectl config use-context <amar-dev-cluster>","title":"update context"},{"location":"k8s/admin/security/#using-the-kubeconfig-file-with-kubectl","text":"You can pass the Kubeconfig file with the Kubectl command to override the current context and KUBECONFIG env variable. kubectl get nodes --kubeconfig = $HOME /.kube/dev_cluster_config","title":"Using the Kubeconfig File With Kubectl"},{"location":"k8s/arch/apiserver/","text":"API Server \u00b6 Main uses are - Authenticate a user - Validate a request - Retrive data - Update ETCD (only component to do that): The list of ETCD servers is updated in the API server - Communicate with Scheduler - Commnunicate with Kubelet View API sever options \u00b6 cat /etc/systemd/system/kube-apisever.service See the process ps aux | grep kube-apiservice","title":"API Server \ud83d\udcbb"},{"location":"k8s/arch/apiserver/#api-server","text":"Main uses are - Authenticate a user - Validate a request - Retrive data - Update ETCD (only component to do that): The list of ETCD servers is updated in the API server - Communicate with Scheduler - Commnunicate with Kubelet","title":"API Server"},{"location":"k8s/arch/apiserver/#view-api-sever-options","text":"cat /etc/systemd/system/kube-apisever.service See the process ps aux | grep kube-apiservice","title":"View API sever options"},{"location":"k8s/arch/arch/","text":"K8S Architecture \u00b6 K8S on high level A Kubernetes cluster consists of a master, which manages the cluster, and nodes, which run the services. Developers and the deployment pipeline interact with Kubernetes through the API server, which along with other cluster-management software runs on the master. Application containers run on nodes. Each node runs a Kubelet, which manages the application container kube-proxy, which routes application requests to the pods, either directly as a proxy or indirectly by configuring iptables routing rules built into the Linux kernel. Master Node \u00b6 The master node consists of four components: The API server - When you run commands on kubectl , this is what it communicated with to perform operations. The API server exposes an API for both external users and other components within the cluster. The scheduler \u2014 This is responsible for selecting an appropriate node where a pod will run, given priority, resource needs, and other constraints. The controller manager \u2014 This is responsible for executing control loops : the continual observe-diff-act operation that underpins the operation of Kubernetes. A distributed key-value data store, etcd \u2014 This stores the underlying state of the cluster and thereby makes sure it persists when nodes fail or restarts are required. Worker node \u00b6 Each worker node uses the following components to run and monitor applications: A container runtime \u2014 This can be Docker. The kubelet \u2014 This interacts with the Kubernetes master to start, stop, and monitor containers on the node. The kube-proxy \u2014 It is managing the virtual networking on each node. The proxy handles network routes and IP addressing for services and pods.","title":"Cluster Architecutre"},{"location":"k8s/arch/arch/#k8s-architecture","text":"K8S on high level A Kubernetes cluster consists of a master, which manages the cluster, and nodes, which run the services. Developers and the deployment pipeline interact with Kubernetes through the API server, which along with other cluster-management software runs on the master. Application containers run on nodes. Each node runs a Kubelet, which manages the application container kube-proxy, which routes application requests to the pods, either directly as a proxy or indirectly by configuring iptables routing rules built into the Linux kernel.","title":"K8S Architecture"},{"location":"k8s/arch/arch/#master-node","text":"The master node consists of four components: The API server - When you run commands on kubectl , this is what it communicated with to perform operations. The API server exposes an API for both external users and other components within the cluster. The scheduler \u2014 This is responsible for selecting an appropriate node where a pod will run, given priority, resource needs, and other constraints. The controller manager \u2014 This is responsible for executing control loops : the continual observe-diff-act operation that underpins the operation of Kubernetes. A distributed key-value data store, etcd \u2014 This stores the underlying state of the cluster and thereby makes sure it persists when nodes fail or restarts are required.","title":"Master Node"},{"location":"k8s/arch/arch/#worker-node","text":"Each worker node uses the following components to run and monitor applications: A container runtime \u2014 This can be Docker. The kubelet \u2014 This interacts with the Kubernetes master to start, stop, and monitor containers on the node. The kube-proxy \u2014 It is managing the virtual networking on each node. The proxy handles network routes and IP addressing for services and pods.","title":"Worker node"},{"location":"k8s/arch/controllermgr/","text":"Controller Manager \u00b6 It manages various controllers They do watch the status remediate the situation Various controllers \u00b6 Info Various controllers are Node controller Replication Controller Namespace controller PV controller RS controller Deployment controller Endpoint controller","title":"Controller Manager \ud83d\udc6e\u200d\u2640\ufe0f"},{"location":"k8s/arch/controllermgr/#controller-manager","text":"It manages various controllers They do watch the status remediate the situation","title":"Controller Manager"},{"location":"k8s/arch/controllermgr/#various-controllers","text":"Info Various controllers are Node controller Replication Controller Namespace controller PV controller RS controller Deployment controller Endpoint controller","title":"Various controllers"},{"location":"k8s/arch/docker/","text":"Docker file \u00b6 A Dockerfile is a plain text file containing all the commands needed to build an image. Dockerfiles are written in a minimal scripting language designed for building and configuring images. They document the operations required to build an image, starting with a base image. Sample Dockerfile FROM mcr.microsoft.com/dotnet/core/sdk:2.2 WORKDIR /app COPY myapp_code . RUN dotnet build -c Release -o /rel EXPOSE 80 WORKDIR /rel ENTRYPOINT [ \"dotnet\" , \"myapp.dll\" ] By convention, applications meant to be packaged as Docker images typically have a Dockerfile located in the root of their source code, and it's almost always named Dockerfile . The docker build command creates a new image by running a Dockerfile. The syntax or this command has several parameters: The f flag indicates the name of the Dockerfile to use. The t flag specifies the name of the image to be created, in this example, myapp:v1 . The final parameter, . , provides the build context for the source files for the COPY command: the set of files on the host computer needed during the build process. docker build -t myapp:v1 . Behind the scenes, the docker build command creates a container, runs commands in it, then commits the changes to a new image Steps \u00b6 Create a Dockerfile for a new container image based on a starter image from Docker Hub Add files to an image using Dockerfile commands Configure an image's startup command with Dockerfile commands Build and run a web application packaged in a Docker image Deploy a Docker image using the Azure Container Instance service","title":"Docker"},{"location":"k8s/arch/docker/#docker-file","text":"A Dockerfile is a plain text file containing all the commands needed to build an image. Dockerfiles are written in a minimal scripting language designed for building and configuring images. They document the operations required to build an image, starting with a base image. Sample Dockerfile FROM mcr.microsoft.com/dotnet/core/sdk:2.2 WORKDIR /app COPY myapp_code . RUN dotnet build -c Release -o /rel EXPOSE 80 WORKDIR /rel ENTRYPOINT [ \"dotnet\" , \"myapp.dll\" ] By convention, applications meant to be packaged as Docker images typically have a Dockerfile located in the root of their source code, and it's almost always named Dockerfile . The docker build command creates a new image by running a Dockerfile. The syntax or this command has several parameters: The f flag indicates the name of the Dockerfile to use. The t flag specifies the name of the image to be created, in this example, myapp:v1 . The final parameter, . , provides the build context for the source files for the COPY command: the set of files on the host computer needed during the build process. docker build -t myapp:v1 . Behind the scenes, the docker build command creates a container, runs commands in it, then commits the changes to a new image","title":"Docker file"},{"location":"k8s/arch/docker/#steps","text":"Create a Dockerfile for a new container image based on a starter image from Docker Hub Add files to an image using Dockerfile commands Configure an image's startup command with Dockerfile commands Build and run a web application packaged in a Docker image Deploy a Docker image using the Azure Container Instance service","title":"Steps"},{"location":"k8s/arch/etcd/","text":"ETCD \u00b6 It is a distributed key-value store Uses RAFT consensus protocol Only once change is written to ETCD, the change is considered as complete ETCD runs in an HA environment and in this case the service should be configured so that various ETCD servers knows about one another The API version used by etcdctl to speak to etcd may be set to version 2 or 3 via the ETCDCTL_API environment variable. By default, etcdctl on master (3.4) uses the v3 API and earlier versions (3.3 and earlier) default to the v2 API. set version export ETCDCTL_API = 3 Warning When API version is not set, it is assumed to be set to version 2. And version 3 commands listed above don't work. When API version is set to version 3, version 2 commands listed above don't work. Write key \u00b6 Applications store keys into the etcd cluster by writing to keys. Every stored key is replicated to all etcd cluster members through the Raft protocol to achieve consistency and reliability. set key $ ./etcdctl.exe put name amar OK Get the key \u00b6 get key $ ./etcdctl.exe get name name amar get keys in k8s \u00b6 k exec etcd-master -n kube-system etcdctl get / --prefix --keys-only","title":"ETCD"},{"location":"k8s/arch/etcd/#etcd","text":"It is a distributed key-value store Uses RAFT consensus protocol Only once change is written to ETCD, the change is considered as complete ETCD runs in an HA environment and in this case the service should be configured so that various ETCD servers knows about one another The API version used by etcdctl to speak to etcd may be set to version 2 or 3 via the ETCDCTL_API environment variable. By default, etcdctl on master (3.4) uses the v3 API and earlier versions (3.3 and earlier) default to the v2 API. set version export ETCDCTL_API = 3 Warning When API version is not set, it is assumed to be set to version 2. And version 3 commands listed above don't work. When API version is set to version 3, version 2 commands listed above don't work.","title":"ETCD"},{"location":"k8s/arch/etcd/#write-key","text":"Applications store keys into the etcd cluster by writing to keys. Every stored key is replicated to all etcd cluster members through the Raft protocol to achieve consistency and reliability. set key $ ./etcdctl.exe put name amar OK","title":"Write key"},{"location":"k8s/arch/etcd/#get-the-key","text":"get key $ ./etcdctl.exe get name name amar","title":"Get the key"},{"location":"k8s/arch/etcd/#get-keys-in-k8s","text":"k exec etcd-master -n kube-system etcdctl get / --prefix --keys-only","title":"get keys in k8s"},{"location":"k8s/arch/kubelet/","text":"Kubelet \u00b6 They are captain of the ship (worker node) It requrests the container runtime engine to do the job We need to install Kubelet on the worker nodes Rememmer The Kubelet monitors the state of worker node and reports back to API server. Check the process \u00b6 ps aux | grep kubelet","title":"Kubelet"},{"location":"k8s/arch/kubelet/#kubelet","text":"They are captain of the ship (worker node) It requrests the container runtime engine to do the job We need to install Kubelet on the worker nodes Rememmer The Kubelet monitors the state of worker node and reports back to API server.","title":"Kubelet"},{"location":"k8s/arch/kubelet/#check-the-process","text":"ps aux | grep kubelet","title":"Check the process"},{"location":"k8s/arch/kubeproxy/","text":"Kube Proxy \u00b6 The Kubernetes network proxy runs on each node. It is used to setup the pod network, which is an internal network Kube proxy create an iptable route for each service, so that the traffic to the service is directed to the right pod.","title":"Kube Proxy"},{"location":"k8s/arch/kubeproxy/#kube-proxy","text":"The Kubernetes network proxy runs on each node. It is used to setup the pod network, which is an internal network Kube proxy create an iptable route for each service, so that the traffic to the service is directed to the right pod.","title":"Kube Proxy"},{"location":"k8s/arch/scheduler/","text":"Scheduler \u00b6 It decides on which node to assign the pod The 2 step process for node selection is Filter nodes: Filter nodes which does not match the critera Rank nodes:","title":"Scheduler"},{"location":"k8s/arch/scheduler/#scheduler","text":"It decides on which node to assign the pod The 2 step process for node selection is Filter nodes: Filter nodes which does not match the critera Rank nodes:","title":"Scheduler"},{"location":"sys_design/concepts/consistentHashing/","text":"Consistent Hashing maps data to physical nodes and ensures that only a small set of keys move when servers are added or removed. Consistent Hashing stores the data managed by a distributed system in a ring. Each node in the ring is assigned a range of data. Server Token Range Start Range End Server 1 1 1 25 Server 2 26 26 50 Server 3 51 51 75 Server 4 76 76 100 The Consistent Hashing scheme described above works great when a node is added or removed from the ring, as in these cases, since only the next node is affected. For example, when a node is removed, the next node becomes responsible for all of the keys stored on the outgoing node. However, this scheme can result in non-uniform data and load distribution. This problem can be solved with the help of Virtual nodes.","title":"Consistent Hashing"},{"location":"sys_design/concepts/dataPartitioning/","text":"Data partitioning : It is the process of distributing data across a set of servers. It improves the scalability and performance of the system. Data replication : It is the process of making multiple copies of data and storing them on different servers. It improves the availability and durability of the data across the system Tip A carefully designed scheme for partitioning and replicating the data enhances the performance, availability, and reliability of the system and also defines how efficiently the system will be scaled and managed. A naive approach will use a suitable hash function to map the data key to a number. Then, find the server by applying modulo on this number and the total number of servers. Warning The scheme described in the above diagram solves the problem of finding a server for storing/retrieving the data. But when we add or remove a server, all our existing mappings will be broken. This is because the total number of servers will be changed, which was used to find the actual server storing the data. So to get things working again, we have to remap all the keys and move our data based on the new server count \ud83d\ude15","title":"Data Partitioning"},{"location":"sys_design/concepts/graphql/","text":"What is GraphQL? \u00b6 GraphQL is a popular data query language developed by Facebook. Issues with REST It was designed as an alternative to REST (Representational State Transfer) because of REST\u2019s perceived weaknesses such as: 1. Multiple round-trips 2. Over-fetching 3. Problems with versioning GraphQL attempts to solve these problems by providing a hierarchical, declarative way of performing queries from a single end point. why GraphQL is powerful? \u00b6 GraphQL gives power to the client. Instead of specifying the structure of the response on the server, it\u2019s defined on the client. The client can specify what properties and relationships to return. GraphQL aggregates data from multiple sources and returns it to the client in a single round trip, which makes it an efficient system for retrieving data.","title":"GraphQL"},{"location":"sys_design/concepts/graphql/#what-is-graphql","text":"GraphQL is a popular data query language developed by Facebook. Issues with REST It was designed as an alternative to REST (Representational State Transfer) because of REST\u2019s perceived weaknesses such as: 1. Multiple round-trips 2. Over-fetching 3. Problems with versioning GraphQL attempts to solve these problems by providing a hierarchical, declarative way of performing queries from a single end point.","title":"What is GraphQL?"},{"location":"sys_design/concepts/graphql/#why-graphql-is-powerful","text":"GraphQL gives power to the client. Instead of specifying the structure of the response on the server, it\u2019s defined on the client. The client can specify what properties and relationships to return. GraphQL aggregates data from multiple sources and returns it to the client in a single round trip, which makes it an efficient system for retrieving data.","title":"why GraphQL is powerful?"},{"location":"sys_design/concepts/jwt/","text":"JWT \u00b6 Structure \u00b6 JWTs consist of three parts separated by dots (.), which are: Header Payload Signature Therefore, a JWT typically looks like the following. JWT structure encoded_Header.encoded_Payload.encoded_Signature Structure is shown in the below example from jwt.io Header \u00b6 The header typically consists of two parts: the type of the token, which is JWT, and the hashing algorithm such as HMAC SHA256 or RSA . JWT Header example { 'alg' : 'HS 256 ' , ' t yp' : 'JWT' } Payload \u00b6 The second part of the token is the payload, which contains the claims. Claims are statements about an entity (typically, the user) and additional metadata. JWT Payload example (claims + metadata) { 'sub' : ' 1234567890 ' , ' na me' : 'Amarji t Si n gh' , 'admi n ' : true } There are three types of claims: Reserved claim Public claim Private claim Reserved claims \u00b6 These are a set of predefined claims, which are not mandatory but recommended, thought to provide a set of useful, interoperable claims. Some of them are: iss (issuer) , exp (expiration time) , sub (subject) , aud (audience) Notice that the claim names are only three characters long as JWT is meant to be compact. Public claims \u00b6 These can be defined at will by those using JWTs. But to avoid collisions they should be defined in the IANA JSON Web Token Registry or be defined as a URI that contains a collision resistant namespace. Private claims \u00b6 These are the custom claims created to share information between parties that agree on using them. Signature \u00b6 To create the signature part you have to take the encoded header (base64), the encoded payload (base64), a secret, the algorithm specified in the header, and sign that. For example if you want to use the HMAC SHA256 algorithm , the signature will be created in the following way. JWT Signature example HMACSHA 256 ( base 64 UrlE n code(header) + '.' + base 64 UrlE n code(payload) , secre t ) The signature is used to verify that the sender of the JWT is who it says it is and to ensure that the message was\u2019t changed in the way. JWT vs SAML \u00b6 Size : As JSON is less verbose than XML, when it is encoded its size is also smaller; making JWT more compact than SAML. This makes JWT a good choice to be passed in HTML and HTTP environments. Security : While JWT and SAML tokens can also use a public/private key pair in the form of a X.509 certificate to sign them. However, signing XML with XML Digital Signature without introducing obscure security holes is very difficult compared to the simplicity of signing JSON. Parsing : JSON parsers are common in most programming languages, because they map directly to objects, conversely XML doesn\u2019t have a natural document-to-object mapping. This makes it easier to work with JWT than SAML assertions. Bearer Schema/Format \u00b6 The Authorization: <type> <credentials> pattern was introduced by the W3C in HTTP 1.0, and has been reused in many places since. Many web servers support multiple methods of authorization. In those cases sending just the token isn't sufficient. Sites that use the Authorization : Bearer xxxxxx format are most likely implementing OAuth 2.0 bearer tokens . Sending GET Request with Bearer Token Authorization Header To send a GET request with a Bearer Token authorization header , you need to make an HTTP GET request and provide your Bearer Token with the Authorization: Bearer {token} HTTP header. Bearer Authentication (also called token authentication) is an HTTP authentication scheme created as part of OAuth 2.0 but is now used on its own. For security reasons, bearer tokens are only sent over HTTPS (SSL). GET /echo/get/json HTTP / 1.1 Host : amarjitdhillon.com Accept : application/json Authorization : Bearer {token}","title":"JWT"},{"location":"sys_design/concepts/jwt/#jwt","text":"","title":"JWT"},{"location":"sys_design/concepts/jwt/#structure","text":"JWTs consist of three parts separated by dots (.), which are: Header Payload Signature Therefore, a JWT typically looks like the following. JWT structure encoded_Header.encoded_Payload.encoded_Signature Structure is shown in the below example from jwt.io","title":"Structure"},{"location":"sys_design/concepts/jwt/#header","text":"The header typically consists of two parts: the type of the token, which is JWT, and the hashing algorithm such as HMAC SHA256 or RSA . JWT Header example { 'alg' : 'HS 256 ' , ' t yp' : 'JWT' }","title":"Header"},{"location":"sys_design/concepts/jwt/#payload","text":"The second part of the token is the payload, which contains the claims. Claims are statements about an entity (typically, the user) and additional metadata. JWT Payload example (claims + metadata) { 'sub' : ' 1234567890 ' , ' na me' : 'Amarji t Si n gh' , 'admi n ' : true } There are three types of claims: Reserved claim Public claim Private claim","title":"Payload"},{"location":"sys_design/concepts/jwt/#reserved-claims","text":"These are a set of predefined claims, which are not mandatory but recommended, thought to provide a set of useful, interoperable claims. Some of them are: iss (issuer) , exp (expiration time) , sub (subject) , aud (audience) Notice that the claim names are only three characters long as JWT is meant to be compact.","title":"Reserved claims"},{"location":"sys_design/concepts/jwt/#public-claims","text":"These can be defined at will by those using JWTs. But to avoid collisions they should be defined in the IANA JSON Web Token Registry or be defined as a URI that contains a collision resistant namespace.","title":"Public claims"},{"location":"sys_design/concepts/jwt/#private-claims","text":"These are the custom claims created to share information between parties that agree on using them.","title":"Private claims"},{"location":"sys_design/concepts/jwt/#signature","text":"To create the signature part you have to take the encoded header (base64), the encoded payload (base64), a secret, the algorithm specified in the header, and sign that. For example if you want to use the HMAC SHA256 algorithm , the signature will be created in the following way. JWT Signature example HMACSHA 256 ( base 64 UrlE n code(header) + '.' + base 64 UrlE n code(payload) , secre t ) The signature is used to verify that the sender of the JWT is who it says it is and to ensure that the message was\u2019t changed in the way.","title":"Signature"},{"location":"sys_design/concepts/jwt/#jwt-vs-saml","text":"Size : As JSON is less verbose than XML, when it is encoded its size is also smaller; making JWT more compact than SAML. This makes JWT a good choice to be passed in HTML and HTTP environments. Security : While JWT and SAML tokens can also use a public/private key pair in the form of a X.509 certificate to sign them. However, signing XML with XML Digital Signature without introducing obscure security holes is very difficult compared to the simplicity of signing JSON. Parsing : JSON parsers are common in most programming languages, because they map directly to objects, conversely XML doesn\u2019t have a natural document-to-object mapping. This makes it easier to work with JWT than SAML assertions.","title":"JWT vs SAML"},{"location":"sys_design/concepts/jwt/#bearer-schemaformat","text":"The Authorization: <type> <credentials> pattern was introduced by the W3C in HTTP 1.0, and has been reused in many places since. Many web servers support multiple methods of authorization. In those cases sending just the token isn't sufficient. Sites that use the Authorization : Bearer xxxxxx format are most likely implementing OAuth 2.0 bearer tokens . Sending GET Request with Bearer Token Authorization Header To send a GET request with a Bearer Token authorization header , you need to make an HTTP GET request and provide your Bearer Token with the Authorization: Bearer {token} HTTP header. Bearer Authentication (also called token authentication) is an HTTP authentication scheme created as part of OAuth 2.0 but is now used on its own. For security reasons, bearer tokens are only sent over HTTPS (SSL). GET /echo/get/json HTTP / 1.1 Host : amarjitdhillon.com Accept : application/json Authorization : Bearer {token}","title":"Bearer Schema/Format"},{"location":"sys_design/concepts/microservices/","text":"What are microservices? \u00b6","title":"Micro Services"},{"location":"sys_design/concepts/microservices/#what-are-microservices","text":"","title":"What are microservices?"},{"location":"sys_design/concepts/protobuf/","text":"Protobuf \u00b6 Protobuf (Protocol Buffer) is a data serializing protocol like a JSON or XML. But unlike them, the protobuf is not for humans, serialized data is compiled bytes and hard for the human reading. Why is it not so popular yet? Because protobuf is not so as simple as Google says. Protobuf must preprocess from .proto files to the sources of your programming language. Unfortunately, for some platforms, the protoc generator produces a very impractical code and it\u2019s too hard to debug it. Also protobuf is hard to develop when services are so many and we have more than one team in development, because it\u2019s not a human reading standard.","title":"Protobuf"},{"location":"sys_design/concepts/protobuf/#protobuf","text":"Protobuf (Protocol Buffer) is a data serializing protocol like a JSON or XML. But unlike them, the protobuf is not for humans, serialized data is compiled bytes and hard for the human reading. Why is it not so popular yet? Because protobuf is not so as simple as Google says. Protobuf must preprocess from .proto files to the sources of your programming language. Unfortunately, for some platforms, the protoc generator produces a very impractical code and it\u2019s too hard to debug it. Also protobuf is hard to develop when services are so many and we have more than one team in development, because it\u2019s not a human reading standard.","title":"Protobuf"},{"location":"sys_design/concepts/webhook/","text":"Webhook \u00b6 A webhook can be thought of as a type of API that is driven by events rather than requests. Webhooks are a way for one application to provide other applications with real-time information . They allow one application to send a notification to another application when a certain event occurs rather than constantly polling for new data. This can help save on server resources and costs. Instead of one application making a request to another to receive a response, a webhook is a service that allows one program to send data to another as soon as a particular event takes place. Webhooks are sometimes referred to as push APIs or reverse APIs because instead of pulling data from one system to another, they push the data to update it in real-time. In either case, the webhook's meaning is the same; it enables you to share data. Webhooks work by making an HTTP request from one application to another. With web services becoming increasingly interconnected, webhooks are seeing more action as a lightweight solution for enabling real-time notifications and data updates without the need to develop a full-scale API. Example Say for instance you want to receive Slack notifications when tweets that mention a certain account and contain a specific hashtag are published. Instead of Slack continuously asking Twitter for new posts meeting these criteria, it makes much more sense for Twitter to send a notification to Slack only when this event takes place. This is the purpose of a webhook\u2013\u2013instead of having to repeatedly request the data, the receiving application can sit back and get what it needs without having to send repeated requests to another system. Webhook VS API's \u00b6 Webhooks are different from APIs, which allow for communication between different applications but work in a different way. An API is a set of protocols and routines for building and interacting with software applications, whereas a webhook is a way for one application to notify another application when a specific event occurs. In other words, an API allows you to retrieve data, while a webhook allows data to be pushed to you. This means that instead of having to poll for new data, you can receive data in real time through webhooks. Setup time for Webhook vs APIs \u00b6 One might think that since webhooks are real-time events that they are technically difficult to implement. Actually, a key advantage of webhooks is that they are easier to set up and less resource-intensive than APIs. Creating an API is a complex process that in some cases can be as challenging as designing and building an application itself, but implementing a webhook simply requires setting up a single POST request on the sending end, establishing a URL on the receiving end to accept the data, then performing some action on the data once it is received. Why webhook is limited? \u00b6 Also, unlike APIs, webhooks do not allow the sending system to add, update and delete data on the receiving end, which is why webhooks alone are too limited to offer full integration between two applications. limitations of using webhooks? \u00b6 In the case of webhooks vs. APIs, you might wonder why anyone would still use an API since they're less efficient and productive. A few limitations of webhooks include the following: Not always supported \u00b6 Unfortunately, not all applications support webhooks . However, several types of third-party app providers can help you send webhooks by connecting apps that don't have integrations and allowing them to pass data. Less functionality than APIs \u00b6 Webhooks only allow for data to be received from one application for another. Therefore, they can't be used for complicated integrations that require bi-directional communication. Potential for lost data \u00b6 With webhooks, you won't be alerted if an application or server is down for tails to send data. Since you'll only receive data when events occur, you won't receive any information if the other system is down. However, with APIs, you'll receive an error response alerting you that the system isn't functional. Webhooks will attempt to resend data, but they will only try so many times before stopping. Therefore, you'll need another system to know when an application is down to prevent you from losing information","title":"WebHook"},{"location":"sys_design/concepts/webhook/#webhook","text":"A webhook can be thought of as a type of API that is driven by events rather than requests. Webhooks are a way for one application to provide other applications with real-time information . They allow one application to send a notification to another application when a certain event occurs rather than constantly polling for new data. This can help save on server resources and costs. Instead of one application making a request to another to receive a response, a webhook is a service that allows one program to send data to another as soon as a particular event takes place. Webhooks are sometimes referred to as push APIs or reverse APIs because instead of pulling data from one system to another, they push the data to update it in real-time. In either case, the webhook's meaning is the same; it enables you to share data. Webhooks work by making an HTTP request from one application to another. With web services becoming increasingly interconnected, webhooks are seeing more action as a lightweight solution for enabling real-time notifications and data updates without the need to develop a full-scale API. Example Say for instance you want to receive Slack notifications when tweets that mention a certain account and contain a specific hashtag are published. Instead of Slack continuously asking Twitter for new posts meeting these criteria, it makes much more sense for Twitter to send a notification to Slack only when this event takes place. This is the purpose of a webhook\u2013\u2013instead of having to repeatedly request the data, the receiving application can sit back and get what it needs without having to send repeated requests to another system.","title":"Webhook"},{"location":"sys_design/concepts/webhook/#webhook-vs-apis","text":"Webhooks are different from APIs, which allow for communication between different applications but work in a different way. An API is a set of protocols and routines for building and interacting with software applications, whereas a webhook is a way for one application to notify another application when a specific event occurs. In other words, an API allows you to retrieve data, while a webhook allows data to be pushed to you. This means that instead of having to poll for new data, you can receive data in real time through webhooks.","title":"Webhook VS API's"},{"location":"sys_design/concepts/webhook/#setup-time-for-webhook-vs-apis","text":"One might think that since webhooks are real-time events that they are technically difficult to implement. Actually, a key advantage of webhooks is that they are easier to set up and less resource-intensive than APIs. Creating an API is a complex process that in some cases can be as challenging as designing and building an application itself, but implementing a webhook simply requires setting up a single POST request on the sending end, establishing a URL on the receiving end to accept the data, then performing some action on the data once it is received.","title":"Setup time for Webhook vs APIs"},{"location":"sys_design/concepts/webhook/#why-webhook-is-limited","text":"Also, unlike APIs, webhooks do not allow the sending system to add, update and delete data on the receiving end, which is why webhooks alone are too limited to offer full integration between two applications.","title":"Why webhook is limited?"},{"location":"sys_design/concepts/webhook/#limitations-of-using-webhooks","text":"In the case of webhooks vs. APIs, you might wonder why anyone would still use an API since they're less efficient and productive. A few limitations of webhooks include the following:","title":"limitations of using webhooks?"},{"location":"sys_design/concepts/webhook/#not-always-supported","text":"Unfortunately, not all applications support webhooks . However, several types of third-party app providers can help you send webhooks by connecting apps that don't have integrations and allowing them to pass data.","title":"Not always supported"},{"location":"sys_design/concepts/webhook/#less-functionality-than-apis","text":"Webhooks only allow for data to be received from one application for another. Therefore, they can't be used for complicated integrations that require bi-directional communication.","title":"Less functionality than APIs"},{"location":"sys_design/concepts/webhook/#potential-for-lost-data","text":"With webhooks, you won't be alerted if an application or server is down for tails to send data. Since you'll only receive data when events occur, you won't receive any information if the other system is down. However, with APIs, you'll receive an error response alerting you that the system isn't functional. Webhooks will attempt to resend data, but they will only try so many times before stopping. Therefore, you'll need another system to know when an application is down to prevent you from losing information","title":"Potential for lost data"},{"location":"sys_design/concepts/websockets/","text":"HTTP, SSE, WebSockets \u00b6 HTTP \u00b6 Long and Short Polling \u00b6 SSE \u00b6 According to the definition, it is a server push technology enabling a client to receive automatic updates from a server. Using SSE, the clients make a persistent long-term connection with the server. Then, the server uses this connection to send the data to the client. Client can't send request twice It is unidirectional, meaning once the client sends the request it can only receive the responses without the ability to send new requests over the same connection. The client makes a request to the server. The connection between client and server is established, and it remains open. The server sends responses or events to the client when new data is available. WebSockets \u00b6 WebSocket provides full-duplex communication channels over a single TCP connection. It is a persistent connection between a client and a server that both parties can use to start sending data at any time. How this connection works? The client establishes a WebSocket connection through a process known as the WebSocket handshake. If the process succeeds, then the server and client can exchange data in both directions at any time. The WebSocket protocol enables the communication between a client and a server with lower overheads, facilitating real-time data transfer from and to the server. The client initiates a WebSocket handshake process by sending a request. The request also contains an HTTP Upgrade header that allows the request to switch to the WebSocket protocol ws:// The server sends a response to the client, acknowledging the WebSocket handshake request. A WebSocket connection will be opened once the client receives a successful handshake response. Now the client and server can start sending data in both directions allowing real-time communication. The connection is closed once the server or the client decides to close the connection.","title":"Web Sockets"},{"location":"sys_design/concepts/websockets/#http-sse-websockets","text":"","title":"HTTP, SSE, WebSockets"},{"location":"sys_design/concepts/websockets/#http","text":"","title":"HTTP"},{"location":"sys_design/concepts/websockets/#long-and-short-polling","text":"","title":"Long and Short Polling"},{"location":"sys_design/concepts/websockets/#sse","text":"According to the definition, it is a server push technology enabling a client to receive automatic updates from a server. Using SSE, the clients make a persistent long-term connection with the server. Then, the server uses this connection to send the data to the client. Client can't send request twice It is unidirectional, meaning once the client sends the request it can only receive the responses without the ability to send new requests over the same connection. The client makes a request to the server. The connection between client and server is established, and it remains open. The server sends responses or events to the client when new data is available.","title":"SSE"},{"location":"sys_design/concepts/websockets/#websockets","text":"WebSocket provides full-duplex communication channels over a single TCP connection. It is a persistent connection between a client and a server that both parties can use to start sending data at any time. How this connection works? The client establishes a WebSocket connection through a process known as the WebSocket handshake. If the process succeeds, then the server and client can exchange data in both directions at any time. The WebSocket protocol enables the communication between a client and a server with lower overheads, facilitating real-time data transfer from and to the server. The client initiates a WebSocket handshake process by sending a request. The request also contains an HTTP Upgrade header that allows the request to switch to the WebSocket protocol ws:// The server sends a response to the client, acknowledging the WebSocket handshake request. A WebSocket connection will be opened once the client receives a successful handshake response. Now the client and server can start sending data in both directions allowing real-time communication. The connection is closed once the server or the client decides to close the connection.","title":"WebSockets"},{"location":"sys_design/designPatterns/bff/","text":"Backend for Frontend (BFF) \u00b6 BFF is essentially a variant of the API Gateway pattern. It also provides an additional layer between microservices and clients. But rather than being a single point of entry, it introduces multiple gateways for each client as shown below. With BFF, you can add an API tailored to the needs of each client, removing a lot of the bloat caused by keeping it all in one place. Abstract BFF is a variant of the API Gateway pattern, but it also provides an additional layer between microservices and each client type separately. Instead of a single point of entry, it introduces multiple gateways. Because of that, you can have a tailored API that targets the needs of each client (mobile, web, desktop, voice assistant, etc.), and remove a lot of the bloat caused by keeping it all in one place. Benefits \u00b6 Decoupling \u00b6 Decoupling of Backend and Frontend\u200b for sure gives us faster time to market as frontend teams can have dedicated backend teams serving their unique needs. The release of new features of one frontend does not affect the other. Faster development \u00b6 We can much easier maintain and modify APIs\u200b and even provide API versioning dedicated for specific frontend, which is a big plus from a mobile app perspective as many users do not update the app immediately. Multiple device types can call the backend in parallel \u00b6 While the browser is making a request to the browser BFF, the mobile devices can do the same. It will help obtain responses from the services faster. Better security \u00b6 Certain sensitive information can be hidden, and unnecessary data to the frontend can be omitted when sending back a response to the frontend. The abstraction will make it harder for attackers to target the application.","title":"BFF"},{"location":"sys_design/designPatterns/bff/#backend-for-frontend-bff","text":"BFF is essentially a variant of the API Gateway pattern. It also provides an additional layer between microservices and clients. But rather than being a single point of entry, it introduces multiple gateways for each client as shown below. With BFF, you can add an API tailored to the needs of each client, removing a lot of the bloat caused by keeping it all in one place. Abstract BFF is a variant of the API Gateway pattern, but it also provides an additional layer between microservices and each client type separately. Instead of a single point of entry, it introduces multiple gateways. Because of that, you can have a tailored API that targets the needs of each client (mobile, web, desktop, voice assistant, etc.), and remove a lot of the bloat caused by keeping it all in one place.","title":"Backend for Frontend (BFF)"},{"location":"sys_design/designPatterns/bff/#benefits","text":"","title":"Benefits"},{"location":"sys_design/designPatterns/bff/#decoupling","text":"Decoupling of Backend and Frontend\u200b for sure gives us faster time to market as frontend teams can have dedicated backend teams serving their unique needs. The release of new features of one frontend does not affect the other.","title":"Decoupling"},{"location":"sys_design/designPatterns/bff/#faster-development","text":"We can much easier maintain and modify APIs\u200b and even provide API versioning dedicated for specific frontend, which is a big plus from a mobile app perspective as many users do not update the app immediately.","title":"Faster development"},{"location":"sys_design/designPatterns/bff/#multiple-device-types-can-call-the-backend-in-parallel","text":"While the browser is making a request to the browser BFF, the mobile devices can do the same. It will help obtain responses from the services faster.","title":"Multiple device types can call the backend in parallel"},{"location":"sys_design/designPatterns/bff/#better-security","text":"Certain sensitive information can be hidden, and unnecessary data to the frontend can be omitted when sending back a response to the frontend. The abstraction will make it harder for attackers to target the application.","title":"Better security"},{"location":"sys_design/designPatterns/cdn/","text":"CDN \u00b6 A content delivery network (CDN) is a distributed network of servers that can efficiently deliver web content to users. CDNs store cached content on edge servers that are close to the end users to minimize latency. Caching and CDN Caching is the process of storing data locally so that future requests for that data can be accessed more quickly. In the most common type of caching, web browser caching , a web browser stores copies of static data locally on a local hard drive. By using caching, the web browser can avoid making multiple round-trips to the server and instead access the same data locally, thus saving time and resources. Caching is well-suited for locally managing small, static data such as static images, CSS files, and JavaScript files. Similarly, caching is used by a content delivery network on edge servers close to the user to avoid requests traveling back to the origin and reducing end-user latency. Unlike a web browser cache, which is used only for a single user, the CDN has a shared cache. In a CDN shared cache, a file that is requested by one user can be accessed later by other users, which greatly decreases the number of requests to the origin server. CDNs are typically used to deliver static content such as images, style sheets, documents, client-side scripts, and HTML pages. The major advantages of using a CDN are lower latency of content to users , regardless of their geographical location in relation to the datacenter where the application is hosted. CDNs can also help to reduce load on a web application , because the application does not have to service requests for the content that is hosted in the CDN. Tip Static content is easy to serve becasue it does not need to be modified for each request. Caching at various levels \u00b6 Caching can occur at multiple levels between the origin server and the end user: Web server (Origin) : Uses a shared cache (for multiple users). Content delivery network : Uses a shared cache (for multiple users). Internet service provider (ISP) : Uses a shared cache (for multiple users). Web browser (Client) : Uses a private cache (for one user). CDN Internal Architecture \u00b6 A user types in www.amarjitdhillon.com in the browser. The browser looks up the domain name in the local DNS cache. If the domain name does not exist in the local DNS cache, the browser goes to the DNS resolver to resolve the name. The DNS resolver usually sits in the Internet Service Provider (ISP) . The DNS resolver recursively resolves the domain name. Finally, it asks the authoritative name server to resolve the domain name. If we don\u2019t use CDN, the authoritative name server returns the IP address for www.amarjitdhillon.com . But with CDN, the authoritative name server has an alias pointing to www.amarjitdhillon.cdn.com (the domain name of the CDN server). The DNS resolver asks the authoritative name server to resolve www.amarjitdhillon.cdn.com . The authoritative name server returns the domain name for the load balancer of CDN www.amarjitdhillon.lb.com . The DNS resolver asks the CDN load balancer to resolve www.amarjitdhillon.lb.com . The load balancer chooses an optimal CDN edge server based on the user\u2019s IP address, user\u2019s ISP, the content requested, and the server load. The CDN load balancer returns the CDN edge server\u2019s IP address for www.amarjitdhillon.lb.com . Now we finally get the actual IP address to visit. The DNS resolver returns the IP address to the browser. The browser visits the CDN edge server to load the content. There are two types of contents cached on the CDN servers: static contents and dynamic contents. The former contains static pages, pictures, and videos; the latter one includes results of edge computing. If the edge CDN server cache doesn't contain the content, it goes upward to the regional CDN server . If the content is still not found, it will go upward to the central CDN server Challenges of using CDN \u00b6 There are several challenges to take into account when planning to use a CDN. Deployment : Decide the origin from which the CDN fetches the content, and whether you need to deploy the content in more than one storage system. Take into account the process for deploying static content and resources. For example, you may need to implement a separate step to load content into Azure blob storage. Versioning and cache-control : Consider how you will update static content and deploy new versions. Understand how the CDN performs caching and time-to-live (TTL). Testing . It can be difficult to perform local testing of your CDN settings when developing and testing an application locally or in a staging environment. Search engine optimization (SEO) : Content such as images and documents are served from a different domain when you use the CDN. This can have an effect on SEO for this content. Content security . Not all CDNs offer any form of access control for the content. Some CDN services, including Azure CDN, support token-based authentication to protect CDN content. Client security . Clients might connect from an environment that does not allow access to resources on the CDN. This could be a security-constrained environment that limits access to only a set of known sources, or one that prevents loading of resources from anything other than the page origin. A fallback implementation is required to handle these cases.","title":"CDN"},{"location":"sys_design/designPatterns/cdn/#cdn","text":"A content delivery network (CDN) is a distributed network of servers that can efficiently deliver web content to users. CDNs store cached content on edge servers that are close to the end users to minimize latency. Caching and CDN Caching is the process of storing data locally so that future requests for that data can be accessed more quickly. In the most common type of caching, web browser caching , a web browser stores copies of static data locally on a local hard drive. By using caching, the web browser can avoid making multiple round-trips to the server and instead access the same data locally, thus saving time and resources. Caching is well-suited for locally managing small, static data such as static images, CSS files, and JavaScript files. Similarly, caching is used by a content delivery network on edge servers close to the user to avoid requests traveling back to the origin and reducing end-user latency. Unlike a web browser cache, which is used only for a single user, the CDN has a shared cache. In a CDN shared cache, a file that is requested by one user can be accessed later by other users, which greatly decreases the number of requests to the origin server. CDNs are typically used to deliver static content such as images, style sheets, documents, client-side scripts, and HTML pages. The major advantages of using a CDN are lower latency of content to users , regardless of their geographical location in relation to the datacenter where the application is hosted. CDNs can also help to reduce load on a web application , because the application does not have to service requests for the content that is hosted in the CDN. Tip Static content is easy to serve becasue it does not need to be modified for each request.","title":"CDN"},{"location":"sys_design/designPatterns/cdn/#caching-at-various-levels","text":"Caching can occur at multiple levels between the origin server and the end user: Web server (Origin) : Uses a shared cache (for multiple users). Content delivery network : Uses a shared cache (for multiple users). Internet service provider (ISP) : Uses a shared cache (for multiple users). Web browser (Client) : Uses a private cache (for one user).","title":"Caching at various levels"},{"location":"sys_design/designPatterns/cdn/#cdn-internal-architecture","text":"A user types in www.amarjitdhillon.com in the browser. The browser looks up the domain name in the local DNS cache. If the domain name does not exist in the local DNS cache, the browser goes to the DNS resolver to resolve the name. The DNS resolver usually sits in the Internet Service Provider (ISP) . The DNS resolver recursively resolves the domain name. Finally, it asks the authoritative name server to resolve the domain name. If we don\u2019t use CDN, the authoritative name server returns the IP address for www.amarjitdhillon.com . But with CDN, the authoritative name server has an alias pointing to www.amarjitdhillon.cdn.com (the domain name of the CDN server). The DNS resolver asks the authoritative name server to resolve www.amarjitdhillon.cdn.com . The authoritative name server returns the domain name for the load balancer of CDN www.amarjitdhillon.lb.com . The DNS resolver asks the CDN load balancer to resolve www.amarjitdhillon.lb.com . The load balancer chooses an optimal CDN edge server based on the user\u2019s IP address, user\u2019s ISP, the content requested, and the server load. The CDN load balancer returns the CDN edge server\u2019s IP address for www.amarjitdhillon.lb.com . Now we finally get the actual IP address to visit. The DNS resolver returns the IP address to the browser. The browser visits the CDN edge server to load the content. There are two types of contents cached on the CDN servers: static contents and dynamic contents. The former contains static pages, pictures, and videos; the latter one includes results of edge computing. If the edge CDN server cache doesn't contain the content, it goes upward to the regional CDN server . If the content is still not found, it will go upward to the central CDN server","title":"CDN Internal Architecture"},{"location":"sys_design/designPatterns/cdn/#challenges-of-using-cdn","text":"There are several challenges to take into account when planning to use a CDN. Deployment : Decide the origin from which the CDN fetches the content, and whether you need to deploy the content in more than one storage system. Take into account the process for deploying static content and resources. For example, you may need to implement a separate step to load content into Azure blob storage. Versioning and cache-control : Consider how you will update static content and deploy new versions. Understand how the CDN performs caching and time-to-live (TTL). Testing . It can be difficult to perform local testing of your CDN settings when developing and testing an application locally or in a staging environment. Search engine optimization (SEO) : Content such as images and documents are served from a different domain when you use the CDN. This can have an effect on SEO for this content. Content security . Not all CDNs offer any form of access control for the content. Some CDN services, including Azure CDN, support token-based authentication to protect CDN content. Client security . Clients might connect from an environment that does not allow access to resources on the CDN. This could be a security-constrained environment that limits access to only a set of known sources, or one that prevents loading of resources from anything other than the page origin. A fallback implementation is required to handle these cases.","title":"Challenges of using CDN"},{"location":"sys_design/designPatterns/choreography/","text":"Choreography \u00b6 Tldr The goal of this pattern is to have each component of the system participate in the decision-making process about the workflow of a business transaction, instead of relying on a central point of control (Orchestration) . Problem \u00b6 In Microservices, a common pattern for communication is to use a centralized service that acts as the orchestrator. It acknowledges all incoming requests and delegates operations to the respective services. In doing so, it also manages the workflow of the entire business transaction. Each service just completes an operation and is not aware of the overall workflow. Solution \u00b6 A client request publishes messages to a message queue . As messages arrive, they are pushed to subscribers, or services, interested in that message. Each subscribed service does their operation as indicated by the message and responds to the message queue with success or failure of the operation. In case of success, the service can push a message back to the same queue or a different message queue so that another service can continue the workflow if needed. If an operation fails, the message bus can retry that operation.","title":"Choreography"},{"location":"sys_design/designPatterns/choreography/#choreography","text":"Tldr The goal of this pattern is to have each component of the system participate in the decision-making process about the workflow of a business transaction, instead of relying on a central point of control (Orchestration) .","title":"Choreography"},{"location":"sys_design/designPatterns/choreography/#problem","text":"In Microservices, a common pattern for communication is to use a centralized service that acts as the orchestrator. It acknowledges all incoming requests and delegates operations to the respective services. In doing so, it also manages the workflow of the entire business transaction. Each service just completes an operation and is not aware of the overall workflow.","title":"Problem"},{"location":"sys_design/designPatterns/choreography/#solution","text":"A client request publishes messages to a message queue . As messages arrive, they are pushed to subscribers, or services, interested in that message. Each subscribed service does their operation as indicated by the message and responds to the message queue with success or failure of the operation. In case of success, the service can push a message back to the same queue or a different message queue so that another service can continue the workflow if needed. If an operation fails, the message bus can retry that operation.","title":"Solution"},{"location":"sys_design/designPatterns/cqrs/","text":"Problem statement \u00b6 This pattern is used to solve an issue of how to implement a query that retrieves data from multiple services in a microservice architecture? Solution \u00b6 Define a view database, which is a read-only replica that is designed to support that query. The application keeps the replica up to data by subscribing to Domain events published by the service that own the data.","title":"CQRS"},{"location":"sys_design/designPatterns/cqrs/#problem-statement","text":"This pattern is used to solve an issue of how to implement a query that retrieves data from multiple services in a microservice architecture?","title":"Problem statement"},{"location":"sys_design/designPatterns/cqrs/#solution","text":"Define a view database, which is a read-only replica that is designed to support that query. The application keeps the replica up to data by subscribing to Domain events published by the service that own the data.","title":"Solution"},{"location":"sys_design/designPatterns/durableFunctions/","text":"Durable Functions \u00b6 Durable Functions is an extension of Azure Functions that lets you write stateful functions in a serverless compute environment. The extension lets you define stateful workflows by writing orchestrator functions and stateful entities by writing entity functions using the Azure Functions programming model. Patterns \u00b6 Function chaining \u00b6 In the function chaining pattern, a sequence of functions executes in a specific order. In this pattern, the output of one function is applied to the input of another function. The use of queues between each function ensures that the system stays durable and scalable, even though there is a flow of control from one function to the next. In this example, the values F1, F2, F3, and F4 are the names of other functions in the same function app. You can implement control flow by using normal imperative coding constructs. Code executes from the top down. Fan-out/fan-in \u00b6 In the fan out/fan in pattern, you execute multiple functions in parallel and then wait for all functions to finish. Often, some aggregation work is done on the results that are returned from the functions. How to do fan in? Fanning back in is much more challenging. To fan in, in a normal function, you write code to track when the queue-triggered functions end, and then store function outputs Async HTTP APIs \u00b6 The async HTTP API pattern addresses the problem of coordinating the state of long-running operations with external clients. A common way to implement this pattern is by having an HTTP endpoint trigger the long-running action. Then, redirect the client to a status endpoint that the client polls to learn when the operation is finished. HTTP reference \u00b6 reference docs","title":"Durable functions"},{"location":"sys_design/designPatterns/durableFunctions/#durable-functions","text":"Durable Functions is an extension of Azure Functions that lets you write stateful functions in a serverless compute environment. The extension lets you define stateful workflows by writing orchestrator functions and stateful entities by writing entity functions using the Azure Functions programming model.","title":"Durable Functions"},{"location":"sys_design/designPatterns/durableFunctions/#patterns","text":"","title":"Patterns"},{"location":"sys_design/designPatterns/durableFunctions/#function-chaining","text":"In the function chaining pattern, a sequence of functions executes in a specific order. In this pattern, the output of one function is applied to the input of another function. The use of queues between each function ensures that the system stays durable and scalable, even though there is a flow of control from one function to the next. In this example, the values F1, F2, F3, and F4 are the names of other functions in the same function app. You can implement control flow by using normal imperative coding constructs. Code executes from the top down.","title":"Function chaining"},{"location":"sys_design/designPatterns/durableFunctions/#fan-outfan-in","text":"In the fan out/fan in pattern, you execute multiple functions in parallel and then wait for all functions to finish. Often, some aggregation work is done on the results that are returned from the functions. How to do fan in? Fanning back in is much more challenging. To fan in, in a normal function, you write code to track when the queue-triggered functions end, and then store function outputs","title":"Fan-out/fan-in"},{"location":"sys_design/designPatterns/durableFunctions/#async-http-apis","text":"The async HTTP API pattern addresses the problem of coordinating the state of long-running operations with external clients. A common way to implement this pattern is by having an HTTP endpoint trigger the long-running action. Then, redirect the client to a status endpoint that the client polls to learn when the operation is finished.","title":"Async HTTP APIs"},{"location":"sys_design/designPatterns/durableFunctions/#http-reference","text":"reference docs","title":"HTTP reference"},{"location":"sys_design/designPatterns/materializedView/","text":"Materialized View \u00b6 Tldr Generate prepopulated views over the data in one or more data stores when the data is formatted in a way that does not favor the required query operations. This pattern can help to support efficient querying and data extraction, and improve application performance. Problem Context \u00b6 When storing data, the priority for developers and data administrators is often focused on how the data is stored, as opposed to how it is read. The chosen storage format is usually closely related to the format of the data, requirements for managing data size and data integrity, and the kind of store in use. For example , when using NoSQL Document store such as MongoDB, the data is often represented as a series of aggregates, each of which contains all of the information for that entity. However, this may have a negative effect on queries. When a query requires only a subset of the data from some entities, such as a summary of orders for several customers without all of the order details, it must extract all of the data for the relevant entities in order to obtain the required in for Solution \u00b6 To support efficient querying, a common solution is to generate, in advance, a view that materializes the data in a format most suited to the required results set. The Materialized View pattern describes generating prepopulated views of data in environments where the source data is not in a format that is suitable for querying, where generating a suitable query is difficult, or where query performance is poor due to the nature of the data or the data store. These materialized views , which contain only data required by a query, allow applications to quickly obtain the information they need. In additionto joining tables or combining data entities, materialized views may include the current values of calculated columns or data items, the results of combining values or executing transformations on the data items, and values specified as part of the query. A materialized view may even be optimized for just a single query. A key point is that a materialized view and the data it contains is completely disposable because it can be entirely rebuilt from the source data stores. A materialized view is never updated directly by an application, and so it is effectively a specialized cache. When the source data for the view changes, the view must be updated to include the new information. This may occur automatically on an appropriate schedule, or when the system detects a change to the original data. In other cases it may be necessary to regenerate the view manually.","title":"Materialized View"},{"location":"sys_design/designPatterns/materializedView/#materialized-view","text":"Tldr Generate prepopulated views over the data in one or more data stores when the data is formatted in a way that does not favor the required query operations. This pattern can help to support efficient querying and data extraction, and improve application performance.","title":"Materialized View"},{"location":"sys_design/designPatterns/materializedView/#problem-context","text":"When storing data, the priority for developers and data administrators is often focused on how the data is stored, as opposed to how it is read. The chosen storage format is usually closely related to the format of the data, requirements for managing data size and data integrity, and the kind of store in use. For example , when using NoSQL Document store such as MongoDB, the data is often represented as a series of aggregates, each of which contains all of the information for that entity. However, this may have a negative effect on queries. When a query requires only a subset of the data from some entities, such as a summary of orders for several customers without all of the order details, it must extract all of the data for the relevant entities in order to obtain the required in for","title":"Problem Context"},{"location":"sys_design/designPatterns/materializedView/#solution","text":"To support efficient querying, a common solution is to generate, in advance, a view that materializes the data in a format most suited to the required results set. The Materialized View pattern describes generating prepopulated views of data in environments where the source data is not in a format that is suitable for querying, where generating a suitable query is difficult, or where query performance is poor due to the nature of the data or the data store. These materialized views , which contain only data required by a query, allow applications to quickly obtain the information they need. In additionto joining tables or combining data entities, materialized views may include the current values of calculated columns or data items, the results of combining values or executing transformations on the data items, and values specified as part of the query. A materialized view may even be optimized for just a single query. A key point is that a materialized view and the data it contains is completely disposable because it can be entirely rebuilt from the source data stores. A materialized view is never updated directly by an application, and so it is effectively a specialized cache. When the source data for the view changes, the view must be updated to include the new information. This may occur automatically on an appropriate schedule, or when the system detects a change to the original data. In other cases it may be necessary to regenerate the view manually.","title":"Solution"},{"location":"sys_design/designPatterns/priority_queue/","text":"Priority Queue Pattern \u00b6 The goal is to prioritize requests sent to services so that requests with a higher priority are received and processed more quickly than those with a lower priority . This pattern is useful in applications that offer different service level guarantees to individual clients.","title":"Priority Queue"},{"location":"sys_design/designPatterns/priority_queue/#priority-queue-pattern","text":"The goal is to prioritize requests sent to services so that requests with a higher priority are received and processed more quickly than those with a lower priority . This pattern is useful in applications that offer different service level guarantees to individual clients.","title":"Priority Queue Pattern"},{"location":"sys_design/designPatterns/saga/","text":"Saga \u00b6 Tldr The Saga design pattern is a way to manage data consistency across microservices in distributed transaction scenarios. A saga is a sequence of transactions that updates each service and publishes a message or event to trigger the next transaction step. If a step fails, the saga executes compensating transactions that counteract the preceding transactions. How Saga functions? \u00b6 The Saga pattern provides transaction management using a sequence of local transactions . A local transaction is the atomic work effort performed by a saga participant. Each local transaction updates the database and publishes a message or event to trigger the next local transaction in the saga. If a local transaction fails, the saga executes a series of compensating transactions that undo the changes that were made by the preceding local transactions. When to use Saga? \u00b6 Use the Saga pattern when you need to: Ensure data consistency in a distributed system without tight coupling. Roll back or compensate if one of the operations in the sequence fails. The Saga pattern is less suitable for: Tightly coupled transactions. Compensating transactions that occur in earlier participants. Cyclic dependencies. Orchestration based Saga \u00b6 The saga orchestrator sends a Verify Consumer command to Consumer Service . Consumer Service replies with a Consumer Verified message. The saga orchestrator sends a Create Ticket command to Kitchen Service . Kitchen Service replies with a Ticket Created message. The saga orchestrator sends an Authorize Card message to Accounting Service . Accounting Service replies with a Card Authorized message.\u2260 The saga orchestrator sends an Approve Ticket command to Kitchen Service. The saga orchestrator sends an Approve Order command to Order Service. Choreo based Saga \u00b6 Order Service creates an Order in the APPROVAL_PENDING state and publishes an OrderCreated event . Consumer Service consumes the OrderCreated event , verifies that the con- sumer can place the order, and publishes a ConsumerVerified event . Kitchen Service consumes the OrderCreated event , validates the Order, cre- ates a Ticket in a CREATE_PENDING state , and publishes the TicketCreated event . Accounting Service consumes the OrderCreated event and creates a Credit- CardAuthorization in a PENDING state. Accounting Service consumes the TicketCreated and ConsumerVerified events , charges the consumer\u2019s credit card, and publishes the CreditCard-Authorized event . Kitchen Service consumes the CreditCardAuthorized event and changes the state of the Ticket to AWAITING_ACCEPTANCE. Order Service receives the CreditCardAuthorized events , changes the state of the Order to APPROVED, and publishes an OrderApproved event.","title":"Saga"},{"location":"sys_design/designPatterns/saga/#saga","text":"Tldr The Saga design pattern is a way to manage data consistency across microservices in distributed transaction scenarios. A saga is a sequence of transactions that updates each service and publishes a message or event to trigger the next transaction step. If a step fails, the saga executes compensating transactions that counteract the preceding transactions.","title":"Saga"},{"location":"sys_design/designPatterns/saga/#how-saga-functions","text":"The Saga pattern provides transaction management using a sequence of local transactions . A local transaction is the atomic work effort performed by a saga participant. Each local transaction updates the database and publishes a message or event to trigger the next local transaction in the saga. If a local transaction fails, the saga executes a series of compensating transactions that undo the changes that were made by the preceding local transactions.","title":"How Saga functions?"},{"location":"sys_design/designPatterns/saga/#when-to-use-saga","text":"Use the Saga pattern when you need to: Ensure data consistency in a distributed system without tight coupling. Roll back or compensate if one of the operations in the sequence fails. The Saga pattern is less suitable for: Tightly coupled transactions. Compensating transactions that occur in earlier participants. Cyclic dependencies.","title":"When to use Saga?"},{"location":"sys_design/designPatterns/saga/#orchestration-based-saga","text":"The saga orchestrator sends a Verify Consumer command to Consumer Service . Consumer Service replies with a Consumer Verified message. The saga orchestrator sends a Create Ticket command to Kitchen Service . Kitchen Service replies with a Ticket Created message. The saga orchestrator sends an Authorize Card message to Accounting Service . Accounting Service replies with a Card Authorized message.\u2260 The saga orchestrator sends an Approve Ticket command to Kitchen Service. The saga orchestrator sends an Approve Order command to Order Service.","title":"Orchestration based Saga"},{"location":"sys_design/designPatterns/saga/#choreo-based-saga","text":"Order Service creates an Order in the APPROVAL_PENDING state and publishes an OrderCreated event . Consumer Service consumes the OrderCreated event , verifies that the con- sumer can place the order, and publishes a ConsumerVerified event . Kitchen Service consumes the OrderCreated event , validates the Order, cre- ates a Ticket in a CREATE_PENDING state , and publishes the TicketCreated event . Accounting Service consumes the OrderCreated event and creates a Credit- CardAuthorization in a PENDING state. Accounting Service consumes the TicketCreated and ConsumerVerified events , charges the consumer\u2019s credit card, and publishes the CreditCard-Authorized event . Kitchen Service consumes the CreditCardAuthorized event and changes the state of the Ticket to AWAITING_ACCEPTANCE. Order Service receives the CreditCardAuthorized events , changes the state of the Order to APPROVED, and publishes an OrderApproved event.","title":"Choreo based Saga"},{"location":"sys_design/designPatterns/serviceRegistry/","text":"Service Registry Pattern \u00b6 Services discovery in Monolithic/SOA/Microservices \u00b6 Services typically need to call one another. In a monolithic application , services invoke one another through language-level method or procedure calls. In a traditional distributed system SOA deployment, services run at fixed, well known locations (hosts and ports) and so can easily call one another using HTTP/REST or some RPC mechanism. However, a modern microservice-based application typically runs in a virtualized or containerized environments where the number of instances of a service and their locations changes dynamically. Why we need it? \u00b6 Each microservice has a unique name/URL that's used to resolve its location. Your microservice needs to be addressable wherever it's running. In the same way that DNS resolves a URL to a particular computer IP, your microservice needs to have a unique name so that its current location is discoverable. Handling service failure If a computer fails, the registry service must be able to indicate where the service is running now. Caching/Storing the results The registry is a database containing the network locations of service instances. A service registry needs to be highly available and up-to-date. Clients could cache network locations obtained from the service registry. Types of service discovery \u00b6 Client-side \u00b6 When using client\u2011side discovery , the client is responsible for determining the network locations of available service instances and load balancing requests across them. The client queries a service registry , which is a database of available service instances. The client then uses a load\u2011balancing algorithm to select one of the available service instances and makes a request. Note The network location of a service instance is registered with the service registry when it starts up. It is removed from the service registry when the instance terminates. The service instance\u2019s registration is typically refreshed periodically using a heartbeat mechanism. Sever-side \u00b6 When making a request to a service, the client makes a request via a router (a.k.a load balancer) that runs at a well known location. The router queries a service registry, which might be built into the router, and forwards the request to an available service instance. Registry for AKS \u00b6 Using service registy in AKS In some microservice deployment environments (called clusters, to be covered in a later section), service discovery is built in . For example, an Azure Kubernetes Service (AKS) environment can handle service instance registration and deregistration . It also runs a proxy on each cluster host that plays the role of server-side discovery router. servcie discovery example \u00b6 ELB as Service registry and server-side discovery router An AWS Elastic Load Balancer (ELB) is an example of a server-side discovery router . A client makes HTTP requests (or opens TCP connections) to the ELB, which load balances the traffic amongst a set of EC2 instances. An ELB can load balance either external traffic from the Internet or, when deployed in a VPC, load balance internal traffic. An ELB also functions as a Service Registry . EC2 instances are registered with the ELB either explicitly via an API call or automatically as part of an auto-scaling group. Service Registry \u00b6 The service registry is a key part of service discovery. It is a database containing the network locations of service instances. A service registry needs to be highly available and up to date. Clients can cache network locations obtained from the service registry. However, that information eventually becomes out of date and clients become unable to discover service instances. How to stay updated? A service registry consists of a cluster of servers that use a replication protocol to maintain consistency. Examples \u00b6 Netflix Eureka : it is a RESTful (Representational State Transfer) service that is primarily used in the AWS cloud for the purpose of discovery, load balancing and failover of middle-tier servers. It plays a critical role in Netflix mid-tier infra. Etcd \u2013 A highly available, distributed, consistent, key\u2011value store that is used for shared configuration and service discovery. Two notable projects that use etcd are Kubernetes and Cloud Foundry. Consul \u2013 A tool for discovering and configuring services. It provides an API that allows clients to register and discover services. Consul can perform health checks to determine service availability. Apache Zookeeper \u2013 A widely used, high\u2011performance coordination service for distributed applications. Apache Zookeeper was originally a subproject of Hadoop but is now a top\u2011level project.","title":"Service Registry"},{"location":"sys_design/designPatterns/serviceRegistry/#service-registry-pattern","text":"","title":"Service Registry Pattern"},{"location":"sys_design/designPatterns/serviceRegistry/#services-discovery-in-monolithicsoamicroservices","text":"Services typically need to call one another. In a monolithic application , services invoke one another through language-level method or procedure calls. In a traditional distributed system SOA deployment, services run at fixed, well known locations (hosts and ports) and so can easily call one another using HTTP/REST or some RPC mechanism. However, a modern microservice-based application typically runs in a virtualized or containerized environments where the number of instances of a service and their locations changes dynamically.","title":"Services discovery in Monolithic/SOA/Microservices"},{"location":"sys_design/designPatterns/serviceRegistry/#why-we-need-it","text":"Each microservice has a unique name/URL that's used to resolve its location. Your microservice needs to be addressable wherever it's running. In the same way that DNS resolves a URL to a particular computer IP, your microservice needs to have a unique name so that its current location is discoverable. Handling service failure If a computer fails, the registry service must be able to indicate where the service is running now. Caching/Storing the results The registry is a database containing the network locations of service instances. A service registry needs to be highly available and up-to-date. Clients could cache network locations obtained from the service registry.","title":"Why we need it?"},{"location":"sys_design/designPatterns/serviceRegistry/#types-of-service-discovery","text":"","title":"Types of service discovery"},{"location":"sys_design/designPatterns/serviceRegistry/#client-side","text":"When using client\u2011side discovery , the client is responsible for determining the network locations of available service instances and load balancing requests across them. The client queries a service registry , which is a database of available service instances. The client then uses a load\u2011balancing algorithm to select one of the available service instances and makes a request. Note The network location of a service instance is registered with the service registry when it starts up. It is removed from the service registry when the instance terminates. The service instance\u2019s registration is typically refreshed periodically using a heartbeat mechanism.","title":"Client-side"},{"location":"sys_design/designPatterns/serviceRegistry/#sever-side","text":"When making a request to a service, the client makes a request via a router (a.k.a load balancer) that runs at a well known location. The router queries a service registry, which might be built into the router, and forwards the request to an available service instance.","title":"Sever-side"},{"location":"sys_design/designPatterns/serviceRegistry/#registry-for-aks","text":"Using service registy in AKS In some microservice deployment environments (called clusters, to be covered in a later section), service discovery is built in . For example, an Azure Kubernetes Service (AKS) environment can handle service instance registration and deregistration . It also runs a proxy on each cluster host that plays the role of server-side discovery router.","title":"Registry for AKS"},{"location":"sys_design/designPatterns/serviceRegistry/#servcie-discovery-example","text":"ELB as Service registry and server-side discovery router An AWS Elastic Load Balancer (ELB) is an example of a server-side discovery router . A client makes HTTP requests (or opens TCP connections) to the ELB, which load balances the traffic amongst a set of EC2 instances. An ELB can load balance either external traffic from the Internet or, when deployed in a VPC, load balance internal traffic. An ELB also functions as a Service Registry . EC2 instances are registered with the ELB either explicitly via an API call or automatically as part of an auto-scaling group.","title":"servcie discovery example"},{"location":"sys_design/designPatterns/serviceRegistry/#service-registry","text":"The service registry is a key part of service discovery. It is a database containing the network locations of service instances. A service registry needs to be highly available and up to date. Clients can cache network locations obtained from the service registry. However, that information eventually becomes out of date and clients become unable to discover service instances. How to stay updated? A service registry consists of a cluster of servers that use a replication protocol to maintain consistency.","title":"Service Registry"},{"location":"sys_design/designPatterns/serviceRegistry/#examples","text":"Netflix Eureka : it is a RESTful (Representational State Transfer) service that is primarily used in the AWS cloud for the purpose of discovery, load balancing and failover of middle-tier servers. It plays a critical role in Netflix mid-tier infra. Etcd \u2013 A highly available, distributed, consistent, key\u2011value store that is used for shared configuration and service discovery. Two notable projects that use etcd are Kubernetes and Cloud Foundry. Consul \u2013 A tool for discovering and configuring services. It provides an API that allows clients to register and discover services. Consul can perform health checks to determine service availability. Apache Zookeeper \u2013 A widely used, high\u2011performance coordination service for distributed applications. Apache Zookeeper was originally a subproject of Hadoop but is now a top\u2011level project.","title":"Examples"},{"location":"sys_design/designPatterns/sidecar/","text":"Sidecar \u00b6 This pattern is named Sidecar because it resembles a sidecar attached to a motorcycle. In the pattern, the sidecar is attached to a parent application and provides supporting features for the application. The sidecar also shares the same lifecycle as the parent application, being created and retired alongside the parent. The sidecar pattern is sometimes referred to as the sidekick pattern and is a decomposition pattern. Why this pattern is needed? Applications and services often require cross-cutting-concerns functionality, such as monitoring , logging , configuration , and networking services . These peripheral tasks can be implemented as separate components or services.","title":"Sidecar"},{"location":"sys_design/designPatterns/sidecar/#sidecar","text":"This pattern is named Sidecar because it resembles a sidecar attached to a motorcycle. In the pattern, the sidecar is attached to a parent application and provides supporting features for the application. The sidecar also shares the same lifecycle as the parent application, being created and retired alongside the parent. The sidecar pattern is sometimes referred to as the sidekick pattern and is a decomposition pattern. Why this pattern is needed? Applications and services often require cross-cutting-concerns functionality, such as monitoring , logging , configuration , and networking services . These peripheral tasks can be implemented as separate components or services.","title":"Sidecar"},{"location":"technical_stuff/powerbi/","text":"Power BI \ud83d\udcc8 \u00b6 Concepts \u00b6 The five major building blocks of Power BI are: Dashboards Reports Workbooks Datasets Dataflows They're all organized into workspaces , and they're created on capacities . It's important to understand capacities and workspaces before we dig into the five building blocks, so let's start there. Capacity \u00b6 Capacities are a core Power BI concept representing a set of resources (storage, processor, and memory) used to host and deliver your Power BI content. Capacities are either shared or reserved . A shared capacity is shared with other Microsoft customers, while a reserved capacity is reserved for a single customer. Workspaces \u00b6 How to share powerBI content? A workspace is like a shared folder between a team of users. This can be a place to share some of the Power BI content There are two types of licenses for Power BI; capacity-based and user-based licensing. The capacity-based licensing is usually better for organizations with more than hundreds of users, and user-based licensing is good for small to medium size businesses There are two types of workspaces: My workspace (user based) Workspaces (Capacity based) My workspace \u00b6 It is the personal workspace for any Power BI customer to work with your own content. Only you have access to your My workspace. You can share dashboards and reports from your My Workspace. Workspaces \u00b6 Workspaces are used to collaborate and share content with colleagues. You can add colleagues to your workspaces and collaborate on dashboards, reports, workbooks, and datasets. Warning Each workspace member needs a Power BI Pro or Premium Per User (PPU) license. Dashboard \u00b6 Tldr A Power BI dashboard is a single page, often called a canvas, that uses visualizations to tell a story. Because it is limited to one page, a well-designed dashboard contains only the most-important elements of that story. It is a single screen with tiles of interactive visuals, text, and graphics. A dashboard collects your most important metrics, on one screen, to tell a story or answer a question. if a business user is given permissions to the report, they can build their own dashboards too. As the capacity must share resources, limitations are imposed to ensure \"fair play\", such as the maximum model size (1 GB) and maximum daily refresh frequency ( 8 times per day ). The visualizations on a dashboard come from reports and each report is based on one dataset. In fact, one way to think of a dashboard is as an entryway into the underlying reports and datasets. Selecting a visualization takes you to the report that was used to create it. Visualization: \u00b6 It is a type of chart built by Power BI designers. The visuals display the data from reports and datasets. How many datasets are feasible? All of the visualizations in a report come from a single dataset. Power BI Desktop can combine more than one data source into a single dataset in a report, and that report can be imported into Power BI. Report \u00b6 Report is one or more pages of interactive visuals, text, and graphics that together make up a single report. Power BI bases a report on a single dataset. Tile \u00b6 A tile is a snapshot of your data, pinned to a dashboard by a designer. Designers can create tiles from a report, dataset, dashboard, the Q&A question box, Excel, SQL Server Reporting Services (SSRS), and more. DAX \u00b6 Data Analysis Expressions\u202f(DAX) is a programming language that is used throughout Microsoft Power BI for creating calculated columns, measures, and custom tables. It is a collection of functions, operators, and constants that can be used in a formula, or expression, to calculate and return one or more values. using DAX to calculate columns DAX allows you to augment the data that you bring in from different data sources by creating a calculated column that didn't originally exist in the data source. This feature should be used sparingly DAX and MScript \u00b6 Power BI has 2 mighty hands, DAX (powered by SQL Server Analysis Service ) and M Script (powered by Power Query ), to perform the ETL jobs and visualization related calculations. Both of these engines are efficient with their way of performing calculations. where not to use DAX and MScript? When it comes to performing calculations in an optimized way, both under-perform compared to database engines, since they are in-memory calculation engines best suited to play with data. They lack indexing of data, which database handles while storing of data. Also, database engine re-uses query results of last few queries, by keeping track of changing data, which is something complex for both DAX & Power Query. As the data size grows, performance difference is quite noticeable. Calculated column and measure \u00b6 The fundamental difference between a calculated column and a measure is that a calculated column creates a value for each row in a table. For example, if the table has 1,000 rows, it will have 1,000 values in the calculated column. Calculated column values are stored in the Power BI .pbix file. Each calculated column will increase the space that is used in that file and potentially increase the refresh time. Measures are calculated on demand. Power BI calculates the correct value when the user requests it. Measures do not add to the overall disk space of the Power BI .pbix file . Dataflow \u00b6 Tldr Dataflow is a collection of Power Query queries that are scheduled and executed together. It's up to you how you organize the staged data in dataflows. For example, if you need to stage some tables from Dynamics 365, you can create one dataflow that has a query for each table you want to stage. So, dataflows allow you to logically group related Power Query queries Query folding \u00b6 Query folding is the ability for a Power Query query to generate a single query statement that retrieves and transforms source data. Query folding may occur for an entire Power Query query, or for a subset of its steps. When query folding cannot be achieved\u2014either partially or fully\u2014the Power Query mashup engine must compensate by processing data transformations itself. This process can involve retrieving source query results, which for large datasets is very resource intensive and slow. PowerBI Service \u00b6 There are two hosting options for Power BI reports Cloud-based hosting (called Power BI Service or website): In the Power BI Service , organizations are separated using Tenants . Tenants can be managed by Azure Active Directory or Office 365 . Under tenants, there will be users. These users are Azure Active Directory users. On-premises hosting (called Power BI Report Server ) Storage mode \u00b6 Today PowerBI offers 4 different storage modes for tables: Import mode \u00b6 In this mode, Power BI connects with underlying data source & downloads entire data from the datasource. This data is stored in Power BI model (in an in-memory cache). Fresh copy of this data can be downloaded by pressing Refresh button. PBIX file internally stores model data in compressed format. This published datset model on Power BI Service, internally is stored on Common Data Model, which is sort of Azure Managed SQL Server instance in the backend. Which is the fastest method The fastest method is Import mode . Essentially, this mode allows you to load the data once into Power BI, where it is then stored. Power BI uses the in-memory VertiPaq/xVelocity engine , which is exceptionally fast and delivers results almost immediately. It offers full DAX and PowerQuery support . A quick rule of thumb is that you can typically expect about 10x compression when importing data into Power BI The drawback is that it only permits you to refresh data 8 times per day. DirectQuery mode \u00b6 With DirectQuery datasets , no data is imported into Power BI. Instead, your Power BI dataset is simply metadata (e.g., tables, relationships) of how your model is structured to query the data source. Data is only brought into Power BI reports and dashboards at query-time (e.g., a user runs a report, or changes filters). Pros: - Dataset size limits do not apply as all data still resides in the underlying database. - Datasets do not require a scheduled refresh as data is only retrieved at query-time. Cons: - Typically, query-time performance will suffer, even when querying a cloud data warehouse like Snowflake. - Concurrency (e.g., multiple users running reports) against DirectQuery datasets could cause performance issues against the underlying database. Dual mode \u00b6 Dudata is imported into cache memory, but can also be served directly from the data source at the query time Hybrid tables/ Composite mode \u00b6 Tldr Cold data in Import mode , Hot data in DirectQuery . Hybrid tables can only be applied on a table that incremental refresh is set on it. Hybrid tables are partitioned, so their most recent partition is a DirectQuery from the data source, and their historical data is imported into other partitions. If your data source doesn\u2019t support query folding, For example, it is a CSV file. Then Power BI, when connected to it, reads the entire data anyway. Incremental Refresh doesn\u2019t need a Premium or PPU license. You can even set it up using a Power BI Pro license. However, Hybrid tables require a Power BI Premium capacity or PPU. Composite models aka hybid tables attempt to combine the best aspects from Import and DirectQuery modes into a single dataset. With Composite models, data modelers can configure the storage mode for each table in the model PowerBI Architecture \u00b6 Front-end cluster \u00b6 The overall technical architecture consists of two clusters: a Web Front End (WFE) cluster a Back End cluster AAD \u00b6 The WFE cluster manages connectivity and authentication. Power BI relies on Azure Active Directory (AAD) to manage account authentication and management. ATM \u00b6 Power BI uses the Azure Traffic Manager (ATM) to direct user traffic to the nearest data center. Which data center is used is determined by the DNS record of the client attempting to connect. The DNS Service can communicate with the Azure Traffic Manager to find the nearest data center with a Power BI deployment CDN \u00b6 Power BI uses the Azure Content Delivery Network (CDN) to deliver the necessary static content and files to end users based on their geographical locale. The WFE cluster nearest to the user manages the user login and authentication and provides an access token to the user once authentication is successful. The ASP.NET component within the WFE cluster parses the request to determine which organization the user belongs to, and then consults the Power BI Global Service Global Service \u00b6 The Global Service is implemented as a single Azure Table that is shared among all worldwide WFE and Back End clusters. This service maps users and customer organizations to the datacenter that hosts their Power BI tenant. The WFE specifies to the browser which backend cluster houses the organization's tenant. Once a user is authenticated, subsequent client interactions occur with the backend cluster directly and the WFE cluster is not used. Back-end cluster \u00b6 The backend cluster manages all actions the user does in Power BI Service, including visualizations, dashboards,datasets, reports, data storage, data connections, data refresh, and others. Gateway and APIM \u00b6 The Gateway Role acts as a gateway between user requests and the Power BI service. As you can see in the diagram, only the Gateway Role and Azure API Management (APIM) services are accessible from the public Internet. When an authenticated user connects to the Power BI Service, the connection and any request by the client is accepted and managed by the Gateway Role, which then interacts on the user's behalf with the rest of the Power BI Service. For example, when a client attempts to view a dashboard, the Gateway Role accepts that request, and then sends a request to the Presentation Role to retrieve the data needed by the browser to render the dashboard. Data Storage \u00b6 As far as data storage in the cloud goes, Power BI uses two primary repositories for storing and managing data. Data that is uploaded from users or generated by dataflows is stored in Azure BLOB storage, but all the metadata definitions (dashboards, reports, recent data sources, workspaces, organizational information, tenant information) are stored in Azure SQL Database. Performance Optimization of PowerBI \u00b6 check this ZerbaBI report Analyze the results \u00b6 Power BI Performance Analyzer : The natural starting point is Power BI Performance Analyzer, a built-in feature of Power BI Desktop. Select the View ribbon, and then select Performance Analyzer to display the Performance Analyzer pane. DAX Studio : You can analyze performance using in another tool called DAX Studio . This is a a great tool for performance analysis. Using direct query? \u00b6 There's another situation that can slow down your report. You might be creating tables that use DirectQuery as the connection mode . This means that the data is not loaded into Power BI. Instead, it is loaded using a query that runs on a database server. This means additional time to fetch the data. This shows up as another item on the report, since Power BI first needs to fetch the data, process it using a PowerQuery and pass it on to a DAX command . Avoid Joins \u00b6 People essentially just load all the tables into Power BI and start creating relationships. Then, they need to write complex DAX formulas to tie everything together. To make everything run faster, however, you need to combine tables and merge them before you even load them into Power BI. De-normalize data model \u00b6 Combine or append the tables that are similar in structure and used for the same purpose. For example, fact tables like sales, actuals, plan and different forecasts can be combined or appended into a single table. Incremental Refresh \u00b6 When you load data from the source into the destination (Power BI), there are two methods: Full Load or Incremental Refresh. Full Load means fetching the entire dataset each time and wiping out the previous data. Incremental refresh is an important feature to consider when working with large tables that you would like to import into memory. With Incremental Refresh, partitions are automatically created on your Power BI table based on the amount of history to retain, as well as the partition size you would like to set. Since data modelers can define the size of each partition, refreshes will be faster, more reliable, and able to build history over time. When to use incremental refresh/ Consider you have a large dataset including 20 years of data. From that dataset, probably the data from 20 years ago won\u2019t change anymore, or even the data from 5 years ago, sometimes even a year ago. So why re-processing it again? Why re-loading data that doesn\u2019t update? Incremental Refresh is the process of loading only part of the data that might change and adding it to the previous dataset, which is no longer changing. When setting up Incremental Refresh, keep in mind the following: You must create RangeStart and RangeEnd date/time parameters. These parameters must be set to a date/time data type and must be named RangeStart and RangeEnd. The initial refresh in the Power BI service will take the longest due to the creation of partitions and loading of historical data. Subsequent refreshes will only process the latest partition(s) based on how the feature was configured. Partitioning \u00b6 Incremental Load will split the table into partitions. The quantity of the partitions will be based on the settings applied at the time of Incremental refresh. For example, if you want to have the last year\u2019s data refreshed only, a yearly partition will likely be created for every year, and the one for the current year will be refreshed on a scheduled basis. Hybrid tables \u00b6 Are you using hybrid tables? Query folding \u00b6 Are you using query folding and is ti finishing completely? Avoid row-level security \u00b6 Last but not least, avoid row-level security, which can be a performance killer. Dimentional modelling \u00b6 It is a set of guidelines to design database table structure for easier and faster data retrieval. Remember Data is stored in de-normalized form here. Dimentions : Descriptive entity. Facts : Quantitative entity value. They are mostly values. Dimentional model vs Normal form models Syntax Normal form models Dimentional Purpose Transactional systems For reporting Structure Complex Less comples (Denormalized data) Operations Insert, upate and delete Mostly Select (read only) Bitmap Index Not used Heavily used Various kinds of dimentional modelling are =: Star Schema \u00b6 In this data model, you use a fact table, which is the table like sales that contains facts, meaning your measures. The fact table is related to your dimensions, which are things like your salespeople, products, business units, customers and so on. The fact table then has one-to-many relationships to your dimension tables. Snowflake schema \u00b6 In a snowflake schema, each property of entries in a dimension table is assigned to a new table, creating what seems like a snowflake. Snowflake and PowerBI \u00b6 We need to used datawarehouse in front of PowerBI Data storage mode For larger datasets, DirectQuery and Hybrid tables is a good option. Snowflake will handle the heavy lifting. Best Practices when using DW with PowerBI \u00b6 Make sure the data visualizations are use case driven, this includes the timing of the data, number of visualizations and other components. Data Model Design: How you design your data model will have a major impact on query performance in Power BI. Ideally, use a Star Schema Design for your data in Power BI with Facts and Dimensions . It is also important to have a more relational, normalized Data Vault type of an ODS model in Snowflake to get the best performance in Power BI. Use Import mode for dimension tables and Direct query mode for Fact tables. Use aggregations in Power BI for pre-aggregated data to get better query performance Keep the dashboard simple by limiting the number of data points, visuals and queries in a page to a minimum. This will also help query performance as there will be lesser number of queries to run while refreshing the visualization. Use Query reduction to limit the number of queries generated. This is especially helpful when using slicers in your visualization, where you only want the filters applied when the \u201cApply\u201d button is used. To make modeling easier, use the Assume referential integrity property on relationships. While the default property on a relationship in Power BI is to generate a left outer join , by using the \u201cAssume referential integrity\u201d property, you can force an inner join. This can make the queries faster. This property is available only in a Direct Query mode. Use bi-directional filter on relationships with discretion. More bi-directional filters mean you will generate more SQL queries. These can increase the complexity of the model and increase compute costs in Snowflake. Pruning micro partitions on large data sets in Snowflake, creating additional tables with larger data with alternate sets of keys driven by the end users ad-hoc query needs can result in better performance Auth flow \u00b6 The user logs into Power BI service using Microsoft Azure Active Directory (Azure AD). (Optional) If the identity provider is not Azure AD, then Azure AD verifies the user through SAML authentication before logging the user into the Power BI service. When the user connects to Snowflake, the Power BI service asks Azure AD to give it a token for Snowflake The Power BI service uses the embedded Snowflake driver to send the Azure AD token to Snowflake as part of the connection string. Snowflake validates the token, extracts the username from the token, maps it to the Snowflake user, and creates a Snowflake session for the Power BI service using the user\u2019s default role.","title":"Power BI \ud83d\udcc8"},{"location":"technical_stuff/powerbi/#power-bi","text":"","title":"Power BI \ud83d\udcc8"},{"location":"technical_stuff/powerbi/#concepts","text":"The five major building blocks of Power BI are: Dashboards Reports Workbooks Datasets Dataflows They're all organized into workspaces , and they're created on capacities . It's important to understand capacities and workspaces before we dig into the five building blocks, so let's start there.","title":"Concepts"},{"location":"technical_stuff/powerbi/#capacity","text":"Capacities are a core Power BI concept representing a set of resources (storage, processor, and memory) used to host and deliver your Power BI content. Capacities are either shared or reserved . A shared capacity is shared with other Microsoft customers, while a reserved capacity is reserved for a single customer.","title":"Capacity"},{"location":"technical_stuff/powerbi/#workspaces","text":"How to share powerBI content? A workspace is like a shared folder between a team of users. This can be a place to share some of the Power BI content There are two types of licenses for Power BI; capacity-based and user-based licensing. The capacity-based licensing is usually better for organizations with more than hundreds of users, and user-based licensing is good for small to medium size businesses There are two types of workspaces: My workspace (user based) Workspaces (Capacity based)","title":"Workspaces"},{"location":"technical_stuff/powerbi/#my-workspace","text":"It is the personal workspace for any Power BI customer to work with your own content. Only you have access to your My workspace. You can share dashboards and reports from your My Workspace.","title":"My workspace"},{"location":"technical_stuff/powerbi/#workspaces_1","text":"Workspaces are used to collaborate and share content with colleagues. You can add colleagues to your workspaces and collaborate on dashboards, reports, workbooks, and datasets. Warning Each workspace member needs a Power BI Pro or Premium Per User (PPU) license.","title":"Workspaces"},{"location":"technical_stuff/powerbi/#dashboard","text":"Tldr A Power BI dashboard is a single page, often called a canvas, that uses visualizations to tell a story. Because it is limited to one page, a well-designed dashboard contains only the most-important elements of that story. It is a single screen with tiles of interactive visuals, text, and graphics. A dashboard collects your most important metrics, on one screen, to tell a story or answer a question. if a business user is given permissions to the report, they can build their own dashboards too. As the capacity must share resources, limitations are imposed to ensure \"fair play\", such as the maximum model size (1 GB) and maximum daily refresh frequency ( 8 times per day ). The visualizations on a dashboard come from reports and each report is based on one dataset. In fact, one way to think of a dashboard is as an entryway into the underlying reports and datasets. Selecting a visualization takes you to the report that was used to create it.","title":"Dashboard"},{"location":"technical_stuff/powerbi/#visualization","text":"It is a type of chart built by Power BI designers. The visuals display the data from reports and datasets. How many datasets are feasible? All of the visualizations in a report come from a single dataset. Power BI Desktop can combine more than one data source into a single dataset in a report, and that report can be imported into Power BI.","title":"Visualization:"},{"location":"technical_stuff/powerbi/#report","text":"Report is one or more pages of interactive visuals, text, and graphics that together make up a single report. Power BI bases a report on a single dataset.","title":"Report"},{"location":"technical_stuff/powerbi/#tile","text":"A tile is a snapshot of your data, pinned to a dashboard by a designer. Designers can create tiles from a report, dataset, dashboard, the Q&A question box, Excel, SQL Server Reporting Services (SSRS), and more.","title":"Tile"},{"location":"technical_stuff/powerbi/#dax","text":"Data Analysis Expressions\u202f(DAX) is a programming language that is used throughout Microsoft Power BI for creating calculated columns, measures, and custom tables. It is a collection of functions, operators, and constants that can be used in a formula, or expression, to calculate and return one or more values. using DAX to calculate columns DAX allows you to augment the data that you bring in from different data sources by creating a calculated column that didn't originally exist in the data source. This feature should be used sparingly","title":"DAX"},{"location":"technical_stuff/powerbi/#dax-and-mscript","text":"Power BI has 2 mighty hands, DAX (powered by SQL Server Analysis Service ) and M Script (powered by Power Query ), to perform the ETL jobs and visualization related calculations. Both of these engines are efficient with their way of performing calculations. where not to use DAX and MScript? When it comes to performing calculations in an optimized way, both under-perform compared to database engines, since they are in-memory calculation engines best suited to play with data. They lack indexing of data, which database handles while storing of data. Also, database engine re-uses query results of last few queries, by keeping track of changing data, which is something complex for both DAX & Power Query. As the data size grows, performance difference is quite noticeable.","title":"DAX and MScript"},{"location":"technical_stuff/powerbi/#calculated-column-and-measure","text":"The fundamental difference between a calculated column and a measure is that a calculated column creates a value for each row in a table. For example, if the table has 1,000 rows, it will have 1,000 values in the calculated column. Calculated column values are stored in the Power BI .pbix file. Each calculated column will increase the space that is used in that file and potentially increase the refresh time. Measures are calculated on demand. Power BI calculates the correct value when the user requests it. Measures do not add to the overall disk space of the Power BI .pbix file .","title":"Calculated column and measure"},{"location":"technical_stuff/powerbi/#dataflow","text":"Tldr Dataflow is a collection of Power Query queries that are scheduled and executed together. It's up to you how you organize the staged data in dataflows. For example, if you need to stage some tables from Dynamics 365, you can create one dataflow that has a query for each table you want to stage. So, dataflows allow you to logically group related Power Query queries","title":"Dataflow"},{"location":"technical_stuff/powerbi/#query-folding","text":"Query folding is the ability for a Power Query query to generate a single query statement that retrieves and transforms source data. Query folding may occur for an entire Power Query query, or for a subset of its steps. When query folding cannot be achieved\u2014either partially or fully\u2014the Power Query mashup engine must compensate by processing data transformations itself. This process can involve retrieving source query results, which for large datasets is very resource intensive and slow.","title":"Query folding"},{"location":"technical_stuff/powerbi/#powerbi-service","text":"There are two hosting options for Power BI reports Cloud-based hosting (called Power BI Service or website): In the Power BI Service , organizations are separated using Tenants . Tenants can be managed by Azure Active Directory or Office 365 . Under tenants, there will be users. These users are Azure Active Directory users. On-premises hosting (called Power BI Report Server )","title":"PowerBI Service"},{"location":"technical_stuff/powerbi/#storage-mode","text":"Today PowerBI offers 4 different storage modes for tables:","title":"Storage mode"},{"location":"technical_stuff/powerbi/#import-mode","text":"In this mode, Power BI connects with underlying data source & downloads entire data from the datasource. This data is stored in Power BI model (in an in-memory cache). Fresh copy of this data can be downloaded by pressing Refresh button. PBIX file internally stores model data in compressed format. This published datset model on Power BI Service, internally is stored on Common Data Model, which is sort of Azure Managed SQL Server instance in the backend. Which is the fastest method The fastest method is Import mode . Essentially, this mode allows you to load the data once into Power BI, where it is then stored. Power BI uses the in-memory VertiPaq/xVelocity engine , which is exceptionally fast and delivers results almost immediately. It offers full DAX and PowerQuery support . A quick rule of thumb is that you can typically expect about 10x compression when importing data into Power BI The drawback is that it only permits you to refresh data 8 times per day.","title":"Import mode"},{"location":"technical_stuff/powerbi/#directquery-mode","text":"With DirectQuery datasets , no data is imported into Power BI. Instead, your Power BI dataset is simply metadata (e.g., tables, relationships) of how your model is structured to query the data source. Data is only brought into Power BI reports and dashboards at query-time (e.g., a user runs a report, or changes filters). Pros: - Dataset size limits do not apply as all data still resides in the underlying database. - Datasets do not require a scheduled refresh as data is only retrieved at query-time. Cons: - Typically, query-time performance will suffer, even when querying a cloud data warehouse like Snowflake. - Concurrency (e.g., multiple users running reports) against DirectQuery datasets could cause performance issues against the underlying database.","title":"DirectQuery mode"},{"location":"technical_stuff/powerbi/#dual-mode","text":"Dudata is imported into cache memory, but can also be served directly from the data source at the query time","title":"Dual mode"},{"location":"technical_stuff/powerbi/#hybrid-tables-composite-mode","text":"Tldr Cold data in Import mode , Hot data in DirectQuery . Hybrid tables can only be applied on a table that incremental refresh is set on it. Hybrid tables are partitioned, so their most recent partition is a DirectQuery from the data source, and their historical data is imported into other partitions. If your data source doesn\u2019t support query folding, For example, it is a CSV file. Then Power BI, when connected to it, reads the entire data anyway. Incremental Refresh doesn\u2019t need a Premium or PPU license. You can even set it up using a Power BI Pro license. However, Hybrid tables require a Power BI Premium capacity or PPU. Composite models aka hybid tables attempt to combine the best aspects from Import and DirectQuery modes into a single dataset. With Composite models, data modelers can configure the storage mode for each table in the model","title":"Hybrid tables/ Composite mode"},{"location":"technical_stuff/powerbi/#powerbi-architecture","text":"","title":"PowerBI Architecture"},{"location":"technical_stuff/powerbi/#front-end-cluster","text":"The overall technical architecture consists of two clusters: a Web Front End (WFE) cluster a Back End cluster","title":"Front-end cluster"},{"location":"technical_stuff/powerbi/#aad","text":"The WFE cluster manages connectivity and authentication. Power BI relies on Azure Active Directory (AAD) to manage account authentication and management.","title":"AAD"},{"location":"technical_stuff/powerbi/#atm","text":"Power BI uses the Azure Traffic Manager (ATM) to direct user traffic to the nearest data center. Which data center is used is determined by the DNS record of the client attempting to connect. The DNS Service can communicate with the Azure Traffic Manager to find the nearest data center with a Power BI deployment","title":"ATM"},{"location":"technical_stuff/powerbi/#cdn","text":"Power BI uses the Azure Content Delivery Network (CDN) to deliver the necessary static content and files to end users based on their geographical locale. The WFE cluster nearest to the user manages the user login and authentication and provides an access token to the user once authentication is successful. The ASP.NET component within the WFE cluster parses the request to determine which organization the user belongs to, and then consults the Power BI Global Service","title":"CDN"},{"location":"technical_stuff/powerbi/#global-service","text":"The Global Service is implemented as a single Azure Table that is shared among all worldwide WFE and Back End clusters. This service maps users and customer organizations to the datacenter that hosts their Power BI tenant. The WFE specifies to the browser which backend cluster houses the organization's tenant. Once a user is authenticated, subsequent client interactions occur with the backend cluster directly and the WFE cluster is not used.","title":"Global Service"},{"location":"technical_stuff/powerbi/#back-end-cluster","text":"The backend cluster manages all actions the user does in Power BI Service, including visualizations, dashboards,datasets, reports, data storage, data connections, data refresh, and others.","title":"Back-end cluster"},{"location":"technical_stuff/powerbi/#gateway-and-apim","text":"The Gateway Role acts as a gateway between user requests and the Power BI service. As you can see in the diagram, only the Gateway Role and Azure API Management (APIM) services are accessible from the public Internet. When an authenticated user connects to the Power BI Service, the connection and any request by the client is accepted and managed by the Gateway Role, which then interacts on the user's behalf with the rest of the Power BI Service. For example, when a client attempts to view a dashboard, the Gateway Role accepts that request, and then sends a request to the Presentation Role to retrieve the data needed by the browser to render the dashboard.","title":"Gateway and APIM"},{"location":"technical_stuff/powerbi/#data-storage","text":"As far as data storage in the cloud goes, Power BI uses two primary repositories for storing and managing data. Data that is uploaded from users or generated by dataflows is stored in Azure BLOB storage, but all the metadata definitions (dashboards, reports, recent data sources, workspaces, organizational information, tenant information) are stored in Azure SQL Database.","title":"Data Storage"},{"location":"technical_stuff/powerbi/#performance-optimization-of-powerbi","text":"check this ZerbaBI report","title":"Performance Optimization of PowerBI"},{"location":"technical_stuff/powerbi/#analyze-the-results","text":"Power BI Performance Analyzer : The natural starting point is Power BI Performance Analyzer, a built-in feature of Power BI Desktop. Select the View ribbon, and then select Performance Analyzer to display the Performance Analyzer pane. DAX Studio : You can analyze performance using in another tool called DAX Studio . This is a a great tool for performance analysis.","title":"Analyze the results"},{"location":"technical_stuff/powerbi/#using-direct-query","text":"There's another situation that can slow down your report. You might be creating tables that use DirectQuery as the connection mode . This means that the data is not loaded into Power BI. Instead, it is loaded using a query that runs on a database server. This means additional time to fetch the data. This shows up as another item on the report, since Power BI first needs to fetch the data, process it using a PowerQuery and pass it on to a DAX command .","title":"Using direct query?"},{"location":"technical_stuff/powerbi/#avoid-joins","text":"People essentially just load all the tables into Power BI and start creating relationships. Then, they need to write complex DAX formulas to tie everything together. To make everything run faster, however, you need to combine tables and merge them before you even load them into Power BI.","title":"Avoid Joins"},{"location":"technical_stuff/powerbi/#de-normalize-data-model","text":"Combine or append the tables that are similar in structure and used for the same purpose. For example, fact tables like sales, actuals, plan and different forecasts can be combined or appended into a single table.","title":"De-normalize data model"},{"location":"technical_stuff/powerbi/#incremental-refresh","text":"When you load data from the source into the destination (Power BI), there are two methods: Full Load or Incremental Refresh. Full Load means fetching the entire dataset each time and wiping out the previous data. Incremental refresh is an important feature to consider when working with large tables that you would like to import into memory. With Incremental Refresh, partitions are automatically created on your Power BI table based on the amount of history to retain, as well as the partition size you would like to set. Since data modelers can define the size of each partition, refreshes will be faster, more reliable, and able to build history over time. When to use incremental refresh/ Consider you have a large dataset including 20 years of data. From that dataset, probably the data from 20 years ago won\u2019t change anymore, or even the data from 5 years ago, sometimes even a year ago. So why re-processing it again? Why re-loading data that doesn\u2019t update? Incremental Refresh is the process of loading only part of the data that might change and adding it to the previous dataset, which is no longer changing. When setting up Incremental Refresh, keep in mind the following: You must create RangeStart and RangeEnd date/time parameters. These parameters must be set to a date/time data type and must be named RangeStart and RangeEnd. The initial refresh in the Power BI service will take the longest due to the creation of partitions and loading of historical data. Subsequent refreshes will only process the latest partition(s) based on how the feature was configured.","title":"Incremental Refresh"},{"location":"technical_stuff/powerbi/#partitioning","text":"Incremental Load will split the table into partitions. The quantity of the partitions will be based on the settings applied at the time of Incremental refresh. For example, if you want to have the last year\u2019s data refreshed only, a yearly partition will likely be created for every year, and the one for the current year will be refreshed on a scheduled basis.","title":"Partitioning"},{"location":"technical_stuff/powerbi/#hybrid-tables","text":"Are you using hybrid tables?","title":"Hybrid tables"},{"location":"technical_stuff/powerbi/#query-folding_1","text":"Are you using query folding and is ti finishing completely?","title":"Query folding"},{"location":"technical_stuff/powerbi/#avoid-row-level-security","text":"Last but not least, avoid row-level security, which can be a performance killer.","title":"Avoid row-level security"},{"location":"technical_stuff/powerbi/#dimentional-modelling","text":"It is a set of guidelines to design database table structure for easier and faster data retrieval. Remember Data is stored in de-normalized form here. Dimentions : Descriptive entity. Facts : Quantitative entity value. They are mostly values. Dimentional model vs Normal form models Syntax Normal form models Dimentional Purpose Transactional systems For reporting Structure Complex Less comples (Denormalized data) Operations Insert, upate and delete Mostly Select (read only) Bitmap Index Not used Heavily used Various kinds of dimentional modelling are =:","title":"Dimentional modelling"},{"location":"technical_stuff/powerbi/#star-schema","text":"In this data model, you use a fact table, which is the table like sales that contains facts, meaning your measures. The fact table is related to your dimensions, which are things like your salespeople, products, business units, customers and so on. The fact table then has one-to-many relationships to your dimension tables.","title":"Star Schema"},{"location":"technical_stuff/powerbi/#snowflake-schema","text":"In a snowflake schema, each property of entries in a dimension table is assigned to a new table, creating what seems like a snowflake.","title":"Snowflake schema"},{"location":"technical_stuff/powerbi/#snowflake-and-powerbi","text":"We need to used datawarehouse in front of PowerBI Data storage mode For larger datasets, DirectQuery and Hybrid tables is a good option. Snowflake will handle the heavy lifting.","title":"Snowflake and PowerBI"},{"location":"technical_stuff/powerbi/#best-practices-when-using-dw-with-powerbi","text":"Make sure the data visualizations are use case driven, this includes the timing of the data, number of visualizations and other components. Data Model Design: How you design your data model will have a major impact on query performance in Power BI. Ideally, use a Star Schema Design for your data in Power BI with Facts and Dimensions . It is also important to have a more relational, normalized Data Vault type of an ODS model in Snowflake to get the best performance in Power BI. Use Import mode for dimension tables and Direct query mode for Fact tables. Use aggregations in Power BI for pre-aggregated data to get better query performance Keep the dashboard simple by limiting the number of data points, visuals and queries in a page to a minimum. This will also help query performance as there will be lesser number of queries to run while refreshing the visualization. Use Query reduction to limit the number of queries generated. This is especially helpful when using slicers in your visualization, where you only want the filters applied when the \u201cApply\u201d button is used. To make modeling easier, use the Assume referential integrity property on relationships. While the default property on a relationship in Power BI is to generate a left outer join , by using the \u201cAssume referential integrity\u201d property, you can force an inner join. This can make the queries faster. This property is available only in a Direct Query mode. Use bi-directional filter on relationships with discretion. More bi-directional filters mean you will generate more SQL queries. These can increase the complexity of the model and increase compute costs in Snowflake. Pruning micro partitions on large data sets in Snowflake, creating additional tables with larger data with alternate sets of keys driven by the end users ad-hoc query needs can result in better performance","title":"Best Practices when using DW with PowerBI"},{"location":"technical_stuff/powerbi/#auth-flow","text":"The user logs into Power BI service using Microsoft Azure Active Directory (Azure AD). (Optional) If the identity provider is not Azure AD, then Azure AD verifies the user through SAML authentication before logging the user into the Power BI service. When the user connects to Snowflake, the Power BI service asks Azure AD to give it a token for Snowflake The Power BI service uses the embedded Snowflake driver to send the Azure AD token to Snowflake as part of the connection string. Snowflake validates the token, extracts the username from the token, maps it to the Snowflake user, and creates a Snowflake session for the Power BI service using the user\u2019s default role.","title":"Auth flow"},{"location":"technical_stuff/snowflake/","text":"Snowflake \u00b6 Snowflake is true Software-as-a-Service (SaaS) for data Its purely cloud based. All components of Snowflake\u2019s service (other than optional command line clients, drivers, and connectors), run in public cloud infrastructures. Snowflake enables data storage, processing, and analytic solutions that are faster, easier to use, and far more flexible than traditional offerings. Warning Snowflake cannot be run on private cloud infrastructures (on-premises or hosted). Architecture \u00b6 Snowflake\u2019s architecture is a hybrid of shared-disk + shared-nothing architecture Shared-disk architecture : Snowflake uses a central data repository for persisted data that is accessible from all compute nodes in the platform. Shared-nothing architecture : Snowflake processes queries using MPP (massively parallel processing) compute clusters where each node in the cluster stores a portion of the entire data set locally. Snowflake\u2019s unique architecture consists of three key layers: Database Storage Query Processing Cloud Services Database Storage \u00b6 Snowflake\u2019s centralized database storage layer holds all data, including structured and semi-structured data. As data is loaded into Snowflake, it is optimally reorganized into a compressed, columnar format and stored and maintained in Snowflake databases. Can I view Snowflake data? The data objects stored by Snowflake are not directly visible nor accessible by customers; they are only accessible through SQL query operations run using Snowflake Data stored in Snowflake databases is always compressed and encrypted. Snowflake takes care of managing every aspect of how the data is stored. How data is stored in snowflake? Snowflake automatically organizes stored data into micro-partitions : which are an optimized, immutable, compressed columnar format which is encrypted using AES-256 encryption Zero-Copy Cloning \u00b6 Zero-copy cloning offers the user a way to snapshot a Snowflake database, schema, or table along with its associated data. There is no additional storage charge until changes are made to the cloned object, because zero-copy data cloning is a metadataonly operation Time Travel \u00b6 Time Travel allows you to restore a previous version of a database, table, or schema. This is an incredibly helpful feature that gives you an opportunity to fix previous edits that were done incorrectly or restore items deleted in error. Caching \u00b6 When you submit a query, Snowflake checks to see whether that query has been previously run and, if so, whether the results are still cached. Snowflake will use the cached result set if it is still available rather than executing the query you just submitted There are three Snowflake caching types: Query result cache : The fastest way to retrieve data from Snowflake is by using the query result cache. The results of a Snowflake query are cached, or persisted, for 24 hours and then purged. The result cache is fully managed by the Snowflake global cloud services (GCS) layer, as shown in Figure 2-18, and is available across all virtual warehouses since virtual warehouses have access to all data. Virtual warehouse cache : Running virtual warehouses use SSD storage to store the micro-partitions that are pulled from the centralized database storage layer when a query is processed. Danger This cache is dropped once the virtual warehouse is suspended, so you\u2019ll want to consider the trade-off between the credits that will be consumed by keeping a virtual warehouse running and the value from maintaining the cache of data from previous queries to improve performance. Metadata cache : The Snowflake metadata repository includes table definitions and references to the micro-partition files for that table. The range of values in terms of MIN and MAX, the NULL count, and the n Query Processing \u00b6 Query execution is performed in the processing layer. Snowflake processes queries using \u201cvirtual warehouses\u201d. Each virtual warehouse is an MPP compute cluster composed of multiple compute nodes allocated by Snowflake from a cloud provider. The Snowflake compute resources are created and deployed on demand to the Snowflake user, to whom the process is transparent. Tip Snowflake\u2019s unique architecture allows for separation of storage and compute, which means any virtual warehouse can access the same data as another, without any contention or impact on performance of the other warehouses. This is because each Snowflake virtual warehouse operates independently and does not share compute resources with other virtual warehouses Cloud services \u00b6 All interactions with data in a Snowflake instance begin in the cloud services layer, also called the global services layer. The Snowflake cloud services layer is a collection of services that coordinate activities such as authentication, access control, and encryption. Each time a user requests to log in, the request is handled by the cloud services layer. When a user submits a Snowflake query, the SQL query will be sent to the cloud services layer optimizer before being sent to the compute layer for processing. Managing cache across AZ's \u00b6 The Snowflake cloud services layer runs across multiple availability zones in each cloud provider region and holds the result cache, a cached copy of the executed query results . The metadata required for query optimization or data filtering are also stored in the cloud services layer. Auto Suspend and Auto resume \u00b6 Auto Suspend is the number of seconds that the virtual warehouse will wait if no queries need to be executed before going offline. Auto Resume will restart the virtual warehouse once there is an operation that requires compute resources. Notes about snowflake \u00b6 A multicluster virtual warehouse allows Snowflake to scale in and out automatically. We can use different kinds of Warehouses in the snowflake to hanlde different kinds of requirements Common Table Expressions: \u00b6 A Common Table Expression, also called as CTE in short form, is a temporary named result set that you can reference within a SELECT, INSERT, UPDATE, or DELETE statement. The CTE can also be used in a View. The WITH clause, an optional clause that precedes the SELECT statement, is used to define common table expressions (CTEs) which are referenced in the FROM clause WITH expression_name [ ( column_name [,... n ] ) ] AS ( CTE_query_definition ) To view the CTE result we use a Select query with the CTE expression name. Select [ Column1 , Column2 , Column3 \u2026 ..] from expression_name Commands \u00b6 Create a warehouse USE ROLE SYSADMIN ; CREATE WAREHOUSE AmarBlog_WH_S WITH WAREHOUSE_SIZE = MEDIUM AUTO_SUSPEND = 300 AUTO_RESUME = true INITIALLY_SUSPENDED = true ; Alter warehouse size USE ROLE SYSADMIN ; ALTER WAREHOUSE AmarBlog_WH_S SET WAREHOUSE_SIZE = LARGE ; Select a warehouse USE WAREHOUSE AmarBlog_WH_M","title":"Snowflake \u2744\ufe0f"},{"location":"technical_stuff/snowflake/#snowflake","text":"Snowflake is true Software-as-a-Service (SaaS) for data Its purely cloud based. All components of Snowflake\u2019s service (other than optional command line clients, drivers, and connectors), run in public cloud infrastructures. Snowflake enables data storage, processing, and analytic solutions that are faster, easier to use, and far more flexible than traditional offerings. Warning Snowflake cannot be run on private cloud infrastructures (on-premises or hosted).","title":"Snowflake"},{"location":"technical_stuff/snowflake/#architecture","text":"Snowflake\u2019s architecture is a hybrid of shared-disk + shared-nothing architecture Shared-disk architecture : Snowflake uses a central data repository for persisted data that is accessible from all compute nodes in the platform. Shared-nothing architecture : Snowflake processes queries using MPP (massively parallel processing) compute clusters where each node in the cluster stores a portion of the entire data set locally. Snowflake\u2019s unique architecture consists of three key layers: Database Storage Query Processing Cloud Services","title":"Architecture"},{"location":"technical_stuff/snowflake/#database-storage","text":"Snowflake\u2019s centralized database storage layer holds all data, including structured and semi-structured data. As data is loaded into Snowflake, it is optimally reorganized into a compressed, columnar format and stored and maintained in Snowflake databases. Can I view Snowflake data? The data objects stored by Snowflake are not directly visible nor accessible by customers; they are only accessible through SQL query operations run using Snowflake Data stored in Snowflake databases is always compressed and encrypted. Snowflake takes care of managing every aspect of how the data is stored. How data is stored in snowflake? Snowflake automatically organizes stored data into micro-partitions : which are an optimized, immutable, compressed columnar format which is encrypted using AES-256 encryption","title":"Database Storage"},{"location":"technical_stuff/snowflake/#zero-copy-cloning","text":"Zero-copy cloning offers the user a way to snapshot a Snowflake database, schema, or table along with its associated data. There is no additional storage charge until changes are made to the cloned object, because zero-copy data cloning is a metadataonly operation","title":"Zero-Copy Cloning"},{"location":"technical_stuff/snowflake/#time-travel","text":"Time Travel allows you to restore a previous version of a database, table, or schema. This is an incredibly helpful feature that gives you an opportunity to fix previous edits that were done incorrectly or restore items deleted in error.","title":"Time Travel"},{"location":"technical_stuff/snowflake/#caching","text":"When you submit a query, Snowflake checks to see whether that query has been previously run and, if so, whether the results are still cached. Snowflake will use the cached result set if it is still available rather than executing the query you just submitted There are three Snowflake caching types: Query result cache : The fastest way to retrieve data from Snowflake is by using the query result cache. The results of a Snowflake query are cached, or persisted, for 24 hours and then purged. The result cache is fully managed by the Snowflake global cloud services (GCS) layer, as shown in Figure 2-18, and is available across all virtual warehouses since virtual warehouses have access to all data. Virtual warehouse cache : Running virtual warehouses use SSD storage to store the micro-partitions that are pulled from the centralized database storage layer when a query is processed. Danger This cache is dropped once the virtual warehouse is suspended, so you\u2019ll want to consider the trade-off between the credits that will be consumed by keeping a virtual warehouse running and the value from maintaining the cache of data from previous queries to improve performance. Metadata cache : The Snowflake metadata repository includes table definitions and references to the micro-partition files for that table. The range of values in terms of MIN and MAX, the NULL count, and the n","title":"Caching"},{"location":"technical_stuff/snowflake/#query-processing","text":"Query execution is performed in the processing layer. Snowflake processes queries using \u201cvirtual warehouses\u201d. Each virtual warehouse is an MPP compute cluster composed of multiple compute nodes allocated by Snowflake from a cloud provider. The Snowflake compute resources are created and deployed on demand to the Snowflake user, to whom the process is transparent. Tip Snowflake\u2019s unique architecture allows for separation of storage and compute, which means any virtual warehouse can access the same data as another, without any contention or impact on performance of the other warehouses. This is because each Snowflake virtual warehouse operates independently and does not share compute resources with other virtual warehouses","title":"Query Processing"},{"location":"technical_stuff/snowflake/#cloud-services","text":"All interactions with data in a Snowflake instance begin in the cloud services layer, also called the global services layer. The Snowflake cloud services layer is a collection of services that coordinate activities such as authentication, access control, and encryption. Each time a user requests to log in, the request is handled by the cloud services layer. When a user submits a Snowflake query, the SQL query will be sent to the cloud services layer optimizer before being sent to the compute layer for processing.","title":"Cloud services"},{"location":"technical_stuff/snowflake/#managing-cache-across-azs","text":"The Snowflake cloud services layer runs across multiple availability zones in each cloud provider region and holds the result cache, a cached copy of the executed query results . The metadata required for query optimization or data filtering are also stored in the cloud services layer.","title":"Managing cache across AZ's"},{"location":"technical_stuff/snowflake/#auto-suspend-and-auto-resume","text":"Auto Suspend is the number of seconds that the virtual warehouse will wait if no queries need to be executed before going offline. Auto Resume will restart the virtual warehouse once there is an operation that requires compute resources.","title":"Auto Suspend and Auto resume"},{"location":"technical_stuff/snowflake/#notes-about-snowflake","text":"A multicluster virtual warehouse allows Snowflake to scale in and out automatically. We can use different kinds of Warehouses in the snowflake to hanlde different kinds of requirements","title":"Notes about snowflake"},{"location":"technical_stuff/snowflake/#common-table-expressions","text":"A Common Table Expression, also called as CTE in short form, is a temporary named result set that you can reference within a SELECT, INSERT, UPDATE, or DELETE statement. The CTE can also be used in a View. The WITH clause, an optional clause that precedes the SELECT statement, is used to define common table expressions (CTEs) which are referenced in the FROM clause WITH expression_name [ ( column_name [,... n ] ) ] AS ( CTE_query_definition ) To view the CTE result we use a Select query with the CTE expression name. Select [ Column1 , Column2 , Column3 \u2026 ..] from expression_name","title":"Common Table Expressions:"},{"location":"technical_stuff/snowflake/#commands","text":"Create a warehouse USE ROLE SYSADMIN ; CREATE WAREHOUSE AmarBlog_WH_S WITH WAREHOUSE_SIZE = MEDIUM AUTO_SUSPEND = 300 AUTO_RESUME = true INITIALLY_SUSPENDED = true ; Alter warehouse size USE ROLE SYSADMIN ; ALTER WAREHOUSE AmarBlog_WH_S SET WAREHOUSE_SIZE = LARGE ; Select a warehouse USE WAREHOUSE AmarBlog_WH_M","title":"Commands"},{"location":"technical_stuff/sql_and_nosql/","text":"DB, DW, Data Lakes, Data Lakehouse \u00b6 Databases \u00b6 Databases are for OLTP (operational needs). In OLTP, the emphasis is on fast processing, because OLTP databases are read, written, and updated frequently. Characterstics of OLTP We need Normalized databases for efficiency Some OLTP examples are credit card activity, order entry, and ATM transactions. Data Warehouses \u00b6 Data Warehouses are for OLAP (Informational needs) OLAP applies complex queries to large amounts of historical data, aggregated from OLTP databases and other sources, for data mining, analytics, and business intelligence projects. In OLAP, the emphasis is on response time to these complex queries . Each query involves one or more columns of data aggregated from many rows. Data warehousing helps you answer those tough analytical questions that your board may be asking that aren\u2019t possible to address with your standard data analytics tool. Characterstics of OLAP OLAP is a read heavy system Data is non-volatile as it wont be changes often. OLAP is used on data warehouse or some other centralized data store. Data periodically refreshed with scheduled, long-running batch jobs For backup : Lost data can be reloaded from OLTP database as needed in lieu of regular backups. Denormalized databases for analysis (most data warehouses favor denormalized data models, where each table contains as many related attributes as possible. In this way, all the information can be processed by a single pass through the data.) An OLAP database uses a multidimensional data model, which includes features of relational, navigational, and hierarchical databases. It also consists of an OLAP cube which consists of multiple types of data. Unlike a database, the information isn\u2019t updated in real-time and is better for data analysis of broader trends. Data Lake \u00b6 Data lake is for storing any and all raw data that may or may not yet have an intended use case. A data warehouse, on the other hand, holds data that has already been processed and filtered, so it\u2019s ready to be used and analyzed. Data LakeHouse \u00b6 A data lakehouse is an open data management architecture that combines the flexibility and cost-efficiency of data lakes with the data management and structure features of data warehouses, all on one data platform. Simply put: The data lakehouse is the only data architecture that stores all data \u2014 unstructured, semi-structured, AND structured \u2014 in your data lake while still providing the data quality and data governance standards of a data warehouse. How to create OLAP view from OLTP? \u00b6 In the first step, new transactions are copied from the operational source systems and loaded into a temporary staging area. Data in the staging area is then transformed to create fact and dimension tables that are used to build OLAP cube structures. These cubes contain precalculated aggregate structures that contain summary information which must be updated as new facts are added to the fact tables. The information in the OLAP cubes is then accessed from a graphical front-end tool through the security and data services layers. The precise meaning of data in any part of the system is stored in a separate metadata registry database that ensures data is used and interpreted consistently despite the many layers of transformation. Issues in moving data The ETL tools to move data between operational and analytical systems still usually run on single processors, perform costly join operations, and limit the amount of data that can be moved each night between the operational and analytical systems. These challenges and costs are even greater when organizations lack strong data governance policies or have inconsistent category definitions OLAP Concepts \u00b6 Fact table \u00b6 A central table of events that contains foreign keys to other tables and integer and decimal values called measures. Dimension table \u00b6 A table used to categorize every fact. Examples of dimensions include time, geography, product, or promotion. Star schema \u00b6 An arrangement of tables with one fact table surrounded by dimension tables. Each transaction isrepresented by a single row in the central fact table . Categories \u00b6 A way to divide all the facts into two or more classes. For example, products may have a Seasonal category indicating they\u2019re only stocked part of the year. Measures \u00b6 A number used in a column of a fact table that you can sum or average. Measures are usually things like sales counts or prices. Aggregates \u00b6 Precomputed sums used by OLAP systems to quickly display results to users. Data warehouse performance reasons \ud83e\udd14 \u00b6 We will take Amazon Redshift as an example. Info Amazon Redshift is a column-oriented, fully managed, petabyte-scale data warehouse that makes it simple and cost-effective to analyze all your data using your existing business intelligence tools. MPP \u00b6 Massively parallel processing (MPP) enables fast run of the most complex queries operating on large amounts of data. Multiple compute nodes handle all query processing leading up to final result aggregation, with each core of each node running the same compiled query segments on portions of the entire data. Columnar data storage \u00b6 Storing database table information in a columnar fashion reduces the number of disk I/O requests and reduces the amount of data you need to load from disk. Data compression \u00b6 Because columnar storage stores similar data sequentially, Amazon Redshift is able to apply adaptive compression encodings specifically tied to columnar data types Query optimizer \u00b6 The Amazon Redshift query run engine incorporates a query optimizer that is MPP-aware and also takes advantage of the columnar-oriented data storage. Results Caching \u00b6 To reduce query runtime and improve system performance, Amazon Redshift caches the results of certain types of queries in memory on the leader node. When a user submits a query, Amazon Redshift checks the results cache for a valid, cached copy of the query results. If a match is found in the result cache, Amazon Redshift uses the cached results and doesn't run the query. Result caching is transparent to the user. No SQL data patterns \u00b6 Key-value \u00b6 At its core, S3 is a simple key-value store with some enhanced features such as metadata and access control A key-value store is a simple database that when presented with a simple string (the key) returns an arbitrary large BLOB of data (the value). Key-value stores have no query language; they provide a way to add and remove key-value pairs (a combination of key and value where the key is bound to the value until a new value is assigned) into/from a database. The dictionary is a simple key-value store where word entries represent keys and definitions represent values. Various operations on Key value are One of the benefits of not specifying a data type for the value of a key-value store is that you can store any data type that you want in the value. The system will store the information as a BLOB and return the same BLOB when a GET (retrieval) request is made. It\u2019s up to the application to determine what type of data is being used, such as a string, XML file, or binary image. Document store \u00b6 Column Family \u00b6 Examples: Cassandra PostgreSQL HBase Google BigTable Amazon Redshift: Amazon Redshift is based on PostgreSQL. While a relational database is optimized for storing rows of data, typically for transactional applications, a columnar database is optimized for fast retrieval of columns of data, typically in analytical applications. Remember Column-oriented storage for database tables is an important factor in analytic query performance because it drastically reduces the overall disk I/O requirements and reduces the amount of data you need to load from disk. This is how data is stored for RDBMS in disk (sectors and blocks) Data storage pattern for columnar family in disk An added advantage is that, since each block holds the same type of data, block data can use a compression scheme selected specifically for the column data type, further reducing disk space and I/O. Graph DB \u00b6 A graph store is a system that contains a sequence of nodes and relationships that, when combined, create a graph. You know that in a key-value store there two data fields: the key and the value. In contrast, a graph store has three data fields: nodes, relationships, and properties.","title":"DB, DW and Data Lake \ud83d\udcbe"},{"location":"technical_stuff/sql_and_nosql/#db-dw-data-lakes-data-lakehouse","text":"","title":"DB, DW, Data Lakes, Data Lakehouse"},{"location":"technical_stuff/sql_and_nosql/#databases","text":"Databases are for OLTP (operational needs). In OLTP, the emphasis is on fast processing, because OLTP databases are read, written, and updated frequently. Characterstics of OLTP We need Normalized databases for efficiency Some OLTP examples are credit card activity, order entry, and ATM transactions.","title":"Databases"},{"location":"technical_stuff/sql_and_nosql/#data-warehouses","text":"Data Warehouses are for OLAP (Informational needs) OLAP applies complex queries to large amounts of historical data, aggregated from OLTP databases and other sources, for data mining, analytics, and business intelligence projects. In OLAP, the emphasis is on response time to these complex queries . Each query involves one or more columns of data aggregated from many rows. Data warehousing helps you answer those tough analytical questions that your board may be asking that aren\u2019t possible to address with your standard data analytics tool. Characterstics of OLAP OLAP is a read heavy system Data is non-volatile as it wont be changes often. OLAP is used on data warehouse or some other centralized data store. Data periodically refreshed with scheduled, long-running batch jobs For backup : Lost data can be reloaded from OLTP database as needed in lieu of regular backups. Denormalized databases for analysis (most data warehouses favor denormalized data models, where each table contains as many related attributes as possible. In this way, all the information can be processed by a single pass through the data.) An OLAP database uses a multidimensional data model, which includes features of relational, navigational, and hierarchical databases. It also consists of an OLAP cube which consists of multiple types of data. Unlike a database, the information isn\u2019t updated in real-time and is better for data analysis of broader trends.","title":"Data Warehouses"},{"location":"technical_stuff/sql_and_nosql/#data-lake","text":"Data lake is for storing any and all raw data that may or may not yet have an intended use case. A data warehouse, on the other hand, holds data that has already been processed and filtered, so it\u2019s ready to be used and analyzed.","title":"Data Lake"},{"location":"technical_stuff/sql_and_nosql/#data-lakehouse","text":"A data lakehouse is an open data management architecture that combines the flexibility and cost-efficiency of data lakes with the data management and structure features of data warehouses, all on one data platform. Simply put: The data lakehouse is the only data architecture that stores all data \u2014 unstructured, semi-structured, AND structured \u2014 in your data lake while still providing the data quality and data governance standards of a data warehouse.","title":"Data LakeHouse"},{"location":"technical_stuff/sql_and_nosql/#how-to-create-olap-view-from-oltp","text":"In the first step, new transactions are copied from the operational source systems and loaded into a temporary staging area. Data in the staging area is then transformed to create fact and dimension tables that are used to build OLAP cube structures. These cubes contain precalculated aggregate structures that contain summary information which must be updated as new facts are added to the fact tables. The information in the OLAP cubes is then accessed from a graphical front-end tool through the security and data services layers. The precise meaning of data in any part of the system is stored in a separate metadata registry database that ensures data is used and interpreted consistently despite the many layers of transformation. Issues in moving data The ETL tools to move data between operational and analytical systems still usually run on single processors, perform costly join operations, and limit the amount of data that can be moved each night between the operational and analytical systems. These challenges and costs are even greater when organizations lack strong data governance policies or have inconsistent category definitions","title":"How to create OLAP view from OLTP?"},{"location":"technical_stuff/sql_and_nosql/#olap-concepts","text":"","title":"OLAP Concepts"},{"location":"technical_stuff/sql_and_nosql/#fact-table","text":"A central table of events that contains foreign keys to other tables and integer and decimal values called measures.","title":"Fact table"},{"location":"technical_stuff/sql_and_nosql/#dimension-table","text":"A table used to categorize every fact. Examples of dimensions include time, geography, product, or promotion.","title":"Dimension table"},{"location":"technical_stuff/sql_and_nosql/#star-schema","text":"An arrangement of tables with one fact table surrounded by dimension tables. Each transaction isrepresented by a single row in the central fact table .","title":"Star schema"},{"location":"technical_stuff/sql_and_nosql/#categories","text":"A way to divide all the facts into two or more classes. For example, products may have a Seasonal category indicating they\u2019re only stocked part of the year.","title":"Categories"},{"location":"technical_stuff/sql_and_nosql/#measures","text":"A number used in a column of a fact table that you can sum or average. Measures are usually things like sales counts or prices.","title":"Measures"},{"location":"technical_stuff/sql_and_nosql/#aggregates","text":"Precomputed sums used by OLAP systems to quickly display results to users.","title":"Aggregates"},{"location":"technical_stuff/sql_and_nosql/#data-warehouse-performance-reasons","text":"We will take Amazon Redshift as an example. Info Amazon Redshift is a column-oriented, fully managed, petabyte-scale data warehouse that makes it simple and cost-effective to analyze all your data using your existing business intelligence tools.","title":"Data warehouse performance reasons \ud83e\udd14"},{"location":"technical_stuff/sql_and_nosql/#mpp","text":"Massively parallel processing (MPP) enables fast run of the most complex queries operating on large amounts of data. Multiple compute nodes handle all query processing leading up to final result aggregation, with each core of each node running the same compiled query segments on portions of the entire data.","title":"MPP"},{"location":"technical_stuff/sql_and_nosql/#columnar-data-storage","text":"Storing database table information in a columnar fashion reduces the number of disk I/O requests and reduces the amount of data you need to load from disk.","title":"Columnar data storage"},{"location":"technical_stuff/sql_and_nosql/#data-compression","text":"Because columnar storage stores similar data sequentially, Amazon Redshift is able to apply adaptive compression encodings specifically tied to columnar data types","title":"Data compression"},{"location":"technical_stuff/sql_and_nosql/#query-optimizer","text":"The Amazon Redshift query run engine incorporates a query optimizer that is MPP-aware and also takes advantage of the columnar-oriented data storage.","title":"Query optimizer"},{"location":"technical_stuff/sql_and_nosql/#results-caching","text":"To reduce query runtime and improve system performance, Amazon Redshift caches the results of certain types of queries in memory on the leader node. When a user submits a query, Amazon Redshift checks the results cache for a valid, cached copy of the query results. If a match is found in the result cache, Amazon Redshift uses the cached results and doesn't run the query. Result caching is transparent to the user.","title":"Results Caching"},{"location":"technical_stuff/sql_and_nosql/#no-sql-data-patterns","text":"","title":"No SQL data patterns"},{"location":"technical_stuff/sql_and_nosql/#key-value","text":"At its core, S3 is a simple key-value store with some enhanced features such as metadata and access control A key-value store is a simple database that when presented with a simple string (the key) returns an arbitrary large BLOB of data (the value). Key-value stores have no query language; they provide a way to add and remove key-value pairs (a combination of key and value where the key is bound to the value until a new value is assigned) into/from a database. The dictionary is a simple key-value store where word entries represent keys and definitions represent values. Various operations on Key value are One of the benefits of not specifying a data type for the value of a key-value store is that you can store any data type that you want in the value. The system will store the information as a BLOB and return the same BLOB when a GET (retrieval) request is made. It\u2019s up to the application to determine what type of data is being used, such as a string, XML file, or binary image.","title":"Key-value"},{"location":"technical_stuff/sql_and_nosql/#document-store","text":"","title":"Document store"},{"location":"technical_stuff/sql_and_nosql/#column-family","text":"Examples: Cassandra PostgreSQL HBase Google BigTable Amazon Redshift: Amazon Redshift is based on PostgreSQL. While a relational database is optimized for storing rows of data, typically for transactional applications, a columnar database is optimized for fast retrieval of columns of data, typically in analytical applications. Remember Column-oriented storage for database tables is an important factor in analytic query performance because it drastically reduces the overall disk I/O requirements and reduces the amount of data you need to load from disk. This is how data is stored for RDBMS in disk (sectors and blocks) Data storage pattern for columnar family in disk An added advantage is that, since each block holds the same type of data, block data can use a compression scheme selected specifically for the column data type, further reducing disk space and I/O.","title":"Column Family"},{"location":"technical_stuff/sql_and_nosql/#graph-db","text":"A graph store is a system that contains a sequence of nodes and relationships that, when combined, create a graph. You know that in a key-value store there two data fields: the key and the value. In contrast, a graph store has three data fields: nodes, relationships, and properties.","title":"Graph DB"},{"location":"technical_stuff/blogs/cpanel/gitAndCpanel/","text":"How to clone a private Github repo into Cpanel \u00b6 The goal of this blog post is to help you in setting up your Github private repo on Cpanel, so that when you make a push to your blog post, then those changes are synced automatically. So let's get started \ud83d\ude00 Generate a key in Cpanel \u00b6 The goal is the create a public and private key pair. Then copy the public key and save it in Github. First we will create a pair in Cpanel. For this go to Cpanel \u2192 Security \u2192 SSH Access \u2192 Manage Keys \u2192 Generate New Key (enter a strong password) Copy public key from Cpanel \u00b6 After generating a public-private key pair, now is the time to copy the public key. For doing this, go to SSH Access \u2192 Manage SSH Keys \u2192 Public keys \u2192 Authorize (make authorize) \u2192 Go back \u2192 Press view/download \u2192 Download key \u2192 open it using a text editor and copy it. Copy public key to Github \u00b6 The goal is to save the public key in your private git repo, so that Cpanel can fetch data from it. Go to your Github \u2192 Your private repo \u2192 Settings \u2192 Deploy Keys \u2192 add deploy key \u2192 Provide any title and paste the public key. Add an .cpanel.yml file. \u00b6 Before we go ahead, we need to make sure the files we add from the Github are deployed to cpanel. You will need to know 2 important things here yourUserName/repositories : this is the folder where all the repo's will get synced. yourUserName/public_html : This is the folder from where the actual site content is served. So, this should contain an index.html at minimum. The .cpanel.yml file is a basic configuration file. It can be used to copy the contents from your yourUserName/repositories/subdir directory to yourUserName/public_html for example. Make sure you've added a .cpanel.yml file at the root of your project that you are uploading. The contents of this file can be something like --- deployment : tasks : - export DEPLOYPATH = /home/ReplaceItByYourUserName/public_html/ - /bin/rm -Rf $DEPLOYPATH - /bin/mkdir $DEPLOYPATH - /bin/cp -R FileWhereSiteContentIsLocated/* $DEPLOYPATH Clone a repo to CPanel \u00b6 After you upload your project, this file will be hidden (as this is a . file) but can be seen by going to your directory from cPanel home \u2192 Advanced \u2192 Shell cd yourHome/Repositories ls -a Go to Cpanel \u2192 Git\u2122 Version Control \u2192 create clone url and enter the similar Clone URL git@github.com:<user_name>/<repository_name>.git The Repository Path and Repo Name will be autofilled. Then hit create. Pull from the sources. \u00b6 Manage repository from list \u2192 Manage \u2192 pull or deploy from Github \u2192 Click on Update from Remote : works perfectly (any files edit or delete you fetch/pull from GitHub now) Cheers \ud83c\udf7b","title":"Cpanel settings"},{"location":"technical_stuff/blogs/cpanel/gitAndCpanel/#how-to-clone-a-private-github-repo-into-cpanel","text":"The goal of this blog post is to help you in setting up your Github private repo on Cpanel, so that when you make a push to your blog post, then those changes are synced automatically. So let's get started \ud83d\ude00","title":"How to clone a private Github repo into Cpanel"},{"location":"technical_stuff/blogs/cpanel/gitAndCpanel/#generate-a-key-in-cpanel","text":"The goal is the create a public and private key pair. Then copy the public key and save it in Github. First we will create a pair in Cpanel. For this go to Cpanel \u2192 Security \u2192 SSH Access \u2192 Manage Keys \u2192 Generate New Key (enter a strong password)","title":"Generate a key in Cpanel"},{"location":"technical_stuff/blogs/cpanel/gitAndCpanel/#copy-public-key-from-cpanel","text":"After generating a public-private key pair, now is the time to copy the public key. For doing this, go to SSH Access \u2192 Manage SSH Keys \u2192 Public keys \u2192 Authorize (make authorize) \u2192 Go back \u2192 Press view/download \u2192 Download key \u2192 open it using a text editor and copy it.","title":"Copy public key from Cpanel"},{"location":"technical_stuff/blogs/cpanel/gitAndCpanel/#copy-public-key-to-github","text":"The goal is to save the public key in your private git repo, so that Cpanel can fetch data from it. Go to your Github \u2192 Your private repo \u2192 Settings \u2192 Deploy Keys \u2192 add deploy key \u2192 Provide any title and paste the public key.","title":"Copy public key to Github"},{"location":"technical_stuff/blogs/cpanel/gitAndCpanel/#add-an-cpanelyml-file","text":"Before we go ahead, we need to make sure the files we add from the Github are deployed to cpanel. You will need to know 2 important things here yourUserName/repositories : this is the folder where all the repo's will get synced. yourUserName/public_html : This is the folder from where the actual site content is served. So, this should contain an index.html at minimum. The .cpanel.yml file is a basic configuration file. It can be used to copy the contents from your yourUserName/repositories/subdir directory to yourUserName/public_html for example. Make sure you've added a .cpanel.yml file at the root of your project that you are uploading. The contents of this file can be something like --- deployment : tasks : - export DEPLOYPATH = /home/ReplaceItByYourUserName/public_html/ - /bin/rm -Rf $DEPLOYPATH - /bin/mkdir $DEPLOYPATH - /bin/cp -R FileWhereSiteContentIsLocated/* $DEPLOYPATH","title":"Add an .cpanel.yml file."},{"location":"technical_stuff/blogs/cpanel/gitAndCpanel/#clone-a-repo-to-cpanel","text":"After you upload your project, this file will be hidden (as this is a . file) but can be seen by going to your directory from cPanel home \u2192 Advanced \u2192 Shell cd yourHome/Repositories ls -a Go to Cpanel \u2192 Git\u2122 Version Control \u2192 create clone url and enter the similar Clone URL git@github.com:<user_name>/<repository_name>.git The Repository Path and Repo Name will be autofilled. Then hit create.","title":"Clone a repo to CPanel"},{"location":"technical_stuff/blogs/cpanel/gitAndCpanel/#pull-from-the-sources","text":"Manage repository from list \u2192 Manage \u2192 pull or deploy from Github \u2192 Click on Update from Remote : works perfectly (any files edit or delete you fetch/pull from GitHub now) Cheers \ud83c\udf7b","title":"Pull from the sources."},{"location":"technical_stuff/databricks/ManipulatingTableswithDeltaLakeLab/","text":"Manipulating Tables with Delta Lake \u00b6 This notebook provides a hands-on review of some basic functionality of Delta Lake. Learning Objectives \u00b6 By the end of this lab, you should be able to: - Execute standard operations to create and manipulate Delta Lake tables, including: CREATE TABLE INSERT INTO SELECT FROM UPDATE DELETE MERGE DROP TABLE Setup \u00b6 Run the following script to setup necessary variables and clear out past runs of this notebook. Note that re-executing this cell will allow you to start the lab over. % run ../ Includes / Classroom-Setup - 2 . 2L Create a Table \u00b6 In this notebook, we'll be creating a table to track our bean collection. Use the cell below to create a managed Delta Lake table named beans . Provide the following schema: Field Name Field type name STRING color STRING grams FLOAT delicious BOOLEAN create table beans ( name string , color string , grams float , delicious boolean ) Note We'll use Python to run checks occasionally throughout the lab. The following cell will return as error with a message on what needs to change if you have not followed instructions. No output from cell execution means that you have completed this step. assert spark . table ( \"beans\" ), \"Table named `beans` does not exist\" assert spark . table ( \"beans\" ) . columns == [ \"name\" , \"color\" , \"grams\" , \"delicious\" ], \"Please name the columns in the order provided above\" assert spark . table ( \"beans\" ) . dtypes == [( \"name\" , \"string\" ), ( \"color\" , \"string\" ), ( \"grams\" , \"float\" ), ( \"delicious\" , \"boolean\" )], \"Please make sure the column types are identical to those provided above\" Insert Data \u00b6 Run the following cell to insert three rows into the table. INSERT INTO beans VALUES ( \"black\" , \"black\" , 500 , true ), ( \"lentils\" , \"brown\" , 1000 , true ), ( \"jelly\" , \"rainbow\" , 42 . 5 , false ) Manually review the table contents to ensure data was written as expected. select * from beans Insert the additional records provided below. Make sure you execute this as a single transaction. insert into beans values ( 'pinto' , 'brown' , 1 . 5 , true ), ( 'green' , 'green' , 178 . 3 , true ), ( 'beanbag chair' , 'white' , 40000 , false ) Run the cell below to confirm the data is in the proper state. assert spark . conf . get ( \"spark.databricks.delta.lastCommitVersionInSession\" ) == \"2\" , \"Only 3 commits should have been made to the table\" assert spark . table ( \"beans\" ) . count () == 6 , \"The table should have 6 records\" assert set ( row [ \"name\" ] for row in spark . table ( \"beans\" ) . select ( \"name\" ) . collect ()) == { 'beanbag chair' , 'black' , 'green' , 'jelly' , 'lentils' , 'pinto' }, \"Make sure you have not modified the data provided\" Update Records \u00b6 A friend is reviewing your inventory of beans. After much debate, you agree that jelly beans are delicious. Run the following cell to update this record. UPDATE beans SET delicious = true WHERE name = \"jelly\" You realize that you've accidentally entered the weight of your pinto beans incorrectly. Update the grams column for this record to the correct weight of 1500. update beans set grams = 1500 where name = 'pinto' Run the cell below to confirm this has completed properly. assert spark . table ( \"beans\" ) . filter ( \"name='pinto'\" ) . count () == 1 , \"There should only be 1 entry for pinto beans\" row = spark . table ( \"beans\" ) . filter ( \"name='pinto'\" ) . first () assert row [ \"color\" ] == \"brown\" , \"The pinto bean should be labeled as the color brown\" assert row [ \"grams\" ] == 1500 , \"Make sure you correctly specified the `grams` as 1500\" assert row [ \"delicious\" ] == True , \"The pinto bean is a delicious bean\" Delete Records \u00b6 You've decided that you only want to keep track of delicious beans. Execute a query to drop all beans that are not delicious. delete from beans where delicious = false Run the following cell to confirm this operation was successful. assert spark . table ( \"beans\" ) . filter ( \"delicious=true\" ) . count () == 5 , \"There should be 5 delicious beans in your table\" assert spark . table ( \"beans\" ) . filter ( \"delicious=false\" ) . count () == 0 , \"There should be 0 delicious beans in your table\" assert spark . table ( \"beans\" ) . filter ( \"name='beanbag chair'\" ) . count () == 0 , \"Make sure your logic deletes non-delicious beans\" Using Merge to Upsert Records \u00b6 Your friend gives you some new beans. The cell below registers these as a temporary view. CREATE OR REPLACE TEMP VIEW new_beans ( name , color , grams , delicious ) AS VALUES ( 'black' , 'black' , 60 . 5 , true ), ( 'lentils' , 'green' , 500 , true ), ( 'kidney' , 'red' , 387 . 2 , true ), ( 'castor' , 'brown' , 25 , false ); SELECT * FROM new_beans In the cell below, use the above view to write a merge statement to update and insert new records to your beans table as one transaction. Make sure your logic: - Match beans by name and color - Updates existing beans by adding the new weight to the existing weight - Inserts new beans only if they are delicious merge into beans a using new_beans b on a . name = b . name and a . color = b . color when matched then update set grams = a . grams + b . grams when not matched and b . delicious = true then insert * Run the cell below to check your work. version = spark . sql ( \"DESCRIBE HISTORY beans\" ) . selectExpr ( \"max(version)\" ) . first ()[ 0 ] last_tx = spark . sql ( \"DESCRIBE HISTORY beans\" ) . filter ( f \"version= { version } \" ) assert last_tx . select ( \"operation\" ) . first ()[ 0 ] == \"MERGE\" , \"Transaction should be completed as a merge\" metrics = last_tx . select ( \"operationMetrics\" ) . first ()[ 0 ] assert metrics [ \"numOutputRows\" ] == \"3\" , \"Make sure you only insert delicious beans\" assert metrics [ \"numTargetRowsUpdated\" ] == \"1\" , \"Make sure you match on name and color\" assert metrics [ \"numTargetRowsInserted\" ] == \"2\" , \"Make sure you insert newly collected beans\" assert metrics [ \"numTargetRowsDeleted\" ] == \"0\" , \"No rows should be deleted by this operation\" Dropping Tables \u00b6 When working with managed Delta Lake tables, dropping a table results in permanently deleting access to the table and all underlying data files. NOTE : Later in the course, we'll learn about external tables, which approach Delta Lake tables as a collection of files and have different persistence guarantees. In the cell below, write a query to drop the beans table. drop table beans Run the cell below to assert that your table no longer exists. assert spark . sql ( \"SHOW TABLES LIKE 'beans'\" ) . collect () == [], \"Confirm that you have dropped the `beans` table from your current database\"","title":"Delta Lake \ud83d\udc1f"},{"location":"technical_stuff/databricks/ManipulatingTableswithDeltaLakeLab/#manipulating-tables-with-delta-lake","text":"This notebook provides a hands-on review of some basic functionality of Delta Lake.","title":"Manipulating Tables with Delta Lake"},{"location":"technical_stuff/databricks/ManipulatingTableswithDeltaLakeLab/#learning-objectives","text":"By the end of this lab, you should be able to: - Execute standard operations to create and manipulate Delta Lake tables, including: CREATE TABLE INSERT INTO SELECT FROM UPDATE DELETE MERGE DROP TABLE","title":"Learning Objectives"},{"location":"technical_stuff/databricks/ManipulatingTableswithDeltaLakeLab/#setup","text":"Run the following script to setup necessary variables and clear out past runs of this notebook. Note that re-executing this cell will allow you to start the lab over. % run ../ Includes / Classroom-Setup - 2 . 2L","title":"Setup"},{"location":"technical_stuff/databricks/ManipulatingTableswithDeltaLakeLab/#create-a-table","text":"In this notebook, we'll be creating a table to track our bean collection. Use the cell below to create a managed Delta Lake table named beans . Provide the following schema: Field Name Field type name STRING color STRING grams FLOAT delicious BOOLEAN create table beans ( name string , color string , grams float , delicious boolean ) Note We'll use Python to run checks occasionally throughout the lab. The following cell will return as error with a message on what needs to change if you have not followed instructions. No output from cell execution means that you have completed this step. assert spark . table ( \"beans\" ), \"Table named `beans` does not exist\" assert spark . table ( \"beans\" ) . columns == [ \"name\" , \"color\" , \"grams\" , \"delicious\" ], \"Please name the columns in the order provided above\" assert spark . table ( \"beans\" ) . dtypes == [( \"name\" , \"string\" ), ( \"color\" , \"string\" ), ( \"grams\" , \"float\" ), ( \"delicious\" , \"boolean\" )], \"Please make sure the column types are identical to those provided above\"","title":"Create a Table"},{"location":"technical_stuff/databricks/ManipulatingTableswithDeltaLakeLab/#insert-data","text":"Run the following cell to insert three rows into the table. INSERT INTO beans VALUES ( \"black\" , \"black\" , 500 , true ), ( \"lentils\" , \"brown\" , 1000 , true ), ( \"jelly\" , \"rainbow\" , 42 . 5 , false ) Manually review the table contents to ensure data was written as expected. select * from beans Insert the additional records provided below. Make sure you execute this as a single transaction. insert into beans values ( 'pinto' , 'brown' , 1 . 5 , true ), ( 'green' , 'green' , 178 . 3 , true ), ( 'beanbag chair' , 'white' , 40000 , false ) Run the cell below to confirm the data is in the proper state. assert spark . conf . get ( \"spark.databricks.delta.lastCommitVersionInSession\" ) == \"2\" , \"Only 3 commits should have been made to the table\" assert spark . table ( \"beans\" ) . count () == 6 , \"The table should have 6 records\" assert set ( row [ \"name\" ] for row in spark . table ( \"beans\" ) . select ( \"name\" ) . collect ()) == { 'beanbag chair' , 'black' , 'green' , 'jelly' , 'lentils' , 'pinto' }, \"Make sure you have not modified the data provided\"","title":"Insert Data"},{"location":"technical_stuff/databricks/ManipulatingTableswithDeltaLakeLab/#update-records","text":"A friend is reviewing your inventory of beans. After much debate, you agree that jelly beans are delicious. Run the following cell to update this record. UPDATE beans SET delicious = true WHERE name = \"jelly\" You realize that you've accidentally entered the weight of your pinto beans incorrectly. Update the grams column for this record to the correct weight of 1500. update beans set grams = 1500 where name = 'pinto' Run the cell below to confirm this has completed properly. assert spark . table ( \"beans\" ) . filter ( \"name='pinto'\" ) . count () == 1 , \"There should only be 1 entry for pinto beans\" row = spark . table ( \"beans\" ) . filter ( \"name='pinto'\" ) . first () assert row [ \"color\" ] == \"brown\" , \"The pinto bean should be labeled as the color brown\" assert row [ \"grams\" ] == 1500 , \"Make sure you correctly specified the `grams` as 1500\" assert row [ \"delicious\" ] == True , \"The pinto bean is a delicious bean\"","title":"Update Records"},{"location":"technical_stuff/databricks/ManipulatingTableswithDeltaLakeLab/#delete-records","text":"You've decided that you only want to keep track of delicious beans. Execute a query to drop all beans that are not delicious. delete from beans where delicious = false Run the following cell to confirm this operation was successful. assert spark . table ( \"beans\" ) . filter ( \"delicious=true\" ) . count () == 5 , \"There should be 5 delicious beans in your table\" assert spark . table ( \"beans\" ) . filter ( \"delicious=false\" ) . count () == 0 , \"There should be 0 delicious beans in your table\" assert spark . table ( \"beans\" ) . filter ( \"name='beanbag chair'\" ) . count () == 0 , \"Make sure your logic deletes non-delicious beans\"","title":"Delete Records"},{"location":"technical_stuff/databricks/ManipulatingTableswithDeltaLakeLab/#using-merge-to-upsert-records","text":"Your friend gives you some new beans. The cell below registers these as a temporary view. CREATE OR REPLACE TEMP VIEW new_beans ( name , color , grams , delicious ) AS VALUES ( 'black' , 'black' , 60 . 5 , true ), ( 'lentils' , 'green' , 500 , true ), ( 'kidney' , 'red' , 387 . 2 , true ), ( 'castor' , 'brown' , 25 , false ); SELECT * FROM new_beans In the cell below, use the above view to write a merge statement to update and insert new records to your beans table as one transaction. Make sure your logic: - Match beans by name and color - Updates existing beans by adding the new weight to the existing weight - Inserts new beans only if they are delicious merge into beans a using new_beans b on a . name = b . name and a . color = b . color when matched then update set grams = a . grams + b . grams when not matched and b . delicious = true then insert * Run the cell below to check your work. version = spark . sql ( \"DESCRIBE HISTORY beans\" ) . selectExpr ( \"max(version)\" ) . first ()[ 0 ] last_tx = spark . sql ( \"DESCRIBE HISTORY beans\" ) . filter ( f \"version= { version } \" ) assert last_tx . select ( \"operation\" ) . first ()[ 0 ] == \"MERGE\" , \"Transaction should be completed as a merge\" metrics = last_tx . select ( \"operationMetrics\" ) . first ()[ 0 ] assert metrics [ \"numOutputRows\" ] == \"3\" , \"Make sure you only insert delicious beans\" assert metrics [ \"numTargetRowsUpdated\" ] == \"1\" , \"Make sure you match on name and color\" assert metrics [ \"numTargetRowsInserted\" ] == \"2\" , \"Make sure you insert newly collected beans\" assert metrics [ \"numTargetRowsDeleted\" ] == \"0\" , \"No rows should be deleted by this operation\"","title":"Using Merge to Upsert Records"},{"location":"technical_stuff/databricks/ManipulatingTableswithDeltaLakeLab/#dropping-tables","text":"When working with managed Delta Lake tables, dropping a table results in permanently deleting access to the table and all underlying data files. NOTE : Later in the course, we'll learn about external tables, which approach Delta Lake tables as a collection of files and have different persistence guarantees. In the cell below, write a query to drop the beans table. drop table beans Run the cell below to assert that your table no longer exists. assert spark . sql ( \"SHOW TABLES LIKE 'beans'\" ) . collect () == [], \"Confirm that you have dropped the `beans` table from your current database\"","title":"Dropping Tables"},{"location":"technical_stuff/databricks/databricks_basics/","text":"Databricks 101 \ud83e\uddf1 \u00b6 The Databricks Runtime includes additional optimizations and proprietary features that build upon and extend Apache Spark, including Photon which is an optimized version of Apache Spark rewritten in C++ using vectorized query processing. Spark Context You don\u2019t need to worry about configuring or initializing a Spark context or Spark session, as these are managed for you by Databricks. Architecture \u00b6 Databricks operates out of a control plane and a data plane. Image credits: Microsoft Learn Control Plane \u00b6 The control plane includes the backend services that Azure Databricks manages in its own Azure account. Notebook commands and many other workspace configurations are stored in the control plane and encrypted at rest. Data Plane \u00b6 Your Azure account manages the data plane, and is where your data resides. This is also where data is processed Job results reside in storage in your account. Interactive notebook results are stored in a combination of the control plane (partial results for presentation in the UI) and your Azure storage. If you want interactive notebook results stored only in your cloud account storage, you can ask your Databricks representative to enable interactive notebook results in the customer account for your workspace. Spark Concepts \u00b6 DataFrame and RDD \u00b6 Tldr A DataFrame is a two-dimensional labeled data structure with columns of potentially different types. You can think of a DataFrame like a spreadsheet, a SQL table, or a dictionary of series objects. Apache Spark DataFrames provide a rich set of functions (select columns, filter, join, aggregate) that allow you to solve common data analysis problems efficiently. Apache Spark DataFrames are an abstraction built on top of Resilient Distributed Datasets (RDDs). Use of Lazy loading in Spark Dataframe instead of Pandas One of the key differences between Pandas and Spark dataframes is eager versus lazy execution . In PySpark, operations are delayed until a result is actually requested in the pipeline. For example, you can specify operations for loading a data set from Amazon S3 and applying a number of transformations to the dataframe, but these operations won\u2019t be applied immediately. Instead, a graph of transformations is recorded, and once the data are actually needed, for example when writing the results back to S3, then the transformations are applied as a single pipeline operation. This approach is used to avoid pulling the full dataframe into memory, and enables more effective processing across a cluster of machines. Spark SQL \u00b6 The term Spark SQL technically applies to all operations that use Spark DataFrames. Spark SQL replaced the Spark RDD API in Spark 2.x, introducing support for SQL queries and the DataFrame API for Python, Scala, R, and Java. PySpark \u00b6 PySpark is the Python API for Apache Spark, an open source, distributed computing framework and set of libraries for real-time, large-scale data processing. Databricks Concepts \u00b6 Databricks File System (DBFS) \u00b6 A filesystem abstraction layer over a blob store. It contains directories, which can contain files (data files, libraries, and images), and other directories. DBFS is automatically populated with some datasets that you can use to learn Azure Databricks. DBFS is an abstraction on top of scalable object storage that maps Unix-like filesystem calls to native cloud storage API calls. Mount blob to DBFS \u00b6 Mounting object storage to DBFS allows you to access objects in object storage as if they were on the local file system. Mounts store Hadoop configurations necessary for accessing storage, so you do not need to specify these settings in code or during cluster configuration. DBFS root \u00b6 The DBFS root is the default storage location for a Databricks workspace, provisioned as part of workspace creation in the cloud account containing the Databricks workspace It is important to differentiate that DBFS is a file system used for interacting with data in cloud object storage, and the DBFS root is a cloud object storage location. Auto Loader \u00b6 Tldr Auto Loader incrementally and efficiently processes new data files as they arrive in cloud storage without any additional setup. Auto Loader incrementally and efficiently processes new data files as they arrive in cloud storage. Auto Loader can load data files from AWS S3 (s3://) Azure Data Lake Storage Gen2 (ADLS Gen2, abfss:// ) Google Cloud Storage (GCS, gs:// ) Azure Blob Storage ( wasbs:// ) ADLS Gen1 ( adl:// ) Databricks File System (DBFS, dbfs:/ ) Auto Loader can ingest JSON, CSV, PARQUET, AVRO, ORC, TEXT, and BINARYFILE file formats. How does Auto Loader track ingestion progress? As files are discovered, their metadata is persisted in a scalable key-value store ( RocksDB ) in the checkpoint location of your Auto Loader pipeline. This key-value store ensures that data is processed exactly once . Delta Lake \u00b6 Delta Lake is open source software that extends Parquet data files with a file-based transaction log for ACID transactions and scalable metadata handling. Delta Lake is fully compatible with Apache Spark APIs, and was developed for tight integration with Structured Streaming, allowing you to easily use a single copy of data for both batch and streaming operations and providing incremental processing at scale. Who created delta lake format? Delta Lake is the default storage format for all operations on Databricks. Unless otherwise specified, all tables on Databricks are Delta tables. Databricks originally developed the Delta Lake protocol and continues to actively contribute to the open source project. Connecting to blob and ADLS \u00b6 We can use the Azure Blob Filesystem driver (ABFS) to connect to Azure Blob Storage and Azure Data Lake Storage (ADLS) Gen2 from Databricks The connection can be scoped to either 1. Databricks cluster 2. Databricks Notebook ABFS vs WASB The legacy Windows Azure Storage Blob driver (WASB) has been deprecated. ABFS has numerous benefits over WASB. Credentials walkthrough When you enable Azure Data Lake Storage credential passthrough for your cluster, commands that you run on that cluster can read and write data in Azure Data Lake Storage without requiring you to configure service principal credentials for access to storage. Azure Data Lake Storage credential passthrough is supported with Azure Data Lake Storage Gen1 and Gen2 only. Azure Blob storage does not support credential passthrough. Delta table \u00b6 A Delta table stores data as a directory of files on cloud object storage and registers table metadata to the metastore within a catalog and schema. Hive metastore \u00b6 The component that stores all the structure information of the various tables and partitions in the data warehouse including column and column type information, the serializers and deserializers necessary to read and write data, and the corresponding files where the data is stored. Delta live tables \u00b6 Instead of defining your data pipelines using a series of separate Apache Spark tasks, Delta Live Tables manages how your data is transformed based on a target schema you define for each processing step. You can also enforce data quality with Delta Live Tables expectations. Expectations allow you to define expected data quality and specify how to handle records that fail those expectations. Authentication and authorization \u00b6 User \u00b6 A unique individual who has access to the system. User identities are represented by email addresses. Service principal \u00b6 A service identity for use with jobs, automated tools, and systems such as scripts, apps, and CI/CD platforms. Service principals are represented by an application ID. Group \u00b6 Groups simplify identity management, making it easier to assign access to workspaces, data, and other securable objects. All Databricks identities can be assigned as members of groups. ACL \u00b6 A list of permissions attached to the workspace, cluster, job, table, or experiment. An ACL specifies which users or system processes are granted access to the objects, as well as what operations are allowed on the assets PAT \u00b6 An opaque string is used to authenticate to the REST API and by tools in the Databricks integrations to connect to SQL warehouses. DS & Engineering Space \u00b6 Workspace \u00b6 A workspace is an environment for accessing all of your Azure Databricks assets. A workspace organizes objects (notebooks, libraries, dashboards, and experiments) into folders and provides access to data objects and computational resources. Notebook \u00b6 A web-based interface to documents that contain runnable commands, visualizations, and narrative text. Repo \u00b6 A folder whose contents are co-versioned together by syncing them to a remote Git repository. Databricks Workflow \u00b6 Azure Databricks Workflows orchestrates data processing, machine learning, and analytics pipelines in the Azure Databricks Lakehouse Platform. Workflows has fully managed orchestration services integrated with the Azure Databricks platform, including Azure Databricks Jobs to run non-interactive code in your Azure Databricks workspace and Delta Live Tables to build reliable and maintainable ETL pipelines.","title":"Databricks 101 \ud83e\uddf1"},{"location":"technical_stuff/databricks/databricks_basics/#databricks-101","text":"The Databricks Runtime includes additional optimizations and proprietary features that build upon and extend Apache Spark, including Photon which is an optimized version of Apache Spark rewritten in C++ using vectorized query processing. Spark Context You don\u2019t need to worry about configuring or initializing a Spark context or Spark session, as these are managed for you by Databricks.","title":"Databricks 101 \ud83e\uddf1"},{"location":"technical_stuff/databricks/databricks_basics/#architecture","text":"Databricks operates out of a control plane and a data plane. Image credits: Microsoft Learn","title":"Architecture"},{"location":"technical_stuff/databricks/databricks_basics/#control-plane","text":"The control plane includes the backend services that Azure Databricks manages in its own Azure account. Notebook commands and many other workspace configurations are stored in the control plane and encrypted at rest.","title":"Control Plane"},{"location":"technical_stuff/databricks/databricks_basics/#data-plane","text":"Your Azure account manages the data plane, and is where your data resides. This is also where data is processed Job results reside in storage in your account. Interactive notebook results are stored in a combination of the control plane (partial results for presentation in the UI) and your Azure storage. If you want interactive notebook results stored only in your cloud account storage, you can ask your Databricks representative to enable interactive notebook results in the customer account for your workspace.","title":"Data Plane"},{"location":"technical_stuff/databricks/databricks_basics/#spark-concepts","text":"","title":"Spark Concepts"},{"location":"technical_stuff/databricks/databricks_basics/#dataframe-and-rdd","text":"Tldr A DataFrame is a two-dimensional labeled data structure with columns of potentially different types. You can think of a DataFrame like a spreadsheet, a SQL table, or a dictionary of series objects. Apache Spark DataFrames provide a rich set of functions (select columns, filter, join, aggregate) that allow you to solve common data analysis problems efficiently. Apache Spark DataFrames are an abstraction built on top of Resilient Distributed Datasets (RDDs). Use of Lazy loading in Spark Dataframe instead of Pandas One of the key differences between Pandas and Spark dataframes is eager versus lazy execution . In PySpark, operations are delayed until a result is actually requested in the pipeline. For example, you can specify operations for loading a data set from Amazon S3 and applying a number of transformations to the dataframe, but these operations won\u2019t be applied immediately. Instead, a graph of transformations is recorded, and once the data are actually needed, for example when writing the results back to S3, then the transformations are applied as a single pipeline operation. This approach is used to avoid pulling the full dataframe into memory, and enables more effective processing across a cluster of machines.","title":"DataFrame and RDD"},{"location":"technical_stuff/databricks/databricks_basics/#spark-sql","text":"The term Spark SQL technically applies to all operations that use Spark DataFrames. Spark SQL replaced the Spark RDD API in Spark 2.x, introducing support for SQL queries and the DataFrame API for Python, Scala, R, and Java.","title":"Spark SQL"},{"location":"technical_stuff/databricks/databricks_basics/#pyspark","text":"PySpark is the Python API for Apache Spark, an open source, distributed computing framework and set of libraries for real-time, large-scale data processing.","title":"PySpark"},{"location":"technical_stuff/databricks/databricks_basics/#databricks-concepts","text":"","title":"Databricks Concepts"},{"location":"technical_stuff/databricks/databricks_basics/#databricks-file-system-dbfs","text":"A filesystem abstraction layer over a blob store. It contains directories, which can contain files (data files, libraries, and images), and other directories. DBFS is automatically populated with some datasets that you can use to learn Azure Databricks. DBFS is an abstraction on top of scalable object storage that maps Unix-like filesystem calls to native cloud storage API calls.","title":"Databricks File System (DBFS)"},{"location":"technical_stuff/databricks/databricks_basics/#mount-blob-to-dbfs","text":"Mounting object storage to DBFS allows you to access objects in object storage as if they were on the local file system. Mounts store Hadoop configurations necessary for accessing storage, so you do not need to specify these settings in code or during cluster configuration.","title":"Mount blob to DBFS"},{"location":"technical_stuff/databricks/databricks_basics/#dbfs-root","text":"The DBFS root is the default storage location for a Databricks workspace, provisioned as part of workspace creation in the cloud account containing the Databricks workspace It is important to differentiate that DBFS is a file system used for interacting with data in cloud object storage, and the DBFS root is a cloud object storage location.","title":"DBFS root"},{"location":"technical_stuff/databricks/databricks_basics/#auto-loader","text":"Tldr Auto Loader incrementally and efficiently processes new data files as they arrive in cloud storage without any additional setup. Auto Loader incrementally and efficiently processes new data files as they arrive in cloud storage. Auto Loader can load data files from AWS S3 (s3://) Azure Data Lake Storage Gen2 (ADLS Gen2, abfss:// ) Google Cloud Storage (GCS, gs:// ) Azure Blob Storage ( wasbs:// ) ADLS Gen1 ( adl:// ) Databricks File System (DBFS, dbfs:/ ) Auto Loader can ingest JSON, CSV, PARQUET, AVRO, ORC, TEXT, and BINARYFILE file formats. How does Auto Loader track ingestion progress? As files are discovered, their metadata is persisted in a scalable key-value store ( RocksDB ) in the checkpoint location of your Auto Loader pipeline. This key-value store ensures that data is processed exactly once .","title":"Auto Loader"},{"location":"technical_stuff/databricks/databricks_basics/#delta-lake","text":"Delta Lake is open source software that extends Parquet data files with a file-based transaction log for ACID transactions and scalable metadata handling. Delta Lake is fully compatible with Apache Spark APIs, and was developed for tight integration with Structured Streaming, allowing you to easily use a single copy of data for both batch and streaming operations and providing incremental processing at scale. Who created delta lake format? Delta Lake is the default storage format for all operations on Databricks. Unless otherwise specified, all tables on Databricks are Delta tables. Databricks originally developed the Delta Lake protocol and continues to actively contribute to the open source project.","title":"Delta Lake"},{"location":"technical_stuff/databricks/databricks_basics/#connecting-to-blob-and-adls","text":"We can use the Azure Blob Filesystem driver (ABFS) to connect to Azure Blob Storage and Azure Data Lake Storage (ADLS) Gen2 from Databricks The connection can be scoped to either 1. Databricks cluster 2. Databricks Notebook ABFS vs WASB The legacy Windows Azure Storage Blob driver (WASB) has been deprecated. ABFS has numerous benefits over WASB. Credentials walkthrough When you enable Azure Data Lake Storage credential passthrough for your cluster, commands that you run on that cluster can read and write data in Azure Data Lake Storage without requiring you to configure service principal credentials for access to storage. Azure Data Lake Storage credential passthrough is supported with Azure Data Lake Storage Gen1 and Gen2 only. Azure Blob storage does not support credential passthrough.","title":"Connecting to blob and ADLS"},{"location":"technical_stuff/databricks/databricks_basics/#delta-table","text":"A Delta table stores data as a directory of files on cloud object storage and registers table metadata to the metastore within a catalog and schema.","title":"Delta table"},{"location":"technical_stuff/databricks/databricks_basics/#hive-metastore","text":"The component that stores all the structure information of the various tables and partitions in the data warehouse including column and column type information, the serializers and deserializers necessary to read and write data, and the corresponding files where the data is stored.","title":"Hive metastore"},{"location":"technical_stuff/databricks/databricks_basics/#delta-live-tables","text":"Instead of defining your data pipelines using a series of separate Apache Spark tasks, Delta Live Tables manages how your data is transformed based on a target schema you define for each processing step. You can also enforce data quality with Delta Live Tables expectations. Expectations allow you to define expected data quality and specify how to handle records that fail those expectations.","title":"Delta live tables"},{"location":"technical_stuff/databricks/databricks_basics/#authentication-and-authorization","text":"","title":"Authentication and authorization"},{"location":"technical_stuff/databricks/databricks_basics/#user","text":"A unique individual who has access to the system. User identities are represented by email addresses.","title":"User"},{"location":"technical_stuff/databricks/databricks_basics/#service-principal","text":"A service identity for use with jobs, automated tools, and systems such as scripts, apps, and CI/CD platforms. Service principals are represented by an application ID.","title":"Service principal"},{"location":"technical_stuff/databricks/databricks_basics/#group","text":"Groups simplify identity management, making it easier to assign access to workspaces, data, and other securable objects. All Databricks identities can be assigned as members of groups.","title":"Group"},{"location":"technical_stuff/databricks/databricks_basics/#acl","text":"A list of permissions attached to the workspace, cluster, job, table, or experiment. An ACL specifies which users or system processes are granted access to the objects, as well as what operations are allowed on the assets","title":"ACL"},{"location":"technical_stuff/databricks/databricks_basics/#pat","text":"An opaque string is used to authenticate to the REST API and by tools in the Databricks integrations to connect to SQL warehouses.","title":"PAT"},{"location":"technical_stuff/databricks/databricks_basics/#ds-engineering-space","text":"","title":"DS &amp; Engineering Space"},{"location":"technical_stuff/databricks/databricks_basics/#workspace","text":"A workspace is an environment for accessing all of your Azure Databricks assets. A workspace organizes objects (notebooks, libraries, dashboards, and experiments) into folders and provides access to data objects and computational resources.","title":"Workspace"},{"location":"technical_stuff/databricks/databricks_basics/#notebook","text":"A web-based interface to documents that contain runnable commands, visualizations, and narrative text.","title":"Notebook"},{"location":"technical_stuff/databricks/databricks_basics/#repo","text":"A folder whose contents are co-versioned together by syncing them to a remote Git repository.","title":"Repo"},{"location":"technical_stuff/databricks/databricks_basics/#databricks-workflow","text":"Azure Databricks Workflows orchestrates data processing, machine learning, and analytics pipelines in the Azure Databricks Lakehouse Platform. Workflows has fully managed orchestration services integrated with the Azure Databricks platform, including Azure Databricks Jobs to run non-interactive code in your Azure Databricks workspace and Delta Live Tables to build reliable and maintainable ETL pipelines.","title":"Databricks Workflow"},{"location":"technical_stuff/databricks/hive/","text":"Hive \u00b6 Apache Hive is open-source data warehouse software designed to read, write, and manage large datasets extracted from the Apache Hadoop Distributed File System (HDFS) , one aspect of a larger Hadoop Ecosystem. Hive processing support Apache Hive supports the analysis of large datasets stored in Hadoop's HDFS and compatible file systems such as Amazon S3 , Azure Blob Storage , Azure Data Lake Storage , Google Cloud Storage etc. It provides a SQL-like query language called HiveQL with schema-on-read and transparently converts queries to Apache Spark jobs, MapReduce job, and Apache Tez jobs. The Hive Metastore \u00b6 The central repository of the Apache Hive infrastructure, the metastore is where all of the Hive\u2019s metadata is stored. In the metastore, metadata can also be formatted into Hive tables and partitions to compare data across relational databases.","title":"Hive 101"},{"location":"technical_stuff/databricks/hive/#hive","text":"Apache Hive is open-source data warehouse software designed to read, write, and manage large datasets extracted from the Apache Hadoop Distributed File System (HDFS) , one aspect of a larger Hadoop Ecosystem. Hive processing support Apache Hive supports the analysis of large datasets stored in Hadoop's HDFS and compatible file systems such as Amazon S3 , Azure Blob Storage , Azure Data Lake Storage , Google Cloud Storage etc. It provides a SQL-like query language called HiveQL with schema-on-read and transparently converts queries to Apache Spark jobs, MapReduce job, and Apache Tez jobs.","title":"Hive"},{"location":"technical_stuff/databricks/hive/#the-hive-metastore","text":"The central repository of the Apache Hive infrastructure, the metastore is where all of the Hive\u2019s metadata is stored. In the metastore, metadata can also be formatted into Hive tables and partitions to compare data across relational databases.","title":"The Hive Metastore"},{"location":"technical_stuff/installing_flink/flink/","text":"Apache Flink \u00b6 Installing Apache Flink \u00b6 Building Apache Flink is very easy yet it took approximately 30 minutes. Steps for installing apache Flink on Mac/ubuntu \u00b6 Steps are: Unix-like environment such as Linux, Mac OS X, Cygwin. Git Make sure you have java installed, check it in terminal using java -version Maven is used as build tool, if you do not have maven install it using brew install maven Unix-like environment (We use Linux, Mac OS X, Cygwin) is required Go to this link and download the source version. You can also clone the source form git by entering following command in your terminal. git clone https://github.com/apache/flink cd to the downloaded file and then unpack it by using tar xzf *.tgz where * is filename. Alternatively in Mac you can also double click the tar file and it will be un-tared and unzipped. Then cd to un-tarred file and enter following command in terminal mvn clean install -DskipTests Let build will start and will take almost 30 minutes and finally if everything is done successfully, then you will see following message. In my system the Flink is installed at the following location. /Users/YOUR_USER_NAME/.m2/repository/org/apache/flink Success Congrats, we have successfully build Apache-Flink on our system.","title":"Flink"},{"location":"technical_stuff/installing_flink/flink/#apache-flink","text":"","title":"Apache Flink"},{"location":"technical_stuff/installing_flink/flink/#installing-apache-flink","text":"Building Apache Flink is very easy yet it took approximately 30 minutes.","title":"Installing Apache Flink"},{"location":"technical_stuff/installing_flink/flink/#steps-for-installing-apache-flink-on-macubuntu","text":"Steps are: Unix-like environment such as Linux, Mac OS X, Cygwin. Git Make sure you have java installed, check it in terminal using java -version Maven is used as build tool, if you do not have maven install it using brew install maven Unix-like environment (We use Linux, Mac OS X, Cygwin) is required Go to this link and download the source version. You can also clone the source form git by entering following command in your terminal. git clone https://github.com/apache/flink cd to the downloaded file and then unpack it by using tar xzf *.tgz where * is filename. Alternatively in Mac you can also double click the tar file and it will be un-tared and unzipped. Then cd to un-tarred file and enter following command in terminal mvn clean install -DskipTests Let build will start and will take almost 30 minutes and finally if everything is done successfully, then you will see following message. In my system the Flink is installed at the following location. /Users/YOUR_USER_NAME/.m2/repository/org/apache/flink Success Congrats, we have successfully build Apache-Flink on our system.","title":"Steps for installing apache Flink on Mac/ubuntu"},{"location":"technical_stuff/linux/android_device/","text":"Attach android device to android studio in Ubuntu \u00b6 Make sure you have enabled developer options , if not then follow these steps Go to Settings \u2192 About phone \u2192 Build number . On a Samsung Galaxy device, go to Settings \u2192 About device \u2192 Build number . Tap build number 7 times. Go back to Settings, where you\u2019ll find a developer options entry in the menu. In terminal type adb devices Error If you see something like ?????????? no permissions , then create a new file using nano as sudo nano /etc/udev/rules.d/51-android.rules Then paste the following code into that file SUBSYSTEM == \"usb\" , ATTRS { idVendor }== \"0bb4\" , MODE = \"0666\" SUBSYSTEM == \"usb\" , ATTRS { idVendor }== \"0e79\" , MODE = \"0666\" SUBSYSTEM == \"usb\" , ATTRS { idVendor }== \"0502\" , MODE = \"0666\" SUBSYSTEM == \"usb\" , ATTRS { idVendor }== \"0b05\" , MODE = \"0666\" SUBSYSTEM == \"usb\" , ATTRS { idVendor }== \"413c\" , MODE = \"0666\" SUBSYSTEM == \"usb\" , ATTRS { idVendor }== \"0489\" , MODE = \"0666\" SUBSYSTEM == \"usb\" , ATTRS { idVendor }== \"091e\" , MODE = \"0666\" SUBSYSTEM == \"usb\" , ATTRS { idVendor }== \"18d1\" , MODE = \"0666\" SUBSYSTEM == \"usb\" , ATTRS { idVendor }== \"0bb4\" , MODE = \"0666\" SUBSYSTEM == \"usb\" , ATTRS { idVendor }== \"12d1\" , MODE = \"0666\" SUBSYSTEM == \"usb\" , ATTRS { idVendor }== \"24e3\" , MODE = \"0666\" SUBSYSTEM == \"usb\" , ATTRS { idVendor }== \"2116\" , MODE = \"0666\" SUBSYSTEM == \"usb\" , ATTRS { idVendor }== \"0482\" , MODE = \"0666\" SUBSYSTEM == \"usb\" , ATTRS { idVendor }== \"17ef\" , MODE = \"0666\" SUBSYSTEM == \"usb\" , ATTRS { idVendor }== \"1004\" , MODE = \"0666\" SUBSYSTEM == \"usb\" , ATTRS { idVendor }== \"22b8\" , MODE = \"0666\" SUBSYSTEM == \"usb\" , ATTRS { idVendor }== \"0409\" , MODE = \"0666\" SUBSYSTEM == \"usb\" , ATTRS { idVendor }== \"2080\" , MODE = \"0666\" SUBSYSTEM == \"usb\" , ATTRS { idVendor }== \"0955\" , MODE = \"0666\" SUBSYSTEM == \"usb\" , ATTRS { idVendor }== \"2257\" , MODE = \"0666\" SUBSYSTEM == \"usb\" , ATTRS { idVendor }== \"10a9\" , MODE = \"0666\" SUBSYSTEM == \"usb\" , ATTRS { idVendor }== \"1d4d\" , MODE = \"0666\" SUBSYSTEM == \"usb\" , ATTRS { idVendor }== \"0471\" , MODE = \"0666\" SUBSYSTEM == \"usb\" , ATTRS { idVendor }== \"04da\" , MODE = \"0666\" SUBSYSTEM == \"usb\" , ATTRS { idVendor }== \"05c6\" , MODE = \"0666\" SUBSYSTEM == \"usb\" , ATTRS { idVendor }== \"1f53\" , MODE = \"0666\" SUBSYSTEM == \"usb\" , ATTRS { idVendor }== \"04e8\" , MODE = \"0666\" SUBSYSTEM == \"usb\" , ATTRS { idVendor }== \"04dd\" , MODE = \"0666\" SUBSYSTEM == \"usb\" , ATTRS { idVendor }== \"0fce\" , MODE = \"0666\" SUBSYSTEM == \"usb\" , ATTRS { idVendor }== \"0930\" , MODE = \"0666\" SUBSYSTEM == \"usb\" , ATTRS { idVendor }== \"19d2\" , MODE = \"0666\" SUBSYSTEM == \"usb\" , ATTRS { idVendor }== \"1bbb\" , MODE = \"0666\" Then enter following commands sudo chmod 644 /etc/udev/rules.d/51-android.rules sudo chown root. /etc/udev/rules.d/51-android.rules sudo service udev restart sudo killall adb Remove he USB cable and connect it again. Then enter adb devices","title":"Attach Android device \ud83e\udd16"},{"location":"technical_stuff/linux/android_device/#attach-android-device-to-android-studio-in-ubuntu","text":"Make sure you have enabled developer options , if not then follow these steps Go to Settings \u2192 About phone \u2192 Build number . On a Samsung Galaxy device, go to Settings \u2192 About device \u2192 Build number . Tap build number 7 times. Go back to Settings, where you\u2019ll find a developer options entry in the menu. In terminal type adb devices Error If you see something like ?????????? no permissions , then create a new file using nano as sudo nano /etc/udev/rules.d/51-android.rules Then paste the following code into that file SUBSYSTEM == \"usb\" , ATTRS { idVendor }== \"0bb4\" , MODE = \"0666\" SUBSYSTEM == \"usb\" , ATTRS { idVendor }== \"0e79\" , MODE = \"0666\" SUBSYSTEM == \"usb\" , ATTRS { idVendor }== \"0502\" , MODE = \"0666\" SUBSYSTEM == \"usb\" , ATTRS { idVendor }== \"0b05\" , MODE = \"0666\" SUBSYSTEM == \"usb\" , ATTRS { idVendor }== \"413c\" , MODE = \"0666\" SUBSYSTEM == \"usb\" , ATTRS { idVendor }== \"0489\" , MODE = \"0666\" SUBSYSTEM == \"usb\" , ATTRS { idVendor }== \"091e\" , MODE = \"0666\" SUBSYSTEM == \"usb\" , ATTRS { idVendor }== \"18d1\" , MODE = \"0666\" SUBSYSTEM == \"usb\" , ATTRS { idVendor }== \"0bb4\" , MODE = \"0666\" SUBSYSTEM == \"usb\" , ATTRS { idVendor }== \"12d1\" , MODE = \"0666\" SUBSYSTEM == \"usb\" , ATTRS { idVendor }== \"24e3\" , MODE = \"0666\" SUBSYSTEM == \"usb\" , ATTRS { idVendor }== \"2116\" , MODE = \"0666\" SUBSYSTEM == \"usb\" , ATTRS { idVendor }== \"0482\" , MODE = \"0666\" SUBSYSTEM == \"usb\" , ATTRS { idVendor }== \"17ef\" , MODE = \"0666\" SUBSYSTEM == \"usb\" , ATTRS { idVendor }== \"1004\" , MODE = \"0666\" SUBSYSTEM == \"usb\" , ATTRS { idVendor }== \"22b8\" , MODE = \"0666\" SUBSYSTEM == \"usb\" , ATTRS { idVendor }== \"0409\" , MODE = \"0666\" SUBSYSTEM == \"usb\" , ATTRS { idVendor }== \"2080\" , MODE = \"0666\" SUBSYSTEM == \"usb\" , ATTRS { idVendor }== \"0955\" , MODE = \"0666\" SUBSYSTEM == \"usb\" , ATTRS { idVendor }== \"2257\" , MODE = \"0666\" SUBSYSTEM == \"usb\" , ATTRS { idVendor }== \"10a9\" , MODE = \"0666\" SUBSYSTEM == \"usb\" , ATTRS { idVendor }== \"1d4d\" , MODE = \"0666\" SUBSYSTEM == \"usb\" , ATTRS { idVendor }== \"0471\" , MODE = \"0666\" SUBSYSTEM == \"usb\" , ATTRS { idVendor }== \"04da\" , MODE = \"0666\" SUBSYSTEM == \"usb\" , ATTRS { idVendor }== \"05c6\" , MODE = \"0666\" SUBSYSTEM == \"usb\" , ATTRS { idVendor }== \"1f53\" , MODE = \"0666\" SUBSYSTEM == \"usb\" , ATTRS { idVendor }== \"04e8\" , MODE = \"0666\" SUBSYSTEM == \"usb\" , ATTRS { idVendor }== \"04dd\" , MODE = \"0666\" SUBSYSTEM == \"usb\" , ATTRS { idVendor }== \"0fce\" , MODE = \"0666\" SUBSYSTEM == \"usb\" , ATTRS { idVendor }== \"0930\" , MODE = \"0666\" SUBSYSTEM == \"usb\" , ATTRS { idVendor }== \"19d2\" , MODE = \"0666\" SUBSYSTEM == \"usb\" , ATTRS { idVendor }== \"1bbb\" , MODE = \"0666\" Then enter following commands sudo chmod 644 /etc/udev/rules.d/51-android.rules sudo chown root. /etc/udev/rules.d/51-android.rules sudo service udev restart sudo killall adb Remove he USB cable and connect it again. Then enter adb devices","title":"Attach android device to android studio in Ubuntu"},{"location":"technical_stuff/linux/linuxCommands/","text":"Linux commands \u00b6 Linux includes a large number of commands, but I have chosen 37 of the most important ones to present here. Learn these commands, and you\u2019ll be much more at home at the Linux command prompt. The below list is presented in alphabetical order. A command\u2019s position in the list is not representative of its usefulness or simplicity. Info For the final word on a command\u2019s usage, refer to its man pages. The man command is in our list, of course\u2014it\u2019s short for \u201cmanual.\u201d alias \u00b6 The alias command lets you give your own name to a command or sequence of commands. You can then type your short name, and the shell will execute the command or sequence of commands for you. alias cls = clear This sets up an alias called cls . It will be another name for clear . When you type cls, it will clear the screen just as though you had typed clear . Your alias saves a few keystrokes, sure. But, if you frequently move between Windows and Linux command line, you can find yourself typing the Windows cls command on a Linux machine that doesn\u2019t know what you mean. Now it will know. Aliases can be much more intricate than that simple example. Here\u2019s an alias called pf (for process find) that is just a little more complex. Note the use of quotation marks around the command sequence. This is required if the command sequence has spaces in it. This alias uses the ps command to list the running processes and then pipes them through the grep command. The grep command looks for entries in the output from ps that match the command line parameter $1 . alias pf = \"ps -e | grep $1 \" If you wanted to discover the process ID (PID) of the shutter process\u2014or to find out if shutter was even running\u2014you could use the alias like this. Type pf followed by a space, and the name of the process you are intere$$sted in: pf shutter alias command in terminal window Warn Aliases defined on the command line will die with the terminal window. When you close it, they are gone. To make your aliases always be available to you, add them to the .bash_aliases file in your home directory such as .bash_profile and .bashrc cat \u00b6 The cat command (short for \u201cconcatenate\u201d) lists the contents of files to the terminal window. This is faster than opening the file in an editor, and there\u2019s no chance you can accidentally alter the file. To read the contents of your .bash_log_out file, type the following command while the home directory is your current working directory, as it is by default. cat .bash_logout With files longer than the number of lines in your terminal window, the text will whip past too fast for you to read. You can pipe the output from cat through less to make the process more manageable. With less you can scroll forward and backward through the file using the Up and Down Arrow keys, the PgUp and PgDn keys, and the Home and End keys. Type q to quit from less . cat .bashrc | less cat .bashrc | Less in a terminal window cd \u00b6 what is cd command? The cd command changes your current directory. In other words, it moves you to a new place in the filesystem.If you are changing to a directory that is within your current directory, you can simply type cd and the name of the other directory. cd work If you are changing to a directory elsewhere within the filesystem directory tree, provide the path to the directory with a leading / cd /usr/local/bin To quickly return to your home directory, use the ~ (tilde) character as the directory name. cd ~ Tip You can use the double dot symbol .. to represent the parent of the current directory. You can type the following command to go up a directory: cd .. Imagine you are in a directory. The parent directory has other directories in it, as well as the directory you\u2019re currently in. To change into one of those other directories, you can use the .. symbol to shorten what you have to type. cd ../games chmod \u00b6 The chmod command sets the file permissions flags on a file or folder. The flags define who can read, write to or execute the file. When you list files with the -l (long format) option you\u2019ll see a string of characters that look like -rwxrwxrwx If the first character is a - the item is a file. if it is a d the item is a directory. The rest of the string is three sets of three characters. From the left, the first three represent the file permissions of the owner , the middle three represent the file permissions of the group and the rightmost three characters represent the permissions for others . In each set:- r stands for read w stands for write x stands for execute. If the r, w, or x character is present that file permission is granted. If the letter is not present and a - appears instead, that file permission is not granted. Permission Levels One way to use chmod is to provide the permissions you wish to give to the owner, group, and others as a 3 digit number. The leftmost digit represents the owner. The middle digit represents the group. The rightmost digit represents the others. The digits you can use and what they represent are listed here: 0 : No permission 1 : Execute permission 2 : Write permission 3 : Write and execute permissions 4 : Read permission 5 : Read and execute permissions 6 : Read and write permissions 7 : Read, write and execute permissions Looking at our example.txt file, we can see that all three sets of characters are rwx . That means everyone has read, write and execute rights with the file. To set the permission to be read, write, and execute (7 from our list) for the owner; read and write (6 from our list) for the group; and read and execute (5 from our list) for the others we\u2019d need to use the digits 765 with the chmod command: chmod -R 765 example.txt chmod command in a terminal window To set the permission to be read, write and execute (7 from our list) for the owner, and read and write (6 from our list) for the group and for the others we\u2019d need to use the digits 766 with the chmod command: chmod 766 example.txt chown \u00b6 The chown command allows you to change the owner and group owner of a file. Listing our example.txt file with ls -l we can see amar amar in the file description. The first of these indicates the name of the file owner, which in this case is the user amar. The second entry shows that the name of the group owner is also amar. Each user has a default group created when the user is created. That user is the only member of that group. This shows that the file is not shared with any other groups of users. Tip You can use chown to change the owner or group, or both of a file. You must provide the name of the owner and the group, separated by a : character. You will need to use sudo. To retain amar as the owner of the file but to set mary as the group owner, use this command: sudo chown amar:mary example.txt To change both the owner and the group owner to mary, you would use the following command; sudo chown mary:mary example.txt To change the file so that amar is once more the file owner and the group owner, use this command: sudo chown amar:amar example.txt curl \u00b6 The curl command is a tool to retrieve information and files from Uniform Resource Locators (URLs) or internet addresses. Not able to find curl on your system? The curl command may not be provided as a standard part of your Linux distribution. Use apt-get to install this package onto your system if you\u2019re using Ubuntu or another Debian-based distribution. On other Linux distributions, use your Linux distribution\u2019s package management tool instead. sudo apt-get install curl Suppose you want to retrieve a single file from a GitHub repository. There is no officially supported way to this. You\u2019re forced to clone the entire repository. With curl however, we can retrieve the file we want on its own. This command retrieves the file for us. Note that you need to specify the name of the file to save it in, using the -o (output) option. Tip If you do not do this, the contents of the file are scrolled rapidly in the terminal window but not saved to your computer. curl https://raw.githubusercontent.com/torvalds/linux/master/kernel/events/core.c -o core.c If you don\u2019t want to see the download progress information use the -s (silent) option. curl -s https://raw.githubusercontent.com/torvalds/linux/master/kernel/events/core.c -o core.c df \u00b6 The df command shows the size, used space, and available space on the mounted filesystems of your computer. Two of the most useful options are the -h (human readable) and -x (exclude) options. The human-readable option displays the sizes in Mb or Gb instead of in bytes. The exclude option allows you to tell df to discount filesystems you are not interested in. For example, the squashfs pseudo-filesystems that are created when you install an application with the snap command. df -h -x squashfs diff \u00b6 The diff command compares two text files and shows the differences between them. There are many options to tailor the display to your requirements. The -y (side by side) option shows the line differences side by side. The -w (width) option lets you specify the maximum line width to use to avoid wraparound lines. The two files are called alpha1.txt and alpha2.txt in this example. The --suppress-common-lines prevents diff from listing the matching lines, letting you focus on the lines which have differences. diff -y -W 70 alpha1.txt alpha2.txt --suppress-common-lines echo \u00b6 The echo command prints (echoes) a string of text to the terminal window. The command below will print the words \u201cA string of text\u201d on the terminal window. echo A string of text The echo command can show the value of environment variables, for example, the $USER, $HOME, and $PATH environment variables. These hold the values of the name of the user, the user\u2019s home directory, and the path searched for matching commands when the user types something on the command line. echo $USER echo $HOME echo $PATH The following command will cause a bleep to be issued. The -e (escape code) option interprets the escaped a character as a \u2018bell\u2019 character. echo -e \"\\a\" The echo command is also invaluable in shell scripts. A script can use this command to generate visible output to indicate the progress or results of the script as it is executed. exit \u00b6 The exit command will close a terminal window, end the execution of a shell script, or log you out of an SSH remote access session. exit find \u00b6 Use the find command to track down files that you know exist if you can\u2019t remember where you put them. You must tell find where to start searching from and what it is looking for. In this example, the . matches the current folder and the -name option tells find to look for files with a name that matches the search pattern. You can use wildcards, where * represents any sequence of characters and ? represents any single character. We\u2019re using ones to match any file name containing the sequence \u201cones.\u201d This would match words like bones, stones, and lonesome. find . -name *ones* find command in a terminal window As we can see, find has returned a list of matches. One of them is a directory called Ramones. We can tell find to restrict the search to files only. We do this using the -type option with the f parameter. The f parameter stands for files. find . -type f -name *ones* If you want the search to be case insensitive use the -iname (insensitive name) option. find . -iname *wild* finger \u00b6 The finger command gives you a short dump of information about a user, including the time of the user\u2019s last login, the user\u2019s home directory, and the user account\u2019s full name. free \u00b6 The free command gives you a summary of the memory usage with your computer. It does this for both the main Random Access Memory (RAM) and swap memory. The -h (human) option is used to provide human-friendly numbers and units. Without this option, the figures are presented in bytes. free -h grep \u00b6 The grep utility searches for lines which contain a search pattern. When we looked at the alias command, we used grep to search through the output of another program, ps . The grep command can also search the contents of files. Here we\u2019re searching for the word \u201ctrain\u201d in all text files in the current directory. grep train *.txt The output lists the name of the file and shows the lines that match. The matching text is highlighted. grep command in a terminal window The functionality and sheer usefulness of grep definitely warrants you checking out its man page. groups \u00b6 The groups command tells you which groups a user is a member of. groups dave groups mary gzip \u00b6 The gzip command compresses files. By default, it removes the original file and leaves you with the compressed version. To retain both the original and the compressed version, use the -k (keep) option. gzip -k core.c head \u00b6 The head command gives you a listing of the first 10 lines of a file. If you want to see fewer or more lines, use the -n (number) option. In this example, we use head with its default of 10 lines. We then repeat the command asking for only five lines. head -core.c head -n 5 core.c history \u00b6 The history command lists the commands you have previously issued on the command line. You can repeat any of the commands from your history by typing an exclamation point ! and the number of the command from the history list. !188 Typing two exclamation points repeats your previous command. !! kill \u00b6 The kill command allows you to terminate a process from the command line. You do this by providing the process ID (PID) of the process to kill. Don\u2019t kill processes willy-nilly. You need to have a good reason to do so. In this example, we\u2019ll pretend the shutter program has locked up. To find the PID of shutter we\u2019ll use our ps and grep trick from the section about the alias command, above. We can search for the shutter process and obtain its PID as follows: ps -e | grep shutter. Once we have determined the PID\u20141692 in this case\u2014we can kill it as follows: kill 1692 kill command in a terminal window less \u00b6 The less command allows you to view files without opening an editor. It\u2019s faster to use, and there\u2019s no chance of you inadvertently modifying the file. With less you can scroll forward and backward through the file using the Up and Down Arrow keys, the PgUp and PgDn keys and the Home and End keys. Press the Q key to quit from less. To view a file provide its name to less as follows: less core.c less command in a terminal window You can also pipe the output from other commands into less. To see the output from ls for a listing of your entire hard drive, use the following command: ls -R / | less less command in a terminal window Use / to search forward in the file and use ? to search backward. ls \u00b6 This might be the first command the majority of Linux users meet. It lists the files and folders in the directory you specify. By default, ls looks in the current directory. There are a great many options you can use with ls , and we strongly advise reviewing its the man page. Some common examples are presented here. To list the files and folders in the current directory: ls To list the files and folders in the current directory with a detailed listing use the -l (long) option: ls -l To use human-friendly file sizes include the -h (human) option: ls -lh To include hidden files use the -a (all files) option: ls -lha ls command in a terminal window man \u00b6 The man command displays the \u201cman pages\u201d for a command in less . The man pages are the user manual for that command. Because man uses less to display the man pages, you can use the search capabilities of less. For example, to see the man pages for chown, use the following command: man chown Use the Up and Down arrow or PgUp and PgDn keys to scroll through the document. Press q to quit the man page or pressh for help. man command in a terminal window mkdir \u00b6 The mkdir command allows you to create new directories in the filesystem. You must provide the name of the new directory to mkdir. If the new directory is not going to be within the current directory, you must provide the path to the new directory. To create two new directories in the current directory called \u201cinvoices\u201d and \u201cquotes,\u201d use these two commands: mkdir invoices mkdir quotes To create a new directory called \u201c2019\u201d inside the \u201cinvoices\u201d directory, use this command: mkdir invoices/2109 If you are going to create a directory, but its parent directory does not exist, you can use the -p (parents) option to have mkdir create all of the required parent directories too. In the following command, we are creating the \u201c2019\u201d directory inside the \u201cyearly\u201d directory inside the \u201cquotes\u201d directory. The \u201cyearly\u201d directory does not exist, but we can have mkdir create all the specified directories at once: mkdir -p quotes/yearly/2019 The \u201cyearly\u201d directory is also created. mv \u00b6 The mv command allows you to move files and directories from directory to directory. It also allows you to rename files. To move a file you must tell mv where the file is and where you want it to be moved to. In this example, we\u2019re moving a file called apache.pdf from the ~/Document/Ukulele directory and placing it in the current directory, represented by the single . character. mv ~/Documents/Ukulele/Apache.pdf . To rename the file, you \u201cmove\u201d it into a new file with the new name. mv Apache.pdf The_Shadows_Apache.pdf The file move and rename action could have been achieved in one step: mv ~/Documents/Ukulele/Apache.pdf ./The_Shadows_Apache.pdf passwd \u00b6 The passwd command lets you change the password for a user. Just type passwd to change your own password. You can also change the password of another user account, but you must use sudo. You will be asked to enter the new password twice. sudo passwd mary ping \u00b6 The ping command lets you verify that you have network connectivity with another network device. It is commonly used to help troubleshoot networking issues. To use ping, provide the IP address or machine name of the other device. ping 192 .168.4.18 The ping command will run until you stop it with Ctrl+C . Here\u2019s what\u2019s going on here: The device at IP address 192.168.4.18 is responding to our ping requests and is sending back packets of 64 bytes. The Internet Control Messaging Protocol (ICMP) sequence numbering allows us to check for missed responses (dropped packets). The TTL figure is the \u201ctime to live\u201d for a packet. Each time the packet goes through a router, it is (supposed to be) decremented by one. If it reaches zero the packet is thrown away. The aim of this is to prevent network loopback problems from flooding the network. The time value is the duration of the round trip from your computer to the device and back. Simply put, the lower this time, the better. To ask ping to run for a specific number of ping attempts, use the -c (count) option. ping -c 5 192 .168.4.18 To hear a ping, use the -a (audible) option. ping -a 192 .168.4.18 ps \u00b6 The ps command lists running processes. Using ps without any options causes it to list the processes running in the current shell. ps To see all the processes related to a particular user, use the -u (user) option. This is likely to be a long list, so for convenience pipe it through less. ps -u dave | less To see every process that is running, use the -e (every process) option: ps -e | less pwd \u00b6 Nice and simple, the pwd command prints the working directory (the current directory) from the root / directory. pwd pwd command in a terminal window shutdown \u00b6 The shutdown command lets you shut down or reboot your Linux system. Using shutdown with no parameters will shut down your computer in one minute. shutdown To shut down immediately, use the now parameter. shutdown now You can also schedule a shutdown and inform any logged in users of the pending shutdown. To let the shutdown command know when you want it to shut down, you provide it with a time. This can be a set number of minutes from now, such as +90 or a precise time, like 23:00. Any text message you provide is broadcast to logged in users. shutdown 23 :00 Shutdown tonight at 23 :00, save your work and log out before then ! To cancel a shutdown, use the -c (cancel) option. Here we have scheduled a shutdown for fifteen minutes time from now\u2014and then changed our minds. shutdown +15 Shutting down in 15 minutes! shutdown -c Shutdown -c cancel command SSH \u00b6 Use the ssh command to make a connection to a remote Linux computer and log into your account. To make a connection, you must provide your user name and the IP address or domain name of the remote computer. In this example, the user mary is logging into the computer at 192.168.4.23. Once the connection is established, she is asked for her password. ssh mary@192.168.4.23 ssh command in a terminal window Her user name and password are verified and accepted, and she is logged in. Notice that her prompt has changed from \u201cNostromo\u201d to \u201chowtogeek.\u201d Mary issues the w command to list the current users on \u201chowtogeek\u201d system. She is listed as being connected from pts/1, which is a pseudo-terminal slave. That is, it is not a terminal directly connected to the computer. To close the session, mary types exit and is returned to the shell on the \u201cNostromo\u201d computer. w exit w and exit commands in a terminal window sudo \u00b6 The sudo command is required when performing actions that require root or superuser permissions, such as changing the password for another user. sudo passwd mary passwd command in a terminal window tail \u00b6 The tail command gives you a listing of the last 10 lines of a file. If you want to see fewer or more lines, use the -n (number) option. In this example, we use tail with its default of 10 lines. We then repeat the command asking for only five lines. tail core.c tail -n 5 core.c tail command in a terminal window tar \u00b6 With the tar command, you can create an archive file (also called a tarball) that can contain many other files. This makes it much more convenient to distribute a collection of files. You can also use tar to extract the files from an archive file. It is common to ask tar to compress the archive. If you do not ask for compression, the archive file is created uncompressed. To create an archive file, you need to tell tar which files to include in the archive file, and the name you wish the archive file to have. In this example, the user is going to archive all of the files in the Ukulele directory, which is in the current directory. ls command in the terminal window They have used the -c (create) option and the -v (verbose) option. The verbose option gives some visual feedback by listing the files to the terminal window as they are added to the archive. The -f (filename) option is followed by the desired name of the archive. In this case, it is songs.tar. tar -cvf songs.tar Ukulele/ tar -cvf command in a terminal window The files are listed to the terminal window as they are added to the archive file. There are two ways to tell tar that you want the archive file to be compressed. The first is with the -z (gzip) option. This tells tar to use the gzip utility to compress the archive once it has been created. It is usual to add .gz as suffix to this type of archive. That allows anyone who is extracting files from it to know which commands to pass to tar to correctly retrieve the files. tar -cvzf songs.tar.gz Ukulele/ The files are listed to the terminal window as they are added to the archive file as before, but the creation of the archive will take a little longer because of the time required for the compression. To create an archive file that is compressed using a superior compression algorithm giving a smaller archive file use the -j (bzip2) option. tar -cvjf songs.tar.bz2 Ukulele/ Once again, the files are listed as the archive is created. The -j option is noticeably slower than the -z option. If you are archiving a great many files, you must choose between the -z option for decent compression and reasonable speed, or the -j option for better compression and slower speed. As can be seen in the screenshot below, the \u201c.tar\u201d file is the largest, the \u201c.tar.gz\u201d is smaller, and the \u201c.tar.bz2\u201d is the smallest of the archives. To extract files from an archive file use the -x (extract) option. The -v (verbose) and -f (filename) options behave as they do when creating archives. Use ls to confirm which type of archive you are going to extract the files from, then issue the following command. ls tar -xvf songs.tar ls and tar -xvf commands in a terminal window The files are listed as they are extracted. Note that the Ukulele directory is also recreated for you. To extract files from a \u201c.tar.gz\u201d archive, use the -z (gzip) option. tar -xvzf songs.tar.gz tar -xvzf command in a terminal window Finally, to extract files from a \u201c.tar.bz2\u201d archive use the -j option instead of the -z (gzip) option. tar -xvjf songs.tar.bz2 tar -xvjf command in a terminal window RELATED: How to Extract Files From a .tar.gz or .tar.bz2 File on Linux top \u00b6 The top command shows you a real-time display of the data relating to your Linux machine. The top of the screen is a status summary. The first line shows you the time and how long your computer has been running for, how many users are logged into it, and what the load average has been over the past one, five, and fifteen minutes. The second line shows the number of tasks and their states: running, stopped, sleeping and zombie. The third line shows CPU information. Here\u2019s what the fields mean: us: value is the CPU time the CPU spends executing processes for users, in \u201cuser space\u201d sy: value is the CPU time spent on running system \u201ckernel space\u201d processes ni: value is the CPU time spent on executing processes with a manually set nice value id: is the amount of CPU idle time wa: value is the time the CPU spends waiting for I/O to complete hi: The CPU time spent servicing hardware interrupts si: The CPU time spent servicing software interrupts st: The CPU time lost due to running virtual machines ( \u201csteal time\u201d ) The fourth line shows the total amount of physical memory, and how much is free, used and buffered or cached. The fifth line shows the total amount of swap memory, and how much is free, used and available (taking into account memory that is expected to be recoverable from caches). top command in a terminal window The user has pressed the E key to change the display into more humanly digestible figures instead of long integers representing bytes. The columns in the main display are made up of: PID: Process ID USER: Name of the owner of the process PR: Process priority NI: The nice value of the process VIRT: Virtual memory used by the process RES: Resident memory used by the process SHR: Shared memory used by the process S: Status of the process. See the list below of the values this field can take %CPU: the share of CPU time used by the process since last update %MEM: share of physical memory used TIME+: total CPU time used by the task in hundredths of a second COMMAND: command name or command line ( name + options ) The status of the process can be one of: D: Uninterruptible sleep R: Running S: Sleeping T: Traced ( stopped ) Z: Zombie Press the Q key to exit from top. RELATED: How to Set Process Priorities With nice and renice on Linux uname \u00b6 You can obtain some system information regarding the Linux computer you\u2019re working on with the uname command. Use the -a (all) option to see everything. Use the -s (kernel name) option to see the type of kernel. Use the -r (kernel release) option to see the kernel release. Use the -v (kernel version) option to see the kernel version. uname -a uname -s uname -r uname -v uname command in a terminal window w \u00b6 The w command lists the currently logged in users. w w command in a terminal window whoami \u00b6 Use whoami to find out who you are logged in as or who is logged into an unmanned Linux terminal.","title":"Commands 101"},{"location":"technical_stuff/linux/linuxCommands/#linux-commands","text":"Linux includes a large number of commands, but I have chosen 37 of the most important ones to present here. Learn these commands, and you\u2019ll be much more at home at the Linux command prompt. The below list is presented in alphabetical order. A command\u2019s position in the list is not representative of its usefulness or simplicity. Info For the final word on a command\u2019s usage, refer to its man pages. The man command is in our list, of course\u2014it\u2019s short for \u201cmanual.\u201d","title":"Linux commands"},{"location":"technical_stuff/linux/linuxCommands/#alias","text":"The alias command lets you give your own name to a command or sequence of commands. You can then type your short name, and the shell will execute the command or sequence of commands for you. alias cls = clear This sets up an alias called cls . It will be another name for clear . When you type cls, it will clear the screen just as though you had typed clear . Your alias saves a few keystrokes, sure. But, if you frequently move between Windows and Linux command line, you can find yourself typing the Windows cls command on a Linux machine that doesn\u2019t know what you mean. Now it will know. Aliases can be much more intricate than that simple example. Here\u2019s an alias called pf (for process find) that is just a little more complex. Note the use of quotation marks around the command sequence. This is required if the command sequence has spaces in it. This alias uses the ps command to list the running processes and then pipes them through the grep command. The grep command looks for entries in the output from ps that match the command line parameter $1 . alias pf = \"ps -e | grep $1 \" If you wanted to discover the process ID (PID) of the shutter process\u2014or to find out if shutter was even running\u2014you could use the alias like this. Type pf followed by a space, and the name of the process you are intere$$sted in: pf shutter alias command in terminal window Warn Aliases defined on the command line will die with the terminal window. When you close it, they are gone. To make your aliases always be available to you, add them to the .bash_aliases file in your home directory such as .bash_profile and .bashrc","title":"alias"},{"location":"technical_stuff/linux/linuxCommands/#cat","text":"The cat command (short for \u201cconcatenate\u201d) lists the contents of files to the terminal window. This is faster than opening the file in an editor, and there\u2019s no chance you can accidentally alter the file. To read the contents of your .bash_log_out file, type the following command while the home directory is your current working directory, as it is by default. cat .bash_logout With files longer than the number of lines in your terminal window, the text will whip past too fast for you to read. You can pipe the output from cat through less to make the process more manageable. With less you can scroll forward and backward through the file using the Up and Down Arrow keys, the PgUp and PgDn keys, and the Home and End keys. Type q to quit from less . cat .bashrc | less cat .bashrc | Less in a terminal window","title":"cat"},{"location":"technical_stuff/linux/linuxCommands/#cd","text":"what is cd command? The cd command changes your current directory. In other words, it moves you to a new place in the filesystem.If you are changing to a directory that is within your current directory, you can simply type cd and the name of the other directory. cd work If you are changing to a directory elsewhere within the filesystem directory tree, provide the path to the directory with a leading / cd /usr/local/bin To quickly return to your home directory, use the ~ (tilde) character as the directory name. cd ~ Tip You can use the double dot symbol .. to represent the parent of the current directory. You can type the following command to go up a directory: cd .. Imagine you are in a directory. The parent directory has other directories in it, as well as the directory you\u2019re currently in. To change into one of those other directories, you can use the .. symbol to shorten what you have to type. cd ../games","title":"cd"},{"location":"technical_stuff/linux/linuxCommands/#chmod","text":"The chmod command sets the file permissions flags on a file or folder. The flags define who can read, write to or execute the file. When you list files with the -l (long format) option you\u2019ll see a string of characters that look like -rwxrwxrwx If the first character is a - the item is a file. if it is a d the item is a directory. The rest of the string is three sets of three characters. From the left, the first three represent the file permissions of the owner , the middle three represent the file permissions of the group and the rightmost three characters represent the permissions for others . In each set:- r stands for read w stands for write x stands for execute. If the r, w, or x character is present that file permission is granted. If the letter is not present and a - appears instead, that file permission is not granted. Permission Levels One way to use chmod is to provide the permissions you wish to give to the owner, group, and others as a 3 digit number. The leftmost digit represents the owner. The middle digit represents the group. The rightmost digit represents the others. The digits you can use and what they represent are listed here: 0 : No permission 1 : Execute permission 2 : Write permission 3 : Write and execute permissions 4 : Read permission 5 : Read and execute permissions 6 : Read and write permissions 7 : Read, write and execute permissions Looking at our example.txt file, we can see that all three sets of characters are rwx . That means everyone has read, write and execute rights with the file. To set the permission to be read, write, and execute (7 from our list) for the owner; read and write (6 from our list) for the group; and read and execute (5 from our list) for the others we\u2019d need to use the digits 765 with the chmod command: chmod -R 765 example.txt chmod command in a terminal window To set the permission to be read, write and execute (7 from our list) for the owner, and read and write (6 from our list) for the group and for the others we\u2019d need to use the digits 766 with the chmod command: chmod 766 example.txt","title":"chmod"},{"location":"technical_stuff/linux/linuxCommands/#chown","text":"The chown command allows you to change the owner and group owner of a file. Listing our example.txt file with ls -l we can see amar amar in the file description. The first of these indicates the name of the file owner, which in this case is the user amar. The second entry shows that the name of the group owner is also amar. Each user has a default group created when the user is created. That user is the only member of that group. This shows that the file is not shared with any other groups of users. Tip You can use chown to change the owner or group, or both of a file. You must provide the name of the owner and the group, separated by a : character. You will need to use sudo. To retain amar as the owner of the file but to set mary as the group owner, use this command: sudo chown amar:mary example.txt To change both the owner and the group owner to mary, you would use the following command; sudo chown mary:mary example.txt To change the file so that amar is once more the file owner and the group owner, use this command: sudo chown amar:amar example.txt","title":"chown"},{"location":"technical_stuff/linux/linuxCommands/#curl","text":"The curl command is a tool to retrieve information and files from Uniform Resource Locators (URLs) or internet addresses. Not able to find curl on your system? The curl command may not be provided as a standard part of your Linux distribution. Use apt-get to install this package onto your system if you\u2019re using Ubuntu or another Debian-based distribution. On other Linux distributions, use your Linux distribution\u2019s package management tool instead. sudo apt-get install curl Suppose you want to retrieve a single file from a GitHub repository. There is no officially supported way to this. You\u2019re forced to clone the entire repository. With curl however, we can retrieve the file we want on its own. This command retrieves the file for us. Note that you need to specify the name of the file to save it in, using the -o (output) option. Tip If you do not do this, the contents of the file are scrolled rapidly in the terminal window but not saved to your computer. curl https://raw.githubusercontent.com/torvalds/linux/master/kernel/events/core.c -o core.c If you don\u2019t want to see the download progress information use the -s (silent) option. curl -s https://raw.githubusercontent.com/torvalds/linux/master/kernel/events/core.c -o core.c","title":"curl"},{"location":"technical_stuff/linux/linuxCommands/#df","text":"The df command shows the size, used space, and available space on the mounted filesystems of your computer. Two of the most useful options are the -h (human readable) and -x (exclude) options. The human-readable option displays the sizes in Mb or Gb instead of in bytes. The exclude option allows you to tell df to discount filesystems you are not interested in. For example, the squashfs pseudo-filesystems that are created when you install an application with the snap command. df -h -x squashfs","title":"df"},{"location":"technical_stuff/linux/linuxCommands/#diff","text":"The diff command compares two text files and shows the differences between them. There are many options to tailor the display to your requirements. The -y (side by side) option shows the line differences side by side. The -w (width) option lets you specify the maximum line width to use to avoid wraparound lines. The two files are called alpha1.txt and alpha2.txt in this example. The --suppress-common-lines prevents diff from listing the matching lines, letting you focus on the lines which have differences. diff -y -W 70 alpha1.txt alpha2.txt --suppress-common-lines","title":"diff"},{"location":"technical_stuff/linux/linuxCommands/#echo","text":"The echo command prints (echoes) a string of text to the terminal window. The command below will print the words \u201cA string of text\u201d on the terminal window. echo A string of text The echo command can show the value of environment variables, for example, the $USER, $HOME, and $PATH environment variables. These hold the values of the name of the user, the user\u2019s home directory, and the path searched for matching commands when the user types something on the command line. echo $USER echo $HOME echo $PATH The following command will cause a bleep to be issued. The -e (escape code) option interprets the escaped a character as a \u2018bell\u2019 character. echo -e \"\\a\" The echo command is also invaluable in shell scripts. A script can use this command to generate visible output to indicate the progress or results of the script as it is executed.","title":"echo"},{"location":"technical_stuff/linux/linuxCommands/#exit","text":"The exit command will close a terminal window, end the execution of a shell script, or log you out of an SSH remote access session. exit","title":"exit"},{"location":"technical_stuff/linux/linuxCommands/#find","text":"Use the find command to track down files that you know exist if you can\u2019t remember where you put them. You must tell find where to start searching from and what it is looking for. In this example, the . matches the current folder and the -name option tells find to look for files with a name that matches the search pattern. You can use wildcards, where * represents any sequence of characters and ? represents any single character. We\u2019re using ones to match any file name containing the sequence \u201cones.\u201d This would match words like bones, stones, and lonesome. find . -name *ones* find command in a terminal window As we can see, find has returned a list of matches. One of them is a directory called Ramones. We can tell find to restrict the search to files only. We do this using the -type option with the f parameter. The f parameter stands for files. find . -type f -name *ones* If you want the search to be case insensitive use the -iname (insensitive name) option. find . -iname *wild*","title":"find"},{"location":"technical_stuff/linux/linuxCommands/#finger","text":"The finger command gives you a short dump of information about a user, including the time of the user\u2019s last login, the user\u2019s home directory, and the user account\u2019s full name.","title":"finger"},{"location":"technical_stuff/linux/linuxCommands/#free","text":"The free command gives you a summary of the memory usage with your computer. It does this for both the main Random Access Memory (RAM) and swap memory. The -h (human) option is used to provide human-friendly numbers and units. Without this option, the figures are presented in bytes. free -h","title":"free"},{"location":"technical_stuff/linux/linuxCommands/#grep","text":"The grep utility searches for lines which contain a search pattern. When we looked at the alias command, we used grep to search through the output of another program, ps . The grep command can also search the contents of files. Here we\u2019re searching for the word \u201ctrain\u201d in all text files in the current directory. grep train *.txt The output lists the name of the file and shows the lines that match. The matching text is highlighted. grep command in a terminal window The functionality and sheer usefulness of grep definitely warrants you checking out its man page.","title":"grep"},{"location":"technical_stuff/linux/linuxCommands/#groups","text":"The groups command tells you which groups a user is a member of. groups dave groups mary","title":"groups"},{"location":"technical_stuff/linux/linuxCommands/#gzip","text":"The gzip command compresses files. By default, it removes the original file and leaves you with the compressed version. To retain both the original and the compressed version, use the -k (keep) option. gzip -k core.c","title":"gzip"},{"location":"technical_stuff/linux/linuxCommands/#head","text":"The head command gives you a listing of the first 10 lines of a file. If you want to see fewer or more lines, use the -n (number) option. In this example, we use head with its default of 10 lines. We then repeat the command asking for only five lines. head -core.c head -n 5 core.c","title":"head"},{"location":"technical_stuff/linux/linuxCommands/#history","text":"The history command lists the commands you have previously issued on the command line. You can repeat any of the commands from your history by typing an exclamation point ! and the number of the command from the history list. !188 Typing two exclamation points repeats your previous command. !!","title":"history"},{"location":"technical_stuff/linux/linuxCommands/#kill","text":"The kill command allows you to terminate a process from the command line. You do this by providing the process ID (PID) of the process to kill. Don\u2019t kill processes willy-nilly. You need to have a good reason to do so. In this example, we\u2019ll pretend the shutter program has locked up. To find the PID of shutter we\u2019ll use our ps and grep trick from the section about the alias command, above. We can search for the shutter process and obtain its PID as follows: ps -e | grep shutter. Once we have determined the PID\u20141692 in this case\u2014we can kill it as follows: kill 1692 kill command in a terminal window","title":"kill"},{"location":"technical_stuff/linux/linuxCommands/#less","text":"The less command allows you to view files without opening an editor. It\u2019s faster to use, and there\u2019s no chance of you inadvertently modifying the file. With less you can scroll forward and backward through the file using the Up and Down Arrow keys, the PgUp and PgDn keys and the Home and End keys. Press the Q key to quit from less. To view a file provide its name to less as follows: less core.c less command in a terminal window You can also pipe the output from other commands into less. To see the output from ls for a listing of your entire hard drive, use the following command: ls -R / | less less command in a terminal window Use / to search forward in the file and use ? to search backward.","title":"less"},{"location":"technical_stuff/linux/linuxCommands/#ls","text":"This might be the first command the majority of Linux users meet. It lists the files and folders in the directory you specify. By default, ls looks in the current directory. There are a great many options you can use with ls , and we strongly advise reviewing its the man page. Some common examples are presented here. To list the files and folders in the current directory: ls To list the files and folders in the current directory with a detailed listing use the -l (long) option: ls -l To use human-friendly file sizes include the -h (human) option: ls -lh To include hidden files use the -a (all files) option: ls -lha ls command in a terminal window","title":"ls"},{"location":"technical_stuff/linux/linuxCommands/#man","text":"The man command displays the \u201cman pages\u201d for a command in less . The man pages are the user manual for that command. Because man uses less to display the man pages, you can use the search capabilities of less. For example, to see the man pages for chown, use the following command: man chown Use the Up and Down arrow or PgUp and PgDn keys to scroll through the document. Press q to quit the man page or pressh for help. man command in a terminal window","title":"man"},{"location":"technical_stuff/linux/linuxCommands/#mkdir","text":"The mkdir command allows you to create new directories in the filesystem. You must provide the name of the new directory to mkdir. If the new directory is not going to be within the current directory, you must provide the path to the new directory. To create two new directories in the current directory called \u201cinvoices\u201d and \u201cquotes,\u201d use these two commands: mkdir invoices mkdir quotes To create a new directory called \u201c2019\u201d inside the \u201cinvoices\u201d directory, use this command: mkdir invoices/2109 If you are going to create a directory, but its parent directory does not exist, you can use the -p (parents) option to have mkdir create all of the required parent directories too. In the following command, we are creating the \u201c2019\u201d directory inside the \u201cyearly\u201d directory inside the \u201cquotes\u201d directory. The \u201cyearly\u201d directory does not exist, but we can have mkdir create all the specified directories at once: mkdir -p quotes/yearly/2019 The \u201cyearly\u201d directory is also created.","title":"mkdir"},{"location":"technical_stuff/linux/linuxCommands/#mv","text":"The mv command allows you to move files and directories from directory to directory. It also allows you to rename files. To move a file you must tell mv where the file is and where you want it to be moved to. In this example, we\u2019re moving a file called apache.pdf from the ~/Document/Ukulele directory and placing it in the current directory, represented by the single . character. mv ~/Documents/Ukulele/Apache.pdf . To rename the file, you \u201cmove\u201d it into a new file with the new name. mv Apache.pdf The_Shadows_Apache.pdf The file move and rename action could have been achieved in one step: mv ~/Documents/Ukulele/Apache.pdf ./The_Shadows_Apache.pdf","title":"mv"},{"location":"technical_stuff/linux/linuxCommands/#passwd","text":"The passwd command lets you change the password for a user. Just type passwd to change your own password. You can also change the password of another user account, but you must use sudo. You will be asked to enter the new password twice. sudo passwd mary","title":"passwd"},{"location":"technical_stuff/linux/linuxCommands/#ping","text":"The ping command lets you verify that you have network connectivity with another network device. It is commonly used to help troubleshoot networking issues. To use ping, provide the IP address or machine name of the other device. ping 192 .168.4.18 The ping command will run until you stop it with Ctrl+C . Here\u2019s what\u2019s going on here: The device at IP address 192.168.4.18 is responding to our ping requests and is sending back packets of 64 bytes. The Internet Control Messaging Protocol (ICMP) sequence numbering allows us to check for missed responses (dropped packets). The TTL figure is the \u201ctime to live\u201d for a packet. Each time the packet goes through a router, it is (supposed to be) decremented by one. If it reaches zero the packet is thrown away. The aim of this is to prevent network loopback problems from flooding the network. The time value is the duration of the round trip from your computer to the device and back. Simply put, the lower this time, the better. To ask ping to run for a specific number of ping attempts, use the -c (count) option. ping -c 5 192 .168.4.18 To hear a ping, use the -a (audible) option. ping -a 192 .168.4.18","title":"ping"},{"location":"technical_stuff/linux/linuxCommands/#ps","text":"The ps command lists running processes. Using ps without any options causes it to list the processes running in the current shell. ps To see all the processes related to a particular user, use the -u (user) option. This is likely to be a long list, so for convenience pipe it through less. ps -u dave | less To see every process that is running, use the -e (every process) option: ps -e | less","title":"ps"},{"location":"technical_stuff/linux/linuxCommands/#pwd","text":"Nice and simple, the pwd command prints the working directory (the current directory) from the root / directory. pwd pwd command in a terminal window","title":"pwd"},{"location":"technical_stuff/linux/linuxCommands/#shutdown","text":"The shutdown command lets you shut down or reboot your Linux system. Using shutdown with no parameters will shut down your computer in one minute. shutdown To shut down immediately, use the now parameter. shutdown now You can also schedule a shutdown and inform any logged in users of the pending shutdown. To let the shutdown command know when you want it to shut down, you provide it with a time. This can be a set number of minutes from now, such as +90 or a precise time, like 23:00. Any text message you provide is broadcast to logged in users. shutdown 23 :00 Shutdown tonight at 23 :00, save your work and log out before then ! To cancel a shutdown, use the -c (cancel) option. Here we have scheduled a shutdown for fifteen minutes time from now\u2014and then changed our minds. shutdown +15 Shutting down in 15 minutes! shutdown -c Shutdown -c cancel command","title":"shutdown"},{"location":"technical_stuff/linux/linuxCommands/#ssh","text":"Use the ssh command to make a connection to a remote Linux computer and log into your account. To make a connection, you must provide your user name and the IP address or domain name of the remote computer. In this example, the user mary is logging into the computer at 192.168.4.23. Once the connection is established, she is asked for her password. ssh mary@192.168.4.23 ssh command in a terminal window Her user name and password are verified and accepted, and she is logged in. Notice that her prompt has changed from \u201cNostromo\u201d to \u201chowtogeek.\u201d Mary issues the w command to list the current users on \u201chowtogeek\u201d system. She is listed as being connected from pts/1, which is a pseudo-terminal slave. That is, it is not a terminal directly connected to the computer. To close the session, mary types exit and is returned to the shell on the \u201cNostromo\u201d computer. w exit w and exit commands in a terminal window","title":"SSH"},{"location":"technical_stuff/linux/linuxCommands/#sudo","text":"The sudo command is required when performing actions that require root or superuser permissions, such as changing the password for another user. sudo passwd mary passwd command in a terminal window","title":"sudo"},{"location":"technical_stuff/linux/linuxCommands/#tail","text":"The tail command gives you a listing of the last 10 lines of a file. If you want to see fewer or more lines, use the -n (number) option. In this example, we use tail with its default of 10 lines. We then repeat the command asking for only five lines. tail core.c tail -n 5 core.c tail command in a terminal window","title":"tail"},{"location":"technical_stuff/linux/linuxCommands/#tar","text":"With the tar command, you can create an archive file (also called a tarball) that can contain many other files. This makes it much more convenient to distribute a collection of files. You can also use tar to extract the files from an archive file. It is common to ask tar to compress the archive. If you do not ask for compression, the archive file is created uncompressed. To create an archive file, you need to tell tar which files to include in the archive file, and the name you wish the archive file to have. In this example, the user is going to archive all of the files in the Ukulele directory, which is in the current directory. ls command in the terminal window They have used the -c (create) option and the -v (verbose) option. The verbose option gives some visual feedback by listing the files to the terminal window as they are added to the archive. The -f (filename) option is followed by the desired name of the archive. In this case, it is songs.tar. tar -cvf songs.tar Ukulele/ tar -cvf command in a terminal window The files are listed to the terminal window as they are added to the archive file. There are two ways to tell tar that you want the archive file to be compressed. The first is with the -z (gzip) option. This tells tar to use the gzip utility to compress the archive once it has been created. It is usual to add .gz as suffix to this type of archive. That allows anyone who is extracting files from it to know which commands to pass to tar to correctly retrieve the files. tar -cvzf songs.tar.gz Ukulele/ The files are listed to the terminal window as they are added to the archive file as before, but the creation of the archive will take a little longer because of the time required for the compression. To create an archive file that is compressed using a superior compression algorithm giving a smaller archive file use the -j (bzip2) option. tar -cvjf songs.tar.bz2 Ukulele/ Once again, the files are listed as the archive is created. The -j option is noticeably slower than the -z option. If you are archiving a great many files, you must choose between the -z option for decent compression and reasonable speed, or the -j option for better compression and slower speed. As can be seen in the screenshot below, the \u201c.tar\u201d file is the largest, the \u201c.tar.gz\u201d is smaller, and the \u201c.tar.bz2\u201d is the smallest of the archives. To extract files from an archive file use the -x (extract) option. The -v (verbose) and -f (filename) options behave as they do when creating archives. Use ls to confirm which type of archive you are going to extract the files from, then issue the following command. ls tar -xvf songs.tar ls and tar -xvf commands in a terminal window The files are listed as they are extracted. Note that the Ukulele directory is also recreated for you. To extract files from a \u201c.tar.gz\u201d archive, use the -z (gzip) option. tar -xvzf songs.tar.gz tar -xvzf command in a terminal window Finally, to extract files from a \u201c.tar.bz2\u201d archive use the -j option instead of the -z (gzip) option. tar -xvjf songs.tar.bz2 tar -xvjf command in a terminal window RELATED: How to Extract Files From a .tar.gz or .tar.bz2 File on Linux","title":"tar"},{"location":"technical_stuff/linux/linuxCommands/#top","text":"The top command shows you a real-time display of the data relating to your Linux machine. The top of the screen is a status summary. The first line shows you the time and how long your computer has been running for, how many users are logged into it, and what the load average has been over the past one, five, and fifteen minutes. The second line shows the number of tasks and their states: running, stopped, sleeping and zombie. The third line shows CPU information. Here\u2019s what the fields mean: us: value is the CPU time the CPU spends executing processes for users, in \u201cuser space\u201d sy: value is the CPU time spent on running system \u201ckernel space\u201d processes ni: value is the CPU time spent on executing processes with a manually set nice value id: is the amount of CPU idle time wa: value is the time the CPU spends waiting for I/O to complete hi: The CPU time spent servicing hardware interrupts si: The CPU time spent servicing software interrupts st: The CPU time lost due to running virtual machines ( \u201csteal time\u201d ) The fourth line shows the total amount of physical memory, and how much is free, used and buffered or cached. The fifth line shows the total amount of swap memory, and how much is free, used and available (taking into account memory that is expected to be recoverable from caches). top command in a terminal window The user has pressed the E key to change the display into more humanly digestible figures instead of long integers representing bytes. The columns in the main display are made up of: PID: Process ID USER: Name of the owner of the process PR: Process priority NI: The nice value of the process VIRT: Virtual memory used by the process RES: Resident memory used by the process SHR: Shared memory used by the process S: Status of the process. See the list below of the values this field can take %CPU: the share of CPU time used by the process since last update %MEM: share of physical memory used TIME+: total CPU time used by the task in hundredths of a second COMMAND: command name or command line ( name + options ) The status of the process can be one of: D: Uninterruptible sleep R: Running S: Sleeping T: Traced ( stopped ) Z: Zombie Press the Q key to exit from top. RELATED: How to Set Process Priorities With nice and renice on Linux","title":"top"},{"location":"technical_stuff/linux/linuxCommands/#uname","text":"You can obtain some system information regarding the Linux computer you\u2019re working on with the uname command. Use the -a (all) option to see everything. Use the -s (kernel name) option to see the type of kernel. Use the -r (kernel release) option to see the kernel release. Use the -v (kernel version) option to see the kernel version. uname -a uname -s uname -r uname -v uname command in a terminal window","title":"uname"},{"location":"technical_stuff/linux/linuxCommands/#w","text":"The w command lists the currently logged in users. w w command in a terminal window","title":"w"},{"location":"technical_stuff/linux/linuxCommands/#whoami","text":"Use whoami to find out who you are logged in as or who is logged into an unmanned Linux terminal.","title":"whoami"},{"location":"technical_stuff/linux/java_home/","text":"If you are getting java_home is not defined correctly in Ubuntu , then you are at right place. I was getting this error as I was trying to update the Java home in ~\\.bashrc and ~\\.bash_profile . Follow these steps to solve this issue. Open the /etc/environment file as vim /etc/environment Then set java_home using JAVA_HOME = \"/usr/lib/jvm/java-8-oracle\" export JAVA_HOME Open bash profile as vim ~ \\. bash_profile Then add the following, . /etc/environment It will load the /etc/environment every time terminal is started Then confirm the path using echo $JAVA_HOME","title":"Java home \u2615\ufe0f"},{"location":"technical_stuff/maven/downgrade_maven/","text":"Downgrading maven version in brew \u00b6 Check current version of maven using mvn -verison This will give you current version and the installation path as well. In my case current version is 3.5.0 as shown below Then search for available versions of maven using brew search maven This will show you all the available versions as shown below If you want to install maven 3.0 for instance, then type brew install maven@3.0 This will install the downgraded version and show some caveats as well Now the next step is to unlink older maven version and then overwrite it with version you just installed brew unlink maven brew link --overwrite maven@3.0 Now check version of maven again. if everything went well ,then you will see that version has been downgraded.","title":"Downgrade Maven"},{"location":"technical_stuff/maven/downgrade_maven/#downgrading-maven-version-in-brew","text":"Check current version of maven using mvn -verison This will give you current version and the installation path as well. In my case current version is 3.5.0 as shown below Then search for available versions of maven using brew search maven This will show you all the available versions as shown below If you want to install maven 3.0 for instance, then type brew install maven@3.0 This will install the downgraded version and show some caveats as well Now the next step is to unlink older maven version and then overwrite it with version you just installed brew unlink maven brew link --overwrite maven@3.0 Now check version of maven again. if everything went well ,then you will see that version has been downgraded.","title":"Downgrading maven version in brew"},{"location":"technical_stuff/mysql/JdbcStatements/","text":"Difference between Prepared Statement and Statement in Java \u00b6 JDBC API provides 3 types of statements for wrapping an SQL query and sending for execution to the database. Statement : It is used to execute normal SQL queries such as select count(*) from Courses . You can also use it to execute DDL, DML and DCL SQL statements. Prepared Statement : is specialized to execute parameterized queries such as select * from Courses where courseId=? , you can execute this SQL multiple times by just changing the course id parameters. They are compiled and cached at database end, hence quite fast for repeated execution. Callable Statement : it is used to execute or call stored procedures stored in the database. Each of the Statement class has a different purpose and you should use them for what they have designed for. It's very important to understand what they are and what is their purpose, along with how to use it correctly. In this article, we will focus on understanding the difference between Statement and Prepared Statement by asking the below questions. 1. Which one to use and when? \u00b6 Prepared Statement's sole purpose is to execute bind queries. If you need to execute a query multiple times with just different data then use Prepared Statement and use a placeholder, the question mark sign (?) for the variable data. Why it is called a prepared statement? When you first execute the prepared SQL query, the database will compile it and cache it for future reuse, next time you call the same query but with a different parameter, then the database will return the result almost immediately. Because of this pre-compilation, this class is called Prepared Statement in Java. It's very useful to build search and insert queries e.g. if your application provides an interface to search some data such as course details, let's say by course, name, instructor, price, or topic. You can create Prepared Statement to handle that for better performance. On the other hand, the sole purpose of Statement object is to execute a SQL query. You give them any query and it will execute it, but unlike Prepared Statement , it will not provide pre-compilation. 2. Syntax for these statements? \u00b6 The syntax for Statement is same as SQL query, you can actually copy SQL from your favorite SQL editor and pass it as String to Statement for execution, but for Prepared Statement , you need to include placeholders i.e. questions mark (?) sign in SQL query e.g. select count ( * ) from Books ; // Statement select * from Books where book_id =? ; // Prepared Statement The actual value is set before executing the query at runtime by using the various setXXX() methods e.g. if placeholder refers to a varchar column then you can use setString(value) to set the value. Similarly, if placeholder refers to an integer column then you can use setInteger(value) method. 3. Which has better performance? \u00b6 Prepared Statement provides better performance than Statement object because of pre-compilation of SQL query on the database server. This is because when you use Prepared Statement, the query is compiled the first time but after that it is cached at the database server, making subsequent run faster. On the other hand, with the Statement object, even if you execute the same query again and again, they are always first compiled and then executed, making them slower compared to Prepared Statement queries. 4. Which one is more secure? \u00b6 The Prepared Statement also provides safety against SQL injection, but the incorrect use of Statement can cause SQL injection. If you remember, the cause of SQL injection is malicious SQL code which is injected by malicious users. For example, you could have written above query which returns a book after passing Id as below: String id = getFromUser (); String SQL = \"select * from Books where book_id=\" + id ; If you pass this SQL to Statement object then it can cause SQL injection if a user sends malicious SQL code in form of id e.g. 1 == 1 OR id, which will return every single book from the database. Though books, may not sound a sensitive data it could happen with any sensitive user data as well. Prepared Statement guards against this. That's all about the difference between Statement and Prepared Statement in Java. You can use Statement to execute SQL queries but it's not recommended, especially if you can use Prepared Statement , which is more secure and fast approach to get the data from the database. If you have to pass parameters always use PreparedStatment, never create dynamic SQL queries by concatenating String, it's not safe and prone to SQL injection attack.","title":"JDBC Statements"},{"location":"technical_stuff/mysql/JdbcStatements/#difference-between-prepared-statement-and-statement-in-java","text":"JDBC API provides 3 types of statements for wrapping an SQL query and sending for execution to the database. Statement : It is used to execute normal SQL queries such as select count(*) from Courses . You can also use it to execute DDL, DML and DCL SQL statements. Prepared Statement : is specialized to execute parameterized queries such as select * from Courses where courseId=? , you can execute this SQL multiple times by just changing the course id parameters. They are compiled and cached at database end, hence quite fast for repeated execution. Callable Statement : it is used to execute or call stored procedures stored in the database. Each of the Statement class has a different purpose and you should use them for what they have designed for. It's very important to understand what they are and what is their purpose, along with how to use it correctly. In this article, we will focus on understanding the difference between Statement and Prepared Statement by asking the below questions.","title":"Difference between Prepared Statement and Statement in Java"},{"location":"technical_stuff/mysql/JdbcStatements/#1-which-one-to-use-and-when","text":"Prepared Statement's sole purpose is to execute bind queries. If you need to execute a query multiple times with just different data then use Prepared Statement and use a placeholder, the question mark sign (?) for the variable data. Why it is called a prepared statement? When you first execute the prepared SQL query, the database will compile it and cache it for future reuse, next time you call the same query but with a different parameter, then the database will return the result almost immediately. Because of this pre-compilation, this class is called Prepared Statement in Java. It's very useful to build search and insert queries e.g. if your application provides an interface to search some data such as course details, let's say by course, name, instructor, price, or topic. You can create Prepared Statement to handle that for better performance. On the other hand, the sole purpose of Statement object is to execute a SQL query. You give them any query and it will execute it, but unlike Prepared Statement , it will not provide pre-compilation.","title":"1. Which one to use and when?"},{"location":"technical_stuff/mysql/JdbcStatements/#2-syntax-for-these-statements","text":"The syntax for Statement is same as SQL query, you can actually copy SQL from your favorite SQL editor and pass it as String to Statement for execution, but for Prepared Statement , you need to include placeholders i.e. questions mark (?) sign in SQL query e.g. select count ( * ) from Books ; // Statement select * from Books where book_id =? ; // Prepared Statement The actual value is set before executing the query at runtime by using the various setXXX() methods e.g. if placeholder refers to a varchar column then you can use setString(value) to set the value. Similarly, if placeholder refers to an integer column then you can use setInteger(value) method.","title":"2. Syntax for these statements?"},{"location":"technical_stuff/mysql/JdbcStatements/#3-which-has-better-performance","text":"Prepared Statement provides better performance than Statement object because of pre-compilation of SQL query on the database server. This is because when you use Prepared Statement, the query is compiled the first time but after that it is cached at the database server, making subsequent run faster. On the other hand, with the Statement object, even if you execute the same query again and again, they are always first compiled and then executed, making them slower compared to Prepared Statement queries.","title":"3. Which has better performance?"},{"location":"technical_stuff/mysql/JdbcStatements/#4-which-one-is-more-secure","text":"The Prepared Statement also provides safety against SQL injection, but the incorrect use of Statement can cause SQL injection. If you remember, the cause of SQL injection is malicious SQL code which is injected by malicious users. For example, you could have written above query which returns a book after passing Id as below: String id = getFromUser (); String SQL = \"select * from Books where book_id=\" + id ; If you pass this SQL to Statement object then it can cause SQL injection if a user sends malicious SQL code in form of id e.g. 1 == 1 OR id, which will return every single book from the database. Though books, may not sound a sensitive data it could happen with any sensitive user data as well. Prepared Statement guards against this. That's all about the difference between Statement and Prepared Statement in Java. You can use Statement to execute SQL queries but it's not recommended, especially if you can use Prepared Statement , which is more secure and fast approach to get the data from the database. If you have to pass parameters always use PreparedStatment, never create dynamic SQL queries by concatenating String, it's not safe and prone to SQL injection attack.","title":"4. Which one is more secure?"},{"location":"technical_stuff/mysql/mysql_basics/","text":"Installing MySQL \u00b6 brew install mysql Now, you have installed your MySQL database without a root password. To secure it run the following mysql_secure_installation Connecting to MySQL \u00b6 Enter the below command mysql -u root Use brew to manage MySQL using following commands brew services start mysql # This will register to run it at bot brew services run # Run the service formula without registering to launch at login brew services stop # Stop service immediately and unregister it To run as background process, by default run the following mysql.server start # start MySQL mysql.server status # check status mysql.server stop # stop the server To reset the password for MySQL you first must create a new file with the following contents ALTER USER 'root'@'localhost' IDENTIFIED BY 'PASSWORD'; Now, login using the password by using the below command mysql -u root -p Forgot MySQL password? \u00b6 perform the following steps Stop the server Start server using sudo mysqld_safe -skip-grant-tables -skip-netwroking & Connect to server using root as mysql -u root . Now, run the following commands in terminal use mysql ; \u200b update user set authentication_string = password ( 'NEWPASSWORD' ) where user = 'root' ; \u200b flush privileges ; \u200b quit Login to MySQL server again and enter the new password. change database using \u00b6 show databases ; use yourDatabaseName ; # replace yourDatabaseName with your database name show tables ; Downloading using GUI (preferred option) \u00b6 Go to https://dev.mysql.com/downloads/mysql/ and download MySQL for your OS.","title":"Mysql 101"},{"location":"technical_stuff/mysql/mysql_basics/#installing-mysql","text":"brew install mysql Now, you have installed your MySQL database without a root password. To secure it run the following mysql_secure_installation","title":"Installing MySQL"},{"location":"technical_stuff/mysql/mysql_basics/#connecting-to-mysql","text":"Enter the below command mysql -u root Use brew to manage MySQL using following commands brew services start mysql # This will register to run it at bot brew services run # Run the service formula without registering to launch at login brew services stop # Stop service immediately and unregister it To run as background process, by default run the following mysql.server start # start MySQL mysql.server status # check status mysql.server stop # stop the server To reset the password for MySQL you first must create a new file with the following contents ALTER USER 'root'@'localhost' IDENTIFIED BY 'PASSWORD'; Now, login using the password by using the below command mysql -u root -p","title":"Connecting to MySQL"},{"location":"technical_stuff/mysql/mysql_basics/#forgot-mysql-password","text":"perform the following steps Stop the server Start server using sudo mysqld_safe -skip-grant-tables -skip-netwroking & Connect to server using root as mysql -u root . Now, run the following commands in terminal use mysql ; \u200b update user set authentication_string = password ( 'NEWPASSWORD' ) where user = 'root' ; \u200b flush privileges ; \u200b quit Login to MySQL server again and enter the new password.","title":"Forgot MySQL password?"},{"location":"technical_stuff/mysql/mysql_basics/#change-database-using","text":"show databases ; use yourDatabaseName ; # replace yourDatabaseName with your database name show tables ;","title":"change database using"},{"location":"technical_stuff/mysql/mysql_basics/#downloading-using-gui-preferred-option","text":"Go to https://dev.mysql.com/downloads/mysql/ and download MySQL for your OS.","title":"Downloading using GUI (preferred option)"},{"location":"technical_stuff/react/react/","text":"React \u00b6 React Native \u00b6 React Native is a JavaScript framework for writing real, natively rendering mobile applications for iOS and Android . It\u2019s based on React, Facebook\u2019s JavaScript library for building user interfaces, but instead of targeting the browser, it targets mobile platforms. In other words: web developers can now write mobile applications that look and feel truly \u201cnative,\u201d all from the comfort of a JavaScript library that we already know and love. JSX \u00b6 JSX stands for JavaScript XML. It allows us to write HTML code directly in our React project. Using TypeScript with React provides better IntelliSense and code completion for JSX. .tsx file extension is used when you create a React component and use JSX elements and syntax. Concepts \u00b6 Components \u00b6 Components are independent and reusable bits of code. They serve the same purpose as JavaScript functions, but work in isolation and return HTML . React Router \u00b6 React Router is a popular standard library for routing among various view components in React applications. It helps keep the user interface in sync with the URL. In addition, React Router allows defining which view to display for a specified URL.","title":"React"},{"location":"technical_stuff/react/react/#react","text":"","title":"React"},{"location":"technical_stuff/react/react/#react-native","text":"React Native is a JavaScript framework for writing real, natively rendering mobile applications for iOS and Android . It\u2019s based on React, Facebook\u2019s JavaScript library for building user interfaces, but instead of targeting the browser, it targets mobile platforms. In other words: web developers can now write mobile applications that look and feel truly \u201cnative,\u201d all from the comfort of a JavaScript library that we already know and love.","title":"React Native"},{"location":"technical_stuff/react/react/#jsx","text":"JSX stands for JavaScript XML. It allows us to write HTML code directly in our React project. Using TypeScript with React provides better IntelliSense and code completion for JSX. .tsx file extension is used when you create a React component and use JSX elements and syntax.","title":"JSX"},{"location":"technical_stuff/react/react/#concepts","text":"","title":"Concepts"},{"location":"technical_stuff/react/react/#components","text":"Components are independent and reusable bits of code. They serve the same purpose as JavaScript functions, but work in isolation and return HTML .","title":"Components"},{"location":"technical_stuff/react/react/#react-router","text":"React Router is a popular standard library for routing among various view components in React applications. It helps keep the user interface in sync with the URL. In addition, React Router allows defining which view to display for a specified URL.","title":"React Router"},{"location":"technical_stuff/wget/wget/","text":"Mac provides curl , however sometime using wget is more handy. This post is for installing wget for Mac. by following the below mentined steps: Method 1 \u00b6 In most of tutorials, installing wget using curl is recommended . However I ran into several issues using this approach. I will share it with you guys. First you need to install the xcode command line tools , easiest way to do so by running in the Terminal xcode-select --install Download wget using curl curl -O http://ftp.gnu.org/gnu/wget/wget-1.15.tar.gz Unpack it using tar -zxvf wget-1.15.tar.gz cd to folder using cd wget-1.15/ 4. Then configure ./configure I have got following error that Error configure: error: --with-ssl=gnutls was given, but GNUTLS is not available. we can skip this error using ./configure --with-ssl = openssl However, this also leads to another error shown below configure: error: --with-ssl = openssl was given, but SSL is not available Method 2 \u00b6 We can easily go around aforementioned error as brew will automatically install dependencies for wget and openssl . Gor doing this you must have brew installed brew install wget as shown below, it will automatically download the dependency == > Installing dependencies for wget: openssl == > Installing wget dependency: openssl == > Downloading https://homebrew.bintray.com/bottles/openssl-1.0.2j.sierra.bottle.tar.gz ######################################################################## 100.0% == > Pouring openssl-1.0.2j.sierra.bottle.tar.gz For checking is wget is installed successfully, please cd to paricular folder and enter wget -O sample.txt http://www.gutenberg.org/cache/epub/1787/pg1787.txt text will be generated from link http://www.gutenberg.org/cache/epub/1787/pg1787.txt and saved as sample.txt Success Congrats, we have successfully installed wget on mac.If you have some issues or suggestions, please feel free to comment below.","title":"Wget"},{"location":"technical_stuff/wget/wget/#method-1","text":"In most of tutorials, installing wget using curl is recommended . However I ran into several issues using this approach. I will share it with you guys. First you need to install the xcode command line tools , easiest way to do so by running in the Terminal xcode-select --install Download wget using curl curl -O http://ftp.gnu.org/gnu/wget/wget-1.15.tar.gz Unpack it using tar -zxvf wget-1.15.tar.gz cd to folder using cd wget-1.15/ 4. Then configure ./configure I have got following error that Error configure: error: --with-ssl=gnutls was given, but GNUTLS is not available. we can skip this error using ./configure --with-ssl = openssl However, this also leads to another error shown below configure: error: --with-ssl = openssl was given, but SSL is not available","title":"Method 1"},{"location":"technical_stuff/wget/wget/#method-2","text":"We can easily go around aforementioned error as brew will automatically install dependencies for wget and openssl . Gor doing this you must have brew installed brew install wget as shown below, it will automatically download the dependency == > Installing dependencies for wget: openssl == > Installing wget dependency: openssl == > Downloading https://homebrew.bintray.com/bottles/openssl-1.0.2j.sierra.bottle.tar.gz ######################################################################## 100.0% == > Pouring openssl-1.0.2j.sierra.bottle.tar.gz For checking is wget is installed successfully, please cd to paricular folder and enter wget -O sample.txt http://www.gutenberg.org/cache/epub/1787/pg1787.txt text will be generated from link http://www.gutenberg.org/cache/epub/1787/pg1787.txt and saved as sample.txt Success Congrats, we have successfully installed wget on mac.If you have some issues or suggestions, please feel free to comment below.","title":"Method 2"},{"location":"technical_stuff/wso2/h2/wso2/","text":"Accessing the H2 Database for WSO2 Products \u00b6 Most of the WSO2 products comes with the H2 database, I have been facing some issues in order to access this database. You can follow approach A or B, in my opinion B is easier. Common step is a must Common Step \u00b6 Open the carbon.xml file to enable the access to h2 database. As I am using the WSO2 IoT server, this file is located in the IoT_HOME/conf/carbon.xml as shown above. Open this file and uncomment the following <H2DatabaseConfiguration> <property name=\u201dweb\u201d /> <property name=\u201dwebPort\u201d>8082</property> <property name=\u201dwebAllowOthers\u201d /> <property name=\u201dwebSSL\u201d /> <property name=\u201dtcp\u201d /> <property name=\u201dtcpPort\u201d>9092</property> <property name=\u201dtcpAllowOthers\u201d /> <property name=\u201dtcpSSL\u201d /> <property name=\u201dpg\u201d /> <property name=\u201dpgPort\u201d>5435</property> <property name=\u201dpgAllowOthers\u201d /> <property name=\u201dtrace\u201d /> <property name=\u201dbaseDir\u201d>${carbon.home}</property> </H2DatabaseConfiguration> Approach A: Using Web Browser \u00b6 Install the H2 database: If you are Mac user, then enter brew install h2 in terminal to install h2 database and then type h2 in terminal to start it .The h2 database will start and can be accessed from http://192.168.0.16:8082/ . If you are using the windows or Linux then install h2 from here Then copy the path for the database you want to open, In my case the database path I want to access is shown below Watch Do not copy the h2.db part of the file. For example path I copied is /Users/amar/Documents/ThesisCode/CEP_codes/wso2iot-3.3.0_new/wso2/broker/repository/database/WSO2MB_DB Now go the http://192.168.0.16:8082/ and choose the generic H2 then enter the JDBC url . Append the jdbc:h2:file: in front of path as shown in below screen-shot. Then enter username and password as wso2carbon . Press connect Approach B: Using IntelliJ IDEA \u00b6 Now the same can be done in IntelliJ Idea which also provides access to databases. For accessing databases maybe you need a ultimate version of it, which is free for students under University account. Open IntelliJ Idea and click on add new database as shown below Select H2 . You have to install the H2 drivers for first time. Info Before going further, you have to close all existing connections to the H2 , and stop H2 if its running in terminal. As shown above, first choose Embedded database type from drop down menu, then browse your file using ... option. Make sure the h2.db path does not contain h2.db . Enter username and password as wso2carbon . Thats it! Hope, it helped \ud83d\ude00","title":"H2 Db in WSO2"},{"location":"technical_stuff/wso2/h2/wso2/#accessing-the-h2-database-for-wso2-products","text":"Most of the WSO2 products comes with the H2 database, I have been facing some issues in order to access this database. You can follow approach A or B, in my opinion B is easier. Common step is a must","title":"Accessing the H2 Database for WSO2 Products"},{"location":"technical_stuff/wso2/h2/wso2/#common-step","text":"Open the carbon.xml file to enable the access to h2 database. As I am using the WSO2 IoT server, this file is located in the IoT_HOME/conf/carbon.xml as shown above. Open this file and uncomment the following <H2DatabaseConfiguration> <property name=\u201dweb\u201d /> <property name=\u201dwebPort\u201d>8082</property> <property name=\u201dwebAllowOthers\u201d /> <property name=\u201dwebSSL\u201d /> <property name=\u201dtcp\u201d /> <property name=\u201dtcpPort\u201d>9092</property> <property name=\u201dtcpAllowOthers\u201d /> <property name=\u201dtcpSSL\u201d /> <property name=\u201dpg\u201d /> <property name=\u201dpgPort\u201d>5435</property> <property name=\u201dpgAllowOthers\u201d /> <property name=\u201dtrace\u201d /> <property name=\u201dbaseDir\u201d>${carbon.home}</property> </H2DatabaseConfiguration>","title":"Common Step"},{"location":"technical_stuff/wso2/h2/wso2/#approach-a-using-web-browser","text":"Install the H2 database: If you are Mac user, then enter brew install h2 in terminal to install h2 database and then type h2 in terminal to start it .The h2 database will start and can be accessed from http://192.168.0.16:8082/ . If you are using the windows or Linux then install h2 from here Then copy the path for the database you want to open, In my case the database path I want to access is shown below Watch Do not copy the h2.db part of the file. For example path I copied is /Users/amar/Documents/ThesisCode/CEP_codes/wso2iot-3.3.0_new/wso2/broker/repository/database/WSO2MB_DB Now go the http://192.168.0.16:8082/ and choose the generic H2 then enter the JDBC url . Append the jdbc:h2:file: in front of path as shown in below screen-shot. Then enter username and password as wso2carbon . Press connect","title":"Approach A: Using Web Browser"},{"location":"technical_stuff/wso2/h2/wso2/#approach-b-using-intellij-idea","text":"Now the same can be done in IntelliJ Idea which also provides access to databases. For accessing databases maybe you need a ultimate version of it, which is free for students under University account. Open IntelliJ Idea and click on add new database as shown below Select H2 . You have to install the H2 drivers for first time. Info Before going further, you have to close all existing connections to the H2 , and stop H2 if its running in terminal. As shown above, first choose Embedded database type from drop down menu, then browse your file using ... option. Make sure the h2.db path does not contain h2.db . Enter username and password as wso2carbon . Thats it! Hope, it helped \ud83d\ude00","title":"Approach B: Using IntelliJ IDEA"},{"location":"technical_stuff/wso2/jms/","text":"Setting up JMS Queue in WSO2 IoT Server \u00b6 I was struggling for few days to get JMS queue up-and-running in WSO2 IoT Server . In my experience, setting the queue is easy in programming, but when it comes to complicated systems, sometime small mistakes can consume lot of time. So, without any further ado, lets get going. Please make sure the WSO2 IoT server is in off state. I will let you know when to turn it on. Knowing the configuration file paths \u00b6 The WSO2 IoT server consists of 3 tiers: Broker , Core and Analytics as shown below Now, for using the JMS there are 3 options in WSO2 IoT server, which are Using Apache ActiveMQ Using Apache Qpid Using WSO2 Message Broker (MB) Out of these, the first option worked for me. The important thing to understand is that, we need to enable the ActiveMQ configurations for all three tiers i.e. Broker, Core and Analytics. The most confusing part is the file structure for configurations, as WSO2 has so many products that even most of paths mentioned in documentation got me confused. So, I would advice you to write the paths down or mark the folders using color coding. Path for IoT Core is shown below Mark both .xml files which are selected in fugure. Next, the path for IoT Analytics as shown below The path of IoT broker is in wso2/broker/conf/axis2 similar to IoT analytics. Setting up the configuration for JMS \u00b6 Once you know all the paths, lets make the required changes in axis2.xml file as shown below Enable Transport Receiver \u00b6 Please note that JMS queue has 2 components: receiver and sender. First we will add the receiver part. If you open the axis.xml file and search for jms you will find the three blocks of code related to receiver part. Please uncomment the code related to ApacheMQ as shown below. <transportReceiver name= \"jms\" class= \"org.apache.axis2.transport.jms.JMSListener\" > <parameter name= \"myTopicConnectionFactory\" > <parameter name= \"java.naming.factory.initial\" > org.apache.activemq.jndi.ActiveMQInitialContextFactory </parameter> <parameter name= \"java.naming.provider.url\" locked= \"false\" > failover:tcp://localhost:61616 </parameter> <parameter name= \"transport.jms.ConnectionFactoryJNDIName\" locked= \"false\" > TopicConnectionFactory </parameter> <parameter name= \"transport.jms.ConnectionFactoryType\" locked= \"false\" > topic </parameter> <property name= \"userName\" value= \"admin\" /> <property name= \"password\" value= \"admin\" /> </parameter> <parameter name= \"myQueueConnectionFactory\" > <parameter name= \"java.naming.factory.initial\" > org.apache.activemq.jndi.ActiveMQInitialContextFactory </parameter> <parameter name= \"java.naming.provider.url\" locked= \"false\" > failover:tcp://localhost:61616 </parameter> <parameter name= \"transport.jms.ConnectionFactoryJNDIName\" locked= \"false\" > QueueConnectionFactory </parameter> <parameter name= \"transport.jms.ConnectionFactoryType\" locked= \"false\" > topic </parameter> <property name= \"userName\" value= \"admin\" /> <property name= \"password\" value= \"admin\" /> </parameter> <parameter name= \"default\" > <parameter name= \"java.naming.factory.initial\" > org.apache.activemq.jndi.ActiveMQInitialContextFactory </parameter> <parameter name= \"java.naming.provider.url\" locked= \"false\" > failover:tcp://localhost:61616 </parameter> <parameter name= \"transport.jms.ConnectionFactoryJNDIName\" locked= \"false\" > TopicConnectionFactory </parameter> <parameter name= \"transport.jms.ConnectionFactoryType\" locked= \"false\" > topic </parameter> <property name= \"userName\" value= \"admin\" /> <property name= \"password\" value= \"admin\" /> </parameter> </transportReceiver> As you have noticed, I have made some additional changes in given code, one of which is to add failover in front of tcp://localhost:61616 and other is to add both username and password . The failover helps to reestablish the connection when connection breaks which often happen in real world scenarios. Enable Transport Sender \u00b6 <transportSender name= \"jms\" class= \"org.apache.axis2.transport.jms.JMSSender\" /> Un-comment the above code which is also available in file. Please make sure that there is no error when you un-comment the code. If you think there is some issue, please go ahead and copy paste the below code. Making Changes in axis2_client.xml file \u00b6 Just enable the JMS sender by un-commenting the given sender code as shown below. There are no settings for JMS receiver in this file. <transportSender name= \"jms\" class= \"org.apache.axis2.transport.jms.JMSSender\" /> Enable Receiver and Sender for Broker, Core and Analytics \u00b6 Yes, you have to enable aforementioned settings for both axis2.xml and axis2_client.xml for all three tiers except the exeption mentioned below. Take some time to make sure that there is no mistake while un-commenting the code. Exception Please dot not enable Transport Sender in IoT_HOME/conf/axis2.xml and IoT_HOME/conf/axis2_client.xml. However, enable Transport sender for all the tiers. A summarized view is given in below table. Download the required dependencies. \u00b6 Caveat The ActiveMQ libraries need to be available in various classpath folder of IoT server. You need to copy the jars from the lib folder of the ActiveMQ as shown below. In case you use the dependency manager such as brew or apt-get to install ActiveMQ, then please find the right version of the ActiveMQ and download the binary version to get jars or go to installation folder. As I used ActiveMQ version 5.5.1 , thus these are the required jars. Download these jar files form maven central activemq-core-5.5.1.jar axis2-transport-all-1.0.0.jar geronimo-j2ee-management_1.0_spec-1.0.1.jar geronimo-jms_1.1_spec-1.1.1.jar Place them in following 3 locations. wso2_home/lib wso2_home/wso2/lib wso2_home/wso2/components/lib Here, the wso2_home is the main installation directory for IoT server. Congrats, we are halfway thorough. Now is time to do some installation Install and start ActiveMQ \u00b6 If you are mac user, then type brew install activemq to download ActiveMQ. Further, start the ActiveMQ using activemq start . Now, check status using activemq status . Please use your favorite package manager such as apt-get or choco to download the ActiveMQ, if you are using Linux or Windows respectively. Caveat If you run activemq start multiple times, it will kick off multiple JVMs to run the broker. However since you did not specify any unique broker configuration, each instance will use the same default broker configuration from conf/activemq.xml . And that typically means each broker instance will compete for the lock on the default KahaDB store. Only one instance will get the lock on that store and fully start up (the master broker), the other instances will continue to compete for the lock (slave brokers). Start the IoT Server i.e. broker, core and analytics. \u00b6 While the products are starting, run a find search in terminal for jms keyword. This way you will know if there is some issue related to JMS. Message broker should say that jms sender started as shown below Make pipeline \u00b6 First make the basic pipeline so that you have the streaming data arriving at IoT server to a particular stream say stream A . Now we will add the stream A to JMS queue publisher. Then we need the JMS receiver to get the data from JMS publisher and further sends it to a logger. Lets understand it by below figure. As shows in the above figure the JMS publisher receives the stream A and en-queues the tuples/events in queue1 . Further the JMS Subscriber subscribes to queue1 and send it to Stream C . Further Stream C is published to Logger publisher which shows the logs in Analytics terminal console . Setting for JMS publisher is given below <?xml version=\"1.0\" encoding=\"UTF-8\"?> <eventPublisher name= \"jms_pub\" processing= \"enable\" statistics= \"enable\" trace= \"enable\" xmlns= \"http://wso2.org/carbon/eventpublisher\" > <from streamName= \"stream1_scep_timestamped\" version= \"1.0.0\" /> <mapping customMapping= \"disable\" type= \"json\" /> <to eventAdapterType= \"jms\" > <property name= \"transport.jms.DestinationType\" > queue </property> <property name= \"transport.jms.Destination\" > queue1 </property> <property name= \"transport.jms.ConcurrentPublishers\" > allow </property> <property name= \"java.naming.factory.initial\" > org.apache.activemq.jndi.ActiveMQInitialContextFactory </property> <property name= \"java.naming.provider.url\" > tcp://localhost:61616 </property> <property name= \"transport.jms.ConnectionFactoryJNDIName\" > QueueConnectionFactory </property> <property name= \"transport.jms.UserName\" > admin </property> <property encrypted= \"true\" name= \"transport.jms.Password\" > aa47+5/q7d9AvOHUyYAJDXrx0Q6GQmgzIKS/hOkzp6huHrxslJJk6Oqmv2mrW159DOTfJ7Rw2nBbfGWjGiMckTFAO9p9YVF3kDDHhiyirWEJPSESSSJeBB782qnwoXEDSAjgiiUYWSRuYIfxdibXUUZr3JPSmjaxvy+EVMjjWgouMrid51UQTW50wl3C0fX03/nak4P9+GWx14T1JGAb07fKQlgK/AwYtJ8esNyiV1j0Z2jgGM9OLpqgZ9gqjsA95htzdqy2DgC/U74qfhkUKISAXUWZdGS+rCEYBFaVzAj0aPKtXmRWTrC6OTDSTVLQCKZPfcHqnU652PUQZqqKCA== </property> </to> </eventPublisher> The username and password are admin and admin . Only the setting you need form here are the configuration setting like url etc. Make sure to use a unique queue name like I used queue1 . Take a note of it as we need to subscribe to it in JMS receiver. Also, one more important thing, there are 2 options in JMS i.e. queue or topic . Make sure you use the same in JMS publisher and JMS Receiver. Also as I am using the queue, hence I used QueueConnectionFactory . The code for JMS receiver is also similar as shown below <?xml version=\"1.0\" encoding=\"UTF-8\"?> <eventReceiver name= \"jms_sub\" statistics= \"disable\" trace= \"disable\" xmlns= \"http://wso2.org/carbon/eventreceiver\" > <from eventAdapterType= \"jms\" > <property name= \"transport.jms.DestinationType\" > queue </property> <property name= \"transport.jms.Destination\" > queue1 </property> <property name= \"java.naming.factory.initial\" > org.apache.activemq.jndi.ActiveMQInitialContextFactory </property> <property name= \"java.naming.provider.url\" > tcp://localhost:61616 </property> <property name= \"transport.jms.SubscriptionDurable\" > false </property> <property name= \"transport.jms.ConnectionFactoryJNDIName\" > QueueConnectionFactory </property> <property name= \"transport.jms.UserName\" > admin </property> <property encrypted= \"true\" name= \"transport.jms.Password\" > aa47+5/q7d9AvOHUyYAJDXrx0Q6GQmgzIKS/hOkzp6huHrxslJJk6Oqmv2mrW159DOTfJ7Rw2nBbfGWjGiMckTFAO9p9YVF3kDDHhiyirWEJPSESSSJeBB782qnwoXEDSAjgiiUYWSRuYIfxdibXUUZr3JPSmjaxvy+EVMjjWgouMrid51UQTW50wl3C0fX03/nak4P9+GWx14T1JGAb07fKQlgK/AwYtJ8esNyiV1j0Z2jgGM9OLpqgZ9gqjsA95htzdqy2DgC/U74qfhkUKISAXUWZdGS+rCEYBFaVzAj0aPKtXmRWTrC6OTDSTVLQCKZPfcHqnU652PUQZqqKCA== </property> </from> <mapping customMapping= \"disable\" type= \"json\" /> <to streamName= \"stream1_scep_timestamped\" version= \"2.0.0\" /> </eventReceiver> Now add the logger and send some events to stream A . If all went right, you should see the output at analytics tier console.","title":"JMS Queue in WSO2"},{"location":"technical_stuff/wso2/jms/#setting-up-jms-queue-in-wso2-iot-server","text":"I was struggling for few days to get JMS queue up-and-running in WSO2 IoT Server . In my experience, setting the queue is easy in programming, but when it comes to complicated systems, sometime small mistakes can consume lot of time. So, without any further ado, lets get going. Please make sure the WSO2 IoT server is in off state. I will let you know when to turn it on.","title":"Setting up JMS Queue in WSO2 IoT Server"},{"location":"technical_stuff/wso2/jms/#knowing-the-configuration-file-paths","text":"The WSO2 IoT server consists of 3 tiers: Broker , Core and Analytics as shown below Now, for using the JMS there are 3 options in WSO2 IoT server, which are Using Apache ActiveMQ Using Apache Qpid Using WSO2 Message Broker (MB) Out of these, the first option worked for me. The important thing to understand is that, we need to enable the ActiveMQ configurations for all three tiers i.e. Broker, Core and Analytics. The most confusing part is the file structure for configurations, as WSO2 has so many products that even most of paths mentioned in documentation got me confused. So, I would advice you to write the paths down or mark the folders using color coding. Path for IoT Core is shown below Mark both .xml files which are selected in fugure. Next, the path for IoT Analytics as shown below The path of IoT broker is in wso2/broker/conf/axis2 similar to IoT analytics.","title":"Knowing the configuration file paths"},{"location":"technical_stuff/wso2/jms/#setting-up-the-configuration-for-jms","text":"Once you know all the paths, lets make the required changes in axis2.xml file as shown below","title":"Setting up the configuration for JMS"},{"location":"technical_stuff/wso2/jms/#enable-transport-receiver","text":"Please note that JMS queue has 2 components: receiver and sender. First we will add the receiver part. If you open the axis.xml file and search for jms you will find the three blocks of code related to receiver part. Please uncomment the code related to ApacheMQ as shown below. <transportReceiver name= \"jms\" class= \"org.apache.axis2.transport.jms.JMSListener\" > <parameter name= \"myTopicConnectionFactory\" > <parameter name= \"java.naming.factory.initial\" > org.apache.activemq.jndi.ActiveMQInitialContextFactory </parameter> <parameter name= \"java.naming.provider.url\" locked= \"false\" > failover:tcp://localhost:61616 </parameter> <parameter name= \"transport.jms.ConnectionFactoryJNDIName\" locked= \"false\" > TopicConnectionFactory </parameter> <parameter name= \"transport.jms.ConnectionFactoryType\" locked= \"false\" > topic </parameter> <property name= \"userName\" value= \"admin\" /> <property name= \"password\" value= \"admin\" /> </parameter> <parameter name= \"myQueueConnectionFactory\" > <parameter name= \"java.naming.factory.initial\" > org.apache.activemq.jndi.ActiveMQInitialContextFactory </parameter> <parameter name= \"java.naming.provider.url\" locked= \"false\" > failover:tcp://localhost:61616 </parameter> <parameter name= \"transport.jms.ConnectionFactoryJNDIName\" locked= \"false\" > QueueConnectionFactory </parameter> <parameter name= \"transport.jms.ConnectionFactoryType\" locked= \"false\" > topic </parameter> <property name= \"userName\" value= \"admin\" /> <property name= \"password\" value= \"admin\" /> </parameter> <parameter name= \"default\" > <parameter name= \"java.naming.factory.initial\" > org.apache.activemq.jndi.ActiveMQInitialContextFactory </parameter> <parameter name= \"java.naming.provider.url\" locked= \"false\" > failover:tcp://localhost:61616 </parameter> <parameter name= \"transport.jms.ConnectionFactoryJNDIName\" locked= \"false\" > TopicConnectionFactory </parameter> <parameter name= \"transport.jms.ConnectionFactoryType\" locked= \"false\" > topic </parameter> <property name= \"userName\" value= \"admin\" /> <property name= \"password\" value= \"admin\" /> </parameter> </transportReceiver> As you have noticed, I have made some additional changes in given code, one of which is to add failover in front of tcp://localhost:61616 and other is to add both username and password . The failover helps to reestablish the connection when connection breaks which often happen in real world scenarios.","title":"Enable Transport Receiver"},{"location":"technical_stuff/wso2/jms/#enable-transport-sender","text":"<transportSender name= \"jms\" class= \"org.apache.axis2.transport.jms.JMSSender\" /> Un-comment the above code which is also available in file. Please make sure that there is no error when you un-comment the code. If you think there is some issue, please go ahead and copy paste the below code.","title":"Enable Transport Sender"},{"location":"technical_stuff/wso2/jms/#making-changes-in-axis2_clientxml-file","text":"Just enable the JMS sender by un-commenting the given sender code as shown below. There are no settings for JMS receiver in this file. <transportSender name= \"jms\" class= \"org.apache.axis2.transport.jms.JMSSender\" />","title":"Making Changes in axis2_client.xml file"},{"location":"technical_stuff/wso2/jms/#enable-receiver-and-sender-for-broker-core-and-analytics","text":"Yes, you have to enable aforementioned settings for both axis2.xml and axis2_client.xml for all three tiers except the exeption mentioned below. Take some time to make sure that there is no mistake while un-commenting the code. Exception Please dot not enable Transport Sender in IoT_HOME/conf/axis2.xml and IoT_HOME/conf/axis2_client.xml. However, enable Transport sender for all the tiers. A summarized view is given in below table.","title":"Enable Receiver and Sender for Broker, Core and Analytics"},{"location":"technical_stuff/wso2/jms/#download-the-required-dependencies","text":"Caveat The ActiveMQ libraries need to be available in various classpath folder of IoT server. You need to copy the jars from the lib folder of the ActiveMQ as shown below. In case you use the dependency manager such as brew or apt-get to install ActiveMQ, then please find the right version of the ActiveMQ and download the binary version to get jars or go to installation folder. As I used ActiveMQ version 5.5.1 , thus these are the required jars. Download these jar files form maven central activemq-core-5.5.1.jar axis2-transport-all-1.0.0.jar geronimo-j2ee-management_1.0_spec-1.0.1.jar geronimo-jms_1.1_spec-1.1.1.jar Place them in following 3 locations. wso2_home/lib wso2_home/wso2/lib wso2_home/wso2/components/lib Here, the wso2_home is the main installation directory for IoT server. Congrats, we are halfway thorough. Now is time to do some installation","title":"Download the required dependencies."},{"location":"technical_stuff/wso2/jms/#install-and-start-activemq","text":"If you are mac user, then type brew install activemq to download ActiveMQ. Further, start the ActiveMQ using activemq start . Now, check status using activemq status . Please use your favorite package manager such as apt-get or choco to download the ActiveMQ, if you are using Linux or Windows respectively. Caveat If you run activemq start multiple times, it will kick off multiple JVMs to run the broker. However since you did not specify any unique broker configuration, each instance will use the same default broker configuration from conf/activemq.xml . And that typically means each broker instance will compete for the lock on the default KahaDB store. Only one instance will get the lock on that store and fully start up (the master broker), the other instances will continue to compete for the lock (slave brokers).","title":"Install and start ActiveMQ"},{"location":"technical_stuff/wso2/jms/#start-the-iot-server-ie-broker-core-and-analytics","text":"While the products are starting, run a find search in terminal for jms keyword. This way you will know if there is some issue related to JMS. Message broker should say that jms sender started as shown below","title":"Start the IoT Server i.e. broker, core and analytics."},{"location":"technical_stuff/wso2/jms/#make-pipeline","text":"First make the basic pipeline so that you have the streaming data arriving at IoT server to a particular stream say stream A . Now we will add the stream A to JMS queue publisher. Then we need the JMS receiver to get the data from JMS publisher and further sends it to a logger. Lets understand it by below figure. As shows in the above figure the JMS publisher receives the stream A and en-queues the tuples/events in queue1 . Further the JMS Subscriber subscribes to queue1 and send it to Stream C . Further Stream C is published to Logger publisher which shows the logs in Analytics terminal console . Setting for JMS publisher is given below <?xml version=\"1.0\" encoding=\"UTF-8\"?> <eventPublisher name= \"jms_pub\" processing= \"enable\" statistics= \"enable\" trace= \"enable\" xmlns= \"http://wso2.org/carbon/eventpublisher\" > <from streamName= \"stream1_scep_timestamped\" version= \"1.0.0\" /> <mapping customMapping= \"disable\" type= \"json\" /> <to eventAdapterType= \"jms\" > <property name= \"transport.jms.DestinationType\" > queue </property> <property name= \"transport.jms.Destination\" > queue1 </property> <property name= \"transport.jms.ConcurrentPublishers\" > allow </property> <property name= \"java.naming.factory.initial\" > org.apache.activemq.jndi.ActiveMQInitialContextFactory </property> <property name= \"java.naming.provider.url\" > tcp://localhost:61616 </property> <property name= \"transport.jms.ConnectionFactoryJNDIName\" > QueueConnectionFactory </property> <property name= \"transport.jms.UserName\" > admin </property> <property encrypted= \"true\" name= \"transport.jms.Password\" > aa47+5/q7d9AvOHUyYAJDXrx0Q6GQmgzIKS/hOkzp6huHrxslJJk6Oqmv2mrW159DOTfJ7Rw2nBbfGWjGiMckTFAO9p9YVF3kDDHhiyirWEJPSESSSJeBB782qnwoXEDSAjgiiUYWSRuYIfxdibXUUZr3JPSmjaxvy+EVMjjWgouMrid51UQTW50wl3C0fX03/nak4P9+GWx14T1JGAb07fKQlgK/AwYtJ8esNyiV1j0Z2jgGM9OLpqgZ9gqjsA95htzdqy2DgC/U74qfhkUKISAXUWZdGS+rCEYBFaVzAj0aPKtXmRWTrC6OTDSTVLQCKZPfcHqnU652PUQZqqKCA== </property> </to> </eventPublisher> The username and password are admin and admin . Only the setting you need form here are the configuration setting like url etc. Make sure to use a unique queue name like I used queue1 . Take a note of it as we need to subscribe to it in JMS receiver. Also, one more important thing, there are 2 options in JMS i.e. queue or topic . Make sure you use the same in JMS publisher and JMS Receiver. Also as I am using the queue, hence I used QueueConnectionFactory . The code for JMS receiver is also similar as shown below <?xml version=\"1.0\" encoding=\"UTF-8\"?> <eventReceiver name= \"jms_sub\" statistics= \"disable\" trace= \"disable\" xmlns= \"http://wso2.org/carbon/eventreceiver\" > <from eventAdapterType= \"jms\" > <property name= \"transport.jms.DestinationType\" > queue </property> <property name= \"transport.jms.Destination\" > queue1 </property> <property name= \"java.naming.factory.initial\" > org.apache.activemq.jndi.ActiveMQInitialContextFactory </property> <property name= \"java.naming.provider.url\" > tcp://localhost:61616 </property> <property name= \"transport.jms.SubscriptionDurable\" > false </property> <property name= \"transport.jms.ConnectionFactoryJNDIName\" > QueueConnectionFactory </property> <property name= \"transport.jms.UserName\" > admin </property> <property encrypted= \"true\" name= \"transport.jms.Password\" > aa47+5/q7d9AvOHUyYAJDXrx0Q6GQmgzIKS/hOkzp6huHrxslJJk6Oqmv2mrW159DOTfJ7Rw2nBbfGWjGiMckTFAO9p9YVF3kDDHhiyirWEJPSESSSJeBB782qnwoXEDSAjgiiUYWSRuYIfxdibXUUZr3JPSmjaxvy+EVMjjWgouMrid51UQTW50wl3C0fX03/nak4P9+GWx14T1JGAb07fKQlgK/AwYtJ8esNyiV1j0Z2jgGM9OLpqgZ9gqjsA95htzdqy2DgC/U74qfhkUKISAXUWZdGS+rCEYBFaVzAj0aPKtXmRWTrC6OTDSTVLQCKZPfcHqnU652PUQZqqKCA== </property> </from> <mapping customMapping= \"disable\" type= \"json\" /> <to streamName= \"stream1_scep_timestamped\" version= \"2.0.0\" /> </eventReceiver> Now add the logger and send some events to stream A . If all went right, you should see the output at analytics tier console.","title":"Make pipeline"}]}